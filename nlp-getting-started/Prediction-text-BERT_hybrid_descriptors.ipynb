{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making an accurate detector for fake disaster tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using BERT pretrained model to compute tweet rich features for ML models by using hybrid approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this to disable GPU usage\n",
    "from tensorflow import config\n",
    "config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.math as tf_math\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "Let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Remove duplicated text from train set\n",
    "train[train['text'].duplicated(keep='first')]\n",
    "\n",
    "# Prepare train and test set\n",
    "X_train, y_train = train['text'], train['target']\n",
    "X_test = test['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing through BERT pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenize input data to fit BERT input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tweet_max_length = 280\n",
    "\n",
    "# Tokenize train y test set according to BERT model input format\n",
    "X_train_tok = tokenizer(text=X_train.tolist(),\n",
    "                        max_length=tweet_max_length,\n",
    "                        padding='max_length',\n",
    "                        add_special_tokens=True,\n",
    "                        return_attention_mask=True,\n",
    "                        return_token_type_ids=True,\n",
    "                        pad_to_max_length=True,\n",
    "                        return_tensors='tf')\n",
    "\n",
    "X_test_tok = tokenizer(text=X_test.tolist(),\n",
    "                        max_length=tweet_max_length,\n",
    "                        padding='max_length',\n",
    "                        add_special_tokens=True,\n",
    "                        return_attention_mask=True,\n",
    "                        return_token_type_ids=True,\n",
    "                        pad_to_max_length=True,\n",
    "                        return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute features by using BERT head pretrained model. Hybrid approach will be used: From the model output hidden states, average pool hidden state and max pool hidden state will be computed and concatened so to make a long feature for each tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "def build_BERT_hry_appr_feat_ext(max_length: int):\n",
    "    \n",
    "    # Prepare input layers with tokenized data\n",
    "    input_ids = Input(shape=(max_length,), dtype='int32', name='input_ids')\n",
    "    token_typ_ids = Input(shape=(max_length,), dtype='int32', name='token_typ_ids')\n",
    "    attention_masks = Input(shape=(max_length,), dtype='int32', name='attention_masks')\n",
    "    \n",
    "    # Load pretrained model\n",
    "    bert_pre_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "    bert_pre_model.trainable = False\n",
    "\n",
    "    sequence_output, pooled_output = bert_pre_model(input_ids, attention_masks, token_typ_ids)\n",
    "    \n",
    "    # Average and max sequence output to obtain a feture of dim 768\n",
    "    out_avg = Lambda(lambda x: tf_math.reduce_mean(x, axis=1))(bert_pre_model.output[0])\n",
    "    out_max = Lambda(lambda x: tf_math.reduce_max(x, axis=1))(bert_pre_model.output[0])\n",
    "    out_layer = Concatenate(axis=1)([out_avg, out_max])\n",
    "    \n",
    "    # Prepare model\n",
    "    model = Model(inputs=(input_ids, attention_masks, token_typ_ids), outputs=out_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Extract features for train and test set and save into disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f54123751e8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7f540eede6c0> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f54123751e8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7f540eede6c0> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:From /home/nico/.local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    }
   ],
   "source": [
    "bert_feat_extr = build_BERT_hry_appr_feat_ext(tweet_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "# Extract features for train set\n",
    "X_train_vect = bert_feat_extr.predict([X_train_tok['input_ids'],\n",
    "                                      X_train_tok['attention_mask'],\n",
    "                                      X_train_tok['token_type_ids']])\n",
    "np.save('X_train_BERT_hyb_appr_hidden_states.npy', X_train_vect)\n",
    "\n",
    "# Extract features for test set\n",
    "X_test_vect = bert_feat_extr.predict([X_test_tok['input_ids'],\n",
    "                                      X_test_tok['attention_mask'],\n",
    "                                      X_test_tok['token_type_ids']])\n",
    "np.save('X_test_BERT_hyb_appr_hidden_states.npy', X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 1536)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vect.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting\n",
    "\n",
    "Let's use these computed features to feed ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load precomputed features from disk\n",
    "X_train_vect = np.load('X_train_BERT_hyb_appr_hidden_states.npy')\n",
    "X_test_vect = np.load('X_test_BERT_hyb_appr_hidden_states.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to fit SVM Models.\n",
    "Validation through stratified K-fold with `K=5` will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following parameters will be used for gridding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_params = {\n",
    "        'C': np.arange(0.5, 10, 0.5),\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    }\n",
    "\n",
    "# KFold\n",
    "K = 5\n",
    "kfold = StratifiedKFold(n_splits=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters {'C': 0.5, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 0.5, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 1.0, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 1.0, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 1.5, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 1.5, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 2.0, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 2.0, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 2.5, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 2.5, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 3.0, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 3.0, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 3.5, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 3.5, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 4.0, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 4.0, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 4.5, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 4.5, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 5.0, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 5.0, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 5.5, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 5.5, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 6.0, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 6.0, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 6.5, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 6.5, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 7.0, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 7.0, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 7.5, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 7.5, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 8.0, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 8.0, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 8.5, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 8.5, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 9.0, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 9.0, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 9.5, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 9.5, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "smv_lin_rbf_perf_metrics = []\n",
    "\n",
    "for param in ParameterGrid(svm_params):\n",
    "    \n",
    "    meta = {\n",
    "        'kernel': param['kernel'],\n",
    "        'C': param['C'],\n",
    "        'accuracy_fold': [],\n",
    "        'precision_fold': [],\n",
    "        'recall_fold': [],\n",
    "        'specificity_fold': [],\n",
    "        'f1_score_fold': []\n",
    "    }\n",
    "    \n",
    "    print('Training with parameters {}'.format(param))\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X_train_vect, y_train):\n",
    "    \n",
    "        # Split train and test partition\n",
    "        X_train_part = X_train_vect[train_index]\n",
    "        y_train_part = y_train[train_index].values\n",
    "        \n",
    "        X_test_part = X_train_vect[test_index]\n",
    "        y_test_part = y_train[test_index].values\n",
    "        \n",
    "        # Fit SVM model with linear or rbf reknel\n",
    "        svm = SVC(kernel=param['kernel'], C=param['C'])\n",
    "        svm.fit(X_train_part, y_train_part)\n",
    "        \n",
    "        # Predict over test\n",
    "        y_pred = svm.predict(X_test_part)\n",
    "        \n",
    "        # Meassure performance metrics\n",
    "        meta['accuracy_fold'].append(metrics.accuracy_score(y_test_part, y_pred))\n",
    "        meta['precision_fold'].append(metrics.precision_score(y_test_part, y_pred))\n",
    "        meta['recall_fold'].append(metrics.recall_score(y_test_part, y_pred))\n",
    "        #  Exchange class 1 and 0 so measure recall for class 0 (specificity)\n",
    "        meta['specificity_fold'].append(metrics.recall_score(np.abs(y_test_part-1), np.abs(y_pred-1)))\n",
    "        meta['f1_score_fold'].append(metrics.f1_score(y_test_part, y_pred))\n",
    "    \n",
    "    # Compute global average metrics\n",
    "    meta['accuracy_avg'] = np.mean(meta['accuracy_fold'])\n",
    "    meta['accuracy_std'] = np.std(meta['accuracy_fold'])\n",
    "    \n",
    "    meta['precision_avg'] = np.mean(meta['precision_fold'])\n",
    "    meta['precision_std'] = np.std(meta['precision_fold'])\n",
    "    \n",
    "    meta['recall_avg'] = np.mean(meta['recall_fold'])\n",
    "    meta['recall_std'] = np.std(meta['recall_fold'])\n",
    "    \n",
    "    meta['specificity_avg'] = np.mean(meta['specificity_fold'])\n",
    "    meta['specificity_std'] = np.std(meta['specificity_fold'])\n",
    "    \n",
    "    meta['f1_score_avg'] = np.mean(meta['f1_score_fold'])\n",
    "    meta['f1_score_std'] = np.std(meta['f1_score_fold'])\n",
    "    \n",
    "    smv_lin_rbf_perf_metrics.append(meta)\n",
    "\n",
    "smv_lin_rbf_perf_metrics = pd.DataFrame(smv_lin_rbf_perf_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kernel</th>\n",
       "      <th>C</th>\n",
       "      <th>accuracy_fold</th>\n",
       "      <th>precision_fold</th>\n",
       "      <th>recall_fold</th>\n",
       "      <th>specificity_fold</th>\n",
       "      <th>f1_score_fold</th>\n",
       "      <th>accuracy_avg</th>\n",
       "      <th>accuracy_std</th>\n",
       "      <th>precision_avg</th>\n",
       "      <th>precision_std</th>\n",
       "      <th>recall_avg</th>\n",
       "      <th>recall_std</th>\n",
       "      <th>specificity_avg</th>\n",
       "      <th>specificity_std</th>\n",
       "      <th>f1_score_avg</th>\n",
       "      <th>f1_score_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[0.7747866053841103, 0.7360472751149048, 0.741...</td>\n",
       "      <td>[0.7680412371134021, 0.7, 0.6851063829787234, ...</td>\n",
       "      <td>[0.6824427480916031, 0.6743119266055045, 0.738...</td>\n",
       "      <td>[0.8444700460829493, 0.7825086306098964, 0.744...</td>\n",
       "      <td>[0.7227162489894907, 0.6869158878504673, 0.710...</td>\n",
       "      <td>0.753055</td>\n",
       "      <td>0.018062</td>\n",
       "      <td>0.714886</td>\n",
       "      <td>0.029114</td>\n",
       "      <td>0.710495</td>\n",
       "      <td>0.039717</td>\n",
       "      <td>0.785132</td>\n",
       "      <td>0.032594</td>\n",
       "      <td>0.711728</td>\n",
       "      <td>0.022326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[0.8063033486539725, 0.788575180564675, 0.8063...</td>\n",
       "      <td>[0.8930131004366813, 0.8155893536121673, 0.830...</td>\n",
       "      <td>[0.6244274809160305, 0.6559633027522935, 0.689...</td>\n",
       "      <td>[0.9435483870967742, 0.8883774453394706, 0.894...</td>\n",
       "      <td>[0.7349505840071878, 0.7271186440677965, 0.753...</td>\n",
       "      <td>0.803627</td>\n",
       "      <td>0.021535</td>\n",
       "      <td>0.835812</td>\n",
       "      <td>0.035147</td>\n",
       "      <td>0.677485</td>\n",
       "      <td>0.046779</td>\n",
       "      <td>0.898668</td>\n",
       "      <td>0.024951</td>\n",
       "      <td>0.747138</td>\n",
       "      <td>0.030877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linear</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.7616546290216678, 0.7406434668417596, 0.734...</td>\n",
       "      <td>[0.7508591065292096, 0.7045813586097947, 0.672...</td>\n",
       "      <td>[0.667175572519084, 0.6819571865443425, 0.7431...</td>\n",
       "      <td>[0.8329493087557603, 0.7848101265822784, 0.727...</td>\n",
       "      <td>[0.7065481002425223, 0.693084693084693, 0.7058...</td>\n",
       "      <td>0.749378</td>\n",
       "      <td>0.016330</td>\n",
       "      <td>0.708384</td>\n",
       "      <td>0.026143</td>\n",
       "      <td>0.711723</td>\n",
       "      <td>0.044082</td>\n",
       "      <td>0.777762</td>\n",
       "      <td>0.033835</td>\n",
       "      <td>0.708886</td>\n",
       "      <td>0.021489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.8082731451083388, 0.793827971109652, 0.8181...</td>\n",
       "      <td>[0.8789144050104384, 0.8183520599250936, 0.836...</td>\n",
       "      <td>[0.6427480916030535, 0.6681957186544343, 0.717...</td>\n",
       "      <td>[0.9331797235023042, 0.8883774453394706, 0.894...</td>\n",
       "      <td>[0.742504409171076, 0.7356902356902357, 0.7720...</td>\n",
       "      <td>0.809406</td>\n",
       "      <td>0.020875</td>\n",
       "      <td>0.833560</td>\n",
       "      <td>0.028495</td>\n",
       "      <td>0.696440</td>\n",
       "      <td>0.049424</td>\n",
       "      <td>0.894520</td>\n",
       "      <td>0.021392</td>\n",
       "      <td>0.757689</td>\n",
       "      <td>0.030850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>linear</td>\n",
       "      <td>1.5</td>\n",
       "      <td>[0.7544320420223244, 0.7314510833880499, 0.732...</td>\n",
       "      <td>[0.7353433835845896, 0.6917057902973396, 0.671...</td>\n",
       "      <td>[0.6702290076335878, 0.6758409785932722, 0.740...</td>\n",
       "      <td>[0.8179723502304147, 0.7733026467203682, 0.727...</td>\n",
       "      <td>[0.7012779552715654, 0.6836813611755608, 0.704...</td>\n",
       "      <td>0.741233</td>\n",
       "      <td>0.017166</td>\n",
       "      <td>0.696971</td>\n",
       "      <td>0.023301</td>\n",
       "      <td>0.705911</td>\n",
       "      <td>0.044504</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.029327</td>\n",
       "      <td>0.700474</td>\n",
       "      <td>0.023702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1.5</td>\n",
       "      <td>[0.8135259356533159, 0.7866053841103086, 0.821...</td>\n",
       "      <td>[0.8778004073319755, 0.8051948051948052, 0.837...</td>\n",
       "      <td>[0.6580152671755726, 0.6636085626911316, 0.724...</td>\n",
       "      <td>[0.9308755760368663, 0.8791714614499425, 0.894...</td>\n",
       "      <td>[0.7521815008726004, 0.7275775356244762, 0.777...</td>\n",
       "      <td>0.810063</td>\n",
       "      <td>0.021735</td>\n",
       "      <td>0.829923</td>\n",
       "      <td>0.029581</td>\n",
       "      <td>0.702857</td>\n",
       "      <td>0.049040</td>\n",
       "      <td>0.890836</td>\n",
       "      <td>0.021913</td>\n",
       "      <td>0.760047</td>\n",
       "      <td>0.031359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>linear</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[0.7524622455679579, 0.7255416940249507, 0.728...</td>\n",
       "      <td>[0.7278688524590164, 0.6820987654320988, 0.667...</td>\n",
       "      <td>[0.6778625954198473, 0.6758409785932722, 0.735...</td>\n",
       "      <td>[0.8087557603686636, 0.762945914844649, 0.7238...</td>\n",
       "      <td>[0.7019762845849803, 0.6789554531490015, 0.699...</td>\n",
       "      <td>0.736898</td>\n",
       "      <td>0.021125</td>\n",
       "      <td>0.690128</td>\n",
       "      <td>0.025541</td>\n",
       "      <td>0.704991</td>\n",
       "      <td>0.044224</td>\n",
       "      <td>0.760948</td>\n",
       "      <td>0.027851</td>\n",
       "      <td>0.696759</td>\n",
       "      <td>0.027474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rbf</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[0.81483913328956, 0.7866053841103086, 0.82140...</td>\n",
       "      <td>[0.8813905930470347, 0.8051948051948052, 0.829...</td>\n",
       "      <td>[0.6580152671755726, 0.6636085626911316, 0.735...</td>\n",
       "      <td>[0.9331797235023042, 0.8791714614499425, 0.886...</td>\n",
       "      <td>[0.7534965034965034, 0.7275775356244762, 0.779...</td>\n",
       "      <td>0.809931</td>\n",
       "      <td>0.020185</td>\n",
       "      <td>0.826374</td>\n",
       "      <td>0.030589</td>\n",
       "      <td>0.707750</td>\n",
       "      <td>0.051520</td>\n",
       "      <td>0.886920</td>\n",
       "      <td>0.024259</td>\n",
       "      <td>0.761091</td>\n",
       "      <td>0.030024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>linear</td>\n",
       "      <td>2.5</td>\n",
       "      <td>[0.7504924491135916, 0.7281680892974393, 0.728...</td>\n",
       "      <td>[0.7250409165302782, 0.6840490797546013, 0.667...</td>\n",
       "      <td>[0.6763358778625954, 0.6819571865443425, 0.735...</td>\n",
       "      <td>[0.8064516129032258, 0.762945914844649, 0.7238...</td>\n",
       "      <td>[0.6998420221169036, 0.6830015313935681, 0.699...</td>\n",
       "      <td>0.736241</td>\n",
       "      <td>0.016805</td>\n",
       "      <td>0.689178</td>\n",
       "      <td>0.021549</td>\n",
       "      <td>0.704992</td>\n",
       "      <td>0.042207</td>\n",
       "      <td>0.759796</td>\n",
       "      <td>0.026768</td>\n",
       "      <td>0.696225</td>\n",
       "      <td>0.023239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rbf</td>\n",
       "      <td>2.5</td>\n",
       "      <td>[0.8135259356533159, 0.7852921864740644, 0.819...</td>\n",
       "      <td>[0.8732394366197183, 0.7989031078610603, 0.825...</td>\n",
       "      <td>[0.6625954198473283, 0.6681957186544343, 0.735...</td>\n",
       "      <td>[0.9274193548387096, 0.8734177215189873, 0.882...</td>\n",
       "      <td>[0.7534722222222222, 0.7277268942547875, 0.777...</td>\n",
       "      <td>0.809012</td>\n",
       "      <td>0.019884</td>\n",
       "      <td>0.821630</td>\n",
       "      <td>0.028913</td>\n",
       "      <td>0.711112</td>\n",
       "      <td>0.050530</td>\n",
       "      <td>0.882775</td>\n",
       "      <td>0.023332</td>\n",
       "      <td>0.761109</td>\n",
       "      <td>0.029442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>linear</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[0.7413000656598818, 0.7248850952068286, 0.726...</td>\n",
       "      <td>[0.7121951219512195, 0.6804915514592934, 0.663...</td>\n",
       "      <td>[0.6687022900763359, 0.6773700305810397, 0.738...</td>\n",
       "      <td>[0.7960829493087558, 0.760644418872267, 0.7180...</td>\n",
       "      <td>[0.6897637795275591, 0.6789272030651342, 0.698...</td>\n",
       "      <td>0.731775</td>\n",
       "      <td>0.016999</td>\n",
       "      <td>0.683145</td>\n",
       "      <td>0.019640</td>\n",
       "      <td>0.701936</td>\n",
       "      <td>0.044775</td>\n",
       "      <td>0.754268</td>\n",
       "      <td>0.025188</td>\n",
       "      <td>0.691638</td>\n",
       "      <td>0.024609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rbf</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[0.8122127380170716, 0.7826657912015759, 0.817...</td>\n",
       "      <td>[0.8653465346534653, 0.7931034482758621, 0.820...</td>\n",
       "      <td>[0.667175572519084, 0.6681957186544343, 0.7354...</td>\n",
       "      <td>[0.9216589861751152, 0.8688147295742232, 0.879...</td>\n",
       "      <td>[0.7534482758620691, 0.7253112033195022, 0.775...</td>\n",
       "      <td>0.807830</td>\n",
       "      <td>0.019313</td>\n",
       "      <td>0.816905</td>\n",
       "      <td>0.026805</td>\n",
       "      <td>0.713863</td>\n",
       "      <td>0.050917</td>\n",
       "      <td>0.878629</td>\n",
       "      <td>0.022441</td>\n",
       "      <td>0.760664</td>\n",
       "      <td>0.029020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>linear</td>\n",
       "      <td>3.5</td>\n",
       "      <td>[0.7386736703873933, 0.7222586999343401, 0.728...</td>\n",
       "      <td>[0.7069243156199678, 0.6779661016949152, 0.662...</td>\n",
       "      <td>[0.6702290076335878, 0.672782874617737, 0.7507...</td>\n",
       "      <td>[0.7903225806451613, 0.759493670886076, 0.7123...</td>\n",
       "      <td>[0.6880877742946708, 0.6753645433614734, 0.703...</td>\n",
       "      <td>0.731907</td>\n",
       "      <td>0.017397</td>\n",
       "      <td>0.682291</td>\n",
       "      <td>0.018797</td>\n",
       "      <td>0.704688</td>\n",
       "      <td>0.047138</td>\n",
       "      <td>0.752426</td>\n",
       "      <td>0.024970</td>\n",
       "      <td>0.692498</td>\n",
       "      <td>0.025738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rbf</td>\n",
       "      <td>3.5</td>\n",
       "      <td>[0.8122127380170716, 0.783322390019698, 0.8181...</td>\n",
       "      <td>[0.8610567514677103, 0.7903225806451613, 0.816...</td>\n",
       "      <td>[0.6717557251908397, 0.6743119266055045, 0.743...</td>\n",
       "      <td>[0.9182027649769585, 0.8653624856156502, 0.874...</td>\n",
       "      <td>[0.7547169811320754, 0.7277227722772278, 0.778...</td>\n",
       "      <td>0.806910</td>\n",
       "      <td>0.018150</td>\n",
       "      <td>0.812372</td>\n",
       "      <td>0.026718</td>\n",
       "      <td>0.717532</td>\n",
       "      <td>0.048416</td>\n",
       "      <td>0.874253</td>\n",
       "      <td>0.022965</td>\n",
       "      <td>0.760825</td>\n",
       "      <td>0.027118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>linear</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[0.7380170715692712, 0.7209455022980958, 0.724...</td>\n",
       "      <td>[0.7064516129032258, 0.6769706336939721, 0.658...</td>\n",
       "      <td>[0.6687022900763359, 0.6697247706422018, 0.744...</td>\n",
       "      <td>[0.7903225806451613, 0.759493670886076, 0.7088...</td>\n",
       "      <td>[0.6870588235294117, 0.6733282090699462, 0.698...</td>\n",
       "      <td>0.729542</td>\n",
       "      <td>0.019092</td>\n",
       "      <td>0.679863</td>\n",
       "      <td>0.020989</td>\n",
       "      <td>0.701325</td>\n",
       "      <td>0.047862</td>\n",
       "      <td>0.750814</td>\n",
       "      <td>0.026299</td>\n",
       "      <td>0.689610</td>\n",
       "      <td>0.027199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rbf</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[0.8135259356533159, 0.7793827971109653, 0.816...</td>\n",
       "      <td>[0.8615984405458089, 0.7839285714285714, 0.810...</td>\n",
       "      <td>[0.6748091603053435, 0.6712538226299695, 0.747...</td>\n",
       "      <td>[0.9182027649769585, 0.8607594936708861, 0.868...</td>\n",
       "      <td>[0.7568493150684931, 0.7232289950576606, 0.778...</td>\n",
       "      <td>0.806385</td>\n",
       "      <td>0.018356</td>\n",
       "      <td>0.809824</td>\n",
       "      <td>0.027765</td>\n",
       "      <td>0.719671</td>\n",
       "      <td>0.048508</td>\n",
       "      <td>0.871721</td>\n",
       "      <td>0.023778</td>\n",
       "      <td>0.760883</td>\n",
       "      <td>0.027245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>linear</td>\n",
       "      <td>4.5</td>\n",
       "      <td>[0.737360472751149, 0.7222586999343401, 0.7235...</td>\n",
       "      <td>[0.7046548956661316, 0.6790697674418604, 0.658...</td>\n",
       "      <td>[0.6702290076335878, 0.6697247706422018, 0.740...</td>\n",
       "      <td>[0.7880184331797235, 0.761795166858458, 0.7111...</td>\n",
       "      <td>[0.6870109546165883, 0.674364896073903, 0.6969...</td>\n",
       "      <td>0.729674</td>\n",
       "      <td>0.018787</td>\n",
       "      <td>0.679900</td>\n",
       "      <td>0.020296</td>\n",
       "      <td>0.701630</td>\n",
       "      <td>0.046696</td>\n",
       "      <td>0.750813</td>\n",
       "      <td>0.025124</td>\n",
       "      <td>0.689836</td>\n",
       "      <td>0.026693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rbf</td>\n",
       "      <td>4.5</td>\n",
       "      <td>[0.8135259356533159, 0.7793827971109653, 0.816...</td>\n",
       "      <td>[0.8601941747572815, 0.7809187279151943, 0.809...</td>\n",
       "      <td>[0.6763358778625954, 0.6758409785932722, 0.749...</td>\n",
       "      <td>[0.9170506912442397, 0.857307249712313, 0.8676...</td>\n",
       "      <td>[0.7572649572649574, 0.7245901639344262, 0.778...</td>\n",
       "      <td>0.805334</td>\n",
       "      <td>0.017851</td>\n",
       "      <td>0.806694</td>\n",
       "      <td>0.028808</td>\n",
       "      <td>0.721200</td>\n",
       "      <td>0.046900</td>\n",
       "      <td>0.868727</td>\n",
       "      <td>0.024960</td>\n",
       "      <td>0.760358</td>\n",
       "      <td>0.026076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>linear</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[0.7406434668417596, 0.7222586999343401, 0.723...</td>\n",
       "      <td>[0.7070063694267515, 0.6801872074882995, 0.658...</td>\n",
       "      <td>[0.6778625954198473, 0.6666666666666666, 0.740...</td>\n",
       "      <td>[0.7880184331797235, 0.7640966628308401, 0.711...</td>\n",
       "      <td>[0.6921278254091972, 0.6733590733590734, 0.696...</td>\n",
       "      <td>0.731250</td>\n",
       "      <td>0.017517</td>\n",
       "      <td>0.681939</td>\n",
       "      <td>0.019283</td>\n",
       "      <td>0.702851</td>\n",
       "      <td>0.045124</td>\n",
       "      <td>0.752655</td>\n",
       "      <td>0.024980</td>\n",
       "      <td>0.691492</td>\n",
       "      <td>0.025184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rbf</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[0.8108995403808273, 0.7787261982928431, 0.814...</td>\n",
       "      <td>[0.8576998050682261, 0.7785588752196837, 0.803...</td>\n",
       "      <td>[0.6717557251908397, 0.6773700305810397, 0.750...</td>\n",
       "      <td>[0.9158986175115207, 0.8550057537399309, 0.861...</td>\n",
       "      <td>[0.7534246575342466, 0.7244480784955029, 0.776...</td>\n",
       "      <td>0.803889</td>\n",
       "      <td>0.017438</td>\n",
       "      <td>0.803960</td>\n",
       "      <td>0.028394</td>\n",
       "      <td>0.720895</td>\n",
       "      <td>0.048305</td>\n",
       "      <td>0.866425</td>\n",
       "      <td>0.025209</td>\n",
       "      <td>0.758877</td>\n",
       "      <td>0.025997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>linear</td>\n",
       "      <td>5.5</td>\n",
       "      <td>[0.7386736703873933, 0.7229152987524623, 0.719...</td>\n",
       "      <td>[0.7036450079239303, 0.6790123456790124, 0.653...</td>\n",
       "      <td>[0.6778625954198473, 0.672782874617737, 0.7400...</td>\n",
       "      <td>[0.7845622119815668, 0.760644418872267, 0.7042...</td>\n",
       "      <td>[0.6905132192846033, 0.6758832565284179, 0.693...</td>\n",
       "      <td>0.729017</td>\n",
       "      <td>0.019530</td>\n",
       "      <td>0.678650</td>\n",
       "      <td>0.021942</td>\n",
       "      <td>0.702545</td>\n",
       "      <td>0.044267</td>\n",
       "      <td>0.748971</td>\n",
       "      <td>0.026462</td>\n",
       "      <td>0.689703</td>\n",
       "      <td>0.026383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rbf</td>\n",
       "      <td>5.5</td>\n",
       "      <td>[0.8102429415627052, 0.7767564018384767, 0.813...</td>\n",
       "      <td>[0.857421875, 0.7744755244755245, 0.8013029315...</td>\n",
       "      <td>[0.6702290076335878, 0.6773700305810397, 0.752...</td>\n",
       "      <td>[0.9158986175115207, 0.8515535097813579, 0.859...</td>\n",
       "      <td>[0.75235646958012, 0.7226753670473083, 0.77602...</td>\n",
       "      <td>0.802970</td>\n",
       "      <td>0.018384</td>\n",
       "      <td>0.801536</td>\n",
       "      <td>0.029697</td>\n",
       "      <td>0.721813</td>\n",
       "      <td>0.050304</td>\n",
       "      <td>0.864122</td>\n",
       "      <td>0.026350</td>\n",
       "      <td>0.758205</td>\n",
       "      <td>0.027179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>linear</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[0.7334208798424163, 0.7209455022980958, 0.722...</td>\n",
       "      <td>[0.6948356807511737, 0.6764252696456087, 0.656...</td>\n",
       "      <td>[0.6778625954198473, 0.6712538226299695, 0.744...</td>\n",
       "      <td>[0.7753456221198156, 0.7583429228998849, 0.706...</td>\n",
       "      <td>[0.6862442040185471, 0.673829623944743, 0.6977...</td>\n",
       "      <td>0.727572</td>\n",
       "      <td>0.019101</td>\n",
       "      <td>0.676105</td>\n",
       "      <td>0.019508</td>\n",
       "      <td>0.702851</td>\n",
       "      <td>0.045629</td>\n",
       "      <td>0.746206</td>\n",
       "      <td>0.023088</td>\n",
       "      <td>0.688578</td>\n",
       "      <td>0.027010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rbf</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[0.8089297439264609, 0.7721602101116218, 0.812...</td>\n",
       "      <td>[0.8540856031128404, 0.7641996557659209, 0.797...</td>\n",
       "      <td>[0.6702290076335878, 0.6788990825688074, 0.753...</td>\n",
       "      <td>[0.913594470046083, 0.8423475258918297, 0.8561...</td>\n",
       "      <td>[0.7510692899914457, 0.7190283400809717, 0.775...</td>\n",
       "      <td>0.800474</td>\n",
       "      <td>0.019497</td>\n",
       "      <td>0.796444</td>\n",
       "      <td>0.031256</td>\n",
       "      <td>0.721813</td>\n",
       "      <td>0.050453</td>\n",
       "      <td>0.859747</td>\n",
       "      <td>0.027443</td>\n",
       "      <td>0.755920</td>\n",
       "      <td>0.028038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>linear</td>\n",
       "      <td>6.5</td>\n",
       "      <td>[0.7334208798424163, 0.7222586999343401, 0.719...</td>\n",
       "      <td>[0.6936236391912908, 0.6774193548387096, 0.653...</td>\n",
       "      <td>[0.6809160305343511, 0.6743119266055045, 0.738...</td>\n",
       "      <td>[0.7730414746543779, 0.7583429228998849, 0.705...</td>\n",
       "      <td>[0.687211093990755, 0.6758620689655173, 0.6934...</td>\n",
       "      <td>0.728229</td>\n",
       "      <td>0.019443</td>\n",
       "      <td>0.676512</td>\n",
       "      <td>0.020375</td>\n",
       "      <td>0.704685</td>\n",
       "      <td>0.042588</td>\n",
       "      <td>0.745976</td>\n",
       "      <td>0.023245</td>\n",
       "      <td>0.689764</td>\n",
       "      <td>0.026212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rbf</td>\n",
       "      <td>6.5</td>\n",
       "      <td>[0.8095863427445831, 0.7728168089297439, 0.812...</td>\n",
       "      <td>[0.8543689320388349, 0.7655172413793103, 0.798...</td>\n",
       "      <td>[0.6717557251908397, 0.6788990825688074, 0.752...</td>\n",
       "      <td>[0.913594470046083, 0.8434982738780207, 0.8573...</td>\n",
       "      <td>[0.7521367521367522, 0.7196110210696921, 0.774...</td>\n",
       "      <td>0.800080</td>\n",
       "      <td>0.019395</td>\n",
       "      <td>0.795877</td>\n",
       "      <td>0.031752</td>\n",
       "      <td>0.721507</td>\n",
       "      <td>0.049449</td>\n",
       "      <td>0.859286</td>\n",
       "      <td>0.027832</td>\n",
       "      <td>0.755523</td>\n",
       "      <td>0.027610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>linear</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[0.7340774786605384, 0.7209455022980958, 0.718...</td>\n",
       "      <td>[0.6947040498442367, 0.6748091603053435, 0.651...</td>\n",
       "      <td>[0.6809160305343511, 0.6758409785932722, 0.738...</td>\n",
       "      <td>[0.7741935483870968, 0.7548906789413119, 0.703...</td>\n",
       "      <td>[0.6877409406322281, 0.6753246753246754, 0.692...</td>\n",
       "      <td>0.727047</td>\n",
       "      <td>0.019903</td>\n",
       "      <td>0.674854</td>\n",
       "      <td>0.021111</td>\n",
       "      <td>0.704379</td>\n",
       "      <td>0.043049</td>\n",
       "      <td>0.744134</td>\n",
       "      <td>0.023968</td>\n",
       "      <td>0.688740</td>\n",
       "      <td>0.026649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>rbf</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[0.8089297439264609, 0.7708470124753776, 0.809...</td>\n",
       "      <td>[0.8527131782945736, 0.7615780445969125, 0.791...</td>\n",
       "      <td>[0.6717557251908397, 0.6788990825688074, 0.755...</td>\n",
       "      <td>[0.9124423963133641, 0.8400460299194477, 0.850...</td>\n",
       "      <td>[0.7514944491887277, 0.7178658043654003, 0.773...</td>\n",
       "      <td>0.798898</td>\n",
       "      <td>0.019988</td>\n",
       "      <td>0.792719</td>\n",
       "      <td>0.032547</td>\n",
       "      <td>0.722730</td>\n",
       "      <td>0.049609</td>\n",
       "      <td>0.856293</td>\n",
       "      <td>0.028411</td>\n",
       "      <td>0.754757</td>\n",
       "      <td>0.028079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>linear</td>\n",
       "      <td>7.5</td>\n",
       "      <td>[0.7334208798424163, 0.7189757058437295, 0.718...</td>\n",
       "      <td>[0.6918335901386748, 0.6712121212121213, 0.651...</td>\n",
       "      <td>[0.6854961832061068, 0.6773700305810397, 0.741...</td>\n",
       "      <td>[0.7695852534562212, 0.7502876869965478, 0.700...</td>\n",
       "      <td>[0.6886503067484663, 0.6742770167427701, 0.693...</td>\n",
       "      <td>0.727310</td>\n",
       "      <td>0.020969</td>\n",
       "      <td>0.674075</td>\n",
       "      <td>0.021945</td>\n",
       "      <td>0.707741</td>\n",
       "      <td>0.042756</td>\n",
       "      <td>0.742062</td>\n",
       "      <td>0.023812</td>\n",
       "      <td>0.689993</td>\n",
       "      <td>0.027445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>rbf</td>\n",
       "      <td>7.5</td>\n",
       "      <td>[0.8056467498358503, 0.7708470124753776, 0.806...</td>\n",
       "      <td>[0.8485436893203884, 0.7606837606837606, 0.785...</td>\n",
       "      <td>[0.667175572519084, 0.6804281345565749, 0.7553...</td>\n",
       "      <td>[0.9101382488479263, 0.8388952819332566, 0.844...</td>\n",
       "      <td>[0.747008547008547, 0.718321226795803, 0.77007...</td>\n",
       "      <td>0.797453</td>\n",
       "      <td>0.019479</td>\n",
       "      <td>0.790018</td>\n",
       "      <td>0.031492</td>\n",
       "      <td>0.722426</td>\n",
       "      <td>0.050810</td>\n",
       "      <td>0.853990</td>\n",
       "      <td>0.028201</td>\n",
       "      <td>0.753293</td>\n",
       "      <td>0.027917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>linear</td>\n",
       "      <td>8.0</td>\n",
       "      <td>[0.7327642810242941, 0.7196323046618516, 0.718...</td>\n",
       "      <td>[0.6907692307692308, 0.6722306525037937, 0.651...</td>\n",
       "      <td>[0.6854961832061068, 0.6773700305810397, 0.741...</td>\n",
       "      <td>[0.7684331797235023, 0.7514384349827388, 0.701...</td>\n",
       "      <td>[0.6881226053639846, 0.6747905559786749, 0.693...</td>\n",
       "      <td>0.727967</td>\n",
       "      <td>0.020975</td>\n",
       "      <td>0.675053</td>\n",
       "      <td>0.022009</td>\n",
       "      <td>0.707741</td>\n",
       "      <td>0.041951</td>\n",
       "      <td>0.743214</td>\n",
       "      <td>0.023611</td>\n",
       "      <td>0.690532</td>\n",
       "      <td>0.027274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>rbf</td>\n",
       "      <td>8.0</td>\n",
       "      <td>[0.8036769533814839, 0.7708470124753776, 0.804...</td>\n",
       "      <td>[0.8463035019455253, 0.7606837606837606, 0.783...</td>\n",
       "      <td>[0.6641221374045801, 0.6804281345565749, 0.753...</td>\n",
       "      <td>[0.9089861751152074, 0.8388952819332566, 0.843...</td>\n",
       "      <td>[0.7442258340461932, 0.718321226795803, 0.7685...</td>\n",
       "      <td>0.797059</td>\n",
       "      <td>0.019593</td>\n",
       "      <td>0.789729</td>\n",
       "      <td>0.030593</td>\n",
       "      <td>0.721509</td>\n",
       "      <td>0.051886</td>\n",
       "      <td>0.853990</td>\n",
       "      <td>0.027571</td>\n",
       "      <td>0.752643</td>\n",
       "      <td>0.028429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>linear</td>\n",
       "      <td>8.5</td>\n",
       "      <td>[0.7307944845699278, 0.7183191070256073, 0.718...</td>\n",
       "      <td>[0.6887519260400616, 0.670196671709531, 0.6514...</td>\n",
       "      <td>[0.6824427480916031, 0.6773700305810397, 0.743...</td>\n",
       "      <td>[0.7672811059907834, 0.7491369390103567, 0.700...</td>\n",
       "      <td>[0.6855828220858896, 0.6737642585551331, 0.694...</td>\n",
       "      <td>0.725865</td>\n",
       "      <td>0.020401</td>\n",
       "      <td>0.672612</td>\n",
       "      <td>0.021358</td>\n",
       "      <td>0.705602</td>\n",
       "      <td>0.042184</td>\n",
       "      <td>0.741141</td>\n",
       "      <td>0.023301</td>\n",
       "      <td>0.688217</td>\n",
       "      <td>0.026948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>rbf</td>\n",
       "      <td>8.5</td>\n",
       "      <td>[0.8036769533814839, 0.7688772160210111, 0.805...</td>\n",
       "      <td>[0.8463035019455253, 0.7568027210884354, 0.785...</td>\n",
       "      <td>[0.6641221374045801, 0.6804281345565749, 0.753...</td>\n",
       "      <td>[0.9089861751152074, 0.8354430379746836, 0.844...</td>\n",
       "      <td>[0.7442258340461932, 0.71658615136876, 0.76911...</td>\n",
       "      <td>0.796797</td>\n",
       "      <td>0.020185</td>\n",
       "      <td>0.788676</td>\n",
       "      <td>0.031427</td>\n",
       "      <td>0.722427</td>\n",
       "      <td>0.052780</td>\n",
       "      <td>0.852839</td>\n",
       "      <td>0.028256</td>\n",
       "      <td>0.752623</td>\n",
       "      <td>0.029062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>linear</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[0.7321076822061721, 0.7189757058437295, 0.720...</td>\n",
       "      <td>[0.6902927580893683, 0.6712121212121213, 0.653...</td>\n",
       "      <td>[0.683969465648855, 0.6773700305810397, 0.7446...</td>\n",
       "      <td>[0.7684331797235023, 0.7502876869965478, 0.703...</td>\n",
       "      <td>[0.6871165644171778, 0.6742770167427701, 0.696...</td>\n",
       "      <td>0.727047</td>\n",
       "      <td>0.020978</td>\n",
       "      <td>0.674072</td>\n",
       "      <td>0.022123</td>\n",
       "      <td>0.706519</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>0.742522</td>\n",
       "      <td>0.023453</td>\n",
       "      <td>0.689440</td>\n",
       "      <td>0.027369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>rbf</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[0.8017071569271176, 0.7662508207485227, 0.805...</td>\n",
       "      <td>[0.8454011741682974, 0.7534013605442177, 0.784...</td>\n",
       "      <td>[0.6595419847328244, 0.6773700305810397, 0.755...</td>\n",
       "      <td>[0.9089861751152074, 0.8331415420023015, 0.843...</td>\n",
       "      <td>[0.7409948542024013, 0.7133655394524958, 0.769...</td>\n",
       "      <td>0.795483</td>\n",
       "      <td>0.020819</td>\n",
       "      <td>0.787069</td>\n",
       "      <td>0.031913</td>\n",
       "      <td>0.720899</td>\n",
       "      <td>0.055172</td>\n",
       "      <td>0.851687</td>\n",
       "      <td>0.028849</td>\n",
       "      <td>0.750940</td>\n",
       "      <td>0.030269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>linear</td>\n",
       "      <td>9.5</td>\n",
       "      <td>[0.7327642810242941, 0.7183191070256073, 0.720...</td>\n",
       "      <td>[0.6901840490797546, 0.669683257918552, 0.6532...</td>\n",
       "      <td>[0.6870229007633588, 0.6788990825688074, 0.746...</td>\n",
       "      <td>[0.7672811059907834, 0.7479861910241657, 0.701...</td>\n",
       "      <td>[0.6885998469778118, 0.6742596810933941, 0.696...</td>\n",
       "      <td>0.726390</td>\n",
       "      <td>0.021137</td>\n",
       "      <td>0.672568</td>\n",
       "      <td>0.022171</td>\n",
       "      <td>0.708047</td>\n",
       "      <td>0.042138</td>\n",
       "      <td>0.740219</td>\n",
       "      <td>0.023286</td>\n",
       "      <td>0.689385</td>\n",
       "      <td>0.027540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>rbf</td>\n",
       "      <td>9.5</td>\n",
       "      <td>[0.8003939592908733, 0.7655942219304005, 0.804...</td>\n",
       "      <td>[0.8434442270058709, 0.7521222410865874, 0.782...</td>\n",
       "      <td>[0.6580152671755726, 0.6773700305810397, 0.755...</td>\n",
       "      <td>[0.9078341013824884, 0.8319907940161104, 0.842...</td>\n",
       "      <td>[0.7392795883361921, 0.7127916331456153, 0.768...</td>\n",
       "      <td>0.795089</td>\n",
       "      <td>0.020765</td>\n",
       "      <td>0.786081</td>\n",
       "      <td>0.031430</td>\n",
       "      <td>0.721205</td>\n",
       "      <td>0.055909</td>\n",
       "      <td>0.850766</td>\n",
       "      <td>0.028730</td>\n",
       "      <td>0.750625</td>\n",
       "      <td>0.030431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    kernel    C                                      accuracy_fold  \\\n",
       "0   linear  0.5  [0.7747866053841103, 0.7360472751149048, 0.741...   \n",
       "1      rbf  0.5  [0.8063033486539725, 0.788575180564675, 0.8063...   \n",
       "2   linear  1.0  [0.7616546290216678, 0.7406434668417596, 0.734...   \n",
       "3      rbf  1.0  [0.8082731451083388, 0.793827971109652, 0.8181...   \n",
       "4   linear  1.5  [0.7544320420223244, 0.7314510833880499, 0.732...   \n",
       "5      rbf  1.5  [0.8135259356533159, 0.7866053841103086, 0.821...   \n",
       "6   linear  2.0  [0.7524622455679579, 0.7255416940249507, 0.728...   \n",
       "7      rbf  2.0  [0.81483913328956, 0.7866053841103086, 0.82140...   \n",
       "8   linear  2.5  [0.7504924491135916, 0.7281680892974393, 0.728...   \n",
       "9      rbf  2.5  [0.8135259356533159, 0.7852921864740644, 0.819...   \n",
       "10  linear  3.0  [0.7413000656598818, 0.7248850952068286, 0.726...   \n",
       "11     rbf  3.0  [0.8122127380170716, 0.7826657912015759, 0.817...   \n",
       "12  linear  3.5  [0.7386736703873933, 0.7222586999343401, 0.728...   \n",
       "13     rbf  3.5  [0.8122127380170716, 0.783322390019698, 0.8181...   \n",
       "14  linear  4.0  [0.7380170715692712, 0.7209455022980958, 0.724...   \n",
       "15     rbf  4.0  [0.8135259356533159, 0.7793827971109653, 0.816...   \n",
       "16  linear  4.5  [0.737360472751149, 0.7222586999343401, 0.7235...   \n",
       "17     rbf  4.5  [0.8135259356533159, 0.7793827971109653, 0.816...   \n",
       "18  linear  5.0  [0.7406434668417596, 0.7222586999343401, 0.723...   \n",
       "19     rbf  5.0  [0.8108995403808273, 0.7787261982928431, 0.814...   \n",
       "20  linear  5.5  [0.7386736703873933, 0.7229152987524623, 0.719...   \n",
       "21     rbf  5.5  [0.8102429415627052, 0.7767564018384767, 0.813...   \n",
       "22  linear  6.0  [0.7334208798424163, 0.7209455022980958, 0.722...   \n",
       "23     rbf  6.0  [0.8089297439264609, 0.7721602101116218, 0.812...   \n",
       "24  linear  6.5  [0.7334208798424163, 0.7222586999343401, 0.719...   \n",
       "25     rbf  6.5  [0.8095863427445831, 0.7728168089297439, 0.812...   \n",
       "26  linear  7.0  [0.7340774786605384, 0.7209455022980958, 0.718...   \n",
       "27     rbf  7.0  [0.8089297439264609, 0.7708470124753776, 0.809...   \n",
       "28  linear  7.5  [0.7334208798424163, 0.7189757058437295, 0.718...   \n",
       "29     rbf  7.5  [0.8056467498358503, 0.7708470124753776, 0.806...   \n",
       "30  linear  8.0  [0.7327642810242941, 0.7196323046618516, 0.718...   \n",
       "31     rbf  8.0  [0.8036769533814839, 0.7708470124753776, 0.804...   \n",
       "32  linear  8.5  [0.7307944845699278, 0.7183191070256073, 0.718...   \n",
       "33     rbf  8.5  [0.8036769533814839, 0.7688772160210111, 0.805...   \n",
       "34  linear  9.0  [0.7321076822061721, 0.7189757058437295, 0.720...   \n",
       "35     rbf  9.0  [0.8017071569271176, 0.7662508207485227, 0.805...   \n",
       "36  linear  9.5  [0.7327642810242941, 0.7183191070256073, 0.720...   \n",
       "37     rbf  9.5  [0.8003939592908733, 0.7655942219304005, 0.804...   \n",
       "\n",
       "                                       precision_fold  \\\n",
       "0   [0.7680412371134021, 0.7, 0.6851063829787234, ...   \n",
       "1   [0.8930131004366813, 0.8155893536121673, 0.830...   \n",
       "2   [0.7508591065292096, 0.7045813586097947, 0.672...   \n",
       "3   [0.8789144050104384, 0.8183520599250936, 0.836...   \n",
       "4   [0.7353433835845896, 0.6917057902973396, 0.671...   \n",
       "5   [0.8778004073319755, 0.8051948051948052, 0.837...   \n",
       "6   [0.7278688524590164, 0.6820987654320988, 0.667...   \n",
       "7   [0.8813905930470347, 0.8051948051948052, 0.829...   \n",
       "8   [0.7250409165302782, 0.6840490797546013, 0.667...   \n",
       "9   [0.8732394366197183, 0.7989031078610603, 0.825...   \n",
       "10  [0.7121951219512195, 0.6804915514592934, 0.663...   \n",
       "11  [0.8653465346534653, 0.7931034482758621, 0.820...   \n",
       "12  [0.7069243156199678, 0.6779661016949152, 0.662...   \n",
       "13  [0.8610567514677103, 0.7903225806451613, 0.816...   \n",
       "14  [0.7064516129032258, 0.6769706336939721, 0.658...   \n",
       "15  [0.8615984405458089, 0.7839285714285714, 0.810...   \n",
       "16  [0.7046548956661316, 0.6790697674418604, 0.658...   \n",
       "17  [0.8601941747572815, 0.7809187279151943, 0.809...   \n",
       "18  [0.7070063694267515, 0.6801872074882995, 0.658...   \n",
       "19  [0.8576998050682261, 0.7785588752196837, 0.803...   \n",
       "20  [0.7036450079239303, 0.6790123456790124, 0.653...   \n",
       "21  [0.857421875, 0.7744755244755245, 0.8013029315...   \n",
       "22  [0.6948356807511737, 0.6764252696456087, 0.656...   \n",
       "23  [0.8540856031128404, 0.7641996557659209, 0.797...   \n",
       "24  [0.6936236391912908, 0.6774193548387096, 0.653...   \n",
       "25  [0.8543689320388349, 0.7655172413793103, 0.798...   \n",
       "26  [0.6947040498442367, 0.6748091603053435, 0.651...   \n",
       "27  [0.8527131782945736, 0.7615780445969125, 0.791...   \n",
       "28  [0.6918335901386748, 0.6712121212121213, 0.651...   \n",
       "29  [0.8485436893203884, 0.7606837606837606, 0.785...   \n",
       "30  [0.6907692307692308, 0.6722306525037937, 0.651...   \n",
       "31  [0.8463035019455253, 0.7606837606837606, 0.783...   \n",
       "32  [0.6887519260400616, 0.670196671709531, 0.6514...   \n",
       "33  [0.8463035019455253, 0.7568027210884354, 0.785...   \n",
       "34  [0.6902927580893683, 0.6712121212121213, 0.653...   \n",
       "35  [0.8454011741682974, 0.7534013605442177, 0.784...   \n",
       "36  [0.6901840490797546, 0.669683257918552, 0.6532...   \n",
       "37  [0.8434442270058709, 0.7521222410865874, 0.782...   \n",
       "\n",
       "                                          recall_fold  \\\n",
       "0   [0.6824427480916031, 0.6743119266055045, 0.738...   \n",
       "1   [0.6244274809160305, 0.6559633027522935, 0.689...   \n",
       "2   [0.667175572519084, 0.6819571865443425, 0.7431...   \n",
       "3   [0.6427480916030535, 0.6681957186544343, 0.717...   \n",
       "4   [0.6702290076335878, 0.6758409785932722, 0.740...   \n",
       "5   [0.6580152671755726, 0.6636085626911316, 0.724...   \n",
       "6   [0.6778625954198473, 0.6758409785932722, 0.735...   \n",
       "7   [0.6580152671755726, 0.6636085626911316, 0.735...   \n",
       "8   [0.6763358778625954, 0.6819571865443425, 0.735...   \n",
       "9   [0.6625954198473283, 0.6681957186544343, 0.735...   \n",
       "10  [0.6687022900763359, 0.6773700305810397, 0.738...   \n",
       "11  [0.667175572519084, 0.6681957186544343, 0.7354...   \n",
       "12  [0.6702290076335878, 0.672782874617737, 0.7507...   \n",
       "13  [0.6717557251908397, 0.6743119266055045, 0.743...   \n",
       "14  [0.6687022900763359, 0.6697247706422018, 0.744...   \n",
       "15  [0.6748091603053435, 0.6712538226299695, 0.747...   \n",
       "16  [0.6702290076335878, 0.6697247706422018, 0.740...   \n",
       "17  [0.6763358778625954, 0.6758409785932722, 0.749...   \n",
       "18  [0.6778625954198473, 0.6666666666666666, 0.740...   \n",
       "19  [0.6717557251908397, 0.6773700305810397, 0.750...   \n",
       "20  [0.6778625954198473, 0.672782874617737, 0.7400...   \n",
       "21  [0.6702290076335878, 0.6773700305810397, 0.752...   \n",
       "22  [0.6778625954198473, 0.6712538226299695, 0.744...   \n",
       "23  [0.6702290076335878, 0.6788990825688074, 0.753...   \n",
       "24  [0.6809160305343511, 0.6743119266055045, 0.738...   \n",
       "25  [0.6717557251908397, 0.6788990825688074, 0.752...   \n",
       "26  [0.6809160305343511, 0.6758409785932722, 0.738...   \n",
       "27  [0.6717557251908397, 0.6788990825688074, 0.755...   \n",
       "28  [0.6854961832061068, 0.6773700305810397, 0.741...   \n",
       "29  [0.667175572519084, 0.6804281345565749, 0.7553...   \n",
       "30  [0.6854961832061068, 0.6773700305810397, 0.741...   \n",
       "31  [0.6641221374045801, 0.6804281345565749, 0.753...   \n",
       "32  [0.6824427480916031, 0.6773700305810397, 0.743...   \n",
       "33  [0.6641221374045801, 0.6804281345565749, 0.753...   \n",
       "34  [0.683969465648855, 0.6773700305810397, 0.7446...   \n",
       "35  [0.6595419847328244, 0.6773700305810397, 0.755...   \n",
       "36  [0.6870229007633588, 0.6788990825688074, 0.746...   \n",
       "37  [0.6580152671755726, 0.6773700305810397, 0.755...   \n",
       "\n",
       "                                     specificity_fold  \\\n",
       "0   [0.8444700460829493, 0.7825086306098964, 0.744...   \n",
       "1   [0.9435483870967742, 0.8883774453394706, 0.894...   \n",
       "2   [0.8329493087557603, 0.7848101265822784, 0.727...   \n",
       "3   [0.9331797235023042, 0.8883774453394706, 0.894...   \n",
       "4   [0.8179723502304147, 0.7733026467203682, 0.727...   \n",
       "5   [0.9308755760368663, 0.8791714614499425, 0.894...   \n",
       "6   [0.8087557603686636, 0.762945914844649, 0.7238...   \n",
       "7   [0.9331797235023042, 0.8791714614499425, 0.886...   \n",
       "8   [0.8064516129032258, 0.762945914844649, 0.7238...   \n",
       "9   [0.9274193548387096, 0.8734177215189873, 0.882...   \n",
       "10  [0.7960829493087558, 0.760644418872267, 0.7180...   \n",
       "11  [0.9216589861751152, 0.8688147295742232, 0.879...   \n",
       "12  [0.7903225806451613, 0.759493670886076, 0.7123...   \n",
       "13  [0.9182027649769585, 0.8653624856156502, 0.874...   \n",
       "14  [0.7903225806451613, 0.759493670886076, 0.7088...   \n",
       "15  [0.9182027649769585, 0.8607594936708861, 0.868...   \n",
       "16  [0.7880184331797235, 0.761795166858458, 0.7111...   \n",
       "17  [0.9170506912442397, 0.857307249712313, 0.8676...   \n",
       "18  [0.7880184331797235, 0.7640966628308401, 0.711...   \n",
       "19  [0.9158986175115207, 0.8550057537399309, 0.861...   \n",
       "20  [0.7845622119815668, 0.760644418872267, 0.7042...   \n",
       "21  [0.9158986175115207, 0.8515535097813579, 0.859...   \n",
       "22  [0.7753456221198156, 0.7583429228998849, 0.706...   \n",
       "23  [0.913594470046083, 0.8423475258918297, 0.8561...   \n",
       "24  [0.7730414746543779, 0.7583429228998849, 0.705...   \n",
       "25  [0.913594470046083, 0.8434982738780207, 0.8573...   \n",
       "26  [0.7741935483870968, 0.7548906789413119, 0.703...   \n",
       "27  [0.9124423963133641, 0.8400460299194477, 0.850...   \n",
       "28  [0.7695852534562212, 0.7502876869965478, 0.700...   \n",
       "29  [0.9101382488479263, 0.8388952819332566, 0.844...   \n",
       "30  [0.7684331797235023, 0.7514384349827388, 0.701...   \n",
       "31  [0.9089861751152074, 0.8388952819332566, 0.843...   \n",
       "32  [0.7672811059907834, 0.7491369390103567, 0.700...   \n",
       "33  [0.9089861751152074, 0.8354430379746836, 0.844...   \n",
       "34  [0.7684331797235023, 0.7502876869965478, 0.703...   \n",
       "35  [0.9089861751152074, 0.8331415420023015, 0.843...   \n",
       "36  [0.7672811059907834, 0.7479861910241657, 0.701...   \n",
       "37  [0.9078341013824884, 0.8319907940161104, 0.842...   \n",
       "\n",
       "                                        f1_score_fold  accuracy_avg  \\\n",
       "0   [0.7227162489894907, 0.6869158878504673, 0.710...      0.753055   \n",
       "1   [0.7349505840071878, 0.7271186440677965, 0.753...      0.803627   \n",
       "2   [0.7065481002425223, 0.693084693084693, 0.7058...      0.749378   \n",
       "3   [0.742504409171076, 0.7356902356902357, 0.7720...      0.809406   \n",
       "4   [0.7012779552715654, 0.6836813611755608, 0.704...      0.741233   \n",
       "5   [0.7521815008726004, 0.7275775356244762, 0.777...      0.810063   \n",
       "6   [0.7019762845849803, 0.6789554531490015, 0.699...      0.736898   \n",
       "7   [0.7534965034965034, 0.7275775356244762, 0.779...      0.809931   \n",
       "8   [0.6998420221169036, 0.6830015313935681, 0.699...      0.736241   \n",
       "9   [0.7534722222222222, 0.7277268942547875, 0.777...      0.809012   \n",
       "10  [0.6897637795275591, 0.6789272030651342, 0.698...      0.731775   \n",
       "11  [0.7534482758620691, 0.7253112033195022, 0.775...      0.807830   \n",
       "12  [0.6880877742946708, 0.6753645433614734, 0.703...      0.731907   \n",
       "13  [0.7547169811320754, 0.7277227722772278, 0.778...      0.806910   \n",
       "14  [0.6870588235294117, 0.6733282090699462, 0.698...      0.729542   \n",
       "15  [0.7568493150684931, 0.7232289950576606, 0.778...      0.806385   \n",
       "16  [0.6870109546165883, 0.674364896073903, 0.6969...      0.729674   \n",
       "17  [0.7572649572649574, 0.7245901639344262, 0.778...      0.805334   \n",
       "18  [0.6921278254091972, 0.6733590733590734, 0.696...      0.731250   \n",
       "19  [0.7534246575342466, 0.7244480784955029, 0.776...      0.803889   \n",
       "20  [0.6905132192846033, 0.6758832565284179, 0.693...      0.729017   \n",
       "21  [0.75235646958012, 0.7226753670473083, 0.77602...      0.802970   \n",
       "22  [0.6862442040185471, 0.673829623944743, 0.6977...      0.727572   \n",
       "23  [0.7510692899914457, 0.7190283400809717, 0.775...      0.800474   \n",
       "24  [0.687211093990755, 0.6758620689655173, 0.6934...      0.728229   \n",
       "25  [0.7521367521367522, 0.7196110210696921, 0.774...      0.800080   \n",
       "26  [0.6877409406322281, 0.6753246753246754, 0.692...      0.727047   \n",
       "27  [0.7514944491887277, 0.7178658043654003, 0.773...      0.798898   \n",
       "28  [0.6886503067484663, 0.6742770167427701, 0.693...      0.727310   \n",
       "29  [0.747008547008547, 0.718321226795803, 0.77007...      0.797453   \n",
       "30  [0.6881226053639846, 0.6747905559786749, 0.693...      0.727967   \n",
       "31  [0.7442258340461932, 0.718321226795803, 0.7685...      0.797059   \n",
       "32  [0.6855828220858896, 0.6737642585551331, 0.694...      0.725865   \n",
       "33  [0.7442258340461932, 0.71658615136876, 0.76911...      0.796797   \n",
       "34  [0.6871165644171778, 0.6742770167427701, 0.696...      0.727047   \n",
       "35  [0.7409948542024013, 0.7133655394524958, 0.769...      0.795483   \n",
       "36  [0.6885998469778118, 0.6742596810933941, 0.696...      0.726390   \n",
       "37  [0.7392795883361921, 0.7127916331456153, 0.768...      0.795089   \n",
       "\n",
       "    accuracy_std  precision_avg  precision_std  recall_avg  recall_std  \\\n",
       "0       0.018062       0.714886       0.029114    0.710495    0.039717   \n",
       "1       0.021535       0.835812       0.035147    0.677485    0.046779   \n",
       "2       0.016330       0.708384       0.026143    0.711723    0.044082   \n",
       "3       0.020875       0.833560       0.028495    0.696440    0.049424   \n",
       "4       0.017166       0.696971       0.023301    0.705911    0.044504   \n",
       "5       0.021735       0.829923       0.029581    0.702857    0.049040   \n",
       "6       0.021125       0.690128       0.025541    0.704991    0.044224   \n",
       "7       0.020185       0.826374       0.030589    0.707750    0.051520   \n",
       "8       0.016805       0.689178       0.021549    0.704992    0.042207   \n",
       "9       0.019884       0.821630       0.028913    0.711112    0.050530   \n",
       "10      0.016999       0.683145       0.019640    0.701936    0.044775   \n",
       "11      0.019313       0.816905       0.026805    0.713863    0.050917   \n",
       "12      0.017397       0.682291       0.018797    0.704688    0.047138   \n",
       "13      0.018150       0.812372       0.026718    0.717532    0.048416   \n",
       "14      0.019092       0.679863       0.020989    0.701325    0.047862   \n",
       "15      0.018356       0.809824       0.027765    0.719671    0.048508   \n",
       "16      0.018787       0.679900       0.020296    0.701630    0.046696   \n",
       "17      0.017851       0.806694       0.028808    0.721200    0.046900   \n",
       "18      0.017517       0.681939       0.019283    0.702851    0.045124   \n",
       "19      0.017438       0.803960       0.028394    0.720895    0.048305   \n",
       "20      0.019530       0.678650       0.021942    0.702545    0.044267   \n",
       "21      0.018384       0.801536       0.029697    0.721813    0.050304   \n",
       "22      0.019101       0.676105       0.019508    0.702851    0.045629   \n",
       "23      0.019497       0.796444       0.031256    0.721813    0.050453   \n",
       "24      0.019443       0.676512       0.020375    0.704685    0.042588   \n",
       "25      0.019395       0.795877       0.031752    0.721507    0.049449   \n",
       "26      0.019903       0.674854       0.021111    0.704379    0.043049   \n",
       "27      0.019988       0.792719       0.032547    0.722730    0.049609   \n",
       "28      0.020969       0.674075       0.021945    0.707741    0.042756   \n",
       "29      0.019479       0.790018       0.031492    0.722426    0.050810   \n",
       "30      0.020975       0.675053       0.022009    0.707741    0.041951   \n",
       "31      0.019593       0.789729       0.030593    0.721509    0.051886   \n",
       "32      0.020401       0.672612       0.021358    0.705602    0.042184   \n",
       "33      0.020185       0.788676       0.031427    0.722427    0.052780   \n",
       "34      0.020978       0.674072       0.022123    0.706519    0.041945   \n",
       "35      0.020819       0.787069       0.031913    0.720899    0.055172   \n",
       "36      0.021137       0.672568       0.022171    0.708047    0.042138   \n",
       "37      0.020765       0.786081       0.031430    0.721205    0.055909   \n",
       "\n",
       "    specificity_avg  specificity_std  f1_score_avg  f1_score_std  \n",
       "0          0.785132         0.032594      0.711728      0.022326  \n",
       "1          0.898668         0.024951      0.747138      0.030877  \n",
       "2          0.777762         0.033835      0.708886      0.021489  \n",
       "3          0.894520         0.021392      0.757689      0.030850  \n",
       "4          0.767857         0.029327      0.700474      0.023702  \n",
       "5          0.890836         0.021913      0.760047      0.031359  \n",
       "6          0.760948         0.027851      0.696759      0.027474  \n",
       "7          0.886920         0.024259      0.761091      0.030024  \n",
       "8          0.759796         0.026768      0.696225      0.023239  \n",
       "9          0.882775         0.023332      0.761109      0.029442  \n",
       "10         0.754268         0.025188      0.691638      0.024609  \n",
       "11         0.878629         0.022441      0.760664      0.029020  \n",
       "12         0.752426         0.024970      0.692498      0.025738  \n",
       "13         0.874253         0.022965      0.760825      0.027118  \n",
       "14         0.750814         0.026299      0.689610      0.027199  \n",
       "15         0.871721         0.023778      0.760883      0.027245  \n",
       "16         0.750813         0.025124      0.689836      0.026693  \n",
       "17         0.868727         0.024960      0.760358      0.026076  \n",
       "18         0.752655         0.024980      0.691492      0.025184  \n",
       "19         0.866425         0.025209      0.758877      0.025997  \n",
       "20         0.748971         0.026462      0.689703      0.026383  \n",
       "21         0.864122         0.026350      0.758205      0.027179  \n",
       "22         0.746206         0.023088      0.688578      0.027010  \n",
       "23         0.859747         0.027443      0.755920      0.028038  \n",
       "24         0.745976         0.023245      0.689764      0.026212  \n",
       "25         0.859286         0.027832      0.755523      0.027610  \n",
       "26         0.744134         0.023968      0.688740      0.026649  \n",
       "27         0.856293         0.028411      0.754757      0.028079  \n",
       "28         0.742062         0.023812      0.689993      0.027445  \n",
       "29         0.853990         0.028201      0.753293      0.027917  \n",
       "30         0.743214         0.023611      0.690532      0.027274  \n",
       "31         0.853990         0.027571      0.752643      0.028429  \n",
       "32         0.741141         0.023301      0.688217      0.026948  \n",
       "33         0.852839         0.028256      0.752623      0.029062  \n",
       "34         0.742522         0.023453      0.689440      0.027369  \n",
       "35         0.851687         0.028849      0.750940      0.030269  \n",
       "36         0.740219         0.023286      0.689385      0.027540  \n",
       "37         0.850766         0.028730      0.750625      0.030431  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smv_lin_rbf_perf_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 280)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_masks (InputLayer)    [(None, 280)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_typ_ids (InputLayer)      [(None, 280)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 109482240   input_ids[0][0]                  \n",
      "                                                                 attention_masks[0][0]            \n",
      "                                                                 token_typ_ids[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1536)         0           lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 109,482,240\n",
      "Trainable params: 0\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_feat_extr.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(perf_df, param, param_name=None):\n",
    "    \n",
    "    if not param_name:\n",
    "        param_name = param\n",
    "\n",
    "    fig, ax = plt.subplots(3,2, figsize=(16,20))\n",
    "\n",
    "    # Plot accuracy\n",
    "    ax[0, 0].fill_between(perf_df[param].values,\n",
    "                     perf_df['accuracy_fold'].apply(np.min),\n",
    "                     perf_df['accuracy_fold'].apply(np.max), color='lightblue')\n",
    "    ax[0, 0].plot(perf_df[param].values, perf_df['accuracy_avg'].values, color='tab:blue')\n",
    "    ax[0, 0].set_xlabel(param_name)\n",
    "    ax[0, 0].set_ylabel('Accuracy')\n",
    "    ax[0, 0].set_title('Accuracy evolution against {}'.format(param_name))\n",
    "    \n",
    "    # Plot precision\n",
    "    ax[0, 1].fill_between(perf_df[param].values,\n",
    "                     perf_df['precision_fold'].apply(np.min),\n",
    "                     perf_df['precision_fold'].apply(np.max), color='coral')\n",
    "    ax[0, 1].plot(perf_df[param].values, perf_df['precision_avg'].values, color='tab:red')\n",
    "    ax[0, 1].set_xlabel(param_name)\n",
    "    ax[0, 1].set_ylabel('Precision')\n",
    "    ax[0, 1].set_title('Precision evolution against {}'.format(param_name))\n",
    "    \n",
    "    # Plot recall\n",
    "    ax[1, 0].fill_between(perf_df[param].values,\n",
    "                     perf_df['recall_fold'].apply(np.min),\n",
    "                     perf_df['recall_fold'].apply(np.max), color='lightgreen')\n",
    "    ax[1, 0].plot(perf_df[param].values, perf_df['recall_avg'].values, color='tab:green')\n",
    "    ax[1, 0].set_xlabel(param_name)\n",
    "    ax[1, 0].set_ylabel('Recall')\n",
    "    ax[1, 0].set_title('Recall evolution against {}'.format(param_name))\n",
    "    \n",
    "    # Plot specificity\n",
    "    ax[1, 1].fill_between(perf_df[param].values,\n",
    "                     perf_df['specificity_fold'].apply(np.min),\n",
    "                     perf_df['specificity_fold'].apply(np.max), color='orange')\n",
    "    ax[1, 1].plot(perf_df[param].values, perf_df['specificity_avg'].values, color='tab:orange')\n",
    "    ax[1, 1].set_xlabel(param_name)\n",
    "    ax[1, 1].set_ylabel('Specificity')\n",
    "    ax[1, 1].set_title('Specificity evolution against {}'.format(param_name))\n",
    "\n",
    "    # Plot f1 score\n",
    "    ax[2, 0].fill_between(perf_df[param].values,\n",
    "                     perf_df['f1_score_fold'].apply(np.min),\n",
    "                     perf_df['f1_score_fold'].apply(np.max), color='lightpink')\n",
    "    ax[2, 0].plot(perf_df[param].values, perf_df['f1_score_avg'].values, color='tab:pink')\n",
    "    ax[2, 0].set_xlabel(param_name)\n",
    "    ax[2, 0].set_ylabel('F1 Score')\n",
    "    ax[2, 0].set_title('F1 Score evolution against {}'.format(param_name))\n",
    "    \n",
    "    plt.show()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot evolution for linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1440 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<Figure size 1152x1440 with 6 Axes>,\n",
       " array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f526012d3c8>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7f5248059240>],\n",
       "        [<matplotlib.axes._subplots.AxesSubplot object at 0x7f5248080358>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7f524802c470>],\n",
       "        [<matplotlib.axes._subplots.AxesSubplot object at 0x7f522f4a1588>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7f522f4526a0>]],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_performance(smv_lin_rbf_perf_metrics[smv_lin_rbf_perf_metrics['kernel']=='linear'], param='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1440 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<Figure size 1152x1440 with 6 Axes>,\n",
       " array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f52240dfcc0>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7f522400fb70>],\n",
       "        [<matplotlib.axes._subplots.AxesSubplot object at 0x7f521f04b080>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7f521f07b5f8>],\n",
       "        [<matplotlib.axes._subplots.AxesSubplot object at 0x7f521f02cba8>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7f521efe9198>]],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_performance(smv_lin_rbf_perf_metrics[smv_lin_rbf_perf_metrics['kernel']=='rbf'], param='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "smv_lin_rbf_perf_metrics.to_csv('smv_lin_rbf_perf_metrics_BERT_hyb_appr.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "Fit models based on Multilayer Perceptro models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import GlorotUniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_MLP_network(input_dim, layers_dim: tuple, dropout=0.1, lr=1e-4, inner_act_func='relu', out_act_func='sigmoid', seed=10):\n",
    "    \n",
    "    if isinstance(dropout, (float, int)):\n",
    "        dropout = (dropout,) * len(layers_dim)\n",
    "    \n",
    "    # Input\n",
    "    input_layer = Input(shape=(input_dim,), dtype='float32')\n",
    "    prev_lay = input_layer\n",
    "    \n",
    "    # Add intern fully-connected layers\n",
    "    for i in range(len(layers_dim)):\n",
    "        fully_lay = Dense(layers_dim[i], activation='relu',\n",
    "                          kernel_initializer=GlorotUniform(seed=seed))(prev_lay)\n",
    "        drop_layer = Dropout(dropout[i])(fully_lay)\n",
    "        prev_lay = drop_layer\n",
    "    \n",
    "    # Add classification output layer\n",
    "    output_layer = Dense(1, activation='sigmoid',\n",
    "                        kernel_initializer=GlorotUniform(seed=seed))(prev_lay)\n",
    "    \n",
    "    # Builad & compile model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    model.optimizer.lr = lr\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, grid over neural networks different architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mulspace_limits(begin, end, factor):\n",
    "    x = begin\n",
    "    \n",
    "    while x <= end:\n",
    "        yield x\n",
    "        x *= factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration with only 1 hidden layer\n",
    "one_hidden_lay_conf = [[x] for x in mulspace_limits(32, X_train_vect.shape[1], 2)]\n",
    "\n",
    "# Configuration with 2 hidden layers\n",
    "two_hidden_lay_conf = [[x[0], y] for x in one_hidden_lay_conf[1:] for y in mulspace_limits(32, x[0], 2)]\n",
    "\n",
    "# Grid parameters\n",
    "params = {\n",
    "        'lay_conf': one_hidden_lay_conf + two_hidden_lay_conf,\n",
    "        'lr': [1e-4],\n",
    "        'dropout': [0.1],\n",
    "        'max_epochs': [300],\n",
    "        'batch_size': [100],\n",
    "        'seed': [123456]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lay_conf': [[32],\n",
       "  [64],\n",
       "  [128],\n",
       "  [256],\n",
       "  [512],\n",
       "  [1024],\n",
       "  [64, 32],\n",
       "  [64, 64],\n",
       "  [128, 32],\n",
       "  [128, 64],\n",
       "  [128, 128],\n",
       "  [256, 32],\n",
       "  [256, 64],\n",
       "  [256, 128],\n",
       "  [256, 256],\n",
       "  [512, 32],\n",
       "  [512, 64],\n",
       "  [512, 128],\n",
       "  [512, 256],\n",
       "  [512, 512],\n",
       "  [1024, 32],\n",
       "  [1024, 64],\n",
       "  [1024, 128],\n",
       "  [1024, 256],\n",
       "  [1024, 512],\n",
       "  [1024, 1024]],\n",
       " 'lr': [0.0001],\n",
       " 'dropout': [0.1],\n",
       " 'max_epochs': [300],\n",
       " 'batch_size': [100],\n",
       " 'seed': [123456]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform experiments by griding parameters above. Simple one-hot validation will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_MLP_exp(X, y, params, val_split=0.1, test_size=0.2):\n",
    "\n",
    "    mlp_perf_metrics = []\n",
    "\n",
    "    # Split train and test sets in a single one-shot validation\n",
    "    X_train_part, X_test_part, y_train_part, y_test_part = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "    for param in ParameterGrid(params):\n",
    "\n",
    "        meta = {\n",
    "            'n_layers': len(param['lay_conf']), #if isinstance(conf, list) else 1\n",
    "            'layer_conf': param['lay_conf'],\n",
    "            'lr': param['lr'],\n",
    "            'dropout': param['dropout'],\n",
    "            'batch_size': param['batch_size']\n",
    "        }\n",
    "\n",
    "        print('Training with parameters {}'.format(param))\n",
    "\n",
    "        # Fit MLP\n",
    "        mlp_model = build_MLP_network(input_dim=X.shape[1],\n",
    "                                      layers_dim=param['lay_conf'],\n",
    "                                      dropout=param['dropout'], lr=param['lr'],\n",
    "                                      seed=param['seed'],\n",
    "                                      inner_act_func=param['inner_act_func'] if 'inner_act_func' in param else 'relu',\n",
    "                                      out_act_func=param['out_act_func'] if 'out_act_func' in param else 'sigmoid')\n",
    "        mlp_model.summary()\n",
    "        hist = mlp_model.fit(X_train_part, y_train_part,\n",
    "                             batch_size=param['batch_size'],\n",
    "                             epochs=param['max_epochs'],\n",
    "                              validation_split=val_split,\n",
    "                              callbacks=[EarlyStopping(monitor='val_loss', patience=5,\n",
    "                                                       restore_best_weights=True,\n",
    "                                                      min_delta=1e-7)])\n",
    "\n",
    "        # Add train & val metrics\n",
    "        meta['accuracy_train'] = hist.history['acc'][-1]\n",
    "        meta['accuracy_val'] = hist.history['val_acc'][-1]\n",
    "        meta['loss_train'] = hist.history['loss'][-1]\n",
    "        meta['loss_val'] = hist.history['val_loss'][-1]\n",
    "        meta['epochs'] = len(hist.history['val_loss'])\n",
    "\n",
    "        # Predict over test\n",
    "        y_pred = mlp_model.predict(X_test_part)\n",
    "        y_pred[y_pred >= 0.5] = 1\n",
    "        y_pred[y_pred < 0.5] = 0\n",
    "        y_pred = y_pred.squeeze().astype('int8')\n",
    "\n",
    "        # Meassure performance metrics\n",
    "        meta['accuracy'] = metrics.accuracy_score(y_test_part, y_pred)\n",
    "        meta['precision'] = metrics.precision_score(y_test_part, y_pred)\n",
    "        meta['recall'] = metrics.recall_score(y_test_part, y_pred)\n",
    "        #  Exchange class 1 and 0 so measure recall for class 0 (specificity)\n",
    "        meta['specificity'] = metrics.recall_score(np.abs(y_test_part-1), np.abs(y_pred-1))\n",
    "        meta['f1_score'] = metrics.f1_score(y_test_part, y_pred)\n",
    "\n",
    "        mlp_perf_metrics.append(meta)\n",
    "\n",
    "    mlp_perf_metrics = pd.DataFrame(mlp_perf_metrics)\n",
    "    return mlp_perf_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                49184     \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 49,217\n",
      "Trainable params: 49,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 7ms/step - loss: 0.6452 - acc: 0.6267 - val_loss: 0.6027 - val_acc: 0.6749\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5685 - acc: 0.7201 - val_loss: 0.5445 - val_acc: 0.7488\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5296 - acc: 0.7555 - val_loss: 0.5334 - val_acc: 0.7553\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5076 - acc: 0.7725 - val_loss: 0.5108 - val_acc: 0.7701\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.4939 - acc: 0.7789 - val_loss: 0.5033 - val_acc: 0.7734\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.4776 - acc: 0.7876 - val_loss: 0.4955 - val_acc: 0.7783\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.4692 - acc: 0.7989 - val_loss: 0.4922 - val_acc: 0.7816\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.4614 - acc: 0.8020 - val_loss: 0.4893 - val_acc: 0.7767\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.4548 - acc: 0.8011 - val_loss: 0.4819 - val_acc: 0.7783\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.4506 - acc: 0.8028 - val_loss: 0.4814 - val_acc: 0.7849\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.4419 - acc: 0.8088 - val_loss: 0.4765 - val_acc: 0.7816\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.4438 - acc: 0.8088 - val_loss: 0.4767 - val_acc: 0.7833\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.4428 - acc: 0.8061 - val_loss: 0.4731 - val_acc: 0.7816\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.4314 - acc: 0.8185 - val_loss: 0.4758 - val_acc: 0.7816\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4322 - acc: 0.8152 - val_loss: 0.4697 - val_acc: 0.7915\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.4268 - acc: 0.8155 - val_loss: 0.4704 - val_acc: 0.7800\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4266 - acc: 0.8143 - val_loss: 0.4735 - val_acc: 0.7783\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4221 - acc: 0.8166 - val_loss: 0.4813 - val_acc: 0.7800\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4246 - acc: 0.8199 - val_loss: 0.4635 - val_acc: 0.7865\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4176 - acc: 0.8181 - val_loss: 0.4614 - val_acc: 0.7882\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4136 - acc: 0.8219 - val_loss: 0.4663 - val_acc: 0.7816\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4179 - acc: 0.8190 - val_loss: 0.4605 - val_acc: 0.7915\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4067 - acc: 0.8232 - val_loss: 0.4600 - val_acc: 0.7882\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.4082 - acc: 0.8216 - val_loss: 0.4573 - val_acc: 0.7833\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4062 - acc: 0.8243 - val_loss: 0.4571 - val_acc: 0.7882\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4058 - acc: 0.8243 - val_loss: 0.4615 - val_acc: 0.7898\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4040 - acc: 0.8267 - val_loss: 0.4556 - val_acc: 0.7915\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4006 - acc: 0.8290 - val_loss: 0.4538 - val_acc: 0.7865\n",
      "Epoch 29/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4036 - acc: 0.8287 - val_loss: 0.4591 - val_acc: 0.7882\n",
      "Epoch 30/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4023 - acc: 0.8289 - val_loss: 0.4523 - val_acc: 0.7882\n",
      "Epoch 31/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3953 - acc: 0.8300 - val_loss: 0.4573 - val_acc: 0.7915\n",
      "Epoch 32/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.3942 - acc: 0.8301 - val_loss: 0.4550 - val_acc: 0.7915\n",
      "Epoch 33/300\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.3916 - acc: 0.8305 - val_loss: 0.4545 - val_acc: 0.7915\n",
      "Epoch 34/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.3878 - acc: 0.8325 - val_loss: 0.4526 - val_acc: 0.7915\n",
      "Epoch 35/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.3920 - acc: 0.8320 - val_loss: 0.4504 - val_acc: 0.7931\n",
      "Epoch 36/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.3892 - acc: 0.8363 - val_loss: 0.4496 - val_acc: 0.7865\n",
      "Epoch 37/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.3909 - acc: 0.8338 - val_loss: 0.4523 - val_acc: 0.7849\n",
      "Epoch 38/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.3867 - acc: 0.8345 - val_loss: 0.4478 - val_acc: 0.7931\n",
      "Epoch 39/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.3863 - acc: 0.8349 - val_loss: 0.4507 - val_acc: 0.7947\n",
      "Epoch 40/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3854 - acc: 0.8365 - val_loss: 0.4511 - val_acc: 0.7849\n",
      "Epoch 41/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3865 - acc: 0.8343 - val_loss: 0.4484 - val_acc: 0.7915\n",
      "Epoch 42/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.3814 - acc: 0.8384 - val_loss: 0.4466 - val_acc: 0.7915\n",
      "Epoch 43/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3837 - acc: 0.8327 - val_loss: 0.4482 - val_acc: 0.7882\n",
      "Epoch 44/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3811 - acc: 0.8385 - val_loss: 0.4719 - val_acc: 0.7783\n",
      "Epoch 45/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3909 - acc: 0.8352 - val_loss: 0.4465 - val_acc: 0.7947\n",
      "Epoch 46/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3810 - acc: 0.8356 - val_loss: 0.4486 - val_acc: 0.7816\n",
      "Epoch 47/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3788 - acc: 0.8363 - val_loss: 0.4512 - val_acc: 0.7849\n",
      "Epoch 48/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3761 - acc: 0.8394 - val_loss: 0.4458 - val_acc: 0.7882\n",
      "Epoch 49/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3753 - acc: 0.8398 - val_loss: 0.4458 - val_acc: 0.7898\n",
      "Epoch 50/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3709 - acc: 0.8389 - val_loss: 0.4459 - val_acc: 0.7947\n",
      "Epoch 51/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3729 - acc: 0.8380 - val_loss: 0.4437 - val_acc: 0.7931\n",
      "Epoch 52/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3685 - acc: 0.8411 - val_loss: 0.4453 - val_acc: 0.7898\n",
      "Epoch 53/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.3684 - acc: 0.8420 - val_loss: 0.4485 - val_acc: 0.7882\n",
      "Epoch 54/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3651 - acc: 0.8446 - val_loss: 0.4440 - val_acc: 0.7931\n",
      "Epoch 55/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3645 - acc: 0.8486 - val_loss: 0.4452 - val_acc: 0.7898\n",
      "Epoch 56/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3634 - acc: 0.8446 - val_loss: 0.4499 - val_acc: 0.7816\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                98368     \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 98,433\n",
      "Trainable params: 98,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 4ms/step - loss: 0.6994 - acc: 0.5911 - val_loss: 0.6054 - val_acc: 0.7044\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5972 - acc: 0.6935 - val_loss: 0.5747 - val_acc: 0.7209\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5563 - acc: 0.7276 - val_loss: 0.5410 - val_acc: 0.7553\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.5253 - acc: 0.7630 - val_loss: 0.5193 - val_acc: 0.7553\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5053 - acc: 0.7719 - val_loss: 0.5091 - val_acc: 0.7521\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.4919 - acc: 0.7730 - val_loss: 0.4994 - val_acc: 0.7635\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.4763 - acc: 0.7873 - val_loss: 0.4894 - val_acc: 0.7619\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.4667 - acc: 0.7924 - val_loss: 0.4902 - val_acc: 0.7734\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.4603 - acc: 0.7940 - val_loss: 0.4821 - val_acc: 0.7652\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.4553 - acc: 0.7999 - val_loss: 0.4875 - val_acc: 0.7750\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4492 - acc: 0.8020 - val_loss: 0.4749 - val_acc: 0.7668\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.4425 - acc: 0.8073 - val_loss: 0.4723 - val_acc: 0.7701\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.4386 - acc: 0.8126 - val_loss: 0.4654 - val_acc: 0.7750\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4317 - acc: 0.8077 - val_loss: 0.4691 - val_acc: 0.7734\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4309 - acc: 0.8157 - val_loss: 0.4625 - val_acc: 0.7718\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4329 - acc: 0.8064 - val_loss: 0.4798 - val_acc: 0.7865\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4302 - acc: 0.8113 - val_loss: 0.4654 - val_acc: 0.7800\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4184 - acc: 0.8186 - val_loss: 0.4652 - val_acc: 0.7800\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4158 - acc: 0.8228 - val_loss: 0.4559 - val_acc: 0.7783\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4112 - acc: 0.8238 - val_loss: 0.4532 - val_acc: 0.7767\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4138 - acc: 0.8239 - val_loss: 0.4569 - val_acc: 0.7783\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4128 - acc: 0.8214 - val_loss: 0.4518 - val_acc: 0.7767\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4075 - acc: 0.8230 - val_loss: 0.4532 - val_acc: 0.7833\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4065 - acc: 0.8245 - val_loss: 0.4506 - val_acc: 0.7849\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4081 - acc: 0.8263 - val_loss: 0.4541 - val_acc: 0.7833\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4049 - acc: 0.8232 - val_loss: 0.4494 - val_acc: 0.7865\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4021 - acc: 0.8258 - val_loss: 0.4553 - val_acc: 0.7849\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4018 - acc: 0.8272 - val_loss: 0.4486 - val_acc: 0.7816\n",
      "Epoch 29/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3954 - acc: 0.8281 - val_loss: 0.4495 - val_acc: 0.7865\n",
      "Epoch 30/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3928 - acc: 0.8300 - val_loss: 0.4478 - val_acc: 0.7783\n",
      "Epoch 31/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3921 - acc: 0.8307 - val_loss: 0.4467 - val_acc: 0.7800\n",
      "Epoch 32/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.3927 - acc: 0.8320 - val_loss: 0.4449 - val_acc: 0.7833\n",
      "Epoch 33/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3911 - acc: 0.8312 - val_loss: 0.4660 - val_acc: 0.7800\n",
      "Epoch 34/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3916 - acc: 0.8336 - val_loss: 0.4449 - val_acc: 0.7816\n",
      "Epoch 35/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.3936 - acc: 0.8296 - val_loss: 0.4441 - val_acc: 0.7849\n",
      "Epoch 36/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3849 - acc: 0.8343 - val_loss: 0.4456 - val_acc: 0.7800\n",
      "Epoch 37/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3853 - acc: 0.8354 - val_loss: 0.4551 - val_acc: 0.7849\n",
      "Epoch 38/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3869 - acc: 0.8369 - val_loss: 0.4407 - val_acc: 0.7849\n",
      "Epoch 39/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3805 - acc: 0.8351 - val_loss: 0.4480 - val_acc: 0.7915\n",
      "Epoch 40/300\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.3825 - acc: 0.8334 - val_loss: 0.4405 - val_acc: 0.7865\n",
      "Epoch 41/300\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.3843 - acc: 0.8338 - val_loss: 0.4547 - val_acc: 0.7865\n",
      "Epoch 42/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3769 - acc: 0.8362 - val_loss: 0.4446 - val_acc: 0.7849\n",
      "Epoch 43/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3763 - acc: 0.8394 - val_loss: 0.4515 - val_acc: 0.7898\n",
      "Epoch 44/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3729 - acc: 0.8369 - val_loss: 0.4403 - val_acc: 0.7865\n",
      "Epoch 45/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3729 - acc: 0.8415 - val_loss: 0.4421 - val_acc: 0.7882\n",
      "Epoch 46/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3707 - acc: 0.8378 - val_loss: 0.4473 - val_acc: 0.7849\n",
      "Epoch 47/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3697 - acc: 0.8433 - val_loss: 0.4406 - val_acc: 0.7882\n",
      "Epoch 48/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3676 - acc: 0.8435 - val_loss: 0.4375 - val_acc: 0.7865\n",
      "Epoch 49/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3677 - acc: 0.8425 - val_loss: 0.4496 - val_acc: 0.7833\n",
      "Epoch 50/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3690 - acc: 0.8431 - val_loss: 0.4581 - val_acc: 0.7898\n",
      "Epoch 51/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3691 - acc: 0.8409 - val_loss: 0.4425 - val_acc: 0.7898\n",
      "Epoch 52/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3632 - acc: 0.8449 - val_loss: 0.4419 - val_acc: 0.7915\n",
      "Epoch 53/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3616 - acc: 0.8471 - val_loss: 0.4386 - val_acc: 0.7849\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 196,865\n",
      "Trainable params: 196,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 5ms/step - loss: 0.6232 - acc: 0.6533 - val_loss: 0.5608 - val_acc: 0.7356\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.5439 - acc: 0.7431 - val_loss: 0.5215 - val_acc: 0.7438\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.5045 - acc: 0.7714 - val_loss: 0.4993 - val_acc: 0.7767\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.4814 - acc: 0.7856 - val_loss: 0.4966 - val_acc: 0.7701\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.4686 - acc: 0.7895 - val_loss: 0.4801 - val_acc: 0.7750\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.4507 - acc: 0.8088 - val_loss: 0.4750 - val_acc: 0.7800\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.4426 - acc: 0.8073 - val_loss: 0.4751 - val_acc: 0.7783\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.4356 - acc: 0.8144 - val_loss: 0.4678 - val_acc: 0.7767\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.4341 - acc: 0.8130 - val_loss: 0.4702 - val_acc: 0.7800\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.4289 - acc: 0.8137 - val_loss: 0.4724 - val_acc: 0.7800\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4244 - acc: 0.8186 - val_loss: 0.4634 - val_acc: 0.7800\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4208 - acc: 0.8186 - val_loss: 0.4696 - val_acc: 0.7882\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4136 - acc: 0.8228 - val_loss: 0.4604 - val_acc: 0.7849\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4094 - acc: 0.8278 - val_loss: 0.4603 - val_acc: 0.7833\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4103 - acc: 0.8241 - val_loss: 0.4548 - val_acc: 0.7865\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4067 - acc: 0.8267 - val_loss: 0.4513 - val_acc: 0.7865\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4057 - acc: 0.8300 - val_loss: 0.4489 - val_acc: 0.7898\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4069 - acc: 0.8265 - val_loss: 0.4524 - val_acc: 0.7849\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4007 - acc: 0.8270 - val_loss: 0.4509 - val_acc: 0.7898\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3944 - acc: 0.8312 - val_loss: 0.4612 - val_acc: 0.7882\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3957 - acc: 0.8331 - val_loss: 0.4746 - val_acc: 0.7816\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3948 - acc: 0.8300 - val_loss: 0.4645 - val_acc: 0.7882\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 393,729\n",
      "Trainable params: 393,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 7ms/step - loss: 0.6470 - acc: 0.6304 - val_loss: 0.5630 - val_acc: 0.7143\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5335 - acc: 0.7491 - val_loss: 0.5158 - val_acc: 0.7750\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5042 - acc: 0.7666 - val_loss: 0.5075 - val_acc: 0.7603\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4753 - acc: 0.7884 - val_loss: 0.4867 - val_acc: 0.7734\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.4597 - acc: 0.7953 - val_loss: 0.4776 - val_acc: 0.7734\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.4468 - acc: 0.8011 - val_loss: 0.4807 - val_acc: 0.7718\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.4427 - acc: 0.8099 - val_loss: 0.4680 - val_acc: 0.7882\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.4361 - acc: 0.8041 - val_loss: 0.4706 - val_acc: 0.7800\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.4276 - acc: 0.8143 - val_loss: 0.4692 - val_acc: 0.7849\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.4274 - acc: 0.8115 - val_loss: 0.4807 - val_acc: 0.7718\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.4241 - acc: 0.8104 - val_loss: 0.4594 - val_acc: 0.7882\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.4137 - acc: 0.8243 - val_loss: 0.4571 - val_acc: 0.7915\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.4095 - acc: 0.8241 - val_loss: 0.4545 - val_acc: 0.7865\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.4066 - acc: 0.8261 - val_loss: 0.4520 - val_acc: 0.7865\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.4043 - acc: 0.8252 - val_loss: 0.4537 - val_acc: 0.7898\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.3999 - acc: 0.8281 - val_loss: 0.4504 - val_acc: 0.7865\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.3972 - acc: 0.8294 - val_loss: 0.4531 - val_acc: 0.7931\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.4037 - acc: 0.8207 - val_loss: 0.4490 - val_acc: 0.7849\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.3907 - acc: 0.8325 - val_loss: 0.4555 - val_acc: 0.7898\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.3913 - acc: 0.8338 - val_loss: 0.4652 - val_acc: 0.7833\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.3896 - acc: 0.8305 - val_loss: 0.4480 - val_acc: 0.7816\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.3842 - acc: 0.8360 - val_loss: 0.4477 - val_acc: 0.7849\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.3811 - acc: 0.8363 - val_loss: 0.4505 - val_acc: 0.7931\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3805 - acc: 0.8404 - val_loss: 0.4427 - val_acc: 0.7849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3750 - acc: 0.8407 - val_loss: 0.4449 - val_acc: 0.7849\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3754 - acc: 0.8387 - val_loss: 0.4495 - val_acc: 0.7865\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.3749 - acc: 0.8371 - val_loss: 0.4629 - val_acc: 0.7882\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.3708 - acc: 0.8402 - val_loss: 0.4448 - val_acc: 0.7882\n",
      "Epoch 29/300\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.3736 - acc: 0.8387 - val_loss: 0.4499 - val_acc: 0.7947\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 787,457\n",
      "Trainable params: 787,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.6155 - acc: 0.6687 - val_loss: 0.5635 - val_acc: 0.7241\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.5129 - acc: 0.7592 - val_loss: 0.4975 - val_acc: 0.7701\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4736 - acc: 0.7880 - val_loss: 0.4856 - val_acc: 0.7750\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4575 - acc: 0.8000 - val_loss: 0.4770 - val_acc: 0.7783\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4446 - acc: 0.8075 - val_loss: 0.4682 - val_acc: 0.7833\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.4379 - acc: 0.8090 - val_loss: 0.4622 - val_acc: 0.7767\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4271 - acc: 0.8176 - val_loss: 0.4627 - val_acc: 0.7882\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4238 - acc: 0.8205 - val_loss: 0.4557 - val_acc: 0.7849\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4169 - acc: 0.8208 - val_loss: 0.4553 - val_acc: 0.7783\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4168 - acc: 0.8230 - val_loss: 0.4599 - val_acc: 0.7882\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4131 - acc: 0.8208 - val_loss: 0.4758 - val_acc: 0.7865\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4060 - acc: 0.8296 - val_loss: 0.4504 - val_acc: 0.7849\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.3977 - acc: 0.8289 - val_loss: 0.4513 - val_acc: 0.7915\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.3995 - acc: 0.8272 - val_loss: 0.4515 - val_acc: 0.7849\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.3966 - acc: 0.8283 - val_loss: 0.4592 - val_acc: 0.7816\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.3930 - acc: 0.8290 - val_loss: 0.4493 - val_acc: 0.7865\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3908 - acc: 0.8343 - val_loss: 0.4537 - val_acc: 0.7931\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3790 - acc: 0.8356 - val_loss: 0.4421 - val_acc: 0.7915\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.3807 - acc: 0.8374 - val_loss: 0.4422 - val_acc: 0.7931\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.3783 - acc: 0.8382 - val_loss: 0.4436 - val_acc: 0.7915\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.3723 - acc: 0.8404 - val_loss: 0.4463 - val_acc: 0.7964\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.3698 - acc: 0.8438 - val_loss: 0.4386 - val_acc: 0.7997\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3734 - acc: 0.8409 - val_loss: 0.4549 - val_acc: 0.7947\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.3697 - acc: 0.8418 - val_loss: 0.4487 - val_acc: 0.7882\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.3640 - acc: 0.8420 - val_loss: 0.4374 - val_acc: 0.7980\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.3667 - acc: 0.8400 - val_loss: 0.5383 - val_acc: 0.7619\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.3748 - acc: 0.8323 - val_loss: 0.4465 - val_acc: 0.8062\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.3604 - acc: 0.8473 - val_loss: 0.4486 - val_acc: 0.7997\n",
      "Epoch 29/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.3599 - acc: 0.8475 - val_loss: 0.4446 - val_acc: 0.7898\n",
      "Epoch 30/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.3521 - acc: 0.8519 - val_loss: 0.4402 - val_acc: 0.7964\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [1024], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 1,574,913\n",
      "Trainable params: 1,574,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.5913 - acc: 0.6862 - val_loss: 0.5144 - val_acc: 0.7537\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 1s 17ms/step - loss: 0.4836 - acc: 0.7823 - val_loss: 0.4798 - val_acc: 0.7783\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.4581 - acc: 0.7938 - val_loss: 0.4712 - val_acc: 0.7849\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.4423 - acc: 0.8051 - val_loss: 0.5060 - val_acc: 0.7619\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.4324 - acc: 0.8101 - val_loss: 0.4599 - val_acc: 0.7849\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.4215 - acc: 0.8174 - val_loss: 0.4546 - val_acc: 0.7898\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.4129 - acc: 0.8238 - val_loss: 0.4587 - val_acc: 0.7816\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 1s 18ms/step - loss: 0.4110 - acc: 0.8219 - val_loss: 0.4480 - val_acc: 0.7816\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.4088 - acc: 0.8227 - val_loss: 0.4475 - val_acc: 0.7882\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.4019 - acc: 0.8252 - val_loss: 0.4447 - val_acc: 0.7898\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.3900 - acc: 0.8329 - val_loss: 0.4517 - val_acc: 0.7915\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 1s 18ms/step - loss: 0.3916 - acc: 0.8323 - val_loss: 0.4435 - val_acc: 0.7816\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.3883 - acc: 0.8334 - val_loss: 0.4423 - val_acc: 0.7882\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.3811 - acc: 0.8371 - val_loss: 0.4473 - val_acc: 0.7849\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.3829 - acc: 0.8378 - val_loss: 0.4579 - val_acc: 0.7947\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.3780 - acc: 0.8380 - val_loss: 0.4382 - val_acc: 0.7849\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 1s 18ms/step - loss: 0.3695 - acc: 0.8393 - val_loss: 0.4640 - val_acc: 0.7898\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 1s 18ms/step - loss: 0.3875 - acc: 0.8311 - val_loss: 0.4431 - val_acc: 0.7898\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.3629 - acc: 0.8491 - val_loss: 0.4438 - val_acc: 0.7947\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.3669 - acc: 0.8400 - val_loss: 0.4780 - val_acc: 0.7882\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.3626 - acc: 0.8418 - val_loss: 0.4432 - val_acc: 0.7997\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [64, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                98368     \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 100,481\n",
      "Trainable params: 100,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 5ms/step - loss: 0.7271 - acc: 0.5630 - val_loss: 0.6376 - val_acc: 0.6995\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.6257 - acc: 0.6606 - val_loss: 0.5772 - val_acc: 0.7356\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5772 - acc: 0.7167 - val_loss: 0.5375 - val_acc: 0.7504\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.5374 - acc: 0.7433 - val_loss: 0.5170 - val_acc: 0.7603\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.5165 - acc: 0.7595 - val_loss: 0.5000 - val_acc: 0.7635\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4980 - acc: 0.7687 - val_loss: 0.4978 - val_acc: 0.7635\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4863 - acc: 0.7785 - val_loss: 0.4916 - val_acc: 0.7603\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4736 - acc: 0.7842 - val_loss: 0.4817 - val_acc: 0.7783\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4648 - acc: 0.7966 - val_loss: 0.4863 - val_acc: 0.7652\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4640 - acc: 0.7926 - val_loss: 0.4758 - val_acc: 0.7701\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4499 - acc: 0.8013 - val_loss: 0.4726 - val_acc: 0.7833\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4475 - acc: 0.8000 - val_loss: 0.4713 - val_acc: 0.7750\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4451 - acc: 0.8092 - val_loss: 0.4685 - val_acc: 0.7783\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4393 - acc: 0.8072 - val_loss: 0.4651 - val_acc: 0.7833\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4329 - acc: 0.8143 - val_loss: 0.4637 - val_acc: 0.7849\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4362 - acc: 0.8119 - val_loss: 0.4614 - val_acc: 0.7833\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4297 - acc: 0.8115 - val_loss: 0.4638 - val_acc: 0.7849\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4249 - acc: 0.8166 - val_loss: 0.4580 - val_acc: 0.7882\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4221 - acc: 0.8179 - val_loss: 0.4596 - val_acc: 0.7816\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4177 - acc: 0.8241 - val_loss: 0.4608 - val_acc: 0.7833\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4172 - acc: 0.8197 - val_loss: 0.4577 - val_acc: 0.7849\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4142 - acc: 0.8227 - val_loss: 0.4639 - val_acc: 0.7882\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4107 - acc: 0.8197 - val_loss: 0.4579 - val_acc: 0.7865\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4075 - acc: 0.8247 - val_loss: 0.4542 - val_acc: 0.7849\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4083 - acc: 0.8258 - val_loss: 0.4638 - val_acc: 0.7833\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4024 - acc: 0.8270 - val_loss: 0.4544 - val_acc: 0.7750\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3972 - acc: 0.8276 - val_loss: 0.4528 - val_acc: 0.7849\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3984 - acc: 0.8305 - val_loss: 0.4546 - val_acc: 0.7783\n",
      "Epoch 29/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3971 - acc: 0.8323 - val_loss: 0.4534 - val_acc: 0.7833\n",
      "Epoch 30/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3948 - acc: 0.8338 - val_loss: 0.4486 - val_acc: 0.7865\n",
      "Epoch 31/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3921 - acc: 0.8351 - val_loss: 0.4497 - val_acc: 0.7816\n",
      "Epoch 32/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3904 - acc: 0.8305 - val_loss: 0.4567 - val_acc: 0.7849\n",
      "Epoch 33/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3890 - acc: 0.8393 - val_loss: 0.4576 - val_acc: 0.7898\n",
      "Epoch 34/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3868 - acc: 0.8407 - val_loss: 0.4494 - val_acc: 0.7997\n",
      "Epoch 35/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3872 - acc: 0.8358 - val_loss: 0.4496 - val_acc: 0.7898\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [64, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 64)                98368     \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 102,593\n",
      "Trainable params: 102,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 5ms/step - loss: 0.6655 - acc: 0.6004 - val_loss: 0.6102 - val_acc: 0.7126\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5995 - acc: 0.6887 - val_loss: 0.5539 - val_acc: 0.7291\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5542 - acc: 0.7272 - val_loss: 0.5223 - val_acc: 0.7521\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5194 - acc: 0.7648 - val_loss: 0.5068 - val_acc: 0.7619\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.5026 - acc: 0.7641 - val_loss: 0.4964 - val_acc: 0.7603\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4856 - acc: 0.7820 - val_loss: 0.4885 - val_acc: 0.7586\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4711 - acc: 0.7895 - val_loss: 0.4818 - val_acc: 0.7701\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4711 - acc: 0.7902 - val_loss: 0.4835 - val_acc: 0.7800\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4572 - acc: 0.7962 - val_loss: 0.4931 - val_acc: 0.7685\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4568 - acc: 0.7933 - val_loss: 0.4814 - val_acc: 0.7767\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4476 - acc: 0.8013 - val_loss: 0.4885 - val_acc: 0.7701\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4457 - acc: 0.8046 - val_loss: 0.4667 - val_acc: 0.7816\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4307 - acc: 0.8124 - val_loss: 0.4692 - val_acc: 0.7816\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4318 - acc: 0.8183 - val_loss: 0.4709 - val_acc: 0.7898\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4300 - acc: 0.8082 - val_loss: 0.4749 - val_acc: 0.7783\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4249 - acc: 0.8163 - val_loss: 0.4665 - val_acc: 0.7849\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4266 - acc: 0.8161 - val_loss: 0.4606 - val_acc: 0.7800\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4184 - acc: 0.8159 - val_loss: 0.4588 - val_acc: 0.7849\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4184 - acc: 0.8179 - val_loss: 0.4642 - val_acc: 0.7833\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4129 - acc: 0.8214 - val_loss: 0.4580 - val_acc: 0.7816\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4112 - acc: 0.8217 - val_loss: 0.4638 - val_acc: 0.7833\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4083 - acc: 0.8243 - val_loss: 0.4559 - val_acc: 0.7833\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4013 - acc: 0.8305 - val_loss: 0.4624 - val_acc: 0.7800\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4016 - acc: 0.8239 - val_loss: 0.4686 - val_acc: 0.7783\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4004 - acc: 0.8274 - val_loss: 0.4529 - val_acc: 0.7833\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3924 - acc: 0.8332 - val_loss: 0.4530 - val_acc: 0.7931\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3932 - acc: 0.8320 - val_loss: 0.4527 - val_acc: 0.7964\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3928 - acc: 0.8342 - val_loss: 0.4612 - val_acc: 0.7898\n",
      "Epoch 29/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3874 - acc: 0.8373 - val_loss: 0.4537 - val_acc: 0.7931\n",
      "Epoch 30/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3871 - acc: 0.8369 - val_loss: 0.4533 - val_acc: 0.7980\n",
      "Epoch 31/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3867 - acc: 0.8382 - val_loss: 0.4546 - val_acc: 0.7931\n",
      "Epoch 32/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3857 - acc: 0.8352 - val_loss: 0.4542 - val_acc: 0.7833\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [128, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 200,897\n",
      "Trainable params: 200,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 5ms/step - loss: 0.6614 - acc: 0.6141 - val_loss: 0.5928 - val_acc: 0.7340\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.5833 - acc: 0.6993 - val_loss: 0.5639 - val_acc: 0.7438\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5405 - acc: 0.7437 - val_loss: 0.5216 - val_acc: 0.7521\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5051 - acc: 0.7727 - val_loss: 0.5117 - val_acc: 0.7553\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.4867 - acc: 0.7734 - val_loss: 0.4926 - val_acc: 0.7701\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4731 - acc: 0.7849 - val_loss: 0.4953 - val_acc: 0.7668\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4647 - acc: 0.7915 - val_loss: 0.4807 - val_acc: 0.7767\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4535 - acc: 0.8009 - val_loss: 0.4799 - val_acc: 0.7767\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4461 - acc: 0.8008 - val_loss: 0.4793 - val_acc: 0.7767\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4381 - acc: 0.8051 - val_loss: 0.4692 - val_acc: 0.7652\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4332 - acc: 0.8077 - val_loss: 0.4689 - val_acc: 0.7734\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4279 - acc: 0.8128 - val_loss: 0.4656 - val_acc: 0.7783\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4244 - acc: 0.8150 - val_loss: 0.4732 - val_acc: 0.7783\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4258 - acc: 0.8134 - val_loss: 0.4617 - val_acc: 0.7767\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4191 - acc: 0.8199 - val_loss: 0.4585 - val_acc: 0.7734\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4147 - acc: 0.8203 - val_loss: 0.4624 - val_acc: 0.7750\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4154 - acc: 0.8217 - val_loss: 0.4580 - val_acc: 0.7734\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4120 - acc: 0.8221 - val_loss: 0.4597 - val_acc: 0.7800\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4029 - acc: 0.8261 - val_loss: 0.4606 - val_acc: 0.7750\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3984 - acc: 0.8303 - val_loss: 0.4563 - val_acc: 0.7767\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3987 - acc: 0.8278 - val_loss: 0.4524 - val_acc: 0.7750\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3942 - acc: 0.8289 - val_loss: 0.4615 - val_acc: 0.7800\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3948 - acc: 0.8307 - val_loss: 0.4617 - val_acc: 0.7783\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3847 - acc: 0.8325 - val_loss: 0.4541 - val_acc: 0.7800\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3836 - acc: 0.8314 - val_loss: 0.4513 - val_acc: 0.7767\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3850 - acc: 0.8365 - val_loss: 0.4548 - val_acc: 0.7865\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3830 - acc: 0.8380 - val_loss: 0.4488 - val_acc: 0.7750\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3825 - acc: 0.8358 - val_loss: 0.4552 - val_acc: 0.7849\n",
      "Epoch 29/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3734 - acc: 0.8431 - val_loss: 0.4562 - val_acc: 0.7767\n",
      "Epoch 30/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3787 - acc: 0.8371 - val_loss: 0.4473 - val_acc: 0.7816\n",
      "Epoch 31/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3690 - acc: 0.8453 - val_loss: 0.4560 - val_acc: 0.7816\n",
      "Epoch 32/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3678 - acc: 0.8429 - val_loss: 0.4532 - val_acc: 0.7849\n",
      "Epoch 33/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3735 - acc: 0.8385 - val_loss: 0.5066 - val_acc: 0.7734\n",
      "Epoch 34/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3665 - acc: 0.8413 - val_loss: 0.4634 - val_acc: 0.7816\n",
      "Epoch 35/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3775 - acc: 0.8336 - val_loss: 0.4488 - val_acc: 0.7833\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [128, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 205,057\n",
      "Trainable params: 205,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 5ms/step - loss: 0.6562 - acc: 0.6183 - val_loss: 0.5730 - val_acc: 0.7438\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.5688 - acc: 0.7188 - val_loss: 0.5309 - val_acc: 0.7603\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.5279 - acc: 0.7473 - val_loss: 0.5079 - val_acc: 0.7586\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.4970 - acc: 0.7703 - val_loss: 0.4962 - val_acc: 0.7685\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4813 - acc: 0.7820 - val_loss: 0.4876 - val_acc: 0.7652\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4656 - acc: 0.7918 - val_loss: 0.4836 - val_acc: 0.7685\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4556 - acc: 0.7978 - val_loss: 0.5090 - val_acc: 0.7685\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4471 - acc: 0.8055 - val_loss: 0.4734 - val_acc: 0.7701\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4385 - acc: 0.8068 - val_loss: 0.4714 - val_acc: 0.7718\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4349 - acc: 0.8101 - val_loss: 0.4676 - val_acc: 0.7718\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4304 - acc: 0.8126 - val_loss: 0.4640 - val_acc: 0.7767\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4258 - acc: 0.8137 - val_loss: 0.4654 - val_acc: 0.7783\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4176 - acc: 0.8201 - val_loss: 0.4751 - val_acc: 0.7833\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4129 - acc: 0.8228 - val_loss: 0.4667 - val_acc: 0.7882\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4176 - acc: 0.8199 - val_loss: 0.4576 - val_acc: 0.7783\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4086 - acc: 0.8245 - val_loss: 0.4584 - val_acc: 0.7767\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4036 - acc: 0.8281 - val_loss: 0.4535 - val_acc: 0.7833\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4047 - acc: 0.8278 - val_loss: 0.4511 - val_acc: 0.7849\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4015 - acc: 0.8256 - val_loss: 0.4506 - val_acc: 0.7816\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3955 - acc: 0.8347 - val_loss: 0.4512 - val_acc: 0.7833\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3962 - acc: 0.8290 - val_loss: 0.4583 - val_acc: 0.7882\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3924 - acc: 0.8311 - val_loss: 0.4560 - val_acc: 0.7833\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3871 - acc: 0.8342 - val_loss: 0.4538 - val_acc: 0.7915\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3841 - acc: 0.8354 - val_loss: 0.4488 - val_acc: 0.7833\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3839 - acc: 0.8373 - val_loss: 0.4443 - val_acc: 0.7849\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3800 - acc: 0.8391 - val_loss: 0.4488 - val_acc: 0.7849\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3777 - acc: 0.8409 - val_loss: 0.4485 - val_acc: 0.7816\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3747 - acc: 0.8389 - val_loss: 0.4483 - val_acc: 0.7800\n",
      "Epoch 29/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3753 - acc: 0.8400 - val_loss: 0.4500 - val_acc: 0.7816\n",
      "Epoch 30/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3651 - acc: 0.8436 - val_loss: 0.4592 - val_acc: 0.7931\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 1s 5ms/step - loss: 0.6590 - acc: 0.6138 - val_loss: 0.5918 - val_acc: 0.7258\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5749 - acc: 0.7150 - val_loss: 0.5354 - val_acc: 0.7570\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5185 - acc: 0.7595 - val_loss: 0.5151 - val_acc: 0.7619\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4908 - acc: 0.7761 - val_loss: 0.4939 - val_acc: 0.7767\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4678 - acc: 0.7926 - val_loss: 0.4844 - val_acc: 0.7800\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4529 - acc: 0.8009 - val_loss: 0.4778 - val_acc: 0.7734\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4469 - acc: 0.8020 - val_loss: 0.4706 - val_acc: 0.7833\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4397 - acc: 0.8103 - val_loss: 0.4746 - val_acc: 0.7816\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4388 - acc: 0.8092 - val_loss: 0.4653 - val_acc: 0.7816\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4231 - acc: 0.8152 - val_loss: 0.4673 - val_acc: 0.7767\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4214 - acc: 0.8161 - val_loss: 0.4623 - val_acc: 0.7849\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4210 - acc: 0.8183 - val_loss: 0.4786 - val_acc: 0.7701\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.4147 - acc: 0.8185 - val_loss: 0.4574 - val_acc: 0.7833\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.4081 - acc: 0.8234 - val_loss: 0.4583 - val_acc: 0.7816\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4069 - acc: 0.8267 - val_loss: 0.4880 - val_acc: 0.7800\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4066 - acc: 0.8250 - val_loss: 0.4943 - val_acc: 0.7734\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3973 - acc: 0.8258 - val_loss: 0.4589 - val_acc: 0.7849\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3911 - acc: 0.8342 - val_loss: 0.4645 - val_acc: 0.7849\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 401,729\n",
      "Trainable params: 401,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 6ms/step - loss: 0.6844 - acc: 0.5878 - val_loss: 0.6117 - val_acc: 0.6995\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5921 - acc: 0.7001 - val_loss: 0.5607 - val_acc: 0.7307\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5338 - acc: 0.7471 - val_loss: 0.5152 - val_acc: 0.7488\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5000 - acc: 0.7670 - val_loss: 0.5140 - val_acc: 0.7586\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4769 - acc: 0.7864 - val_loss: 0.5154 - val_acc: 0.7537\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.4659 - acc: 0.7913 - val_loss: 0.4881 - val_acc: 0.7652\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.4541 - acc: 0.7989 - val_loss: 0.4809 - val_acc: 0.7668\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.4502 - acc: 0.7986 - val_loss: 0.4736 - val_acc: 0.7750\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.4410 - acc: 0.8050 - val_loss: 0.4698 - val_acc: 0.7750\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.4319 - acc: 0.8103 - val_loss: 0.4718 - val_acc: 0.7750\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.4280 - acc: 0.8152 - val_loss: 0.4660 - val_acc: 0.7767\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.4277 - acc: 0.8159 - val_loss: 0.4654 - val_acc: 0.7849\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.4181 - acc: 0.8205 - val_loss: 0.4600 - val_acc: 0.7865\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.4146 - acc: 0.8247 - val_loss: 0.4564 - val_acc: 0.7865\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.4107 - acc: 0.8199 - val_loss: 0.5000 - val_acc: 0.7718\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.4128 - acc: 0.8194 - val_loss: 0.4569 - val_acc: 0.7734\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.4071 - acc: 0.8261 - val_loss: 0.4593 - val_acc: 0.7783\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.4054 - acc: 0.8258 - val_loss: 0.4526 - val_acc: 0.7816\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.3971 - acc: 0.8267 - val_loss: 0.4500 - val_acc: 0.7833\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.3922 - acc: 0.8300 - val_loss: 0.4548 - val_acc: 0.7865\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.3910 - acc: 0.8307 - val_loss: 0.4546 - val_acc: 0.7882\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3874 - acc: 0.8327 - val_loss: 0.4490 - val_acc: 0.7816\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.3873 - acc: 0.8312 - val_loss: 0.4530 - val_acc: 0.7767\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.3815 - acc: 0.8354 - val_loss: 0.4594 - val_acc: 0.7882\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.3770 - acc: 0.8407 - val_loss: 0.4547 - val_acc: 0.7800\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.3734 - acc: 0.8444 - val_loss: 0.4488 - val_acc: 0.7865\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.3805 - acc: 0.8321 - val_loss: 0.4515 - val_acc: 0.7800\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.3790 - acc: 0.8374 - val_loss: 0.4522 - val_acc: 0.7898\n",
      "Epoch 29/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.3670 - acc: 0.8422 - val_loss: 0.4511 - val_acc: 0.7833\n",
      "Epoch 30/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.3697 - acc: 0.8436 - val_loss: 0.4683 - val_acc: 0.7783\n",
      "Epoch 31/300\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.3677 - acc: 0.8424 - val_loss: 0.4424 - val_acc: 0.7849\n",
      "Epoch 32/300\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.3555 - acc: 0.8504 - val_loss: 0.4455 - val_acc: 0.7833\n",
      "Epoch 33/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.3607 - acc: 0.8458 - val_loss: 0.4423 - val_acc: 0.7882\n",
      "Epoch 34/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.3550 - acc: 0.8528 - val_loss: 0.4474 - val_acc: 0.7865\n",
      "Epoch 35/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.3527 - acc: 0.8509 - val_loss: 0.4472 - val_acc: 0.7865\n",
      "Epoch 36/300\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.3484 - acc: 0.8504 - val_loss: 0.4471 - val_acc: 0.7865\n",
      "Epoch 37/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.3464 - acc: 0.8520 - val_loss: 0.4486 - val_acc: 0.7898\n",
      "Epoch 38/300\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.3387 - acc: 0.8555 - val_loss: 0.4539 - val_acc: 0.7931\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,985\n",
      "Trainable params: 409,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 6ms/step - loss: 0.7311 - acc: 0.5829 - val_loss: 0.5979 - val_acc: 0.7291\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5832 - acc: 0.7048 - val_loss: 0.5402 - val_acc: 0.7274\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5314 - acc: 0.7440 - val_loss: 0.5085 - val_acc: 0.7504\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5001 - acc: 0.7712 - val_loss: 0.4974 - val_acc: 0.7570\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4800 - acc: 0.7812 - val_loss: 0.4863 - val_acc: 0.7619\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.4722 - acc: 0.7856 - val_loss: 0.4909 - val_acc: 0.7718\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.4484 - acc: 0.8110 - val_loss: 0.4775 - val_acc: 0.7701\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.4431 - acc: 0.8062 - val_loss: 0.4677 - val_acc: 0.7734\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.4337 - acc: 0.8128 - val_loss: 0.4674 - val_acc: 0.7750\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.4305 - acc: 0.8137 - val_loss: 0.4788 - val_acc: 0.7783\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.4302 - acc: 0.8103 - val_loss: 0.4772 - val_acc: 0.7800\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.4203 - acc: 0.8168 - val_loss: 0.4647 - val_acc: 0.7767\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.4169 - acc: 0.8214 - val_loss: 0.4713 - val_acc: 0.7767\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.4152 - acc: 0.8274 - val_loss: 0.4575 - val_acc: 0.7849\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.4025 - acc: 0.8283 - val_loss: 0.4714 - val_acc: 0.7783\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.4127 - acc: 0.8228 - val_loss: 0.4533 - val_acc: 0.7849\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.4074 - acc: 0.8186 - val_loss: 0.4575 - val_acc: 0.7800\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.3932 - acc: 0.8303 - val_loss: 0.4500 - val_acc: 0.7898\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.3928 - acc: 0.8316 - val_loss: 0.4577 - val_acc: 0.7849\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.3933 - acc: 0.8292 - val_loss: 0.4498 - val_acc: 0.7898\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3926 - acc: 0.8287 - val_loss: 0.4455 - val_acc: 0.7898\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.3849 - acc: 0.8369 - val_loss: 0.4521 - val_acc: 0.7882\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3780 - acc: 0.8405 - val_loss: 0.4528 - val_acc: 0.7865\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.3821 - acc: 0.8389 - val_loss: 0.4440 - val_acc: 0.7915\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3882 - acc: 0.8314 - val_loss: 0.4420 - val_acc: 0.7947\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.3707 - acc: 0.8433 - val_loss: 0.4612 - val_acc: 0.7849\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.3759 - acc: 0.8433 - val_loss: 0.4452 - val_acc: 0.7931\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.3703 - acc: 0.8447 - val_loss: 0.4401 - val_acc: 0.7947\n",
      "Epoch 29/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.3628 - acc: 0.8477 - val_loss: 0.4461 - val_acc: 0.7947\n",
      "Epoch 30/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.3656 - acc: 0.8442 - val_loss: 0.4428 - val_acc: 0.7947\n",
      "Epoch 31/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.3549 - acc: 0.8508 - val_loss: 0.4455 - val_acc: 0.7898\n",
      "Epoch 32/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3561 - acc: 0.8513 - val_loss: 0.4414 - val_acc: 0.8030\n",
      "Epoch 33/300\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.3574 - acc: 0.8484 - val_loss: 0.4425 - val_acc: 0.7980\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [256, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 426,497\n",
      "Trainable params: 426,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 6ms/step - loss: 0.6725 - acc: 0.6229 - val_loss: 0.5822 - val_acc: 0.7323\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.5564 - acc: 0.7283 - val_loss: 0.5253 - val_acc: 0.7422\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.5132 - acc: 0.7568 - val_loss: 0.4980 - val_acc: 0.7652\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.4813 - acc: 0.7838 - val_loss: 0.4834 - val_acc: 0.7619\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.4644 - acc: 0.7869 - val_loss: 0.4718 - val_acc: 0.7783\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.4532 - acc: 0.8028 - val_loss: 0.4670 - val_acc: 0.7734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.4399 - acc: 0.8061 - val_loss: 0.4716 - val_acc: 0.7865\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.4324 - acc: 0.8075 - val_loss: 0.4668 - val_acc: 0.7849\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.4243 - acc: 0.8141 - val_loss: 0.4572 - val_acc: 0.7783\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.4225 - acc: 0.8152 - val_loss: 0.4538 - val_acc: 0.7849\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.4165 - acc: 0.8186 - val_loss: 0.4506 - val_acc: 0.7849\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.4078 - acc: 0.8309 - val_loss: 0.4717 - val_acc: 0.7783\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.4092 - acc: 0.8245 - val_loss: 0.4553 - val_acc: 0.7898\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.4022 - acc: 0.8281 - val_loss: 0.4519 - val_acc: 0.7849\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.3948 - acc: 0.8327 - val_loss: 0.4456 - val_acc: 0.7833\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.3927 - acc: 0.8331 - val_loss: 0.4526 - val_acc: 0.7849\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.3888 - acc: 0.8280 - val_loss: 0.4621 - val_acc: 0.7964\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.3904 - acc: 0.8305 - val_loss: 0.4623 - val_acc: 0.7947\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.3805 - acc: 0.8342 - val_loss: 0.4480 - val_acc: 0.7833\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.3743 - acc: 0.8420 - val_loss: 0.4536 - val_acc: 0.7915\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 6ms/step - loss: 0.7217 - acc: 0.5866 - val_loss: 0.6040 - val_acc: 0.7110\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.5790 - acc: 0.7190 - val_loss: 0.5366 - val_acc: 0.7504\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.5278 - acc: 0.7537 - val_loss: 0.5093 - val_acc: 0.7521\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4893 - acc: 0.7783 - val_loss: 0.4944 - val_acc: 0.7619\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4741 - acc: 0.7882 - val_loss: 0.4854 - val_acc: 0.7718\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4565 - acc: 0.7995 - val_loss: 0.4781 - val_acc: 0.7701\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.4482 - acc: 0.8011 - val_loss: 0.4732 - val_acc: 0.7849\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.4365 - acc: 0.8068 - val_loss: 0.4880 - val_acc: 0.7718\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.4267 - acc: 0.8155 - val_loss: 0.4670 - val_acc: 0.7816\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.4252 - acc: 0.8124 - val_loss: 0.4937 - val_acc: 0.7718\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.4193 - acc: 0.8168 - val_loss: 0.4581 - val_acc: 0.7800\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.4077 - acc: 0.8267 - val_loss: 0.4691 - val_acc: 0.7833\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.4057 - acc: 0.8294 - val_loss: 0.4608 - val_acc: 0.7800\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.3983 - acc: 0.8276 - val_loss: 0.4608 - val_acc: 0.7849\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.4024 - acc: 0.8300 - val_loss: 0.4500 - val_acc: 0.7931\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.3912 - acc: 0.8345 - val_loss: 0.4600 - val_acc: 0.7849\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.3898 - acc: 0.8329 - val_loss: 0.4633 - val_acc: 0.7882\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.3831 - acc: 0.8349 - val_loss: 0.4589 - val_acc: 0.7931\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.3807 - acc: 0.8352 - val_loss: 0.4514 - val_acc: 0.7849\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.3754 - acc: 0.8371 - val_loss: 0.4472 - val_acc: 0.7865\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.3740 - acc: 0.8396 - val_loss: 0.4471 - val_acc: 0.7882\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.3718 - acc: 0.8398 - val_loss: 0.4485 - val_acc: 0.7980\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.3646 - acc: 0.8440 - val_loss: 0.4487 - val_acc: 0.7898\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.3748 - acc: 0.8384 - val_loss: 0.4461 - val_acc: 0.7865\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.3552 - acc: 0.8500 - val_loss: 0.4473 - val_acc: 0.7898\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.3592 - acc: 0.8480 - val_loss: 0.4549 - val_acc: 0.7882\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.3497 - acc: 0.8502 - val_loss: 0.4610 - val_acc: 0.7980\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.3470 - acc: 0.8550 - val_loss: 0.4709 - val_acc: 0.7980\n",
      "Epoch 29/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.3515 - acc: 0.8475 - val_loss: 0.5114 - val_acc: 0.7668\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [512, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 32)                16416     \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 803,393\n",
      "Trainable params: 803,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 1s 8ms/step - loss: 0.6707 - acc: 0.6211 - val_loss: 0.5847 - val_acc: 0.7340\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.5616 - acc: 0.7282 - val_loss: 0.5376 - val_acc: 0.7603\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.5130 - acc: 0.7628 - val_loss: 0.5021 - val_acc: 0.7701\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4844 - acc: 0.7822 - val_loss: 0.5047 - val_acc: 0.7783\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4644 - acc: 0.7929 - val_loss: 0.4753 - val_acc: 0.7718\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.4525 - acc: 0.8008 - val_loss: 0.4736 - val_acc: 0.7783\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4413 - acc: 0.8046 - val_loss: 0.4644 - val_acc: 0.7800\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4338 - acc: 0.8115 - val_loss: 0.4580 - val_acc: 0.7800\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4290 - acc: 0.8113 - val_loss: 0.4565 - val_acc: 0.7833\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4239 - acc: 0.8177 - val_loss: 0.4615 - val_acc: 0.7767\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4160 - acc: 0.8239 - val_loss: 0.4615 - val_acc: 0.7915\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4185 - acc: 0.8192 - val_loss: 0.4868 - val_acc: 0.7685\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4048 - acc: 0.8263 - val_loss: 0.4615 - val_acc: 0.7865\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4064 - acc: 0.8269 - val_loss: 0.4521 - val_acc: 0.7800\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.4002 - acc: 0.8261 - val_loss: 0.4553 - val_acc: 0.7833\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.4093 - acc: 0.8181 - val_loss: 0.4773 - val_acc: 0.7767\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.3905 - acc: 0.8318 - val_loss: 0.4549 - val_acc: 0.7980\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3900 - acc: 0.8338 - val_loss: 0.4587 - val_acc: 0.7816\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.3875 - acc: 0.8323 - val_loss: 0.4442 - val_acc: 0.7767\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3820 - acc: 0.8351 - val_loss: 0.4427 - val_acc: 0.7800\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3847 - acc: 0.8356 - val_loss: 0.4498 - val_acc: 0.7816\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3774 - acc: 0.8362 - val_loss: 0.4422 - val_acc: 0.7816\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.3707 - acc: 0.8433 - val_loss: 0.4510 - val_acc: 0.7882\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3778 - acc: 0.8329 - val_loss: 0.4404 - val_acc: 0.7898\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3674 - acc: 0.8402 - val_loss: 0.4445 - val_acc: 0.7833\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.3618 - acc: 0.8453 - val_loss: 0.4611 - val_acc: 0.7947\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3678 - acc: 0.8458 - val_loss: 0.4377 - val_acc: 0.7849\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3587 - acc: 0.8447 - val_loss: 0.4593 - val_acc: 0.7865\n",
      "Epoch 29/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3565 - acc: 0.8453 - val_loss: 0.4531 - val_acc: 0.7783\n",
      "Epoch 30/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3504 - acc: 0.8519 - val_loss: 0.4380 - val_acc: 0.7947\n",
      "Epoch 31/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3478 - acc: 0.8526 - val_loss: 0.4442 - val_acc: 0.7947\n",
      "Epoch 32/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3412 - acc: 0.8548 - val_loss: 0.4638 - val_acc: 0.7980\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [512, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 819,841\n",
      "Trainable params: 819,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 8ms/step - loss: 0.6618 - acc: 0.6159 - val_loss: 0.5547 - val_acc: 0.7619\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.5418 - acc: 0.7422 - val_loss: 0.5032 - val_acc: 0.7701\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.4906 - acc: 0.7754 - val_loss: 0.5044 - val_acc: 0.7668\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.4768 - acc: 0.7800 - val_loss: 0.4735 - val_acc: 0.7915\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.4494 - acc: 0.8002 - val_loss: 0.4674 - val_acc: 0.7865\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.4478 - acc: 0.8030 - val_loss: 0.4612 - val_acc: 0.7783\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.4313 - acc: 0.8119 - val_loss: 0.4787 - val_acc: 0.7865\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.4256 - acc: 0.8137 - val_loss: 0.4602 - val_acc: 0.7833\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.4203 - acc: 0.8190 - val_loss: 0.4787 - val_acc: 0.7833\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.4117 - acc: 0.8241 - val_loss: 0.4482 - val_acc: 0.7800\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.4054 - acc: 0.8272 - val_loss: 0.4456 - val_acc: 0.7865\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.4040 - acc: 0.8259 - val_loss: 0.4795 - val_acc: 0.7783\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.4001 - acc: 0.8265 - val_loss: 0.4806 - val_acc: 0.7865\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3922 - acc: 0.8311 - val_loss: 0.4437 - val_acc: 0.7898\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3920 - acc: 0.8342 - val_loss: 0.4648 - val_acc: 0.7898\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3978 - acc: 0.8212 - val_loss: 0.4488 - val_acc: 0.7915\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3882 - acc: 0.8318 - val_loss: 0.4705 - val_acc: 0.7947\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3779 - acc: 0.8418 - val_loss: 0.4537 - val_acc: 0.7849\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3722 - acc: 0.8384 - val_loss: 0.4414 - val_acc: 0.7898\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3722 - acc: 0.8378 - val_loss: 0.4409 - val_acc: 0.7964\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3681 - acc: 0.8436 - val_loss: 0.4355 - val_acc: 0.7964\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3672 - acc: 0.8436 - val_loss: 0.4414 - val_acc: 0.7915\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.3706 - acc: 0.8378 - val_loss: 0.4459 - val_acc: 0.7898\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3572 - acc: 0.8458 - val_loss: 0.4377 - val_acc: 0.7898\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3549 - acc: 0.8480 - val_loss: 0.4561 - val_acc: 0.7898\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3477 - acc: 0.8539 - val_loss: 0.4387 - val_acc: 0.7947\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 852,737\n",
      "Trainable params: 852,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 8ms/step - loss: 0.6224 - acc: 0.6603 - val_loss: 0.5337 - val_acc: 0.7553\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.5239 - acc: 0.7497 - val_loss: 0.5070 - val_acc: 0.7521\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.4786 - acc: 0.7800 - val_loss: 0.4813 - val_acc: 0.7685\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.4614 - acc: 0.7933 - val_loss: 0.4766 - val_acc: 0.7882\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.4520 - acc: 0.7975 - val_loss: 0.4709 - val_acc: 0.7964\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.4422 - acc: 0.8072 - val_loss: 0.4733 - val_acc: 0.7619\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.4246 - acc: 0.8141 - val_loss: 0.4799 - val_acc: 0.7849\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.4202 - acc: 0.8152 - val_loss: 0.4593 - val_acc: 0.7931\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.4160 - acc: 0.8186 - val_loss: 0.4542 - val_acc: 0.7800\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.4096 - acc: 0.8210 - val_loss: 0.4545 - val_acc: 0.7767\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.3988 - acc: 0.8283 - val_loss: 0.4593 - val_acc: 0.7833\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.3995 - acc: 0.8245 - val_loss: 0.4487 - val_acc: 0.7882\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.3995 - acc: 0.8225 - val_loss: 0.4434 - val_acc: 0.7849\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3970 - acc: 0.8227 - val_loss: 0.4875 - val_acc: 0.7701\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.3899 - acc: 0.8280 - val_loss: 0.4447 - val_acc: 0.7898\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.3806 - acc: 0.8365 - val_loss: 0.4386 - val_acc: 0.7898\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3735 - acc: 0.8373 - val_loss: 0.4443 - val_acc: 0.7849\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.3723 - acc: 0.8407 - val_loss: 0.4479 - val_acc: 0.7833\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.3664 - acc: 0.8427 - val_loss: 0.4509 - val_acc: 0.8030\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.3635 - acc: 0.8484 - val_loss: 0.4543 - val_acc: 0.7865\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.3649 - acc: 0.8389 - val_loss: 0.4376 - val_acc: 0.7980\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3546 - acc: 0.8491 - val_loss: 0.4372 - val_acc: 0.8013\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.3519 - acc: 0.8469 - val_loss: 0.4515 - val_acc: 0.7833\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3450 - acc: 0.8482 - val_loss: 0.4468 - val_acc: 0.7915\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.3438 - acc: 0.8482 - val_loss: 0.4487 - val_acc: 0.8030\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3346 - acc: 0.8537 - val_loss: 0.4397 - val_acc: 0.7997\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3317 - acc: 0.8593 - val_loss: 0.4661 - val_acc: 0.7980\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 918,529\n",
      "Trainable params: 918,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.6330 - acc: 0.6592 - val_loss: 0.5680 - val_acc: 0.7241\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.5151 - acc: 0.7643 - val_loss: 0.5053 - val_acc: 0.7750\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 0.4727 - acc: 0.7869 - val_loss: 0.4775 - val_acc: 0.7701\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 0.4549 - acc: 0.7966 - val_loss: 0.4879 - val_acc: 0.7767\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.4405 - acc: 0.8073 - val_loss: 0.4642 - val_acc: 0.7816\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 0.4372 - acc: 0.8035 - val_loss: 0.4589 - val_acc: 0.7865\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 0.4224 - acc: 0.8134 - val_loss: 0.4871 - val_acc: 0.7734\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 0.4188 - acc: 0.8166 - val_loss: 0.4510 - val_acc: 0.7898\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 0.4063 - acc: 0.8239 - val_loss: 0.4690 - val_acc: 0.7833\n",
      "Epoch 10/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 1s 15ms/step - loss: 0.4023 - acc: 0.8223 - val_loss: 0.4498 - val_acc: 0.7849\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.3968 - acc: 0.8301 - val_loss: 0.4563 - val_acc: 0.7849\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 0.3943 - acc: 0.8290 - val_loss: 0.4495 - val_acc: 0.7800\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 0.3902 - acc: 0.8307 - val_loss: 0.4573 - val_acc: 0.7816\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 0.3857 - acc: 0.8312 - val_loss: 0.4513 - val_acc: 0.7849\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.3810 - acc: 0.8320 - val_loss: 0.4475 - val_acc: 0.7833\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 0.3778 - acc: 0.8378 - val_loss: 0.4472 - val_acc: 0.7882\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 0.3669 - acc: 0.8407 - val_loss: 0.4394 - val_acc: 0.7882\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 0.3620 - acc: 0.8436 - val_loss: 0.4554 - val_acc: 0.7947\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.3563 - acc: 0.8467 - val_loss: 0.4638 - val_acc: 0.8046\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.3528 - acc: 0.8489 - val_loss: 0.4562 - val_acc: 0.7833\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 0.3505 - acc: 0.8460 - val_loss: 0.4458 - val_acc: 0.7980\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3445 - acc: 0.8493 - val_loss: 0.4591 - val_acc: 0.7964\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,050,113\n",
      "Trainable params: 1,050,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.6077 - acc: 0.6667 - val_loss: 0.5336 - val_acc: 0.7521\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.5183 - acc: 0.7570 - val_loss: 0.4993 - val_acc: 0.7685\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.4741 - acc: 0.7836 - val_loss: 0.4865 - val_acc: 0.7750\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.4466 - acc: 0.8073 - val_loss: 0.5115 - val_acc: 0.7767\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 0.4368 - acc: 0.8081 - val_loss: 0.4864 - val_acc: 0.7734\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.4282 - acc: 0.8143 - val_loss: 0.4614 - val_acc: 0.7734\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.4164 - acc: 0.8181 - val_loss: 0.4705 - val_acc: 0.7718\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.4260 - acc: 0.8134 - val_loss: 0.4603 - val_acc: 0.7964\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.4075 - acc: 0.8259 - val_loss: 0.4574 - val_acc: 0.7783\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.4105 - acc: 0.8216 - val_loss: 0.5076 - val_acc: 0.7668\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.4050 - acc: 0.8270 - val_loss: 0.4522 - val_acc: 0.7734\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.3916 - acc: 0.8305 - val_loss: 0.4559 - val_acc: 0.7931\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.3760 - acc: 0.8345 - val_loss: 0.4493 - val_acc: 0.7865\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.3720 - acc: 0.8415 - val_loss: 0.4612 - val_acc: 0.7849\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.3810 - acc: 0.8325 - val_loss: 0.4555 - val_acc: 0.7964\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.3722 - acc: 0.8380 - val_loss: 0.4742 - val_acc: 0.7964\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.3613 - acc: 0.8462 - val_loss: 0.4479 - val_acc: 0.7833\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.3564 - acc: 0.8425 - val_loss: 0.4810 - val_acc: 0.7898\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.3521 - acc: 0.8489 - val_loss: 0.4842 - val_acc: 0.7882\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 1s 17ms/step - loss: 0.3494 - acc: 0.8453 - val_loss: 0.4547 - val_acc: 0.7964\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.3400 - acc: 0.8495 - val_loss: 0.4459 - val_acc: 0.7865\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.3323 - acc: 0.8604 - val_loss: 0.4789 - val_acc: 0.7898\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.3264 - acc: 0.8630 - val_loss: 0.4605 - val_acc: 0.7882\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.3170 - acc: 0.8650 - val_loss: 0.4917 - val_acc: 0.7964\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.3198 - acc: 0.8608 - val_loss: 0.4718 - val_acc: 0.7947\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.3117 - acc: 0.8699 - val_loss: 0.4589 - val_acc: 0.7898\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [1024, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,606,721\n",
      "Trainable params: 1,606,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.6246 - acc: 0.6570 - val_loss: 0.5326 - val_acc: 0.7570\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.5166 - acc: 0.7583 - val_loss: 0.5004 - val_acc: 0.7750\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.4717 - acc: 0.7865 - val_loss: 0.4847 - val_acc: 0.7816\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.4622 - acc: 0.7946 - val_loss: 0.4837 - val_acc: 0.7816\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.4345 - acc: 0.8092 - val_loss: 0.4727 - val_acc: 0.7767\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.4286 - acc: 0.8159 - val_loss: 0.4658 - val_acc: 0.7750\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.4208 - acc: 0.8196 - val_loss: 0.4705 - val_acc: 0.7767\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.4152 - acc: 0.8221 - val_loss: 0.4729 - val_acc: 0.7767\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.4106 - acc: 0.8179 - val_loss: 0.4643 - val_acc: 0.7865\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.3989 - acc: 0.8278 - val_loss: 0.4618 - val_acc: 0.7849\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.3995 - acc: 0.8285 - val_loss: 0.4527 - val_acc: 0.7849\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.3978 - acc: 0.8269 - val_loss: 0.4511 - val_acc: 0.7833\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.3900 - acc: 0.8280 - val_loss: 0.4800 - val_acc: 0.7898\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.3818 - acc: 0.8347 - val_loss: 0.4442 - val_acc: 0.7915\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.3952 - acc: 0.8336 - val_loss: 0.4495 - val_acc: 0.7898\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.3964 - acc: 0.8250 - val_loss: 0.4476 - val_acc: 0.7849\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.3677 - acc: 0.8420 - val_loss: 0.4465 - val_acc: 0.7931\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.3735 - acc: 0.8398 - val_loss: 0.4488 - val_acc: 0.7816\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.3621 - acc: 0.8482 - val_loss: 0.4496 - val_acc: 0.7833\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [1024, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_22 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,639,553\n",
      "Trainable params: 1,639,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.6109 - acc: 0.6692 - val_loss: 0.5486 - val_acc: 0.7258\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.5019 - acc: 0.7623 - val_loss: 0.4856 - val_acc: 0.7767\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.4758 - acc: 0.7820 - val_loss: 0.4767 - val_acc: 0.7833\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.4482 - acc: 0.7957 - val_loss: 0.4710 - val_acc: 0.7750\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.4395 - acc: 0.8064 - val_loss: 0.4558 - val_acc: 0.7816\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.4284 - acc: 0.8115 - val_loss: 0.4861 - val_acc: 0.7734\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.4193 - acc: 0.8179 - val_loss: 0.4574 - val_acc: 0.7833\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.4137 - acc: 0.8210 - val_loss: 0.4523 - val_acc: 0.7882\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.4067 - acc: 0.8248 - val_loss: 0.4888 - val_acc: 0.7652\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.4037 - acc: 0.8250 - val_loss: 0.4523 - val_acc: 0.7849\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.4015 - acc: 0.8263 - val_loss: 0.4526 - val_acc: 0.7898\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.3964 - acc: 0.8285 - val_loss: 0.4549 - val_acc: 0.7947\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.3995 - acc: 0.8290 - val_loss: 0.4626 - val_acc: 0.7980\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.3852 - acc: 0.8354 - val_loss: 0.4547 - val_acc: 0.7964\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.3819 - acc: 0.8321 - val_loss: 0.4980 - val_acc: 0.7783\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [1024, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_23 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.6453 - acc: 0.6362 - val_loss: 0.5582 - val_acc: 0.7291\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.5154 - acc: 0.7537 - val_loss: 0.5142 - val_acc: 0.7553\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.4770 - acc: 0.7816 - val_loss: 0.4820 - val_acc: 0.7619\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.4495 - acc: 0.8026 - val_loss: 0.4772 - val_acc: 0.7685\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 0.4494 - acc: 0.7964 - val_loss: 0.4747 - val_acc: 0.7734\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.4530 - acc: 0.7978 - val_loss: 0.4755 - val_acc: 0.7718\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.4258 - acc: 0.8106 - val_loss: 0.4618 - val_acc: 0.7865\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.4125 - acc: 0.8197 - val_loss: 0.4533 - val_acc: 0.7833\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.4031 - acc: 0.8269 - val_loss: 0.4506 - val_acc: 0.7734\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.4009 - acc: 0.8272 - val_loss: 0.4516 - val_acc: 0.7734\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.3930 - acc: 0.8283 - val_loss: 0.4498 - val_acc: 0.7849\n",
      "Epoch 12/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 1s 21ms/step - loss: 0.3929 - acc: 0.8289 - val_loss: 0.4453 - val_acc: 0.7783\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.3962 - acc: 0.8267 - val_loss: 0.4538 - val_acc: 0.7898\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.3802 - acc: 0.8387 - val_loss: 0.4822 - val_acc: 0.7865\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.3811 - acc: 0.8371 - val_loss: 0.4626 - val_acc: 0.8013\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.3793 - acc: 0.8371 - val_loss: 0.4444 - val_acc: 0.7849\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.3704 - acc: 0.8407 - val_loss: 0.4628 - val_acc: 0.7964\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.3662 - acc: 0.8416 - val_loss: 0.4403 - val_acc: 0.7833\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.3511 - acc: 0.8478 - val_loss: 0.4409 - val_acc: 0.7931\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.3505 - acc: 0.8504 - val_loss: 0.4415 - val_acc: 0.7849\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 1s 22ms/step - loss: 0.3453 - acc: 0.8519 - val_loss: 0.4513 - val_acc: 0.8013\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 1s 22ms/step - loss: 0.3467 - acc: 0.8513 - val_loss: 0.4520 - val_acc: 0.7816\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.3351 - acc: 0.8646 - val_loss: 0.4334 - val_acc: 0.7947\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.3470 - acc: 0.8460 - val_loss: 0.4456 - val_acc: 0.8013\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 1s 22ms/step - loss: 0.3304 - acc: 0.8602 - val_loss: 0.4403 - val_acc: 0.8046\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.3121 - acc: 0.8664 - val_loss: 0.4484 - val_acc: 0.7980\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.3200 - acc: 0.8623 - val_loss: 0.4523 - val_acc: 0.7915\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.3113 - acc: 0.8646 - val_loss: 0.4503 - val_acc: 0.7947\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [1024, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_24 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,836,545\n",
      "Trainable params: 1,836,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.6117 - acc: 0.6833 - val_loss: 0.5531 - val_acc: 0.7455\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.5012 - acc: 0.7672 - val_loss: 0.4893 - val_acc: 0.7635\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 1s 23ms/step - loss: 0.4618 - acc: 0.7884 - val_loss: 0.4797 - val_acc: 0.7750\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 1s 22ms/step - loss: 0.4507 - acc: 0.7982 - val_loss: 0.4817 - val_acc: 0.7783\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 1s 23ms/step - loss: 0.4437 - acc: 0.8019 - val_loss: 0.4665 - val_acc: 0.7816\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 1s 22ms/step - loss: 0.4236 - acc: 0.8176 - val_loss: 0.4564 - val_acc: 0.7833\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 1s 22ms/step - loss: 0.4171 - acc: 0.8203 - val_loss: 0.4564 - val_acc: 0.7833\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 1s 22ms/step - loss: 0.4072 - acc: 0.8252 - val_loss: 0.4555 - val_acc: 0.7865\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 1s 23ms/step - loss: 0.4011 - acc: 0.8272 - val_loss: 0.4682 - val_acc: 0.7800\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 1s 23ms/step - loss: 0.4031 - acc: 0.8217 - val_loss: 0.4502 - val_acc: 0.7783\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 1s 23ms/step - loss: 0.3936 - acc: 0.8309 - val_loss: 0.4555 - val_acc: 0.7865\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 1s 23ms/step - loss: 0.3959 - acc: 0.8276 - val_loss: 0.4681 - val_acc: 0.7915\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 1s 23ms/step - loss: 0.3748 - acc: 0.8378 - val_loss: 0.4539 - val_acc: 0.7964\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 1s 23ms/step - loss: 0.3770 - acc: 0.8387 - val_loss: 0.4421 - val_acc: 0.7947\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 1s 23ms/step - loss: 0.3819 - acc: 0.8332 - val_loss: 0.4396 - val_acc: 0.7947\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 1s 23ms/step - loss: 0.3670 - acc: 0.8411 - val_loss: 0.4522 - val_acc: 0.7898\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 1s 23ms/step - loss: 0.3635 - acc: 0.8420 - val_loss: 0.4495 - val_acc: 0.7997\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 1s 23ms/step - loss: 0.3638 - acc: 0.8435 - val_loss: 0.4511 - val_acc: 0.7898\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 1s 23ms/step - loss: 0.3486 - acc: 0.8489 - val_loss: 0.4436 - val_acc: 0.7964\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 1s 22ms/step - loss: 0.3410 - acc: 0.8573 - val_loss: 0.4414 - val_acc: 0.8013\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [1024, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 2,099,201\n",
      "Trainable params: 2,099,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 0.6033 - acc: 0.6720 - val_loss: 0.5132 - val_acc: 0.7422\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 0.4906 - acc: 0.7716 - val_loss: 0.4793 - val_acc: 0.7619\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 0.4530 - acc: 0.7949 - val_loss: 0.4883 - val_acc: 0.7800\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 1s 23ms/step - loss: 0.4313 - acc: 0.8141 - val_loss: 0.5039 - val_acc: 0.7734\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 1s 25ms/step - loss: 0.4265 - acc: 0.8137 - val_loss: 0.4586 - val_acc: 0.7882\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 1s 25ms/step - loss: 0.4235 - acc: 0.8112 - val_loss: 0.4565 - val_acc: 0.7849\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 1s 25ms/step - loss: 0.4246 - acc: 0.8126 - val_loss: 0.4968 - val_acc: 0.7668\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 1s 25ms/step - loss: 0.4062 - acc: 0.8212 - val_loss: 0.4605 - val_acc: 0.7849\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 1s 24ms/step - loss: 0.4000 - acc: 0.8214 - val_loss: 0.4455 - val_acc: 0.7816\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 1s 25ms/step - loss: 0.3893 - acc: 0.8292 - val_loss: 0.4553 - val_acc: 0.7915\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 1s 25ms/step - loss: 0.3976 - acc: 0.8203 - val_loss: 0.4449 - val_acc: 0.7997\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.3797 - acc: 0.8373 - val_loss: 0.4796 - val_acc: 0.7865\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.3687 - acc: 0.8365 - val_loss: 0.4443 - val_acc: 0.7964\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 1s 23ms/step - loss: 0.3661 - acc: 0.8420 - val_loss: 0.4359 - val_acc: 0.7947\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 1s 25ms/step - loss: 0.3590 - acc: 0.8433 - val_loss: 0.4521 - val_acc: 0.8013\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.3509 - acc: 0.8488 - val_loss: 0.4401 - val_acc: 0.7964\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3496 - acc: 0.8488 - val_loss: 0.4598 - val_acc: 0.7964\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.3381 - acc: 0.8526 - val_loss: 0.4471 - val_acc: 0.8030\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 0.3354 - acc: 0.8555 - val_loss: 0.4556 - val_acc: 0.7931\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [1024, 1024], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 2,624,513\n",
      "Trainable params: 2,624,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.5983 - acc: 0.6822 - val_loss: 0.5110 - val_acc: 0.7619\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 1s 18ms/step - loss: 0.4840 - acc: 0.7822 - val_loss: 0.4794 - val_acc: 0.7668\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 2s 32ms/step - loss: 0.4518 - acc: 0.7997 - val_loss: 0.4691 - val_acc: 0.7767\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 2s 32ms/step - loss: 0.4462 - acc: 0.8015 - val_loss: 0.5108 - val_acc: 0.7701\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 2s 32ms/step - loss: 0.4243 - acc: 0.8124 - val_loss: 0.4543 - val_acc: 0.7849\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 2s 32ms/step - loss: 0.4071 - acc: 0.8210 - val_loss: 0.4556 - val_acc: 0.7783\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 2s 32ms/step - loss: 0.4020 - acc: 0.8256 - val_loss: 0.4581 - val_acc: 0.7833\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 2s 32ms/step - loss: 0.3930 - acc: 0.8316 - val_loss: 0.4485 - val_acc: 0.7800\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 2s 32ms/step - loss: 0.3862 - acc: 0.8332 - val_loss: 0.4455 - val_acc: 0.7849\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 2s 30ms/step - loss: 0.4284 - acc: 0.8068 - val_loss: 0.4591 - val_acc: 0.7800\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 2s 32ms/step - loss: 0.3789 - acc: 0.8351 - val_loss: 0.4475 - val_acc: 0.7931\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 2s 32ms/step - loss: 0.3655 - acc: 0.8416 - val_loss: 0.4465 - val_acc: 0.7865\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 2s 32ms/step - loss: 0.3699 - acc: 0.8391 - val_loss: 0.4550 - val_acc: 0.7980\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 2s 32ms/step - loss: 0.3561 - acc: 0.8475 - val_loss: 0.4878 - val_acc: 0.7800\n"
     ]
    }
   ],
   "source": [
    "mlp_perf_metrics1 = make_MLP_exp(X_train_vect, y_train.values, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.844554</td>\n",
       "      <td>0.781609</td>\n",
       "      <td>0.363405</td>\n",
       "      <td>0.449858</td>\n",
       "      <td>56</td>\n",
       "      <td>0.826658</td>\n",
       "      <td>0.832168</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.890785</td>\n",
       "      <td>0.782895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.847108</td>\n",
       "      <td>0.784893</td>\n",
       "      <td>0.361574</td>\n",
       "      <td>0.438552</td>\n",
       "      <td>53</td>\n",
       "      <td>0.818779</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.869170</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.829958</td>\n",
       "      <td>0.788177</td>\n",
       "      <td>0.394843</td>\n",
       "      <td>0.464468</td>\n",
       "      <td>22</td>\n",
       "      <td>0.824688</td>\n",
       "      <td>0.811570</td>\n",
       "      <td>0.762422</td>\n",
       "      <td>0.870307</td>\n",
       "      <td>0.786229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.838716</td>\n",
       "      <td>0.794746</td>\n",
       "      <td>0.373633</td>\n",
       "      <td>0.449875</td>\n",
       "      <td>29</td>\n",
       "      <td>0.824032</td>\n",
       "      <td>0.818644</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.878271</td>\n",
       "      <td>0.782820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[512]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.796387</td>\n",
       "      <td>0.352098</td>\n",
       "      <td>0.440152</td>\n",
       "      <td>30</td>\n",
       "      <td>0.826658</td>\n",
       "      <td>0.832168</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.890785</td>\n",
       "      <td>0.782895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>[1024]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.841817</td>\n",
       "      <td>0.799672</td>\n",
       "      <td>0.362611</td>\n",
       "      <td>0.443228</td>\n",
       "      <td>21</td>\n",
       "      <td>0.820749</td>\n",
       "      <td>0.808652</td>\n",
       "      <td>0.754658</td>\n",
       "      <td>0.869170</td>\n",
       "      <td>0.780723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>[64, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.835796</td>\n",
       "      <td>0.789819</td>\n",
       "      <td>0.387205</td>\n",
       "      <td>0.449558</td>\n",
       "      <td>35</td>\n",
       "      <td>0.823375</td>\n",
       "      <td>0.829525</td>\n",
       "      <td>0.732919</td>\n",
       "      <td>0.889647</td>\n",
       "      <td>0.778236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.835249</td>\n",
       "      <td>0.783251</td>\n",
       "      <td>0.385654</td>\n",
       "      <td>0.454196</td>\n",
       "      <td>32</td>\n",
       "      <td>0.820749</td>\n",
       "      <td>0.805601</td>\n",
       "      <td>0.759317</td>\n",
       "      <td>0.865757</td>\n",
       "      <td>0.781775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.833607</td>\n",
       "      <td>0.783251</td>\n",
       "      <td>0.377478</td>\n",
       "      <td>0.448806</td>\n",
       "      <td>35</td>\n",
       "      <td>0.828628</td>\n",
       "      <td>0.851376</td>\n",
       "      <td>0.720497</td>\n",
       "      <td>0.907850</td>\n",
       "      <td>0.780488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.843642</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.365142</td>\n",
       "      <td>0.459225</td>\n",
       "      <td>30</td>\n",
       "      <td>0.827315</td>\n",
       "      <td>0.844485</td>\n",
       "      <td>0.725155</td>\n",
       "      <td>0.902162</td>\n",
       "      <td>0.780284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.834154</td>\n",
       "      <td>0.784893</td>\n",
       "      <td>0.391073</td>\n",
       "      <td>0.464519</td>\n",
       "      <td>18</td>\n",
       "      <td>0.833224</td>\n",
       "      <td>0.836207</td>\n",
       "      <td>0.753106</td>\n",
       "      <td>0.891923</td>\n",
       "      <td>0.792484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.855501</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.338728</td>\n",
       "      <td>0.453931</td>\n",
       "      <td>38</td>\n",
       "      <td>0.826001</td>\n",
       "      <td>0.804173</td>\n",
       "      <td>0.777950</td>\n",
       "      <td>0.861206</td>\n",
       "      <td>0.790845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.848385</td>\n",
       "      <td>0.798030</td>\n",
       "      <td>0.357380</td>\n",
       "      <td>0.442481</td>\n",
       "      <td>33</td>\n",
       "      <td>0.830598</td>\n",
       "      <td>0.842199</td>\n",
       "      <td>0.737578</td>\n",
       "      <td>0.898749</td>\n",
       "      <td>0.786424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.374347</td>\n",
       "      <td>0.453593</td>\n",
       "      <td>20</td>\n",
       "      <td>0.822062</td>\n",
       "      <td>0.836036</td>\n",
       "      <td>0.720497</td>\n",
       "      <td>0.896473</td>\n",
       "      <td>0.773978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.847473</td>\n",
       "      <td>0.766831</td>\n",
       "      <td>0.351490</td>\n",
       "      <td>0.511392</td>\n",
       "      <td>29</td>\n",
       "      <td>0.828628</td>\n",
       "      <td>0.848816</td>\n",
       "      <td>0.723602</td>\n",
       "      <td>0.905575</td>\n",
       "      <td>0.781224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.854771</td>\n",
       "      <td>0.798030</td>\n",
       "      <td>0.341185</td>\n",
       "      <td>0.463847</td>\n",
       "      <td>32</td>\n",
       "      <td>0.822062</td>\n",
       "      <td>0.806240</td>\n",
       "      <td>0.762422</td>\n",
       "      <td>0.865757</td>\n",
       "      <td>0.783719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 64]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.853859</td>\n",
       "      <td>0.794746</td>\n",
       "      <td>0.347676</td>\n",
       "      <td>0.438687</td>\n",
       "      <td>26</td>\n",
       "      <td>0.828628</td>\n",
       "      <td>0.841355</td>\n",
       "      <td>0.732919</td>\n",
       "      <td>0.898749</td>\n",
       "      <td>0.783402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.859332</td>\n",
       "      <td>0.798030</td>\n",
       "      <td>0.331717</td>\n",
       "      <td>0.466150</td>\n",
       "      <td>27</td>\n",
       "      <td>0.820092</td>\n",
       "      <td>0.792722</td>\n",
       "      <td>0.777950</td>\n",
       "      <td>0.850967</td>\n",
       "      <td>0.785266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.849298</td>\n",
       "      <td>0.796387</td>\n",
       "      <td>0.344496</td>\n",
       "      <td>0.459119</td>\n",
       "      <td>22</td>\n",
       "      <td>0.826658</td>\n",
       "      <td>0.829861</td>\n",
       "      <td>0.742236</td>\n",
       "      <td>0.888510</td>\n",
       "      <td>0.783607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.869914</td>\n",
       "      <td>0.789819</td>\n",
       "      <td>0.311703</td>\n",
       "      <td>0.458912</td>\n",
       "      <td>26</td>\n",
       "      <td>0.825345</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.856655</td>\n",
       "      <td>0.791209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.848203</td>\n",
       "      <td>0.783251</td>\n",
       "      <td>0.362140</td>\n",
       "      <td>0.449561</td>\n",
       "      <td>19</td>\n",
       "      <td>0.829284</td>\n",
       "      <td>0.846570</td>\n",
       "      <td>0.728261</td>\n",
       "      <td>0.903299</td>\n",
       "      <td>0.782972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 64]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.832147</td>\n",
       "      <td>0.778325</td>\n",
       "      <td>0.381911</td>\n",
       "      <td>0.498012</td>\n",
       "      <td>15</td>\n",
       "      <td>0.830598</td>\n",
       "      <td>0.862782</td>\n",
       "      <td>0.712733</td>\n",
       "      <td>0.916951</td>\n",
       "      <td>0.780612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.864623</td>\n",
       "      <td>0.794746</td>\n",
       "      <td>0.311344</td>\n",
       "      <td>0.450329</td>\n",
       "      <td>28</td>\n",
       "      <td>0.831911</td>\n",
       "      <td>0.831058</td>\n",
       "      <td>0.756211</td>\n",
       "      <td>0.887372</td>\n",
       "      <td>0.791870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.857325</td>\n",
       "      <td>0.801314</td>\n",
       "      <td>0.340982</td>\n",
       "      <td>0.441352</td>\n",
       "      <td>20</td>\n",
       "      <td>0.826001</td>\n",
       "      <td>0.805153</td>\n",
       "      <td>0.776398</td>\n",
       "      <td>0.862344</td>\n",
       "      <td>0.790514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 512]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.855501</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.335398</td>\n",
       "      <td>0.455620</td>\n",
       "      <td>19</td>\n",
       "      <td>0.827971</td>\n",
       "      <td>0.849817</td>\n",
       "      <td>0.720497</td>\n",
       "      <td>0.906712</td>\n",
       "      <td>0.779832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 1024]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.847473</td>\n",
       "      <td>0.779967</td>\n",
       "      <td>0.356083</td>\n",
       "      <td>0.487753</td>\n",
       "      <td>14</td>\n",
       "      <td>0.816809</td>\n",
       "      <td>0.784711</td>\n",
       "      <td>0.781056</td>\n",
       "      <td>0.843003</td>\n",
       "      <td>0.782879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_layers    layer_conf      lr  dropout  batch_size  accuracy_train  \\\n",
       "0          1          [32]  0.0001      0.1         100        0.844554   \n",
       "1          1          [64]  0.0001      0.1         100        0.847108   \n",
       "2          1         [128]  0.0001      0.1         100        0.829958   \n",
       "3          1         [256]  0.0001      0.1         100        0.838716   \n",
       "4          1         [512]  0.0001      0.1         100        0.851852   \n",
       "5          1        [1024]  0.0001      0.1         100        0.841817   \n",
       "6          2      [64, 32]  0.0001      0.1         100        0.835796   \n",
       "7          2      [64, 64]  0.0001      0.1         100        0.835249   \n",
       "8          2     [128, 32]  0.0001      0.1         100        0.833607   \n",
       "9          2     [128, 64]  0.0001      0.1         100        0.843642   \n",
       "10         2    [128, 128]  0.0001      0.1         100        0.834154   \n",
       "11         2     [256, 32]  0.0001      0.1         100        0.855501   \n",
       "12         2     [256, 64]  0.0001      0.1         100        0.848385   \n",
       "13         2    [256, 128]  0.0001      0.1         100        0.842000   \n",
       "14         2    [256, 256]  0.0001      0.1         100        0.847473   \n",
       "15         2     [512, 32]  0.0001      0.1         100        0.854771   \n",
       "16         2     [512, 64]  0.0001      0.1         100        0.853859   \n",
       "17         2    [512, 128]  0.0001      0.1         100        0.859332   \n",
       "18         2    [512, 256]  0.0001      0.1         100        0.849298   \n",
       "19         2    [512, 512]  0.0001      0.1         100        0.869914   \n",
       "20         2    [1024, 32]  0.0001      0.1         100        0.848203   \n",
       "21         2    [1024, 64]  0.0001      0.1         100        0.832147   \n",
       "22         2   [1024, 128]  0.0001      0.1         100        0.864623   \n",
       "23         2   [1024, 256]  0.0001      0.1         100        0.857325   \n",
       "24         2   [1024, 512]  0.0001      0.1         100        0.855501   \n",
       "25         2  [1024, 1024]  0.0001      0.1         100        0.847473   \n",
       "\n",
       "    accuracy_val  loss_train  loss_val  epochs  accuracy  precision    recall  \\\n",
       "0       0.781609    0.363405  0.449858      56  0.826658   0.832168  0.739130   \n",
       "1       0.784893    0.361574  0.438552      53  0.818779   0.807692  0.750000   \n",
       "2       0.788177    0.394843  0.464468      22  0.824688   0.811570  0.762422   \n",
       "3       0.794746    0.373633  0.449875      29  0.824032   0.818644  0.750000   \n",
       "4       0.796387    0.352098  0.440152      30  0.826658   0.832168  0.739130   \n",
       "5       0.799672    0.362611  0.443228      21  0.820749   0.808652  0.754658   \n",
       "6       0.789819    0.387205  0.449558      35  0.823375   0.829525  0.732919   \n",
       "7       0.783251    0.385654  0.454196      32  0.820749   0.805601  0.759317   \n",
       "8       0.783251    0.377478  0.448806      35  0.828628   0.851376  0.720497   \n",
       "9       0.793103    0.365142  0.459225      30  0.827315   0.844485  0.725155   \n",
       "10      0.784893    0.391073  0.464519      18  0.833224   0.836207  0.753106   \n",
       "11      0.793103    0.338728  0.453931      38  0.826001   0.804173  0.777950   \n",
       "12      0.798030    0.357380  0.442481      33  0.830598   0.842199  0.737578   \n",
       "13      0.791461    0.374347  0.453593      20  0.822062   0.836036  0.720497   \n",
       "14      0.766831    0.351490  0.511392      29  0.828628   0.848816  0.723602   \n",
       "15      0.798030    0.341185  0.463847      32  0.822062   0.806240  0.762422   \n",
       "16      0.794746    0.347676  0.438687      26  0.828628   0.841355  0.732919   \n",
       "17      0.798030    0.331717  0.466150      27  0.820092   0.792722  0.777950   \n",
       "18      0.796387    0.344496  0.459119      22  0.826658   0.829861  0.742236   \n",
       "19      0.789819    0.311703  0.458912      26  0.825345   0.800000  0.782609   \n",
       "20      0.783251    0.362140  0.449561      19  0.829284   0.846570  0.728261   \n",
       "21      0.778325    0.381911  0.498012      15  0.830598   0.862782  0.712733   \n",
       "22      0.794746    0.311344  0.450329      28  0.831911   0.831058  0.756211   \n",
       "23      0.801314    0.340982  0.441352      20  0.826001   0.805153  0.776398   \n",
       "24      0.793103    0.335398  0.455620      19  0.827971   0.849817  0.720497   \n",
       "25      0.779967    0.356083  0.487753      14  0.816809   0.784711  0.781056   \n",
       "\n",
       "    specificity  f1_score  \n",
       "0      0.890785  0.782895  \n",
       "1      0.869170  0.777778  \n",
       "2      0.870307  0.786229  \n",
       "3      0.878271  0.782820  \n",
       "4      0.890785  0.782895  \n",
       "5      0.869170  0.780723  \n",
       "6      0.889647  0.778236  \n",
       "7      0.865757  0.781775  \n",
       "8      0.907850  0.780488  \n",
       "9      0.902162  0.780284  \n",
       "10     0.891923  0.792484  \n",
       "11     0.861206  0.790845  \n",
       "12     0.898749  0.786424  \n",
       "13     0.896473  0.773978  \n",
       "14     0.905575  0.781224  \n",
       "15     0.865757  0.783719  \n",
       "16     0.898749  0.783402  \n",
       "17     0.850967  0.785266  \n",
       "18     0.888510  0.783607  \n",
       "19     0.856655  0.791209  \n",
       "20     0.903299  0.782972  \n",
       "21     0.916951  0.780612  \n",
       "22     0.887372  0.791870  \n",
       "23     0.862344  0.790514  \n",
       "24     0.906712  0.779832  \n",
       "25     0.843003  0.782879  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze better models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.834154</td>\n",
       "      <td>0.784893</td>\n",
       "      <td>0.391073</td>\n",
       "      <td>0.464519</td>\n",
       "      <td>18</td>\n",
       "      <td>0.833224</td>\n",
       "      <td>0.836207</td>\n",
       "      <td>0.753106</td>\n",
       "      <td>0.891923</td>\n",
       "      <td>0.792484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.864623</td>\n",
       "      <td>0.794746</td>\n",
       "      <td>0.311344</td>\n",
       "      <td>0.450329</td>\n",
       "      <td>28</td>\n",
       "      <td>0.831911</td>\n",
       "      <td>0.831058</td>\n",
       "      <td>0.756211</td>\n",
       "      <td>0.887372</td>\n",
       "      <td>0.791870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.848385</td>\n",
       "      <td>0.798030</td>\n",
       "      <td>0.357380</td>\n",
       "      <td>0.442481</td>\n",
       "      <td>33</td>\n",
       "      <td>0.830598</td>\n",
       "      <td>0.842199</td>\n",
       "      <td>0.737578</td>\n",
       "      <td>0.898749</td>\n",
       "      <td>0.786424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 64]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.832147</td>\n",
       "      <td>0.778325</td>\n",
       "      <td>0.381911</td>\n",
       "      <td>0.498012</td>\n",
       "      <td>15</td>\n",
       "      <td>0.830598</td>\n",
       "      <td>0.862782</td>\n",
       "      <td>0.712733</td>\n",
       "      <td>0.916951</td>\n",
       "      <td>0.780612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.848203</td>\n",
       "      <td>0.783251</td>\n",
       "      <td>0.362140</td>\n",
       "      <td>0.449561</td>\n",
       "      <td>19</td>\n",
       "      <td>0.829284</td>\n",
       "      <td>0.846570</td>\n",
       "      <td>0.728261</td>\n",
       "      <td>0.903299</td>\n",
       "      <td>0.782972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_layers   layer_conf      lr  dropout  batch_size  accuracy_train  \\\n",
       "10         2   [128, 128]  0.0001      0.1         100        0.834154   \n",
       "22         2  [1024, 128]  0.0001      0.1         100        0.864623   \n",
       "12         2    [256, 64]  0.0001      0.1         100        0.848385   \n",
       "21         2   [1024, 64]  0.0001      0.1         100        0.832147   \n",
       "20         2   [1024, 32]  0.0001      0.1         100        0.848203   \n",
       "\n",
       "    accuracy_val  loss_train  loss_val  epochs  accuracy  precision    recall  \\\n",
       "10      0.784893    0.391073  0.464519      18  0.833224   0.836207  0.753106   \n",
       "22      0.794746    0.311344  0.450329      28  0.831911   0.831058  0.756211   \n",
       "12      0.798030    0.357380  0.442481      33  0.830598   0.842199  0.737578   \n",
       "21      0.778325    0.381911  0.498012      15  0.830598   0.862782  0.712733   \n",
       "20      0.783251    0.362140  0.449561      19  0.829284   0.846570  0.728261   \n",
       "\n",
       "    specificity  f1_score  \n",
       "10     0.891923  0.792484  \n",
       "22     0.887372  0.791870  \n",
       "12     0.898749  0.786424  \n",
       "21     0.916951  0.780612  \n",
       "20     0.903299  0.782972  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics1.nlargest(5, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.834154</td>\n",
       "      <td>0.784893</td>\n",
       "      <td>0.391073</td>\n",
       "      <td>0.464519</td>\n",
       "      <td>18</td>\n",
       "      <td>0.833224</td>\n",
       "      <td>0.836207</td>\n",
       "      <td>0.753106</td>\n",
       "      <td>0.891923</td>\n",
       "      <td>0.792484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.864623</td>\n",
       "      <td>0.794746</td>\n",
       "      <td>0.311344</td>\n",
       "      <td>0.450329</td>\n",
       "      <td>28</td>\n",
       "      <td>0.831911</td>\n",
       "      <td>0.831058</td>\n",
       "      <td>0.756211</td>\n",
       "      <td>0.887372</td>\n",
       "      <td>0.791870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.869914</td>\n",
       "      <td>0.789819</td>\n",
       "      <td>0.311703</td>\n",
       "      <td>0.458912</td>\n",
       "      <td>26</td>\n",
       "      <td>0.825345</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.856655</td>\n",
       "      <td>0.791209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.855501</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.338728</td>\n",
       "      <td>0.453931</td>\n",
       "      <td>38</td>\n",
       "      <td>0.826001</td>\n",
       "      <td>0.804173</td>\n",
       "      <td>0.777950</td>\n",
       "      <td>0.861206</td>\n",
       "      <td>0.790845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.857325</td>\n",
       "      <td>0.801314</td>\n",
       "      <td>0.340982</td>\n",
       "      <td>0.441352</td>\n",
       "      <td>20</td>\n",
       "      <td>0.826001</td>\n",
       "      <td>0.805153</td>\n",
       "      <td>0.776398</td>\n",
       "      <td>0.862344</td>\n",
       "      <td>0.790514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_layers   layer_conf      lr  dropout  batch_size  accuracy_train  \\\n",
       "10         2   [128, 128]  0.0001      0.1         100        0.834154   \n",
       "22         2  [1024, 128]  0.0001      0.1         100        0.864623   \n",
       "19         2   [512, 512]  0.0001      0.1         100        0.869914   \n",
       "11         2    [256, 32]  0.0001      0.1         100        0.855501   \n",
       "23         2  [1024, 256]  0.0001      0.1         100        0.857325   \n",
       "\n",
       "    accuracy_val  loss_train  loss_val  epochs  accuracy  precision    recall  \\\n",
       "10      0.784893    0.391073  0.464519      18  0.833224   0.836207  0.753106   \n",
       "22      0.794746    0.311344  0.450329      28  0.831911   0.831058  0.756211   \n",
       "19      0.789819    0.311703  0.458912      26  0.825345   0.800000  0.782609   \n",
       "11      0.793103    0.338728  0.453931      38  0.826001   0.804173  0.777950   \n",
       "23      0.801314    0.340982  0.441352      20  0.826001   0.805153  0.776398   \n",
       "\n",
       "    specificity  f1_score  \n",
       "10     0.891923  0.792484  \n",
       "22     0.887372  0.791870  \n",
       "19     0.856655  0.791209  \n",
       "11     0.861206  0.790845  \n",
       "23     0.862344  0.790514  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics1.nlargest(5, 'f1_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gird over batch size by using the best architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_arch = [[128, 128],\n",
    "             [1024, 128],\n",
    "             [256, 64],\n",
    "             [1024, 64],\n",
    "             [1024, 32],\n",
    "             [512, 512],\n",
    "             [256, 32],\n",
    "             [1024, 256]]\n",
    "\n",
    "params2 = {\n",
    "        'lay_conf': best_arch,\n",
    "        'lr': [1e-4],\n",
    "        'dropout': [0.1],\n",
    "        'max_epochs': [300],\n",
    "        'batch_size': [None, 1, 10, 32, 64, 70, 100, 150, 200, 350,\n",
    "                       500, 700, 1000, 1100, 1250, 1500, 2000, 2500,\n",
    "                       3000, 4000, 5000],\n",
    "        'seed': [123456]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters {'batch_size': None, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "172/172 [==============================] - 3s 6ms/step - loss: 0.6200 - acc: 0.6648 - val_loss: 0.5025 - val_acc: 0.8062\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.5205 - acc: 0.7608 - val_loss: 0.4418 - val_acc: 0.8112\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4782 - acc: 0.7873 - val_loss: 0.4083 - val_acc: 0.8342\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4640 - acc: 0.7905 - val_loss: 0.4054 - val_acc: 0.8292\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4462 - acc: 0.8030 - val_loss: 0.3888 - val_acc: 0.8440\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4420 - acc: 0.8041 - val_loss: 0.3877 - val_acc: 0.8374\n",
      "Epoch 7/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4261 - acc: 0.8148 - val_loss: 0.3901 - val_acc: 0.8473\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4211 - acc: 0.8166 - val_loss: 0.4016 - val_acc: 0.8292\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4212 - acc: 0.8166 - val_loss: 0.3762 - val_acc: 0.8473\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4129 - acc: 0.8177 - val_loss: 0.3790 - val_acc: 0.8473\n",
      "Epoch 11/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4051 - acc: 0.8225 - val_loss: 0.3818 - val_acc: 0.8424\n",
      "Epoch 12/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4035 - acc: 0.8263 - val_loss: 0.3800 - val_acc: 0.8522\n",
      "Epoch 13/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3970 - acc: 0.8261 - val_loss: 0.3978 - val_acc: 0.8276\n",
      "Epoch 14/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3881 - acc: 0.8325 - val_loss: 0.3765 - val_acc: 0.8456\n",
      "Training with parameters {'batch_size': None, 'dropout': 0.1, 'lay_conf': [1024, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "172/172 [==============================] - 1s 7ms/step - loss: 0.5810 - acc: 0.6973 - val_loss: 0.4470 - val_acc: 0.8259\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4887 - acc: 0.7756 - val_loss: 0.4096 - val_acc: 0.8391\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4548 - acc: 0.7935 - val_loss: 0.4167 - val_acc: 0.8292\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4479 - acc: 0.7957 - val_loss: 0.3953 - val_acc: 0.8391\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4327 - acc: 0.8075 - val_loss: 0.3818 - val_acc: 0.8539\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4168 - acc: 0.8168 - val_loss: 0.3757 - val_acc: 0.8489\n",
      "Epoch 7/300\n",
      "172/172 [==============================] - 1s 7ms/step - loss: 0.4155 - acc: 0.8135 - val_loss: 0.3692 - val_acc: 0.8506\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4090 - acc: 0.8188 - val_loss: 0.4891 - val_acc: 0.7438\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4045 - acc: 0.8205 - val_loss: 0.3690 - val_acc: 0.8506\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3964 - acc: 0.8301 - val_loss: 0.3709 - val_acc: 0.8407\n",
      "Epoch 11/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3838 - acc: 0.8354 - val_loss: 0.3683 - val_acc: 0.8489\n",
      "Epoch 12/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3968 - acc: 0.8216 - val_loss: 0.3636 - val_acc: 0.8456\n",
      "Epoch 13/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3812 - acc: 0.8294 - val_loss: 0.4136 - val_acc: 0.8112\n",
      "Epoch 14/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3764 - acc: 0.8391 - val_loss: 0.3751 - val_acc: 0.8440\n",
      "Epoch 15/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3691 - acc: 0.8396 - val_loss: 0.3940 - val_acc: 0.8424\n",
      "Epoch 16/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3691 - acc: 0.8385 - val_loss: 0.3814 - val_acc: 0.8489\n",
      "Epoch 17/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3555 - acc: 0.8447 - val_loss: 0.3798 - val_acc: 0.8522\n",
      "Training with parameters {'batch_size': None, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,985\n",
      "Trainable params: 409,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.6335 - acc: 0.6563 - val_loss: 0.5060 - val_acc: 0.7833\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.5244 - acc: 0.7499 - val_loss: 0.5163 - val_acc: 0.7323\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4857 - acc: 0.7758 - val_loss: 0.4368 - val_acc: 0.8210\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4676 - acc: 0.7871 - val_loss: 0.4112 - val_acc: 0.8325\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4565 - acc: 0.7951 - val_loss: 0.4097 - val_acc: 0.8292\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4402 - acc: 0.8062 - val_loss: 0.3934 - val_acc: 0.8440\n",
      "Epoch 7/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4383 - acc: 0.8086 - val_loss: 0.3918 - val_acc: 0.8407\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4368 - acc: 0.8070 - val_loss: 0.3878 - val_acc: 0.8522\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4220 - acc: 0.8163 - val_loss: 0.3777 - val_acc: 0.8522\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4141 - acc: 0.8199 - val_loss: 0.3859 - val_acc: 0.8358\n",
      "Epoch 11/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4068 - acc: 0.8219 - val_loss: 0.3786 - val_acc: 0.8555\n",
      "Epoch 12/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4077 - acc: 0.8221 - val_loss: 0.3899 - val_acc: 0.8424\n",
      "Epoch 13/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4049 - acc: 0.8219 - val_loss: 0.3987 - val_acc: 0.8276\n",
      "Epoch 14/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3961 - acc: 0.8329 - val_loss: 0.4010 - val_acc: 0.8358\n",
      "Training with parameters {'batch_size': None, 'dropout': 0.1, 'lay_conf': [1024, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,639,553\n",
      "Trainable params: 1,639,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "172/172 [==============================] - 2s 7ms/step - loss: 0.5689 - acc: 0.7114 - val_loss: 0.4895 - val_acc: 0.7750\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4853 - acc: 0.7794 - val_loss: 0.4336 - val_acc: 0.8112\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4584 - acc: 0.7929 - val_loss: 0.3894 - val_acc: 0.8424\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4358 - acc: 0.8077 - val_loss: 0.3903 - val_acc: 0.8440\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4424 - acc: 0.8002 - val_loss: 0.3827 - val_acc: 0.8489\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4169 - acc: 0.8165 - val_loss: 0.4126 - val_acc: 0.8177\n",
      "Epoch 7/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4116 - acc: 0.8208 - val_loss: 0.3719 - val_acc: 0.8489\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4069 - acc: 0.8212 - val_loss: 0.3682 - val_acc: 0.8522\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4071 - acc: 0.8190 - val_loss: 0.3845 - val_acc: 0.8473\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3989 - acc: 0.8258 - val_loss: 0.3657 - val_acc: 0.8456\n",
      "Epoch 11/300\n",
      "172/172 [==============================] - 1s 7ms/step - loss: 0.3960 - acc: 0.8239 - val_loss: 0.3635 - val_acc: 0.8489\n",
      "Epoch 12/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3849 - acc: 0.8354 - val_loss: 0.3729 - val_acc: 0.8456\n",
      "Epoch 13/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3949 - acc: 0.8225 - val_loss: 0.3791 - val_acc: 0.8456\n",
      "Epoch 14/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3742 - acc: 0.8338 - val_loss: 0.3685 - val_acc: 0.8456\n",
      "Epoch 15/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3718 - acc: 0.8365 - val_loss: 0.3689 - val_acc: 0.8456\n",
      "Epoch 16/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3789 - acc: 0.8323 - val_loss: 0.3645 - val_acc: 0.8506\n",
      "Training with parameters {'batch_size': None, 'dropout': 0.1, 'lay_conf': [1024, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,606,721\n",
      "Trainable params: 1,606,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.5845 - acc: 0.7024 - val_loss: 0.4573 - val_acc: 0.7997\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4906 - acc: 0.7657 - val_loss: 0.4295 - val_acc: 0.8227\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 1s 7ms/step - loss: 0.4687 - acc: 0.7842 - val_loss: 0.3842 - val_acc: 0.8473\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4526 - acc: 0.7953 - val_loss: 0.3755 - val_acc: 0.8391\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 1s 7ms/step - loss: 0.4353 - acc: 0.8068 - val_loss: 0.3912 - val_acc: 0.8522\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4284 - acc: 0.8088 - val_loss: 0.3811 - val_acc: 0.8424\n",
      "Epoch 7/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4187 - acc: 0.8176 - val_loss: 0.3720 - val_acc: 0.8522\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4149 - acc: 0.8196 - val_loss: 0.3690 - val_acc: 0.8522\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4012 - acc: 0.8208 - val_loss: 0.3795 - val_acc: 0.8456\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4023 - acc: 0.8225 - val_loss: 0.4117 - val_acc: 0.8358\n",
      "Epoch 11/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4010 - acc: 0.8247 - val_loss: 0.3801 - val_acc: 0.8473\n",
      "Epoch 12/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3972 - acc: 0.8274 - val_loss: 0.3616 - val_acc: 0.8506\n",
      "Epoch 13/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3941 - acc: 0.8261 - val_loss: 0.3904 - val_acc: 0.8424\n",
      "Epoch 14/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 1s 7ms/step - loss: 0.3943 - acc: 0.8269 - val_loss: 0.4290 - val_acc: 0.8062\n",
      "Epoch 15/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3841 - acc: 0.8351 - val_loss: 0.3832 - val_acc: 0.8456\n",
      "Epoch 16/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3760 - acc: 0.8325 - val_loss: 0.3638 - val_acc: 0.8506\n",
      "Epoch 17/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3723 - acc: 0.8440 - val_loss: 0.3685 - val_acc: 0.8440\n",
      "Training with parameters {'batch_size': None, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,050,113\n",
      "Trainable params: 1,050,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.5806 - acc: 0.7006 - val_loss: 0.4728 - val_acc: 0.7947\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4926 - acc: 0.7694 - val_loss: 0.4262 - val_acc: 0.8227\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4671 - acc: 0.7843 - val_loss: 0.4062 - val_acc: 0.8259\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4420 - acc: 0.8002 - val_loss: 0.4066 - val_acc: 0.8276\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4257 - acc: 0.8126 - val_loss: 0.3790 - val_acc: 0.8489\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4215 - acc: 0.8108 - val_loss: 0.3747 - val_acc: 0.8539\n",
      "Epoch 7/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4099 - acc: 0.8172 - val_loss: 0.3796 - val_acc: 0.8473\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3979 - acc: 0.8245 - val_loss: 0.3771 - val_acc: 0.8424\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3999 - acc: 0.8225 - val_loss: 0.4043 - val_acc: 0.8342\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3973 - acc: 0.8250 - val_loss: 0.3721 - val_acc: 0.8506\n",
      "Epoch 11/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3852 - acc: 0.8321 - val_loss: 0.4166 - val_acc: 0.8276\n",
      "Epoch 12/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3847 - acc: 0.8351 - val_loss: 0.3780 - val_acc: 0.8506\n",
      "Epoch 13/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3801 - acc: 0.8342 - val_loss: 0.4022 - val_acc: 0.8456\n",
      "Epoch 14/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3698 - acc: 0.8413 - val_loss: 0.4064 - val_acc: 0.8358\n",
      "Epoch 15/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3623 - acc: 0.8422 - val_loss: 0.3734 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': None, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 401,729\n",
      "Trainable params: 401,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.6212 - acc: 0.6574 - val_loss: 0.4993 - val_acc: 0.8013\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.5228 - acc: 0.7504 - val_loss: 0.4492 - val_acc: 0.8128\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4808 - acc: 0.7794 - val_loss: 0.4129 - val_acc: 0.8325\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4652 - acc: 0.7873 - val_loss: 0.4105 - val_acc: 0.8358\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4488 - acc: 0.8031 - val_loss: 0.3915 - val_acc: 0.8391\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4352 - acc: 0.8097 - val_loss: 0.3871 - val_acc: 0.8374\n",
      "Epoch 7/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4326 - acc: 0.8104 - val_loss: 0.3881 - val_acc: 0.8440\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4279 - acc: 0.8108 - val_loss: 0.4191 - val_acc: 0.8177\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4184 - acc: 0.8183 - val_loss: 0.3757 - val_acc: 0.8489\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4094 - acc: 0.8243 - val_loss: 0.3907 - val_acc: 0.8358\n",
      "Epoch 11/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4054 - acc: 0.8232 - val_loss: 0.3714 - val_acc: 0.8473\n",
      "Epoch 12/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3990 - acc: 0.8263 - val_loss: 0.3882 - val_acc: 0.8473\n",
      "Epoch 13/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3972 - acc: 0.8280 - val_loss: 0.3772 - val_acc: 0.8440\n",
      "Epoch 14/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3965 - acc: 0.8303 - val_loss: 0.3936 - val_acc: 0.8358\n",
      "Epoch 15/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3879 - acc: 0.8318 - val_loss: 0.3757 - val_acc: 0.8407\n",
      "Epoch 16/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3825 - acc: 0.8309 - val_loss: 0.3696 - val_acc: 0.8522\n",
      "Epoch 17/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3787 - acc: 0.8396 - val_loss: 0.3726 - val_acc: 0.8489\n",
      "Epoch 18/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3801 - acc: 0.8409 - val_loss: 0.3788 - val_acc: 0.8440\n",
      "Epoch 19/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3719 - acc: 0.8387 - val_loss: 0.3712 - val_acc: 0.8424\n",
      "Epoch 20/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3670 - acc: 0.8391 - val_loss: 0.3711 - val_acc: 0.8489\n",
      "Epoch 21/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3662 - acc: 0.8396 - val_loss: 0.3732 - val_acc: 0.8473\n",
      "Training with parameters {'batch_size': None, 'dropout': 0.1, 'lay_conf': [1024, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,836,545\n",
      "Trainable params: 1,836,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.5754 - acc: 0.7052 - val_loss: 0.4351 - val_acc: 0.8276\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4874 - acc: 0.7708 - val_loss: 0.3967 - val_acc: 0.8440\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4549 - acc: 0.7982 - val_loss: 0.4206 - val_acc: 0.8276\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4504 - acc: 0.7964 - val_loss: 0.3976 - val_acc: 0.8424\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4236 - acc: 0.8108 - val_loss: 0.3790 - val_acc: 0.8522\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4161 - acc: 0.8126 - val_loss: 0.3901 - val_acc: 0.8456\n",
      "Epoch 7/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4088 - acc: 0.8214 - val_loss: 0.3791 - val_acc: 0.8473\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4107 - acc: 0.8216 - val_loss: 0.3872 - val_acc: 0.8506\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4000 - acc: 0.8248 - val_loss: 0.3868 - val_acc: 0.8506\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3940 - acc: 0.8294 - val_loss: 0.4439 - val_acc: 0.7931\n",
      "Training with parameters {'batch_size': 1, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.5700 - acc: 0.7075 - val_loss: 0.4256 - val_acc: 0.8325\n",
      "Epoch 2/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.4827 - acc: 0.7845 - val_loss: 0.4271 - val_acc: 0.8128\n",
      "Epoch 3/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.4593 - acc: 0.7913 - val_loss: 0.3761 - val_acc: 0.8473\n",
      "Epoch 4/300\n",
      "5481/5481 [==============================] - 23s 4ms/step - loss: 0.4415 - acc: 0.7975 - val_loss: 0.3773 - val_acc: 0.8489\n",
      "Epoch 5/300\n",
      "5481/5481 [==============================] - 24s 4ms/step - loss: 0.4309 - acc: 0.8068 - val_loss: 0.3660 - val_acc: 0.8489\n",
      "Epoch 6/300\n",
      "5481/5481 [==============================] - 23s 4ms/step - loss: 0.4212 - acc: 0.8152 - val_loss: 0.3721 - val_acc: 0.8473\n",
      "Epoch 7/300\n",
      "5481/5481 [==============================] - 23s 4ms/step - loss: 0.4155 - acc: 0.8159 - val_loss: 0.4135 - val_acc: 0.8243\n",
      "Epoch 8/300\n",
      "5481/5481 [==============================] - 23s 4ms/step - loss: 0.4078 - acc: 0.8197 - val_loss: 0.3744 - val_acc: 0.8506\n",
      "Epoch 9/300\n",
      "5481/5481 [==============================] - 23s 4ms/step - loss: 0.4078 - acc: 0.8248 - val_loss: 0.3736 - val_acc: 0.8473\n",
      "Epoch 10/300\n",
      "5481/5481 [==============================] - 23s 4ms/step - loss: 0.4012 - acc: 0.8252 - val_loss: 0.4121 - val_acc: 0.8342\n",
      "Training with parameters {'batch_size': 1, 'dropout': 0.1, 'lay_conf': [1024, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5481/5481 [==============================] - 27s 5ms/step - loss: 0.5784 - acc: 0.7137 - val_loss: 0.4210 - val_acc: 0.8358\n",
      "Epoch 2/300\n",
      "5481/5481 [==============================] - 26s 5ms/step - loss: 0.4897 - acc: 0.7763 - val_loss: 0.5556 - val_acc: 0.6897\n",
      "Epoch 3/300\n",
      "5481/5481 [==============================] - 26s 5ms/step - loss: 0.4571 - acc: 0.7929 - val_loss: 0.4156 - val_acc: 0.8227\n",
      "Epoch 4/300\n",
      "5481/5481 [==============================] - 24s 4ms/step - loss: 0.4457 - acc: 0.8035 - val_loss: 0.3749 - val_acc: 0.8489\n",
      "Epoch 5/300\n",
      "5481/5481 [==============================] - 25s 5ms/step - loss: 0.4357 - acc: 0.8070 - val_loss: 0.3678 - val_acc: 0.8506\n",
      "Epoch 6/300\n",
      "5481/5481 [==============================] - 26s 5ms/step - loss: 0.4242 - acc: 0.8104 - val_loss: 0.5123 - val_acc: 0.7504\n",
      "Epoch 7/300\n",
      "5481/5481 [==============================] - 25s 5ms/step - loss: 0.4189 - acc: 0.8183 - val_loss: 0.3623 - val_acc: 0.8539\n",
      "Epoch 8/300\n",
      "5481/5481 [==============================] - 25s 5ms/step - loss: 0.4134 - acc: 0.8159 - val_loss: 0.4358 - val_acc: 0.8095\n",
      "Epoch 9/300\n",
      "5481/5481 [==============================] - 25s 5ms/step - loss: 0.4096 - acc: 0.8174 - val_loss: 0.3722 - val_acc: 0.8489\n",
      "Epoch 10/300\n",
      "5481/5481 [==============================] - 26s 5ms/step - loss: 0.4044 - acc: 0.8186 - val_loss: 0.3738 - val_acc: 0.8473\n",
      "Epoch 11/300\n",
      "5481/5481 [==============================] - 25s 5ms/step - loss: 0.3975 - acc: 0.8254 - val_loss: 0.3709 - val_acc: 0.8489\n",
      "Epoch 12/300\n",
      "5481/5481 [==============================] - 26s 5ms/step - loss: 0.3928 - acc: 0.8238 - val_loss: 0.4301 - val_acc: 0.8391\n",
      "Training with parameters {'batch_size': 1, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,985\n",
      "Trainable params: 409,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.5765 - acc: 0.7039 - val_loss: 0.4461 - val_acc: 0.8112\n",
      "Epoch 2/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.4897 - acc: 0.7796 - val_loss: 0.3818 - val_acc: 0.8424\n",
      "Epoch 3/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.4683 - acc: 0.7887 - val_loss: 0.4897 - val_acc: 0.7701\n",
      "Epoch 4/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.4447 - acc: 0.8020 - val_loss: 0.3897 - val_acc: 0.8374\n",
      "Epoch 5/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.4412 - acc: 0.8020 - val_loss: 0.3778 - val_acc: 0.8522\n",
      "Epoch 6/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.4300 - acc: 0.8103 - val_loss: 0.3831 - val_acc: 0.8407\n",
      "Epoch 7/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.4237 - acc: 0.8170 - val_loss: 0.4053 - val_acc: 0.8522\n",
      "Epoch 8/300\n",
      "5481/5481 [==============================] - 23s 4ms/step - loss: 0.4178 - acc: 0.8155 - val_loss: 0.4264 - val_acc: 0.8227\n",
      "Epoch 9/300\n",
      "5481/5481 [==============================] - 23s 4ms/step - loss: 0.4167 - acc: 0.8152 - val_loss: 0.3823 - val_acc: 0.8588\n",
      "Epoch 10/300\n",
      "5481/5481 [==============================] - 23s 4ms/step - loss: 0.4020 - acc: 0.8267 - val_loss: 0.3729 - val_acc: 0.8424\n",
      "Epoch 11/300\n",
      "5481/5481 [==============================] - 23s 4ms/step - loss: 0.3999 - acc: 0.8263 - val_loss: 0.3721 - val_acc: 0.8473\n",
      "Epoch 12/300\n",
      "5481/5481 [==============================] - 23s 4ms/step - loss: 0.3957 - acc: 0.8278 - val_loss: 0.3793 - val_acc: 0.8539\n",
      "Epoch 13/300\n",
      "5481/5481 [==============================] - 23s 4ms/step - loss: 0.3945 - acc: 0.8239 - val_loss: 0.3671 - val_acc: 0.8539\n",
      "Epoch 14/300\n",
      "5481/5481 [==============================] - 23s 4ms/step - loss: 0.3876 - acc: 0.8301 - val_loss: 0.3845 - val_acc: 0.8342\n",
      "Epoch 15/300\n",
      "5481/5481 [==============================] - 23s 4ms/step - loss: 0.3841 - acc: 0.8327 - val_loss: 0.3763 - val_acc: 0.8456\n",
      "Epoch 16/300\n",
      "5481/5481 [==============================] - 23s 4ms/step - loss: 0.3745 - acc: 0.8321 - val_loss: 0.3706 - val_acc: 0.8506\n",
      "Epoch 17/300\n",
      "5481/5481 [==============================] - 23s 4ms/step - loss: 0.3723 - acc: 0.8376 - val_loss: 0.3664 - val_acc: 0.8522\n",
      "Epoch 18/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.3700 - acc: 0.8351 - val_loss: 0.3670 - val_acc: 0.8539\n",
      "Epoch 19/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.3643 - acc: 0.8400 - val_loss: 0.3772 - val_acc: 0.8473\n",
      "Epoch 20/300\n",
      "5481/5481 [==============================] - 23s 4ms/step - loss: 0.3556 - acc: 0.8424 - val_loss: 0.4044 - val_acc: 0.8374\n",
      "Epoch 21/300\n",
      "5481/5481 [==============================] - 23s 4ms/step - loss: 0.3621 - acc: 0.8398 - val_loss: 0.4153 - val_acc: 0.8506\n",
      "Epoch 22/300\n",
      "5481/5481 [==============================] - 23s 4ms/step - loss: 0.3543 - acc: 0.8447 - val_loss: 0.4085 - val_acc: 0.8506\n",
      "Training with parameters {'batch_size': 1, 'dropout': 0.1, 'lay_conf': [1024, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,639,553\n",
      "Trainable params: 1,639,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5481/5481 [==============================] - 26s 5ms/step - loss: 0.5715 - acc: 0.7072 - val_loss: 0.4236 - val_acc: 0.8259\n",
      "Epoch 2/300\n",
      "5481/5481 [==============================] - 25s 5ms/step - loss: 0.4813 - acc: 0.7739 - val_loss: 0.4145 - val_acc: 0.8194\n",
      "Epoch 3/300\n",
      "5481/5481 [==============================] - 23s 4ms/step - loss: 0.4595 - acc: 0.7927 - val_loss: 0.3790 - val_acc: 0.8440\n",
      "Epoch 4/300\n",
      "5481/5481 [==============================] - 21s 4ms/step - loss: 0.4456 - acc: 0.8050 - val_loss: 0.3830 - val_acc: 0.8407\n",
      "Epoch 5/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.4329 - acc: 0.8050 - val_loss: 0.4150 - val_acc: 0.8276\n",
      "Epoch 6/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.4266 - acc: 0.8104 - val_loss: 0.3958 - val_acc: 0.8309\n",
      "Epoch 7/300\n",
      "5481/5481 [==============================] - 23s 4ms/step - loss: 0.4172 - acc: 0.8150 - val_loss: 0.4211 - val_acc: 0.8276\n",
      "Epoch 8/300\n",
      "5481/5481 [==============================] - 25s 5ms/step - loss: 0.4116 - acc: 0.8181 - val_loss: 0.4515 - val_acc: 0.7898\n",
      "Training with parameters {'batch_size': 1, 'dropout': 0.1, 'lay_conf': [1024, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,606,721\n",
      "Trainable params: 1,606,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5481/5481 [==============================] - 25s 4ms/step - loss: 0.5896 - acc: 0.6977 - val_loss: 0.4512 - val_acc: 0.8243\n",
      "Epoch 2/300\n",
      "5481/5481 [==============================] - 25s 5ms/step - loss: 0.4938 - acc: 0.7772 - val_loss: 0.4076 - val_acc: 0.8161\n",
      "Epoch 3/300\n",
      "5481/5481 [==============================] - 25s 5ms/step - loss: 0.4614 - acc: 0.7926 - val_loss: 0.3750 - val_acc: 0.8555\n",
      "Epoch 4/300\n",
      "5481/5481 [==============================] - 25s 5ms/step - loss: 0.4488 - acc: 0.7978 - val_loss: 0.3948 - val_acc: 0.8440\n",
      "Epoch 5/300\n",
      "5481/5481 [==============================] - 25s 5ms/step - loss: 0.4411 - acc: 0.8017 - val_loss: 0.4174 - val_acc: 0.8259\n",
      "Epoch 6/300\n",
      "5481/5481 [==============================] - 25s 5ms/step - loss: 0.4321 - acc: 0.8092 - val_loss: 0.3820 - val_acc: 0.8489\n",
      "Epoch 7/300\n",
      "5481/5481 [==============================] - 24s 4ms/step - loss: 0.4237 - acc: 0.8124 - val_loss: 0.3762 - val_acc: 0.8489\n",
      "Epoch 8/300\n",
      "5481/5481 [==============================] - 25s 5ms/step - loss: 0.4160 - acc: 0.8194 - val_loss: 0.3764 - val_acc: 0.8456\n",
      "Training with parameters {'batch_size': 1, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,050,113\n",
      "Trainable params: 1,050,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.5582 - acc: 0.7185 - val_loss: 0.4737 - val_acc: 0.7816\n",
      "Epoch 2/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.4796 - acc: 0.7854 - val_loss: 0.4663 - val_acc: 0.7734\n",
      "Epoch 3/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.4559 - acc: 0.7960 - val_loss: 0.4018 - val_acc: 0.8342\n",
      "Epoch 4/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.4412 - acc: 0.8024 - val_loss: 0.3914 - val_acc: 0.8440\n",
      "Epoch 5/300\n",
      "5481/5481 [==============================] - 21s 4ms/step - loss: 0.4301 - acc: 0.8112 - val_loss: 0.4035 - val_acc: 0.8309\n",
      "Epoch 6/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.4267 - acc: 0.8139 - val_loss: 0.5049 - val_acc: 0.7750\n",
      "Epoch 7/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.4154 - acc: 0.8216 - val_loss: 0.3934 - val_acc: 0.8424\n",
      "Epoch 8/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.4107 - acc: 0.8192 - val_loss: 0.4061 - val_acc: 0.8342\n",
      "Epoch 9/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.4076 - acc: 0.8228 - val_loss: 0.3914 - val_acc: 0.8440\n",
      "Epoch 10/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.4001 - acc: 0.8247 - val_loss: 0.3715 - val_acc: 0.8522\n",
      "Epoch 11/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.3948 - acc: 0.8301 - val_loss: 0.3920 - val_acc: 0.8440\n",
      "Epoch 12/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.3882 - acc: 0.8320 - val_loss: 0.3915 - val_acc: 0.8391\n",
      "Epoch 13/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.3827 - acc: 0.8336 - val_loss: 0.3703 - val_acc: 0.8555\n",
      "Epoch 14/300\n",
      "5481/5481 [==============================] - 21s 4ms/step - loss: 0.3813 - acc: 0.8331 - val_loss: 0.3843 - val_acc: 0.8456\n",
      "Epoch 15/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.3718 - acc: 0.8373 - val_loss: 0.4089 - val_acc: 0.8391\n",
      "Epoch 16/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.3711 - acc: 0.8371 - val_loss: 0.3882 - val_acc: 0.8424\n",
      "Epoch 17/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.3620 - acc: 0.8420 - val_loss: 0.4308 - val_acc: 0.8177\n",
      "Epoch 18/300\n",
      "5481/5481 [==============================] - 22s 4ms/step - loss: 0.3585 - acc: 0.8447 - val_loss: 0.3782 - val_acc: 0.8456\n",
      "Training with parameters {'batch_size': 1, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 401,729\n",
      "Trainable params: 401,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5481/5481 [==============================] - 21s 4ms/step - loss: 0.5627 - acc: 0.7139 - val_loss: 0.4323 - val_acc: 0.8144\n",
      "Epoch 2/300\n",
      "5481/5481 [==============================] - 21s 4ms/step - loss: 0.4841 - acc: 0.7783 - val_loss: 0.4137 - val_acc: 0.8259\n",
      "Epoch 3/300\n",
      "5481/5481 [==============================] - 21s 4ms/step - loss: 0.4621 - acc: 0.7951 - val_loss: 0.3979 - val_acc: 0.8440\n",
      "Epoch 4/300\n",
      "5481/5481 [==============================] - 21s 4ms/step - loss: 0.4459 - acc: 0.7997 - val_loss: 0.4187 - val_acc: 0.8144\n",
      "Epoch 5/300\n",
      "5481/5481 [==============================] - 21s 4ms/step - loss: 0.4317 - acc: 0.8106 - val_loss: 0.3883 - val_acc: 0.8440\n",
      "Epoch 6/300\n",
      "5481/5481 [==============================] - 21s 4ms/step - loss: 0.4259 - acc: 0.8101 - val_loss: 0.3687 - val_acc: 0.8473\n",
      "Epoch 7/300\n",
      "5481/5481 [==============================] - 21s 4ms/step - loss: 0.4136 - acc: 0.8219 - val_loss: 0.3684 - val_acc: 0.8489\n",
      "Epoch 8/300\n",
      "5481/5481 [==============================] - 21s 4ms/step - loss: 0.4131 - acc: 0.8165 - val_loss: 0.3735 - val_acc: 0.8506\n",
      "Epoch 9/300\n",
      "5481/5481 [==============================] - 21s 4ms/step - loss: 0.4088 - acc: 0.8234 - val_loss: 0.3649 - val_acc: 0.8424\n",
      "Epoch 10/300\n",
      "5481/5481 [==============================] - 21s 4ms/step - loss: 0.4044 - acc: 0.8241 - val_loss: 0.3915 - val_acc: 0.8325\n",
      "Epoch 11/300\n",
      "5481/5481 [==============================] - 21s 4ms/step - loss: 0.4022 - acc: 0.8283 - val_loss: 0.3733 - val_acc: 0.8456\n",
      "Epoch 12/300\n",
      "5481/5481 [==============================] - 21s 4ms/step - loss: 0.3968 - acc: 0.8287 - val_loss: 0.3607 - val_acc: 0.8473\n",
      "Epoch 13/300\n",
      "5481/5481 [==============================] - 21s 4ms/step - loss: 0.3901 - acc: 0.8294 - val_loss: 0.3659 - val_acc: 0.8473\n",
      "Epoch 14/300\n",
      "5481/5481 [==============================] - 21s 4ms/step - loss: 0.3903 - acc: 0.8329 - val_loss: 0.3598 - val_acc: 0.8456\n",
      "Epoch 15/300\n",
      "5481/5481 [==============================] - 21s 4ms/step - loss: 0.3855 - acc: 0.8305 - val_loss: 0.3862 - val_acc: 0.8489\n",
      "Epoch 16/300\n",
      "5481/5481 [==============================] - 21s 4ms/step - loss: 0.3749 - acc: 0.8352 - val_loss: 0.3740 - val_acc: 0.8489\n",
      "Epoch 17/300\n",
      "5481/5481 [==============================] - 21s 4ms/step - loss: 0.3780 - acc: 0.8345 - val_loss: 0.3746 - val_acc: 0.8522\n",
      "Epoch 18/300\n",
      "5481/5481 [==============================] - 21s 4ms/step - loss: 0.3749 - acc: 0.8345 - val_loss: 0.3821 - val_acc: 0.8506\n",
      "Epoch 19/300\n",
      "5481/5481 [==============================] - 21s 4ms/step - loss: 0.3681 - acc: 0.8349 - val_loss: 0.3814 - val_acc: 0.8555\n",
      "Training with parameters {'batch_size': 1, 'dropout': 0.1, 'lay_conf': [1024, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,836,545\n",
      "Trainable params: 1,836,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5481/5481 [==============================] - 25s 4ms/step - loss: 0.5631 - acc: 0.7207 - val_loss: 0.4511 - val_acc: 0.8210\n",
      "Epoch 2/300\n",
      "5481/5481 [==============================] - 25s 4ms/step - loss: 0.4777 - acc: 0.7778 - val_loss: 0.3812 - val_acc: 0.8555\n",
      "Epoch 3/300\n",
      "5481/5481 [==============================] - 25s 4ms/step - loss: 0.4544 - acc: 0.7984 - val_loss: 0.3760 - val_acc: 0.8539\n",
      "Epoch 4/300\n",
      "5481/5481 [==============================] - 25s 4ms/step - loss: 0.4399 - acc: 0.8026 - val_loss: 0.3697 - val_acc: 0.8506\n",
      "Epoch 5/300\n",
      "5481/5481 [==============================] - 24s 4ms/step - loss: 0.4325 - acc: 0.8072 - val_loss: 0.4073 - val_acc: 0.8309\n",
      "Epoch 6/300\n",
      "5481/5481 [==============================] - 24s 4ms/step - loss: 0.4256 - acc: 0.8092 - val_loss: 0.4461 - val_acc: 0.8128\n",
      "Epoch 7/300\n",
      "5481/5481 [==============================] - 24s 4ms/step - loss: 0.4189 - acc: 0.8168 - val_loss: 0.3753 - val_acc: 0.8489\n",
      "Epoch 8/300\n",
      "5481/5481 [==============================] - 24s 4ms/step - loss: 0.4140 - acc: 0.8152 - val_loss: 0.3759 - val_acc: 0.8456\n",
      "Epoch 9/300\n",
      "5481/5481 [==============================] - 24s 4ms/step - loss: 0.4027 - acc: 0.8210 - val_loss: 0.3946 - val_acc: 0.8424\n",
      "Training with parameters {'batch_size': 10, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.5882 - acc: 0.6962 - val_loss: 0.4711 - val_acc: 0.7865\n",
      "Epoch 2/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4924 - acc: 0.7760 - val_loss: 0.4391 - val_acc: 0.8259\n",
      "Epoch 3/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4661 - acc: 0.7937 - val_loss: 0.3968 - val_acc: 0.8325\n",
      "Epoch 4/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4471 - acc: 0.8006 - val_loss: 0.3932 - val_acc: 0.8342\n",
      "Epoch 5/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4394 - acc: 0.8041 - val_loss: 0.3818 - val_acc: 0.8456\n",
      "Epoch 6/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4242 - acc: 0.8117 - val_loss: 0.3896 - val_acc: 0.8374\n",
      "Epoch 7/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4211 - acc: 0.8172 - val_loss: 0.4604 - val_acc: 0.7980\n",
      "Epoch 8/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4159 - acc: 0.8192 - val_loss: 0.3778 - val_acc: 0.8440\n",
      "Epoch 9/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4079 - acc: 0.8208 - val_loss: 0.3877 - val_acc: 0.8358\n",
      "Epoch 10/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4018 - acc: 0.8239 - val_loss: 0.3755 - val_acc: 0.8456\n",
      "Epoch 11/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3962 - acc: 0.8276 - val_loss: 0.3858 - val_acc: 0.8424\n",
      "Epoch 12/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3893 - acc: 0.8296 - val_loss: 0.3805 - val_acc: 0.8539\n",
      "Epoch 13/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3848 - acc: 0.8352 - val_loss: 0.3737 - val_acc: 0.8473\n",
      "Epoch 14/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3847 - acc: 0.8303 - val_loss: 0.4066 - val_acc: 0.8358\n",
      "Epoch 15/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3809 - acc: 0.8349 - val_loss: 0.3794 - val_acc: 0.8407\n",
      "Epoch 16/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3779 - acc: 0.8373 - val_loss: 0.3773 - val_acc: 0.8424\n",
      "Epoch 17/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3732 - acc: 0.8393 - val_loss: 0.4027 - val_acc: 0.8309\n",
      "Epoch 18/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3656 - acc: 0.8424 - val_loss: 0.3730 - val_acc: 0.8424\n",
      "Epoch 19/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3664 - acc: 0.8429 - val_loss: 0.3766 - val_acc: 0.8424\n",
      "Epoch 20/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3614 - acc: 0.8404 - val_loss: 0.3972 - val_acc: 0.8325\n",
      "Epoch 21/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3593 - acc: 0.8469 - val_loss: 0.3931 - val_acc: 0.8374\n",
      "Epoch 22/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3485 - acc: 0.8502 - val_loss: 0.3783 - val_acc: 0.8407\n",
      "Epoch 23/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3432 - acc: 0.8515 - val_loss: 0.3876 - val_acc: 0.8407\n",
      "Training with parameters {'batch_size': 10, 'dropout': 0.1, 'lay_conf': [1024, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.5575 - acc: 0.7145 - val_loss: 0.4211 - val_acc: 0.8358\n",
      "Epoch 2/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4827 - acc: 0.7738 - val_loss: 0.4458 - val_acc: 0.8112\n",
      "Epoch 3/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4625 - acc: 0.7891 - val_loss: 0.3965 - val_acc: 0.8391\n",
      "Epoch 4/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4403 - acc: 0.8008 - val_loss: 0.4660 - val_acc: 0.7980\n",
      "Epoch 5/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4329 - acc: 0.8059 - val_loss: 0.4755 - val_acc: 0.7668\n",
      "Epoch 6/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4165 - acc: 0.8144 - val_loss: 0.3849 - val_acc: 0.8440\n",
      "Epoch 7/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4143 - acc: 0.8186 - val_loss: 0.4139 - val_acc: 0.8259\n",
      "Epoch 8/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4093 - acc: 0.8234 - val_loss: 0.3819 - val_acc: 0.8555\n",
      "Epoch 9/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4032 - acc: 0.8223 - val_loss: 0.3916 - val_acc: 0.8473\n",
      "Epoch 10/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4019 - acc: 0.8201 - val_loss: 0.3730 - val_acc: 0.8555\n",
      "Epoch 11/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3936 - acc: 0.8292 - val_loss: 0.3635 - val_acc: 0.8522\n",
      "Epoch 12/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3831 - acc: 0.8274 - val_loss: 0.3698 - val_acc: 0.8473\n",
      "Epoch 13/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3868 - acc: 0.8329 - val_loss: 0.3826 - val_acc: 0.8473\n",
      "Epoch 14/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3750 - acc: 0.8356 - val_loss: 0.3623 - val_acc: 0.8456\n",
      "Epoch 15/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3749 - acc: 0.8374 - val_loss: 0.3679 - val_acc: 0.8440\n",
      "Epoch 16/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3727 - acc: 0.8374 - val_loss: 0.3780 - val_acc: 0.8489\n",
      "Epoch 17/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3677 - acc: 0.8373 - val_loss: 0.3877 - val_acc: 0.8473\n",
      "Epoch 18/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3564 - acc: 0.8486 - val_loss: 0.3698 - val_acc: 0.8440\n",
      "Epoch 19/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3598 - acc: 0.8384 - val_loss: 0.3636 - val_acc: 0.8506\n",
      "Training with parameters {'batch_size': 10, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,985\n",
      "Trainable params: 409,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.5909 - acc: 0.6862 - val_loss: 0.4973 - val_acc: 0.7734\n",
      "Epoch 2/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.5013 - acc: 0.7676 - val_loss: 0.4396 - val_acc: 0.8194\n",
      "Epoch 3/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4745 - acc: 0.7823 - val_loss: 0.4890 - val_acc: 0.7570\n",
      "Epoch 4/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4509 - acc: 0.8022 - val_loss: 0.4530 - val_acc: 0.7947\n",
      "Epoch 5/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4396 - acc: 0.8046 - val_loss: 0.3824 - val_acc: 0.8473\n",
      "Epoch 6/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4314 - acc: 0.8099 - val_loss: 0.3798 - val_acc: 0.8506\n",
      "Epoch 7/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4193 - acc: 0.8168 - val_loss: 0.3988 - val_acc: 0.8358\n",
      "Epoch 8/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4181 - acc: 0.8163 - val_loss: 0.3746 - val_acc: 0.8473\n",
      "Epoch 9/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4124 - acc: 0.8170 - val_loss: 0.3759 - val_acc: 0.8489\n",
      "Epoch 10/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4025 - acc: 0.8238 - val_loss: 0.3759 - val_acc: 0.8374\n",
      "Epoch 11/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3982 - acc: 0.8281 - val_loss: 0.3875 - val_acc: 0.8407\n",
      "Epoch 12/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4014 - acc: 0.8227 - val_loss: 0.3708 - val_acc: 0.8522\n",
      "Epoch 13/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3911 - acc: 0.8263 - val_loss: 0.3888 - val_acc: 0.8440\n",
      "Epoch 14/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3892 - acc: 0.8314 - val_loss: 0.3897 - val_acc: 0.8407\n",
      "Epoch 15/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3801 - acc: 0.8382 - val_loss: 0.3836 - val_acc: 0.8407\n",
      "Epoch 16/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3803 - acc: 0.8327 - val_loss: 0.3746 - val_acc: 0.8391\n",
      "Epoch 17/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3667 - acc: 0.8460 - val_loss: 0.3875 - val_acc: 0.8440\n",
      "Training with parameters {'batch_size': 10, 'dropout': 0.1, 'lay_conf': [1024, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,639,553\n",
      "Trainable params: 1,639,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.5602 - acc: 0.7150 - val_loss: 0.4142 - val_acc: 0.8259\n",
      "Epoch 2/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4861 - acc: 0.7752 - val_loss: 0.4002 - val_acc: 0.8440\n",
      "Epoch 3/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4593 - acc: 0.7907 - val_loss: 0.4576 - val_acc: 0.8030\n",
      "Epoch 4/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4448 - acc: 0.8050 - val_loss: 0.4025 - val_acc: 0.8325\n",
      "Epoch 5/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4341 - acc: 0.8022 - val_loss: 0.3741 - val_acc: 0.8539\n",
      "Epoch 6/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4281 - acc: 0.8035 - val_loss: 0.4329 - val_acc: 0.8079\n",
      "Epoch 7/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4138 - acc: 0.8172 - val_loss: 0.3755 - val_acc: 0.8539\n",
      "Epoch 8/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4108 - acc: 0.8152 - val_loss: 0.4843 - val_acc: 0.7603\n",
      "Epoch 9/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4133 - acc: 0.8172 - val_loss: 0.3669 - val_acc: 0.8424\n",
      "Epoch 10/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4015 - acc: 0.8205 - val_loss: 0.3680 - val_acc: 0.8539\n",
      "Epoch 11/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3954 - acc: 0.8267 - val_loss: 0.3947 - val_acc: 0.8407\n",
      "Epoch 12/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3948 - acc: 0.8267 - val_loss: 0.3671 - val_acc: 0.8473\n",
      "Epoch 13/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3889 - acc: 0.8305 - val_loss: 0.3638 - val_acc: 0.8440\n",
      "Epoch 14/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3816 - acc: 0.8360 - val_loss: 0.3768 - val_acc: 0.8489\n",
      "Epoch 15/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3789 - acc: 0.8356 - val_loss: 0.3899 - val_acc: 0.8473\n",
      "Epoch 16/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3725 - acc: 0.8404 - val_loss: 0.3680 - val_acc: 0.8506\n",
      "Epoch 17/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3713 - acc: 0.8396 - val_loss: 0.3847 - val_acc: 0.8440\n",
      "Epoch 18/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3695 - acc: 0.8422 - val_loss: 0.3678 - val_acc: 0.8424\n",
      "Training with parameters {'batch_size': 10, 'dropout': 0.1, 'lay_conf': [1024, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,606,721\n",
      "Trainable params: 1,606,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549/549 [==============================] - 3s 5ms/step - loss: 0.5634 - acc: 0.7125 - val_loss: 0.5580 - val_acc: 0.6782\n",
      "Epoch 2/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4845 - acc: 0.7721 - val_loss: 0.4018 - val_acc: 0.8243\n",
      "Epoch 3/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4622 - acc: 0.7962 - val_loss: 0.4463 - val_acc: 0.8144\n",
      "Epoch 4/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4553 - acc: 0.7991 - val_loss: 0.3746 - val_acc: 0.8374\n",
      "Epoch 5/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4400 - acc: 0.8009 - val_loss: 0.3669 - val_acc: 0.8588\n",
      "Epoch 6/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4361 - acc: 0.8124 - val_loss: 0.3782 - val_acc: 0.8391\n",
      "Epoch 7/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4253 - acc: 0.8135 - val_loss: 0.3788 - val_acc: 0.8539\n",
      "Epoch 8/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4254 - acc: 0.8093 - val_loss: 0.3675 - val_acc: 0.8539\n",
      "Epoch 9/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4129 - acc: 0.8228 - val_loss: 0.3627 - val_acc: 0.8588\n",
      "Epoch 10/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4027 - acc: 0.8263 - val_loss: 0.4162 - val_acc: 0.8374\n",
      "Epoch 11/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3998 - acc: 0.8270 - val_loss: 0.4312 - val_acc: 0.8177\n",
      "Epoch 12/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4003 - acc: 0.8216 - val_loss: 0.3618 - val_acc: 0.8489\n",
      "Epoch 13/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3903 - acc: 0.8265 - val_loss: 0.3698 - val_acc: 0.8522\n",
      "Epoch 14/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3905 - acc: 0.8301 - val_loss: 0.3744 - val_acc: 0.8522\n",
      "Epoch 15/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3795 - acc: 0.8323 - val_loss: 0.3723 - val_acc: 0.8489\n",
      "Epoch 16/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3757 - acc: 0.8376 - val_loss: 0.4283 - val_acc: 0.7997\n",
      "Epoch 17/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3748 - acc: 0.8342 - val_loss: 0.3794 - val_acc: 0.8440\n",
      "Training with parameters {'batch_size': 10, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_22 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,050,113\n",
      "Trainable params: 1,050,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.5520 - acc: 0.7238 - val_loss: 0.4712 - val_acc: 0.7997\n",
      "Epoch 2/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4722 - acc: 0.7885 - val_loss: 0.4586 - val_acc: 0.7718\n",
      "Epoch 3/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4574 - acc: 0.7935 - val_loss: 0.3992 - val_acc: 0.8489\n",
      "Epoch 4/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4351 - acc: 0.8062 - val_loss: 0.3848 - val_acc: 0.8489\n",
      "Epoch 5/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4193 - acc: 0.8128 - val_loss: 0.3701 - val_acc: 0.8506\n",
      "Epoch 6/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4158 - acc: 0.8185 - val_loss: 0.3826 - val_acc: 0.8424\n",
      "Epoch 7/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4148 - acc: 0.8163 - val_loss: 0.3711 - val_acc: 0.8506\n",
      "Epoch 8/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4054 - acc: 0.8174 - val_loss: 0.4174 - val_acc: 0.8177\n",
      "Epoch 9/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4012 - acc: 0.8287 - val_loss: 0.3763 - val_acc: 0.8489\n",
      "Epoch 10/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3867 - acc: 0.8287 - val_loss: 0.4128 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': 10, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_23 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 401,729\n",
      "Trainable params: 401,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.5862 - acc: 0.6979 - val_loss: 0.4794 - val_acc: 0.7783\n",
      "Epoch 2/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4916 - acc: 0.7721 - val_loss: 0.4088 - val_acc: 0.8292\n",
      "Epoch 3/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4701 - acc: 0.7871 - val_loss: 0.4115 - val_acc: 0.8243\n",
      "Epoch 4/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4477 - acc: 0.8011 - val_loss: 0.4120 - val_acc: 0.8309\n",
      "Epoch 5/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4362 - acc: 0.8081 - val_loss: 0.3768 - val_acc: 0.8473\n",
      "Epoch 6/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4261 - acc: 0.8093 - val_loss: 0.3788 - val_acc: 0.8506\n",
      "Epoch 7/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4245 - acc: 0.8139 - val_loss: 0.4256 - val_acc: 0.8276\n",
      "Epoch 8/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4185 - acc: 0.8159 - val_loss: 0.3704 - val_acc: 0.8473\n",
      "Epoch 9/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4112 - acc: 0.8172 - val_loss: 0.3793 - val_acc: 0.8489\n",
      "Epoch 10/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4016 - acc: 0.8214 - val_loss: 0.3634 - val_acc: 0.8473\n",
      "Epoch 11/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3976 - acc: 0.8280 - val_loss: 0.3794 - val_acc: 0.8522\n",
      "Epoch 12/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3956 - acc: 0.8236 - val_loss: 0.3959 - val_acc: 0.8194\n",
      "Epoch 13/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3906 - acc: 0.8281 - val_loss: 0.3716 - val_acc: 0.8456\n",
      "Epoch 14/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3863 - acc: 0.8318 - val_loss: 0.3685 - val_acc: 0.8506\n",
      "Epoch 15/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3814 - acc: 0.8325 - val_loss: 0.3620 - val_acc: 0.8456\n",
      "Epoch 16/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3792 - acc: 0.8338 - val_loss: 0.3678 - val_acc: 0.8456\n",
      "Epoch 17/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3727 - acc: 0.8382 - val_loss: 0.3744 - val_acc: 0.8407\n",
      "Epoch 18/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3643 - acc: 0.8458 - val_loss: 0.4061 - val_acc: 0.8342\n",
      "Epoch 19/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3603 - acc: 0.8405 - val_loss: 0.3668 - val_acc: 0.8456\n",
      "Epoch 20/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3595 - acc: 0.8446 - val_loss: 0.3762 - val_acc: 0.8456\n",
      "Training with parameters {'batch_size': 10, 'dropout': 0.1, 'lay_conf': [1024, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_24 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,836,545\n",
      "Trainable params: 1,836,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.5574 - acc: 0.7168 - val_loss: 0.4514 - val_acc: 0.8030\n",
      "Epoch 2/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4711 - acc: 0.7847 - val_loss: 0.4342 - val_acc: 0.8144\n",
      "Epoch 3/300\n",
      "549/549 [==============================] - 3s 6ms/step - loss: 0.4568 - acc: 0.7922 - val_loss: 0.3895 - val_acc: 0.8539\n",
      "Epoch 4/300\n",
      "549/549 [==============================] - 3s 6ms/step - loss: 0.4328 - acc: 0.8066 - val_loss: 0.4294 - val_acc: 0.8030\n",
      "Epoch 5/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4338 - acc: 0.8024 - val_loss: 0.3814 - val_acc: 0.8473\n",
      "Epoch 6/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4190 - acc: 0.8134 - val_loss: 0.3657 - val_acc: 0.8621\n",
      "Epoch 7/300\n",
      "549/549 [==============================] - 3s 6ms/step - loss: 0.4152 - acc: 0.8134 - val_loss: 0.3879 - val_acc: 0.8374\n",
      "Epoch 8/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4088 - acc: 0.8163 - val_loss: 0.3961 - val_acc: 0.8407\n",
      "Epoch 9/300\n",
      "549/549 [==============================] - 3s 6ms/step - loss: 0.3914 - acc: 0.8316 - val_loss: 0.4015 - val_acc: 0.8259\n",
      "Epoch 10/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.4056 - acc: 0.8172 - val_loss: 0.4421 - val_acc: 0.8062\n",
      "Epoch 11/300\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 0.3912 - acc: 0.8287 - val_loss: 0.3686 - val_acc: 0.8456\n",
      "Training with parameters {'batch_size': 32, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.6214 - acc: 0.6605 - val_loss: 0.5054 - val_acc: 0.7816\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.5216 - acc: 0.7542 - val_loss: 0.4419 - val_acc: 0.8095\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4817 - acc: 0.7781 - val_loss: 0.4173 - val_acc: 0.8177\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4641 - acc: 0.7929 - val_loss: 0.4032 - val_acc: 0.8309\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4510 - acc: 0.7947 - val_loss: 0.4033 - val_acc: 0.8227\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4405 - acc: 0.8046 - val_loss: 0.4002 - val_acc: 0.8342\n",
      "Epoch 7/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4343 - acc: 0.8084 - val_loss: 0.3983 - val_acc: 0.8276\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4242 - acc: 0.8097 - val_loss: 0.4243 - val_acc: 0.8276\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4208 - acc: 0.8124 - val_loss: 0.3885 - val_acc: 0.8407\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4094 - acc: 0.8210 - val_loss: 0.4060 - val_acc: 0.8325\n",
      "Epoch 11/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4114 - acc: 0.8199 - val_loss: 0.3814 - val_acc: 0.8489\n",
      "Epoch 12/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4030 - acc: 0.8230 - val_loss: 0.4123 - val_acc: 0.8210\n",
      "Epoch 13/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3966 - acc: 0.8258 - val_loss: 0.3770 - val_acc: 0.8424\n",
      "Epoch 14/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3920 - acc: 0.8325 - val_loss: 0.3794 - val_acc: 0.8522\n",
      "Epoch 15/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3913 - acc: 0.8334 - val_loss: 0.3835 - val_acc: 0.8391\n",
      "Epoch 16/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3875 - acc: 0.8347 - val_loss: 0.3763 - val_acc: 0.8456\n",
      "Epoch 17/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3786 - acc: 0.8393 - val_loss: 0.3815 - val_acc: 0.8391\n",
      "Epoch 18/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3814 - acc: 0.8354 - val_loss: 0.3761 - val_acc: 0.8407\n",
      "Epoch 19/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3747 - acc: 0.8409 - val_loss: 0.3756 - val_acc: 0.8440\n",
      "Epoch 20/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3684 - acc: 0.8394 - val_loss: 0.3901 - val_acc: 0.8440\n",
      "Epoch 21/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3670 - acc: 0.8431 - val_loss: 0.3825 - val_acc: 0.8407\n",
      "Epoch 22/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3678 - acc: 0.8380 - val_loss: 0.3782 - val_acc: 0.8489\n",
      "Epoch 23/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3673 - acc: 0.8444 - val_loss: 0.4207 - val_acc: 0.8095\n",
      "Epoch 24/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3510 - acc: 0.8477 - val_loss: 0.3952 - val_acc: 0.8309\n",
      "Training with parameters {'batch_size': 32, 'dropout': 0.1, 'lay_conf': [1024, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.5800 - acc: 0.6995 - val_loss: 0.4449 - val_acc: 0.8194\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4876 - acc: 0.7674 - val_loss: 0.4094 - val_acc: 0.8325\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4584 - acc: 0.7949 - val_loss: 0.4943 - val_acc: 0.7849\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4536 - acc: 0.7980 - val_loss: 0.3875 - val_acc: 0.8440\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4278 - acc: 0.8062 - val_loss: 0.3781 - val_acc: 0.8588\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4190 - acc: 0.8183 - val_loss: 0.4098 - val_acc: 0.8309\n",
      "Epoch 7/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4146 - acc: 0.8199 - val_loss: 0.3830 - val_acc: 0.8424\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4042 - acc: 0.8223 - val_loss: 0.4423 - val_acc: 0.8079\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3989 - acc: 0.8217 - val_loss: 0.3779 - val_acc: 0.8374\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3980 - acc: 0.8250 - val_loss: 0.4081 - val_acc: 0.8309\n",
      "Epoch 11/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3951 - acc: 0.8245 - val_loss: 0.3658 - val_acc: 0.8440\n",
      "Epoch 12/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3841 - acc: 0.8338 - val_loss: 0.3805 - val_acc: 0.8424\n",
      "Epoch 13/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3849 - acc: 0.8332 - val_loss: 0.3859 - val_acc: 0.8440\n",
      "Epoch 14/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3722 - acc: 0.8354 - val_loss: 0.3649 - val_acc: 0.8456\n",
      "Epoch 15/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3700 - acc: 0.8323 - val_loss: 0.4051 - val_acc: 0.8227\n",
      "Epoch 16/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3663 - acc: 0.8402 - val_loss: 0.3715 - val_acc: 0.8424\n",
      "Epoch 17/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3557 - acc: 0.8435 - val_loss: 0.3748 - val_acc: 0.8424\n",
      "Epoch 18/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3520 - acc: 0.8469 - val_loss: 0.3714 - val_acc: 0.8522\n",
      "Epoch 19/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3477 - acc: 0.8546 - val_loss: 0.3967 - val_acc: 0.8473\n",
      "Training with parameters {'batch_size': 32, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_27 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,985\n",
      "Trainable params: 409,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.6335 - acc: 0.6601 - val_loss: 0.4934 - val_acc: 0.7898\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.5214 - acc: 0.7528 - val_loss: 0.4460 - val_acc: 0.8144\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4826 - acc: 0.7820 - val_loss: 0.4263 - val_acc: 0.8112\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4664 - acc: 0.7900 - val_loss: 0.4200 - val_acc: 0.8243\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4494 - acc: 0.7966 - val_loss: 0.4000 - val_acc: 0.8276\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4426 - acc: 0.8055 - val_loss: 0.3925 - val_acc: 0.8489\n",
      "Epoch 7/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4301 - acc: 0.8099 - val_loss: 0.3872 - val_acc: 0.8506\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4214 - acc: 0.8112 - val_loss: 0.4043 - val_acc: 0.8259\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4115 - acc: 0.8176 - val_loss: 0.3802 - val_acc: 0.8391\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4109 - acc: 0.8210 - val_loss: 0.3846 - val_acc: 0.8456\n",
      "Epoch 11/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4013 - acc: 0.8290 - val_loss: 0.3844 - val_acc: 0.8489\n",
      "Epoch 12/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4037 - acc: 0.8232 - val_loss: 0.3933 - val_acc: 0.8374\n",
      "Epoch 13/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4010 - acc: 0.8280 - val_loss: 0.3890 - val_acc: 0.8391\n",
      "Epoch 14/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3921 - acc: 0.8292 - val_loss: 0.3863 - val_acc: 0.8391\n",
      "Training with parameters {'batch_size': 32, 'dropout': 0.1, 'lay_conf': [1024, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_28 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,639,553\n",
      "Trainable params: 1,639,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.5685 - acc: 0.7095 - val_loss: 0.4441 - val_acc: 0.8259\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4943 - acc: 0.7690 - val_loss: 0.4115 - val_acc: 0.8292\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4507 - acc: 0.8020 - val_loss: 0.3990 - val_acc: 0.8391\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4434 - acc: 0.8031 - val_loss: 0.3920 - val_acc: 0.8489\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4292 - acc: 0.8037 - val_loss: 0.3728 - val_acc: 0.8506\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4329 - acc: 0.8048 - val_loss: 0.3752 - val_acc: 0.8506\n",
      "Epoch 7/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4118 - acc: 0.8181 - val_loss: 0.3745 - val_acc: 0.8522\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4067 - acc: 0.8217 - val_loss: 0.3722 - val_acc: 0.8456\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4110 - acc: 0.8199 - val_loss: 0.3701 - val_acc: 0.8440\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3944 - acc: 0.8303 - val_loss: 0.3924 - val_acc: 0.8407\n",
      "Epoch 11/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3891 - acc: 0.8298 - val_loss: 0.3685 - val_acc: 0.8456\n",
      "Epoch 12/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3847 - acc: 0.8314 - val_loss: 0.3705 - val_acc: 0.8407\n",
      "Epoch 13/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3823 - acc: 0.8331 - val_loss: 0.3666 - val_acc: 0.8440\n",
      "Epoch 14/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3745 - acc: 0.8351 - val_loss: 0.3700 - val_acc: 0.8473\n",
      "Epoch 15/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3711 - acc: 0.8411 - val_loss: 0.3654 - val_acc: 0.8473\n",
      "Epoch 16/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3694 - acc: 0.8345 - val_loss: 0.3678 - val_acc: 0.8473\n",
      "Epoch 17/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3686 - acc: 0.8382 - val_loss: 0.3688 - val_acc: 0.8456\n",
      "Epoch 18/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3622 - acc: 0.8447 - val_loss: 0.3714 - val_acc: 0.8506\n",
      "Epoch 19/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3555 - acc: 0.8462 - val_loss: 0.3786 - val_acc: 0.8374\n",
      "Epoch 20/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3493 - acc: 0.8493 - val_loss: 0.3801 - val_acc: 0.8325\n",
      "Training with parameters {'batch_size': 32, 'dropout': 0.1, 'lay_conf': [1024, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,606,721\n",
      "Trainable params: 1,606,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.5812 - acc: 0.7019 - val_loss: 0.4456 - val_acc: 0.8309\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4997 - acc: 0.7665 - val_loss: 0.4149 - val_acc: 0.8325\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4543 - acc: 0.7989 - val_loss: 0.4003 - val_acc: 0.8374\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4393 - acc: 0.8000 - val_loss: 0.3780 - val_acc: 0.8489\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4332 - acc: 0.8086 - val_loss: 0.3831 - val_acc: 0.8604\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4354 - acc: 0.8044 - val_loss: 0.3792 - val_acc: 0.8571\n",
      "Epoch 7/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4168 - acc: 0.8172 - val_loss: 0.3754 - val_acc: 0.8473\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4072 - acc: 0.8210 - val_loss: 0.3791 - val_acc: 0.8473\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4058 - acc: 0.8185 - val_loss: 0.4528 - val_acc: 0.7915\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4062 - acc: 0.8157 - val_loss: 0.3863 - val_acc: 0.8424\n",
      "Epoch 11/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3922 - acc: 0.8241 - val_loss: 0.3695 - val_acc: 0.8506\n",
      "Epoch 12/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3845 - acc: 0.8327 - val_loss: 0.3904 - val_acc: 0.8440\n",
      "Epoch 13/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3822 - acc: 0.8329 - val_loss: 0.4026 - val_acc: 0.8210\n",
      "Epoch 14/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3846 - acc: 0.8334 - val_loss: 0.4130 - val_acc: 0.8243\n",
      "Epoch 15/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3822 - acc: 0.8285 - val_loss: 0.4241 - val_acc: 0.8177\n",
      "Epoch 16/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3770 - acc: 0.8290 - val_loss: 0.3687 - val_acc: 0.8489\n",
      "Epoch 17/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3722 - acc: 0.8360 - val_loss: 0.3706 - val_acc: 0.8506\n",
      "Epoch 18/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3734 - acc: 0.8329 - val_loss: 0.3725 - val_acc: 0.8522\n",
      "Epoch 19/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3553 - acc: 0.8471 - val_loss: 0.4056 - val_acc: 0.8407\n",
      "Epoch 20/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3705 - acc: 0.8382 - val_loss: 0.5240 - val_acc: 0.7406\n",
      "Epoch 21/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3679 - acc: 0.8409 - val_loss: 0.3729 - val_acc: 0.8506\n",
      "Training with parameters {'batch_size': 32, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_30 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,050,113\n",
      "Trainable params: 1,050,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.5723 - acc: 0.7035 - val_loss: 0.4435 - val_acc: 0.8177\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4856 - acc: 0.7774 - val_loss: 0.4215 - val_acc: 0.8161\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4523 - acc: 0.8017 - val_loss: 0.4120 - val_acc: 0.8309\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4374 - acc: 0.8062 - val_loss: 0.3896 - val_acc: 0.8391\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4271 - acc: 0.8110 - val_loss: 0.3810 - val_acc: 0.8489\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4157 - acc: 0.8159 - val_loss: 0.3734 - val_acc: 0.8555\n",
      "Epoch 7/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4025 - acc: 0.8239 - val_loss: 0.3768 - val_acc: 0.8539\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4061 - acc: 0.8216 - val_loss: 0.3714 - val_acc: 0.8522\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4002 - acc: 0.8199 - val_loss: 0.3921 - val_acc: 0.8456\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3887 - acc: 0.8305 - val_loss: 0.3868 - val_acc: 0.8407\n",
      "Epoch 11/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3873 - acc: 0.8301 - val_loss: 0.4909 - val_acc: 0.7488\n",
      "Epoch 12/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3780 - acc: 0.8343 - val_loss: 0.3745 - val_acc: 0.8489\n",
      "Epoch 13/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3767 - acc: 0.8305 - val_loss: 0.4330 - val_acc: 0.7947\n",
      "Training with parameters {'batch_size': 32, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_31 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 401,729\n",
      "Trainable params: 401,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.6206 - acc: 0.6621 - val_loss: 0.5029 - val_acc: 0.8046\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.5363 - acc: 0.7435 - val_loss: 0.4527 - val_acc: 0.8112\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4922 - acc: 0.7754 - val_loss: 0.4373 - val_acc: 0.8227\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4710 - acc: 0.7856 - val_loss: 0.4001 - val_acc: 0.8391\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4514 - acc: 0.7989 - val_loss: 0.3964 - val_acc: 0.8440\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4410 - acc: 0.8030 - val_loss: 0.3840 - val_acc: 0.8440\n",
      "Epoch 7/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4318 - acc: 0.8073 - val_loss: 0.3801 - val_acc: 0.8539\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4260 - acc: 0.8093 - val_loss: 0.3987 - val_acc: 0.8325\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4210 - acc: 0.8146 - val_loss: 0.3846 - val_acc: 0.8440\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4224 - acc: 0.8155 - val_loss: 0.4115 - val_acc: 0.8227\n",
      "Epoch 11/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4101 - acc: 0.8236 - val_loss: 0.3685 - val_acc: 0.8506\n",
      "Epoch 12/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4099 - acc: 0.8196 - val_loss: 0.3951 - val_acc: 0.8374\n",
      "Epoch 13/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3974 - acc: 0.8294 - val_loss: 0.3789 - val_acc: 0.8456\n",
      "Epoch 14/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3975 - acc: 0.8239 - val_loss: 0.3664 - val_acc: 0.8539\n",
      "Epoch 15/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3914 - acc: 0.8343 - val_loss: 0.3779 - val_acc: 0.8456\n",
      "Epoch 16/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3879 - acc: 0.8316 - val_loss: 0.3709 - val_acc: 0.8604\n",
      "Epoch 17/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3966 - acc: 0.8248 - val_loss: 0.3738 - val_acc: 0.8506\n",
      "Epoch 18/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3833 - acc: 0.8331 - val_loss: 0.3615 - val_acc: 0.8571\n",
      "Epoch 19/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3795 - acc: 0.8340 - val_loss: 0.3635 - val_acc: 0.8539\n",
      "Epoch 20/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3755 - acc: 0.8352 - val_loss: 0.4171 - val_acc: 0.8112\n",
      "Epoch 21/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3727 - acc: 0.8394 - val_loss: 0.3660 - val_acc: 0.8506\n",
      "Epoch 22/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3701 - acc: 0.8391 - val_loss: 0.3708 - val_acc: 0.8522\n",
      "Epoch 23/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3593 - acc: 0.8464 - val_loss: 0.3612 - val_acc: 0.8539\n",
      "Epoch 24/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3598 - acc: 0.8513 - val_loss: 0.3806 - val_acc: 0.8473\n",
      "Epoch 25/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3548 - acc: 0.8511 - val_loss: 0.3655 - val_acc: 0.8506\n",
      "Epoch 26/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3483 - acc: 0.8506 - val_loss: 0.3650 - val_acc: 0.8522\n",
      "Epoch 27/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3497 - acc: 0.8467 - val_loss: 0.3661 - val_acc: 0.8506\n",
      "Epoch 28/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.3486 - acc: 0.8478 - val_loss: 0.3778 - val_acc: 0.8456\n",
      "Training with parameters {'batch_size': 32, 'dropout': 0.1, 'lay_conf': [1024, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_32 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,836,545\n",
      "Trainable params: 1,836,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.5795 - acc: 0.6915 - val_loss: 0.4904 - val_acc: 0.7570\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 0.4748 - acc: 0.7900 - val_loss: 0.4770 - val_acc: 0.7849\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4624 - acc: 0.7884 - val_loss: 0.3947 - val_acc: 0.8358\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4364 - acc: 0.8008 - val_loss: 0.3921 - val_acc: 0.8358\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4281 - acc: 0.8079 - val_loss: 0.4457 - val_acc: 0.8194\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4198 - acc: 0.8134 - val_loss: 0.3938 - val_acc: 0.8456\n",
      "Epoch 7/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4136 - acc: 0.8174 - val_loss: 0.4094 - val_acc: 0.8309\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.4023 - acc: 0.8199 - val_loss: 0.3781 - val_acc: 0.8456\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3949 - acc: 0.8223 - val_loss: 0.3991 - val_acc: 0.8473\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3988 - acc: 0.8230 - val_loss: 0.3646 - val_acc: 0.8571\n",
      "Epoch 11/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3829 - acc: 0.8270 - val_loss: 0.3785 - val_acc: 0.8489\n",
      "Epoch 12/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3898 - acc: 0.8274 - val_loss: 0.4026 - val_acc: 0.8391\n",
      "Epoch 13/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3798 - acc: 0.8369 - val_loss: 0.3710 - val_acc: 0.8424\n",
      "Epoch 14/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3705 - acc: 0.8365 - val_loss: 0.3649 - val_acc: 0.8522\n",
      "Epoch 15/300\n",
      "172/172 [==============================] - 1s 6ms/step - loss: 0.3740 - acc: 0.8363 - val_loss: 0.4511 - val_acc: 0.8276\n",
      "Training with parameters {'batch_size': 64, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_33 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.6471 - acc: 0.6274 - val_loss: 0.5592 - val_acc: 0.7389\n",
      "Epoch 2/300\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5618 - acc: 0.7334 - val_loss: 0.4909 - val_acc: 0.7849\n",
      "Epoch 3/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.5214 - acc: 0.7568 - val_loss: 0.4430 - val_acc: 0.8210\n",
      "Epoch 4/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4813 - acc: 0.7822 - val_loss: 0.4382 - val_acc: 0.8161\n",
      "Epoch 5/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4661 - acc: 0.7873 - val_loss: 0.4136 - val_acc: 0.8407\n",
      "Epoch 6/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4588 - acc: 0.7946 - val_loss: 0.4090 - val_acc: 0.8292\n",
      "Epoch 7/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4447 - acc: 0.8062 - val_loss: 0.3974 - val_acc: 0.8440\n",
      "Epoch 8/300\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4361 - acc: 0.8072 - val_loss: 0.3990 - val_acc: 0.8391\n",
      "Epoch 9/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4310 - acc: 0.8139 - val_loss: 0.3874 - val_acc: 0.8407\n",
      "Epoch 10/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4279 - acc: 0.8101 - val_loss: 0.3897 - val_acc: 0.8440\n",
      "Epoch 11/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4237 - acc: 0.8150 - val_loss: 0.3961 - val_acc: 0.8391\n",
      "Epoch 12/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4126 - acc: 0.8214 - val_loss: 0.3912 - val_acc: 0.8391\n",
      "Epoch 13/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4104 - acc: 0.8216 - val_loss: 0.3873 - val_acc: 0.8424\n",
      "Epoch 14/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4091 - acc: 0.8205 - val_loss: 0.3922 - val_acc: 0.8358\n",
      "Epoch 15/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4001 - acc: 0.8228 - val_loss: 0.3838 - val_acc: 0.8374\n",
      "Epoch 16/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4022 - acc: 0.8232 - val_loss: 0.3802 - val_acc: 0.8391\n",
      "Epoch 17/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3891 - acc: 0.8307 - val_loss: 0.3870 - val_acc: 0.8407\n",
      "Epoch 18/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3952 - acc: 0.8265 - val_loss: 0.3792 - val_acc: 0.8506\n",
      "Epoch 19/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3896 - acc: 0.8305 - val_loss: 0.3924 - val_acc: 0.8391\n",
      "Epoch 20/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3818 - acc: 0.8321 - val_loss: 0.3789 - val_acc: 0.8407\n",
      "Epoch 21/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3801 - acc: 0.8367 - val_loss: 0.3780 - val_acc: 0.8424\n",
      "Epoch 22/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3788 - acc: 0.8362 - val_loss: 0.3813 - val_acc: 0.8440\n",
      "Epoch 23/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3818 - acc: 0.8321 - val_loss: 0.3994 - val_acc: 0.8358\n",
      "Epoch 24/300\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.3735 - acc: 0.8389 - val_loss: 0.3762 - val_acc: 0.8473\n",
      "Epoch 25/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3610 - acc: 0.8438 - val_loss: 0.3752 - val_acc: 0.8473\n",
      "Epoch 26/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3618 - acc: 0.8431 - val_loss: 0.3771 - val_acc: 0.8456\n",
      "Epoch 27/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3613 - acc: 0.8460 - val_loss: 0.3822 - val_acc: 0.8424\n",
      "Epoch 28/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3552 - acc: 0.8473 - val_loss: 0.3894 - val_acc: 0.8407\n",
      "Epoch 29/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3503 - acc: 0.8526 - val_loss: 0.3822 - val_acc: 0.8456\n",
      "Epoch 30/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3471 - acc: 0.8539 - val_loss: 0.3810 - val_acc: 0.8440\n",
      "Training with parameters {'batch_size': 64, 'dropout': 0.1, 'lay_conf': [1024, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_34 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.6268 - acc: 0.6585 - val_loss: 0.5198 - val_acc: 0.7537\n",
      "Epoch 2/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4995 - acc: 0.7692 - val_loss: 0.4211 - val_acc: 0.8325\n",
      "Epoch 3/300\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4857 - acc: 0.7747 - val_loss: 0.4026 - val_acc: 0.8358\n",
      "Epoch 4/300\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4533 - acc: 0.7958 - val_loss: 0.4027 - val_acc: 0.8358\n",
      "Epoch 5/300\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4411 - acc: 0.8041 - val_loss: 0.3958 - val_acc: 0.8374\n",
      "Epoch 6/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4268 - acc: 0.8134 - val_loss: 0.3870 - val_acc: 0.8440\n",
      "Epoch 7/300\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4173 - acc: 0.8135 - val_loss: 0.3785 - val_acc: 0.8555\n",
      "Epoch 8/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4089 - acc: 0.8219 - val_loss: 0.3854 - val_acc: 0.8391\n",
      "Epoch 9/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4121 - acc: 0.8194 - val_loss: 0.3989 - val_acc: 0.8374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4092 - acc: 0.8219 - val_loss: 0.3737 - val_acc: 0.8489\n",
      "Epoch 11/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4055 - acc: 0.8197 - val_loss: 0.3812 - val_acc: 0.8440\n",
      "Epoch 12/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3962 - acc: 0.8254 - val_loss: 0.3804 - val_acc: 0.8473\n",
      "Epoch 13/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3931 - acc: 0.8311 - val_loss: 0.4115 - val_acc: 0.8276\n",
      "Epoch 14/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3833 - acc: 0.8334 - val_loss: 0.3727 - val_acc: 0.8571\n",
      "Epoch 15/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3744 - acc: 0.8338 - val_loss: 0.4211 - val_acc: 0.8342\n",
      "Epoch 16/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3729 - acc: 0.8385 - val_loss: 0.3815 - val_acc: 0.8440\n",
      "Epoch 17/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3613 - acc: 0.8389 - val_loss: 0.3869 - val_acc: 0.8374\n",
      "Epoch 18/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3610 - acc: 0.8429 - val_loss: 0.3866 - val_acc: 0.8440\n",
      "Epoch 19/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3576 - acc: 0.8473 - val_loss: 0.3832 - val_acc: 0.8456\n",
      "Training with parameters {'batch_size': 64, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_35 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,985\n",
      "Trainable params: 409,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.6933 - acc: 0.6176 - val_loss: 0.5471 - val_acc: 0.7833\n",
      "Epoch 2/300\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.5577 - acc: 0.7260 - val_loss: 0.4980 - val_acc: 0.7915\n",
      "Epoch 3/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.5138 - acc: 0.7634 - val_loss: 0.4425 - val_acc: 0.8194\n",
      "Epoch 4/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4842 - acc: 0.7787 - val_loss: 0.4241 - val_acc: 0.8276\n",
      "Epoch 5/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4664 - acc: 0.7937 - val_loss: 0.4122 - val_acc: 0.8342\n",
      "Epoch 6/300\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4554 - acc: 0.7958 - val_loss: 0.4055 - val_acc: 0.8407\n",
      "Epoch 7/300\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4482 - acc: 0.8009 - val_loss: 0.4023 - val_acc: 0.8374\n",
      "Epoch 8/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4395 - acc: 0.8046 - val_loss: 0.3933 - val_acc: 0.8489\n",
      "Epoch 9/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4345 - acc: 0.8090 - val_loss: 0.4004 - val_acc: 0.8325\n",
      "Epoch 10/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4221 - acc: 0.8126 - val_loss: 0.3993 - val_acc: 0.8210\n",
      "Epoch 11/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4203 - acc: 0.8163 - val_loss: 0.3888 - val_acc: 0.8456\n",
      "Epoch 12/300\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.4157 - acc: 0.8161 - val_loss: 0.4434 - val_acc: 0.8177\n",
      "Epoch 13/300\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4162 - acc: 0.8199 - val_loss: 0.3808 - val_acc: 0.8473\n",
      "Epoch 14/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4051 - acc: 0.8256 - val_loss: 0.4116 - val_acc: 0.8325\n",
      "Epoch 15/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4080 - acc: 0.8245 - val_loss: 0.3774 - val_acc: 0.8473\n",
      "Epoch 16/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4043 - acc: 0.8197 - val_loss: 0.3826 - val_acc: 0.8440\n",
      "Epoch 17/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3921 - acc: 0.8320 - val_loss: 0.3822 - val_acc: 0.8407\n",
      "Epoch 18/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3864 - acc: 0.8336 - val_loss: 0.3858 - val_acc: 0.8374\n",
      "Epoch 19/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3861 - acc: 0.8338 - val_loss: 0.3778 - val_acc: 0.8473\n",
      "Epoch 20/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3829 - acc: 0.8347 - val_loss: 0.3943 - val_acc: 0.8325\n",
      "Training with parameters {'batch_size': 64, 'dropout': 0.1, 'lay_conf': [1024, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_36 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,639,553\n",
      "Trainable params: 1,639,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.6163 - acc: 0.6605 - val_loss: 0.4895 - val_acc: 0.7980\n",
      "Epoch 2/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.5007 - acc: 0.7634 - val_loss: 0.4196 - val_acc: 0.8358\n",
      "Epoch 3/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4662 - acc: 0.7865 - val_loss: 0.3884 - val_acc: 0.8456\n",
      "Epoch 4/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4505 - acc: 0.7982 - val_loss: 0.3831 - val_acc: 0.8489\n",
      "Epoch 5/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4335 - acc: 0.8057 - val_loss: 0.3810 - val_acc: 0.8473\n",
      "Epoch 6/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4449 - acc: 0.8028 - val_loss: 0.3984 - val_acc: 0.8440\n",
      "Epoch 7/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4206 - acc: 0.8117 - val_loss: 0.3841 - val_acc: 0.8456\n",
      "Epoch 8/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4124 - acc: 0.8135 - val_loss: 0.3768 - val_acc: 0.8489\n",
      "Epoch 9/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4193 - acc: 0.8101 - val_loss: 0.3659 - val_acc: 0.8522\n",
      "Epoch 10/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3980 - acc: 0.8250 - val_loss: 0.3648 - val_acc: 0.8506\n",
      "Epoch 11/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4022 - acc: 0.8203 - val_loss: 0.3751 - val_acc: 0.8473\n",
      "Epoch 12/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3891 - acc: 0.8281 - val_loss: 0.3676 - val_acc: 0.8473\n",
      "Epoch 13/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3841 - acc: 0.8298 - val_loss: 0.4084 - val_acc: 0.8177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3801 - acc: 0.8369 - val_loss: 0.3768 - val_acc: 0.8440\n",
      "Epoch 15/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3940 - acc: 0.8188 - val_loss: 0.4037 - val_acc: 0.8342\n",
      "Training with parameters {'batch_size': 64, 'dropout': 0.1, 'lay_conf': [1024, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_37 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,606,721\n",
      "Trainable params: 1,606,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5957 - acc: 0.6869 - val_loss: 0.4659 - val_acc: 0.8144\n",
      "Epoch 2/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.5108 - acc: 0.7559 - val_loss: 0.4377 - val_acc: 0.8112\n",
      "Epoch 3/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4734 - acc: 0.7811 - val_loss: 0.4123 - val_acc: 0.8276\n",
      "Epoch 4/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4524 - acc: 0.7933 - val_loss: 0.4115 - val_acc: 0.8161\n",
      "Epoch 5/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4457 - acc: 0.8050 - val_loss: 0.4109 - val_acc: 0.8407\n",
      "Epoch 6/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4356 - acc: 0.8084 - val_loss: 0.4256 - val_acc: 0.8030\n",
      "Epoch 7/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4248 - acc: 0.8088 - val_loss: 0.3745 - val_acc: 0.8522\n",
      "Epoch 8/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4169 - acc: 0.8176 - val_loss: 0.3717 - val_acc: 0.8539\n",
      "Epoch 9/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4215 - acc: 0.8152 - val_loss: 0.3689 - val_acc: 0.8555\n",
      "Epoch 10/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4044 - acc: 0.8207 - val_loss: 0.3666 - val_acc: 0.8539\n",
      "Epoch 11/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4000 - acc: 0.8225 - val_loss: 0.3648 - val_acc: 0.8539\n",
      "Epoch 12/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3965 - acc: 0.8272 - val_loss: 0.3648 - val_acc: 0.8588\n",
      "Epoch 13/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3942 - acc: 0.8289 - val_loss: 0.3666 - val_acc: 0.8522\n",
      "Epoch 14/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3926 - acc: 0.8261 - val_loss: 0.3681 - val_acc: 0.8571\n",
      "Epoch 15/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3891 - acc: 0.8301 - val_loss: 0.3795 - val_acc: 0.8473\n",
      "Epoch 16/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3737 - acc: 0.8376 - val_loss: 0.3646 - val_acc: 0.8473\n",
      "Epoch 17/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3688 - acc: 0.8438 - val_loss: 0.3631 - val_acc: 0.8456\n",
      "Epoch 18/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3746 - acc: 0.8398 - val_loss: 0.4040 - val_acc: 0.8210\n",
      "Epoch 19/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3802 - acc: 0.8332 - val_loss: 0.3922 - val_acc: 0.8489\n",
      "Epoch 20/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3605 - acc: 0.8442 - val_loss: 0.3699 - val_acc: 0.8539\n",
      "Epoch 21/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3623 - acc: 0.8418 - val_loss: 0.3939 - val_acc: 0.8259\n",
      "Epoch 22/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3564 - acc: 0.8480 - val_loss: 0.3838 - val_acc: 0.8391\n",
      "Training with parameters {'batch_size': 64, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_38 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,050,113\n",
      "Trainable params: 1,050,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5976 - acc: 0.6787 - val_loss: 0.4718 - val_acc: 0.7947\n",
      "Epoch 2/300\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4959 - acc: 0.7690 - val_loss: 0.4238 - val_acc: 0.8342\n",
      "Epoch 3/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4675 - acc: 0.7887 - val_loss: 0.4164 - val_acc: 0.8227\n",
      "Epoch 4/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4464 - acc: 0.8024 - val_loss: 0.4312 - val_acc: 0.8046\n",
      "Epoch 5/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4345 - acc: 0.8066 - val_loss: 0.4172 - val_acc: 0.8112\n",
      "Epoch 6/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4342 - acc: 0.8050 - val_loss: 0.4078 - val_acc: 0.8358\n",
      "Epoch 7/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4141 - acc: 0.8157 - val_loss: 0.4111 - val_acc: 0.8227\n",
      "Epoch 8/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4072 - acc: 0.8248 - val_loss: 0.4923 - val_acc: 0.7865\n",
      "Epoch 9/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4076 - acc: 0.8223 - val_loss: 0.3761 - val_acc: 0.8489\n",
      "Epoch 10/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3977 - acc: 0.8245 - val_loss: 0.4353 - val_acc: 0.8046\n",
      "Epoch 11/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3895 - acc: 0.8287 - val_loss: 0.3807 - val_acc: 0.8407\n",
      "Epoch 12/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3822 - acc: 0.8303 - val_loss: 0.3794 - val_acc: 0.8440\n",
      "Epoch 13/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3801 - acc: 0.8329 - val_loss: 0.3902 - val_acc: 0.8456\n",
      "Epoch 14/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3733 - acc: 0.8358 - val_loss: 0.3737 - val_acc: 0.8555\n",
      "Epoch 15/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3735 - acc: 0.8373 - val_loss: 0.4023 - val_acc: 0.8325\n",
      "Epoch 16/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3556 - acc: 0.8449 - val_loss: 0.3691 - val_acc: 0.8555\n",
      "Epoch 17/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3658 - acc: 0.8374 - val_loss: 0.3695 - val_acc: 0.8473\n",
      "Epoch 18/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3551 - acc: 0.8424 - val_loss: 0.3864 - val_acc: 0.8358\n",
      "Epoch 19/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3404 - acc: 0.8522 - val_loss: 0.3698 - val_acc: 0.8539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3407 - acc: 0.8504 - val_loss: 0.4073 - val_acc: 0.8112\n",
      "Epoch 21/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3307 - acc: 0.8612 - val_loss: 0.3877 - val_acc: 0.8391\n",
      "Training with parameters {'batch_size': 64, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_39 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 401,729\n",
      "Trainable params: 401,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.6476 - acc: 0.6309 - val_loss: 0.5521 - val_acc: 0.7356\n",
      "Epoch 2/300\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.5533 - acc: 0.7316 - val_loss: 0.4756 - val_acc: 0.8128\n",
      "Epoch 3/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.5150 - acc: 0.7634 - val_loss: 0.4606 - val_acc: 0.8144\n",
      "Epoch 4/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4834 - acc: 0.7838 - val_loss: 0.4216 - val_acc: 0.8243\n",
      "Epoch 5/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4739 - acc: 0.7893 - val_loss: 0.4071 - val_acc: 0.8325\n",
      "Epoch 6/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4523 - acc: 0.8048 - val_loss: 0.3948 - val_acc: 0.8440\n",
      "Epoch 7/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4504 - acc: 0.8009 - val_loss: 0.4011 - val_acc: 0.8424\n",
      "Epoch 8/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4395 - acc: 0.8106 - val_loss: 0.4400 - val_acc: 0.8030\n",
      "Epoch 9/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4340 - acc: 0.8101 - val_loss: 0.3897 - val_acc: 0.8407\n",
      "Epoch 10/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4203 - acc: 0.8132 - val_loss: 0.3992 - val_acc: 0.8309\n",
      "Epoch 11/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4191 - acc: 0.8163 - val_loss: 0.3844 - val_acc: 0.8506\n",
      "Epoch 12/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4137 - acc: 0.8203 - val_loss: 0.3878 - val_acc: 0.8440\n",
      "Epoch 13/300\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4054 - acc: 0.8259 - val_loss: 0.3737 - val_acc: 0.8506\n",
      "Epoch 14/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4058 - acc: 0.8250 - val_loss: 0.3706 - val_acc: 0.8489\n",
      "Epoch 15/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3978 - acc: 0.8312 - val_loss: 0.3746 - val_acc: 0.8506\n",
      "Epoch 16/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4046 - acc: 0.8217 - val_loss: 0.3751 - val_acc: 0.8440\n",
      "Epoch 17/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3908 - acc: 0.8329 - val_loss: 0.3793 - val_acc: 0.8391\n",
      "Epoch 18/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3914 - acc: 0.8290 - val_loss: 0.3725 - val_acc: 0.8456\n",
      "Epoch 19/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3880 - acc: 0.8269 - val_loss: 0.3705 - val_acc: 0.8456\n",
      "Epoch 20/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3843 - acc: 0.8345 - val_loss: 0.3790 - val_acc: 0.8473\n",
      "Epoch 21/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3811 - acc: 0.8334 - val_loss: 0.3700 - val_acc: 0.8456\n",
      "Epoch 22/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3736 - acc: 0.8409 - val_loss: 0.3701 - val_acc: 0.8506\n",
      "Epoch 23/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3810 - acc: 0.8385 - val_loss: 0.3720 - val_acc: 0.8407\n",
      "Epoch 24/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3684 - acc: 0.8415 - val_loss: 0.3730 - val_acc: 0.8473\n",
      "Epoch 25/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3683 - acc: 0.8449 - val_loss: 0.3731 - val_acc: 0.8456\n",
      "Epoch 26/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3629 - acc: 0.8456 - val_loss: 0.3729 - val_acc: 0.8456\n",
      "Training with parameters {'batch_size': 64, 'dropout': 0.1, 'lay_conf': [1024, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_40 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,836,545\n",
      "Trainable params: 1,836,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.6067 - acc: 0.6800 - val_loss: 0.5386 - val_acc: 0.7323\n",
      "Epoch 2/300\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4906 - acc: 0.7743 - val_loss: 0.4346 - val_acc: 0.8128\n",
      "Epoch 3/300\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4690 - acc: 0.7845 - val_loss: 0.5267 - val_acc: 0.7176\n",
      "Epoch 4/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4519 - acc: 0.7916 - val_loss: 0.3840 - val_acc: 0.8539\n",
      "Epoch 5/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4273 - acc: 0.8128 - val_loss: 0.4053 - val_acc: 0.8144\n",
      "Epoch 6/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4295 - acc: 0.8066 - val_loss: 0.3849 - val_acc: 0.8424\n",
      "Epoch 7/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4139 - acc: 0.8183 - val_loss: 0.4233 - val_acc: 0.8243\n",
      "Epoch 8/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4117 - acc: 0.8216 - val_loss: 0.3696 - val_acc: 0.8539\n",
      "Epoch 9/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4058 - acc: 0.8205 - val_loss: 0.3695 - val_acc: 0.8555\n",
      "Epoch 10/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3998 - acc: 0.8248 - val_loss: 0.3720 - val_acc: 0.8522\n",
      "Epoch 11/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3959 - acc: 0.8239 - val_loss: 0.3728 - val_acc: 0.8506\n",
      "Epoch 12/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3805 - acc: 0.8321 - val_loss: 0.3711 - val_acc: 0.8555\n",
      "Epoch 13/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3853 - acc: 0.8254 - val_loss: 0.4042 - val_acc: 0.8374\n",
      "Epoch 14/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3700 - acc: 0.8391 - val_loss: 0.3693 - val_acc: 0.8522\n",
      "Epoch 15/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3693 - acc: 0.8449 - val_loss: 0.3704 - val_acc: 0.8473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3556 - acc: 0.8458 - val_loss: 0.4351 - val_acc: 0.8013\n",
      "Epoch 17/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3575 - acc: 0.8404 - val_loss: 0.3711 - val_acc: 0.8440\n",
      "Epoch 18/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3501 - acc: 0.8482 - val_loss: 0.3894 - val_acc: 0.8342\n",
      "Epoch 19/300\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3394 - acc: 0.8531 - val_loss: 0.3964 - val_acc: 0.8424\n",
      "Training with parameters {'batch_size': 70, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_41 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.6453 - acc: 0.6229 - val_loss: 0.5617 - val_acc: 0.7422\n",
      "Epoch 2/300\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.5640 - acc: 0.7269 - val_loss: 0.4823 - val_acc: 0.8128\n",
      "Epoch 3/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.5132 - acc: 0.7645 - val_loss: 0.4389 - val_acc: 0.8161\n",
      "Epoch 4/300\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.4872 - acc: 0.7834 - val_loss: 0.4273 - val_acc: 0.8194\n",
      "Epoch 5/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4630 - acc: 0.7929 - val_loss: 0.4059 - val_acc: 0.8440\n",
      "Epoch 6/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4532 - acc: 0.7969 - val_loss: 0.4010 - val_acc: 0.8440\n",
      "Epoch 7/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4435 - acc: 0.8070 - val_loss: 0.3949 - val_acc: 0.8456\n",
      "Epoch 8/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4388 - acc: 0.8084 - val_loss: 0.3916 - val_acc: 0.8407\n",
      "Epoch 9/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4285 - acc: 0.8139 - val_loss: 0.3897 - val_acc: 0.8424\n",
      "Epoch 10/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4252 - acc: 0.8095 - val_loss: 0.3851 - val_acc: 0.8456\n",
      "Epoch 11/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4232 - acc: 0.8121 - val_loss: 0.3830 - val_acc: 0.8424\n",
      "Epoch 12/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4173 - acc: 0.8177 - val_loss: 0.3803 - val_acc: 0.8489\n",
      "Epoch 13/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4127 - acc: 0.8201 - val_loss: 0.3797 - val_acc: 0.8522\n",
      "Epoch 14/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4090 - acc: 0.8234 - val_loss: 0.3759 - val_acc: 0.8489\n",
      "Epoch 15/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4132 - acc: 0.8236 - val_loss: 0.3786 - val_acc: 0.8489\n",
      "Epoch 16/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4001 - acc: 0.8289 - val_loss: 0.3799 - val_acc: 0.8440\n",
      "Epoch 17/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4037 - acc: 0.8259 - val_loss: 0.3746 - val_acc: 0.8456\n",
      "Epoch 18/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3910 - acc: 0.8278 - val_loss: 0.4071 - val_acc: 0.8391\n",
      "Epoch 19/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3947 - acc: 0.8270 - val_loss: 0.3812 - val_acc: 0.8424\n",
      "Epoch 20/300\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3855 - acc: 0.8360 - val_loss: 0.3755 - val_acc: 0.8473\n",
      "Epoch 21/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3821 - acc: 0.8373 - val_loss: 0.3718 - val_acc: 0.8489\n",
      "Epoch 22/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3838 - acc: 0.8356 - val_loss: 0.3790 - val_acc: 0.8407\n",
      "Epoch 23/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3843 - acc: 0.8340 - val_loss: 0.3754 - val_acc: 0.8456\n",
      "Epoch 24/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3752 - acc: 0.8404 - val_loss: 0.4371 - val_acc: 0.8243\n",
      "Epoch 25/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3821 - acc: 0.8358 - val_loss: 0.3761 - val_acc: 0.8440\n",
      "Epoch 26/300\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3783 - acc: 0.8400 - val_loss: 0.3891 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 70, 'dropout': 0.1, 'lay_conf': [1024, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_42 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_123 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.6165 - acc: 0.6690 - val_loss: 0.4703 - val_acc: 0.8128\n",
      "Epoch 2/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4999 - acc: 0.7619 - val_loss: 0.4157 - val_acc: 0.8342\n",
      "Epoch 3/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.4658 - acc: 0.7895 - val_loss: 0.4256 - val_acc: 0.8243\n",
      "Epoch 4/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4508 - acc: 0.7980 - val_loss: 0.3992 - val_acc: 0.8391\n",
      "Epoch 5/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.4332 - acc: 0.8072 - val_loss: 0.3807 - val_acc: 0.8456\n",
      "Epoch 6/300\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.4194 - acc: 0.8099 - val_loss: 0.3882 - val_acc: 0.8522\n",
      "Epoch 7/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.4249 - acc: 0.8115 - val_loss: 0.3745 - val_acc: 0.8506\n",
      "Epoch 8/300\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.4097 - acc: 0.8181 - val_loss: 0.3791 - val_acc: 0.8456\n",
      "Epoch 9/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.3979 - acc: 0.8243 - val_loss: 0.3798 - val_acc: 0.8424\n",
      "Epoch 10/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.4133 - acc: 0.8223 - val_loss: 0.3758 - val_acc: 0.8440\n",
      "Epoch 11/300\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.3993 - acc: 0.8196 - val_loss: 0.4071 - val_acc: 0.8259\n",
      "Epoch 12/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3863 - acc: 0.8316 - val_loss: 0.3985 - val_acc: 0.8292\n",
      "Training with parameters {'batch_size': 70, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_42\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_43 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_126 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_127 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_85 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_128 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,985\n",
      "Trainable params: 409,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.6997 - acc: 0.6077 - val_loss: 0.5601 - val_acc: 0.7734\n",
      "Epoch 2/300\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.5672 - acc: 0.7172 - val_loss: 0.4882 - val_acc: 0.8030\n",
      "Epoch 3/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.5264 - acc: 0.7590 - val_loss: 0.4507 - val_acc: 0.8128\n",
      "Epoch 4/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4947 - acc: 0.7760 - val_loss: 0.4308 - val_acc: 0.8243\n",
      "Epoch 5/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4843 - acc: 0.7869 - val_loss: 0.4533 - val_acc: 0.7947\n",
      "Epoch 6/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4671 - acc: 0.7927 - val_loss: 0.4050 - val_acc: 0.8424\n",
      "Epoch 7/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4476 - acc: 0.7993 - val_loss: 0.3980 - val_acc: 0.8456\n",
      "Epoch 8/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4357 - acc: 0.8030 - val_loss: 0.3987 - val_acc: 0.8407\n",
      "Epoch 9/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4401 - acc: 0.8061 - val_loss: 0.4012 - val_acc: 0.8391\n",
      "Epoch 10/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4306 - acc: 0.8097 - val_loss: 0.3874 - val_acc: 0.8473\n",
      "Epoch 11/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4213 - acc: 0.8141 - val_loss: 0.3869 - val_acc: 0.8407\n",
      "Epoch 12/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4143 - acc: 0.8199 - val_loss: 0.3837 - val_acc: 0.8424\n",
      "Epoch 13/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4157 - acc: 0.8181 - val_loss: 0.3826 - val_acc: 0.8456\n",
      "Epoch 14/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4049 - acc: 0.8248 - val_loss: 0.3822 - val_acc: 0.8473\n",
      "Epoch 15/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4020 - acc: 0.8248 - val_loss: 0.3850 - val_acc: 0.8424\n",
      "Epoch 16/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3994 - acc: 0.8296 - val_loss: 0.3821 - val_acc: 0.8456\n",
      "Epoch 17/300\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3973 - acc: 0.8307 - val_loss: 0.3764 - val_acc: 0.8506\n",
      "Epoch 18/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3865 - acc: 0.8349 - val_loss: 0.3873 - val_acc: 0.8440\n",
      "Epoch 19/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3877 - acc: 0.8316 - val_loss: 0.3769 - val_acc: 0.8456\n",
      "Epoch 20/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3889 - acc: 0.8311 - val_loss: 0.3846 - val_acc: 0.8456\n",
      "Epoch 21/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3855 - acc: 0.8378 - val_loss: 0.3996 - val_acc: 0.8440\n",
      "Epoch 22/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3746 - acc: 0.8385 - val_loss: 0.3902 - val_acc: 0.8440\n",
      "Training with parameters {'batch_size': 70, 'dropout': 0.1, 'lay_conf': [1024, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_44 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_129 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_86 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_130 (Dense)            (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_131 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,639,553\n",
      "Trainable params: 1,639,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.5845 - acc: 0.6895 - val_loss: 0.4597 - val_acc: 0.8210\n",
      "Epoch 2/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4955 - acc: 0.7685 - val_loss: 0.4137 - val_acc: 0.8325\n",
      "Epoch 3/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4802 - acc: 0.7761 - val_loss: 0.4008 - val_acc: 0.8309\n",
      "Epoch 4/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4480 - acc: 0.7953 - val_loss: 0.4180 - val_acc: 0.8194\n",
      "Epoch 5/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.4390 - acc: 0.8044 - val_loss: 0.3891 - val_acc: 0.8440\n",
      "Epoch 6/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.4289 - acc: 0.8139 - val_loss: 0.3836 - val_acc: 0.8391\n",
      "Epoch 7/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4192 - acc: 0.8170 - val_loss: 0.3959 - val_acc: 0.8243\n",
      "Epoch 8/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4180 - acc: 0.8135 - val_loss: 0.3961 - val_acc: 0.8358\n",
      "Epoch 9/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4028 - acc: 0.8205 - val_loss: 0.4043 - val_acc: 0.8309\n",
      "Epoch 10/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.4082 - acc: 0.8172 - val_loss: 0.3777 - val_acc: 0.8456\n",
      "Epoch 11/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3945 - acc: 0.8283 - val_loss: 0.3699 - val_acc: 0.8456\n",
      "Epoch 12/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.3912 - acc: 0.8311 - val_loss: 0.4570 - val_acc: 0.7865\n",
      "Epoch 13/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.3929 - acc: 0.8270 - val_loss: 0.3717 - val_acc: 0.8489\n",
      "Epoch 14/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.3821 - acc: 0.8332 - val_loss: 0.3703 - val_acc: 0.8407\n",
      "Epoch 15/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3907 - acc: 0.8270 - val_loss: 0.3694 - val_acc: 0.8489\n",
      "Epoch 16/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.3704 - acc: 0.8374 - val_loss: 0.3687 - val_acc: 0.8473\n",
      "Epoch 17/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.3711 - acc: 0.8391 - val_loss: 0.3664 - val_acc: 0.8489\n",
      "Epoch 18/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3712 - acc: 0.8382 - val_loss: 0.3634 - val_acc: 0.8489\n",
      "Epoch 19/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.3610 - acc: 0.8429 - val_loss: 0.4082 - val_acc: 0.8210\n",
      "Epoch 20/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.3583 - acc: 0.8467 - val_loss: 0.3712 - val_acc: 0.8407\n",
      "Epoch 21/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3492 - acc: 0.8495 - val_loss: 0.3897 - val_acc: 0.8506\n",
      "Epoch 22/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3427 - acc: 0.8508 - val_loss: 0.3683 - val_acc: 0.8489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3491 - acc: 0.8458 - val_loss: 0.3763 - val_acc: 0.8473\n",
      "Training with parameters {'batch_size': 70, 'dropout': 0.1, 'lay_conf': [1024, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_45 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_132 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_89 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,606,721\n",
      "Trainable params: 1,606,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.5922 - acc: 0.6904 - val_loss: 0.4388 - val_acc: 0.8177\n",
      "Epoch 2/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.5034 - acc: 0.7687 - val_loss: 0.5054 - val_acc: 0.7635\n",
      "Epoch 3/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4961 - acc: 0.7732 - val_loss: 0.3952 - val_acc: 0.8407\n",
      "Epoch 4/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4518 - acc: 0.7938 - val_loss: 0.3903 - val_acc: 0.8424\n",
      "Epoch 5/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4468 - acc: 0.7980 - val_loss: 0.3846 - val_acc: 0.8456\n",
      "Epoch 6/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.4354 - acc: 0.8090 - val_loss: 0.3864 - val_acc: 0.8456\n",
      "Epoch 7/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4262 - acc: 0.8130 - val_loss: 0.3839 - val_acc: 0.8506\n",
      "Epoch 8/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.4117 - acc: 0.8148 - val_loss: 0.3705 - val_acc: 0.8522\n",
      "Epoch 9/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4195 - acc: 0.8150 - val_loss: 0.3748 - val_acc: 0.8456\n",
      "Epoch 10/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4140 - acc: 0.8155 - val_loss: 0.3923 - val_acc: 0.8292\n",
      "Epoch 11/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.4028 - acc: 0.8236 - val_loss: 0.3803 - val_acc: 0.8473\n",
      "Epoch 12/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3988 - acc: 0.8283 - val_loss: 0.4052 - val_acc: 0.8227\n",
      "Epoch 13/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.3946 - acc: 0.8298 - val_loss: 0.3642 - val_acc: 0.8588\n",
      "Epoch 14/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3895 - acc: 0.8292 - val_loss: 0.3686 - val_acc: 0.8456\n",
      "Epoch 15/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3835 - acc: 0.8294 - val_loss: 0.3734 - val_acc: 0.8473\n",
      "Epoch 16/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3780 - acc: 0.8362 - val_loss: 0.4059 - val_acc: 0.8243\n",
      "Epoch 17/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.3705 - acc: 0.8363 - val_loss: 0.3671 - val_acc: 0.8473\n",
      "Epoch 18/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.3732 - acc: 0.8374 - val_loss: 0.3773 - val_acc: 0.8424\n",
      "Training with parameters {'batch_size': 70, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_46 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_90 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_91 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,050,113\n",
      "Trainable params: 1,050,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.5853 - acc: 0.6931 - val_loss: 0.5007 - val_acc: 0.7767\n",
      "Epoch 2/300\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.4960 - acc: 0.7770 - val_loss: 0.4384 - val_acc: 0.8128\n",
      "Epoch 3/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4615 - acc: 0.7920 - val_loss: 0.4004 - val_acc: 0.8440\n",
      "Epoch 4/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4416 - acc: 0.7991 - val_loss: 0.3958 - val_acc: 0.8391\n",
      "Epoch 5/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4298 - acc: 0.8062 - val_loss: 0.3928 - val_acc: 0.8473\n",
      "Epoch 6/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4322 - acc: 0.8057 - val_loss: 0.3917 - val_acc: 0.8473\n",
      "Epoch 7/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4115 - acc: 0.8239 - val_loss: 0.3830 - val_acc: 0.8489\n",
      "Epoch 8/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4104 - acc: 0.8157 - val_loss: 0.3764 - val_acc: 0.8473\n",
      "Epoch 9/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4003 - acc: 0.8265 - val_loss: 0.3895 - val_acc: 0.8407\n",
      "Epoch 10/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3936 - acc: 0.8312 - val_loss: 0.3827 - val_acc: 0.8456\n",
      "Epoch 11/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3874 - acc: 0.8294 - val_loss: 0.3820 - val_acc: 0.8424\n",
      "Epoch 12/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3957 - acc: 0.8247 - val_loss: 0.3730 - val_acc: 0.8440\n",
      "Epoch 13/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3740 - acc: 0.8378 - val_loss: 0.3775 - val_acc: 0.8473\n",
      "Epoch 14/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3742 - acc: 0.8351 - val_loss: 0.4286 - val_acc: 0.8144\n",
      "Epoch 15/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3689 - acc: 0.8371 - val_loss: 0.4078 - val_acc: 0.8424\n",
      "Epoch 16/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3672 - acc: 0.8413 - val_loss: 0.3982 - val_acc: 0.8309\n",
      "Epoch 17/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3516 - acc: 0.8480 - val_loss: 0.3781 - val_acc: 0.8456\n",
      "Training with parameters {'batch_size': 70, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_47 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_138 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_92 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_93 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_140 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 401,729\n",
      "Trainable params: 401,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "79/79 [==============================] - 1s 5ms/step - loss: 0.6663 - acc: 0.6068 - val_loss: 0.5663 - val_acc: 0.7586\n",
      "Epoch 2/300\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.5767 - acc: 0.7172 - val_loss: 0.4943 - val_acc: 0.7997\n",
      "Epoch 3/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.5240 - acc: 0.7566 - val_loss: 0.4867 - val_acc: 0.7833\n",
      "Epoch 4/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4942 - acc: 0.7683 - val_loss: 0.4340 - val_acc: 0.8177\n",
      "Epoch 5/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4721 - acc: 0.7889 - val_loss: 0.4394 - val_acc: 0.8062\n",
      "Epoch 6/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4650 - acc: 0.7867 - val_loss: 0.4005 - val_acc: 0.8374\n",
      "Epoch 7/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4501 - acc: 0.7995 - val_loss: 0.4026 - val_acc: 0.8407\n",
      "Epoch 8/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4413 - acc: 0.8081 - val_loss: 0.4014 - val_acc: 0.8358\n",
      "Epoch 9/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4312 - acc: 0.8081 - val_loss: 0.3890 - val_acc: 0.8391\n",
      "Epoch 10/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4298 - acc: 0.8132 - val_loss: 0.3984 - val_acc: 0.8539\n",
      "Epoch 11/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4291 - acc: 0.8170 - val_loss: 0.3956 - val_acc: 0.8309\n",
      "Epoch 12/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4177 - acc: 0.8183 - val_loss: 0.3822 - val_acc: 0.8506\n",
      "Epoch 13/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4164 - acc: 0.8203 - val_loss: 0.3789 - val_acc: 0.8489\n",
      "Epoch 14/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4126 - acc: 0.8214 - val_loss: 0.3760 - val_acc: 0.8489\n",
      "Epoch 15/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4069 - acc: 0.8230 - val_loss: 0.3754 - val_acc: 0.8489\n",
      "Epoch 16/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4017 - acc: 0.8270 - val_loss: 0.3794 - val_acc: 0.8489\n",
      "Epoch 17/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4014 - acc: 0.8265 - val_loss: 0.3963 - val_acc: 0.8276\n",
      "Epoch 18/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3914 - acc: 0.8301 - val_loss: 0.4015 - val_acc: 0.8227\n",
      "Epoch 19/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3948 - acc: 0.8301 - val_loss: 0.3742 - val_acc: 0.8522\n",
      "Epoch 20/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3869 - acc: 0.8332 - val_loss: 0.3714 - val_acc: 0.8539\n",
      "Epoch 21/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3869 - acc: 0.8358 - val_loss: 0.3700 - val_acc: 0.8506\n",
      "Epoch 22/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3797 - acc: 0.8358 - val_loss: 0.3794 - val_acc: 0.8473\n",
      "Epoch 23/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3776 - acc: 0.8393 - val_loss: 0.3753 - val_acc: 0.8489\n",
      "Epoch 24/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3799 - acc: 0.8367 - val_loss: 0.3710 - val_acc: 0.8555\n",
      "Epoch 25/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3780 - acc: 0.8369 - val_loss: 0.3709 - val_acc: 0.8407\n",
      "Epoch 26/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3702 - acc: 0.8433 - val_loss: 0.3795 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 70, 'dropout': 0.1, 'lay_conf': [1024, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_48 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_141 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_94 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_95 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,836,545\n",
      "Trainable params: 1,836,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.6135 - acc: 0.6661 - val_loss: 0.4728 - val_acc: 0.8013\n",
      "Epoch 2/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.5051 - acc: 0.7615 - val_loss: 0.4197 - val_acc: 0.8325\n",
      "Epoch 3/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4693 - acc: 0.7880 - val_loss: 0.4102 - val_acc: 0.8276\n",
      "Epoch 4/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4595 - acc: 0.7893 - val_loss: 0.3939 - val_acc: 0.8424\n",
      "Epoch 5/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.4419 - acc: 0.8004 - val_loss: 0.3836 - val_acc: 0.8489\n",
      "Epoch 6/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.4258 - acc: 0.8079 - val_loss: 0.4065 - val_acc: 0.8342\n",
      "Epoch 7/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.4298 - acc: 0.8066 - val_loss: 0.3770 - val_acc: 0.8506\n",
      "Epoch 8/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4045 - acc: 0.8265 - val_loss: 0.3930 - val_acc: 0.8424\n",
      "Epoch 9/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.4096 - acc: 0.8176 - val_loss: 0.4067 - val_acc: 0.8227\n",
      "Epoch 10/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.4067 - acc: 0.8207 - val_loss: 0.4327 - val_acc: 0.8227\n",
      "Epoch 11/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.3938 - acc: 0.8234 - val_loss: 0.3697 - val_acc: 0.8489\n",
      "Epoch 12/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3905 - acc: 0.8307 - val_loss: 0.3759 - val_acc: 0.8473\n",
      "Epoch 13/300\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.3755 - acc: 0.8360 - val_loss: 0.4061 - val_acc: 0.8210\n",
      "Epoch 14/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3850 - acc: 0.8298 - val_loss: 0.3738 - val_acc: 0.8456\n",
      "Epoch 15/300\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3715 - acc: 0.8404 - val_loss: 0.3952 - val_acc: 0.8424\n",
      "Epoch 16/300\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.3713 - acc: 0.8384 - val_loss: 0.3788 - val_acc: 0.8407\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_49 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_144 (Dense)            (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_96 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_145 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_97 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_146 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 6ms/step - loss: 0.6519 - acc: 0.6158 - val_loss: 0.5707 - val_acc: 0.7389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5814 - acc: 0.7123 - val_loss: 0.5031 - val_acc: 0.7783\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5304 - acc: 0.7524 - val_loss: 0.4642 - val_acc: 0.8177\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5029 - acc: 0.7738 - val_loss: 0.4353 - val_acc: 0.8309\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4832 - acc: 0.7763 - val_loss: 0.4223 - val_acc: 0.8259\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4677 - acc: 0.7871 - val_loss: 0.4083 - val_acc: 0.8342\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4590 - acc: 0.7986 - val_loss: 0.4039 - val_acc: 0.8391\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4490 - acc: 0.8037 - val_loss: 0.4025 - val_acc: 0.8424\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4415 - acc: 0.8062 - val_loss: 0.3949 - val_acc: 0.8391\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4342 - acc: 0.8092 - val_loss: 0.3965 - val_acc: 0.8325\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4367 - acc: 0.8115 - val_loss: 0.3902 - val_acc: 0.8407\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4231 - acc: 0.8134 - val_loss: 0.3883 - val_acc: 0.8391\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4189 - acc: 0.8144 - val_loss: 0.4043 - val_acc: 0.8292\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4137 - acc: 0.8168 - val_loss: 0.3813 - val_acc: 0.8424\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4101 - acc: 0.8165 - val_loss: 0.3933 - val_acc: 0.8391\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4074 - acc: 0.8243 - val_loss: 0.3834 - val_acc: 0.8456\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4058 - acc: 0.8212 - val_loss: 0.3795 - val_acc: 0.8407\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4072 - acc: 0.8243 - val_loss: 0.3797 - val_acc: 0.8506\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4009 - acc: 0.8239 - val_loss: 0.3799 - val_acc: 0.8391\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3971 - acc: 0.8259 - val_loss: 0.3923 - val_acc: 0.8358\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3871 - acc: 0.8290 - val_loss: 0.3978 - val_acc: 0.8325\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3876 - acc: 0.8352 - val_loss: 0.3822 - val_acc: 0.8391\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [1024, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_50 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_147 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_98 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_148 (Dense)            (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_99 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 8ms/step - loss: 0.6346 - acc: 0.6566 - val_loss: 0.5016 - val_acc: 0.8046\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.5198 - acc: 0.7506 - val_loss: 0.4372 - val_acc: 0.8259\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4814 - acc: 0.7789 - val_loss: 0.4363 - val_acc: 0.8161\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4582 - acc: 0.7916 - val_loss: 0.3981 - val_acc: 0.8407\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4430 - acc: 0.8020 - val_loss: 0.4374 - val_acc: 0.8210\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4401 - acc: 0.8073 - val_loss: 0.3835 - val_acc: 0.8424\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4225 - acc: 0.8112 - val_loss: 0.3914 - val_acc: 0.8440\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4173 - acc: 0.8165 - val_loss: 0.3790 - val_acc: 0.8424\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4069 - acc: 0.8199 - val_loss: 0.3809 - val_acc: 0.8539\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4171 - acc: 0.8117 - val_loss: 0.3815 - val_acc: 0.8555\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4145 - acc: 0.8183 - val_loss: 0.3764 - val_acc: 0.8473\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3923 - acc: 0.8285 - val_loss: 0.3953 - val_acc: 0.8342\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3856 - acc: 0.8351 - val_loss: 0.3669 - val_acc: 0.8473\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3820 - acc: 0.8345 - val_loss: 0.3723 - val_acc: 0.8391\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3816 - acc: 0.8300 - val_loss: 0.3962 - val_acc: 0.8473\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3755 - acc: 0.8378 - val_loss: 0.3706 - val_acc: 0.8539\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3607 - acc: 0.8431 - val_loss: 0.3681 - val_acc: 0.8473\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3784 - acc: 0.8312 - val_loss: 0.3733 - val_acc: 0.8456\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_51 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_150 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_100 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_101 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,985\n",
      "Trainable params: 409,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 6ms/step - loss: 0.7285 - acc: 0.5964 - val_loss: 0.5898 - val_acc: 0.7291\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5919 - acc: 0.7004 - val_loss: 0.5169 - val_acc: 0.7865\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5406 - acc: 0.7400 - val_loss: 0.4849 - val_acc: 0.8079\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5106 - acc: 0.7625 - val_loss: 0.4547 - val_acc: 0.8144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4994 - acc: 0.7685 - val_loss: 0.4313 - val_acc: 0.8358\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4727 - acc: 0.7864 - val_loss: 0.4220 - val_acc: 0.8194\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4651 - acc: 0.7955 - val_loss: 0.4157 - val_acc: 0.8292\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4557 - acc: 0.7949 - val_loss: 0.4099 - val_acc: 0.8358\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4444 - acc: 0.8033 - val_loss: 0.4116 - val_acc: 0.8243\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4408 - acc: 0.8031 - val_loss: 0.3951 - val_acc: 0.8424\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4369 - acc: 0.8046 - val_loss: 0.3965 - val_acc: 0.8407\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4308 - acc: 0.8110 - val_loss: 0.3888 - val_acc: 0.8456\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4250 - acc: 0.8174 - val_loss: 0.3901 - val_acc: 0.8440\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4232 - acc: 0.8177 - val_loss: 0.3837 - val_acc: 0.8489\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4113 - acc: 0.8223 - val_loss: 0.3959 - val_acc: 0.8407\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4141 - acc: 0.8248 - val_loss: 0.3844 - val_acc: 0.8456\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4121 - acc: 0.8197 - val_loss: 0.3825 - val_acc: 0.8473\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4057 - acc: 0.8232 - val_loss: 0.3841 - val_acc: 0.8473\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4057 - acc: 0.8267 - val_loss: 0.3862 - val_acc: 0.8440\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4039 - acc: 0.8243 - val_loss: 0.3874 - val_acc: 0.8489\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3945 - acc: 0.8332 - val_loss: 0.3775 - val_acc: 0.8456\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3959 - acc: 0.8278 - val_loss: 0.3768 - val_acc: 0.8456\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3871 - acc: 0.8311 - val_loss: 0.3813 - val_acc: 0.8440\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3856 - acc: 0.8345 - val_loss: 0.3818 - val_acc: 0.8440\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3871 - acc: 0.8342 - val_loss: 0.3731 - val_acc: 0.8489\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3780 - acc: 0.8360 - val_loss: 0.3864 - val_acc: 0.8440\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3818 - acc: 0.8352 - val_loss: 0.3756 - val_acc: 0.8456\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3715 - acc: 0.8413 - val_loss: 0.3762 - val_acc: 0.8489\n",
      "Epoch 29/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3701 - acc: 0.8438 - val_loss: 0.3850 - val_acc: 0.8473\n",
      "Epoch 30/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3686 - acc: 0.8402 - val_loss: 0.3799 - val_acc: 0.8440\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [1024, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_52 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_102 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_154 (Dense)            (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_103 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_155 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,639,553\n",
      "Trainable params: 1,639,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 8ms/step - loss: 0.6177 - acc: 0.6727 - val_loss: 0.4847 - val_acc: 0.8128\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.5247 - acc: 0.7539 - val_loss: 0.4437 - val_acc: 0.8227\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4868 - acc: 0.7712 - val_loss: 0.4099 - val_acc: 0.8506\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4701 - acc: 0.7878 - val_loss: 0.4083 - val_acc: 0.8292\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4486 - acc: 0.8020 - val_loss: 0.3995 - val_acc: 0.8292\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4370 - acc: 0.8051 - val_loss: 0.3779 - val_acc: 0.8506\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4309 - acc: 0.8115 - val_loss: 0.3870 - val_acc: 0.8407\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4472 - acc: 0.7997 - val_loss: 0.3767 - val_acc: 0.8539\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4196 - acc: 0.8179 - val_loss: 0.3752 - val_acc: 0.8506\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4089 - acc: 0.8223 - val_loss: 0.3846 - val_acc: 0.8391\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4064 - acc: 0.8192 - val_loss: 0.3677 - val_acc: 0.8571\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3989 - acc: 0.8243 - val_loss: 0.3979 - val_acc: 0.8342\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4028 - acc: 0.8241 - val_loss: 0.3676 - val_acc: 0.8522\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3843 - acc: 0.8358 - val_loss: 0.3722 - val_acc: 0.8506\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3866 - acc: 0.8342 - val_loss: 0.3895 - val_acc: 0.8440\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3820 - acc: 0.8362 - val_loss: 0.3686 - val_acc: 0.8489\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3801 - acc: 0.8345 - val_loss: 0.3783 - val_acc: 0.8489\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3780 - acc: 0.8362 - val_loss: 0.3630 - val_acc: 0.8506\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3722 - acc: 0.8376 - val_loss: 0.3675 - val_acc: 0.8522\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3642 - acc: 0.8436 - val_loss: 0.3661 - val_acc: 0.8440\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3537 - acc: 0.8488 - val_loss: 0.3634 - val_acc: 0.8522\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3574 - acc: 0.8429 - val_loss: 0.3615 - val_acc: 0.8506\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3476 - acc: 0.8475 - val_loss: 0.3736 - val_acc: 0.8473\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3458 - acc: 0.8491 - val_loss: 0.3714 - val_acc: 0.8539\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3380 - acc: 0.8540 - val_loss: 0.3724 - val_acc: 0.8506\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3336 - acc: 0.8601 - val_loss: 0.3707 - val_acc: 0.8506\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3321 - acc: 0.8570 - val_loss: 0.3746 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [1024, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_53 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_156 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_104 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_157 (Dense)            (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_105 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_158 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,606,721\n",
      "Trainable params: 1,606,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 7ms/step - loss: 0.6146 - acc: 0.6656 - val_loss: 0.4820 - val_acc: 0.8046\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.5082 - acc: 0.7610 - val_loss: 0.4158 - val_acc: 0.8391\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4682 - acc: 0.7876 - val_loss: 0.3927 - val_acc: 0.8407\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4553 - acc: 0.7982 - val_loss: 0.3816 - val_acc: 0.8424\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4477 - acc: 0.7933 - val_loss: 0.3769 - val_acc: 0.8506\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4319 - acc: 0.8073 - val_loss: 0.3947 - val_acc: 0.8309\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4320 - acc: 0.8035 - val_loss: 0.3849 - val_acc: 0.8489\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4182 - acc: 0.8165 - val_loss: 0.3668 - val_acc: 0.8506\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4168 - acc: 0.8154 - val_loss: 0.3746 - val_acc: 0.8473\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4083 - acc: 0.8219 - val_loss: 0.3985 - val_acc: 0.8407\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4040 - acc: 0.8256 - val_loss: 0.3743 - val_acc: 0.8424\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4094 - acc: 0.8219 - val_loss: 0.3647 - val_acc: 0.8506\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3950 - acc: 0.8228 - val_loss: 0.3794 - val_acc: 0.8489\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3961 - acc: 0.8267 - val_loss: 0.3655 - val_acc: 0.8539\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3894 - acc: 0.8261 - val_loss: 0.3951 - val_acc: 0.8456\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3783 - acc: 0.8374 - val_loss: 0.3642 - val_acc: 0.8555\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3814 - acc: 0.8320 - val_loss: 0.3702 - val_acc: 0.8489\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3739 - acc: 0.8365 - val_loss: 0.3687 - val_acc: 0.8539\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3748 - acc: 0.8373 - val_loss: 0.3890 - val_acc: 0.8227\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3612 - acc: 0.8398 - val_loss: 0.3715 - val_acc: 0.8424\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3644 - acc: 0.8415 - val_loss: 0.3699 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_54 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_159 (Dense)            (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_106 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_160 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_107 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,050,113\n",
      "Trainable params: 1,050,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 5ms/step - loss: 0.6233 - acc: 0.6552 - val_loss: 0.5277 - val_acc: 0.7340\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5203 - acc: 0.7552 - val_loss: 0.4488 - val_acc: 0.8227\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.4804 - acc: 0.7833 - val_loss: 0.4120 - val_acc: 0.8342\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.4560 - acc: 0.7977 - val_loss: 0.3971 - val_acc: 0.8489\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.4386 - acc: 0.8062 - val_loss: 0.4001 - val_acc: 0.8325\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4329 - acc: 0.8062 - val_loss: 0.4004 - val_acc: 0.8407\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4291 - acc: 0.8028 - val_loss: 0.4024 - val_acc: 0.8391\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4262 - acc: 0.8108 - val_loss: 0.3810 - val_acc: 0.8506\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4060 - acc: 0.8232 - val_loss: 0.3792 - val_acc: 0.8539\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4001 - acc: 0.8248 - val_loss: 0.3776 - val_acc: 0.8456\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3956 - acc: 0.8250 - val_loss: 0.3879 - val_acc: 0.8456\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3883 - acc: 0.8307 - val_loss: 0.3736 - val_acc: 0.8522\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3930 - acc: 0.8250 - val_loss: 0.3760 - val_acc: 0.8456\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3833 - acc: 0.8307 - val_loss: 0.3723 - val_acc: 0.8555\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3953 - acc: 0.8227 - val_loss: 0.3714 - val_acc: 0.8489\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3664 - acc: 0.8433 - val_loss: 0.3836 - val_acc: 0.8473\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3641 - acc: 0.8360 - val_loss: 0.3684 - val_acc: 0.8440\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3579 - acc: 0.8469 - val_loss: 0.3670 - val_acc: 0.8571\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3600 - acc: 0.8424 - val_loss: 0.3731 - val_acc: 0.8506\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3535 - acc: 0.8478 - val_loss: 0.3916 - val_acc: 0.8506\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3414 - acc: 0.8544 - val_loss: 0.3755 - val_acc: 0.8473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3421 - acc: 0.8517 - val_loss: 0.3746 - val_acc: 0.8473\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3314 - acc: 0.8564 - val_loss: 0.3731 - val_acc: 0.8522\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_55 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_162 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_108 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_163 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_109 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_164 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 401,729\n",
      "Trainable params: 401,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 6ms/step - loss: 0.6736 - acc: 0.6026 - val_loss: 0.5835 - val_acc: 0.7438\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.5910 - acc: 0.7086 - val_loss: 0.5242 - val_acc: 0.7980\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5479 - acc: 0.7433 - val_loss: 0.4886 - val_acc: 0.8030\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5193 - acc: 0.7597 - val_loss: 0.4536 - val_acc: 0.8210\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4971 - acc: 0.7747 - val_loss: 0.4359 - val_acc: 0.8194\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4738 - acc: 0.7871 - val_loss: 0.4198 - val_acc: 0.8292\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4652 - acc: 0.8006 - val_loss: 0.4042 - val_acc: 0.8358\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4560 - acc: 0.7940 - val_loss: 0.4027 - val_acc: 0.8440\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4412 - acc: 0.8068 - val_loss: 0.3928 - val_acc: 0.8358\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4392 - acc: 0.8006 - val_loss: 0.3930 - val_acc: 0.8489\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4329 - acc: 0.8139 - val_loss: 0.3861 - val_acc: 0.8440\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4240 - acc: 0.8161 - val_loss: 0.3817 - val_acc: 0.8539\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4233 - acc: 0.8150 - val_loss: 0.3785 - val_acc: 0.8407\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4221 - acc: 0.8181 - val_loss: 0.3905 - val_acc: 0.8325\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4116 - acc: 0.8185 - val_loss: 0.3803 - val_acc: 0.8440\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4097 - acc: 0.8179 - val_loss: 0.3764 - val_acc: 0.8473\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4083 - acc: 0.8250 - val_loss: 0.3780 - val_acc: 0.8506\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3989 - acc: 0.8285 - val_loss: 0.3718 - val_acc: 0.8489\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3943 - acc: 0.8307 - val_loss: 0.3739 - val_acc: 0.8473\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3947 - acc: 0.8314 - val_loss: 0.3892 - val_acc: 0.8473\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3935 - acc: 0.8305 - val_loss: 0.3864 - val_acc: 0.8342\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3961 - acc: 0.8312 - val_loss: 0.3724 - val_acc: 0.8473\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3938 - acc: 0.8331 - val_loss: 0.3722 - val_acc: 0.8506\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [1024, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_56 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_165 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_110 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_166 (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_111 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_167 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,836,545\n",
      "Trainable params: 1,836,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 8ms/step - loss: 0.6411 - acc: 0.6460 - val_loss: 0.5170 - val_acc: 0.7783\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5150 - acc: 0.7594 - val_loss: 0.4317 - val_acc: 0.8309\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4770 - acc: 0.7827 - val_loss: 0.4414 - val_acc: 0.8128\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4540 - acc: 0.7957 - val_loss: 0.3960 - val_acc: 0.8391\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4409 - acc: 0.8041 - val_loss: 0.4425 - val_acc: 0.8144\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4274 - acc: 0.8123 - val_loss: 0.3813 - val_acc: 0.8440\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4224 - acc: 0.8112 - val_loss: 0.3901 - val_acc: 0.8325\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4066 - acc: 0.8197 - val_loss: 0.3797 - val_acc: 0.8473\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4062 - acc: 0.8236 - val_loss: 0.3767 - val_acc: 0.8506\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3947 - acc: 0.8289 - val_loss: 0.3852 - val_acc: 0.8440\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3992 - acc: 0.8219 - val_loss: 0.3943 - val_acc: 0.8456\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3919 - acc: 0.8305 - val_loss: 0.3753 - val_acc: 0.8489\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3823 - acc: 0.8343 - val_loss: 0.4214 - val_acc: 0.8095\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3789 - acc: 0.8342 - val_loss: 0.4128 - val_acc: 0.8194\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3752 - acc: 0.8387 - val_loss: 0.3898 - val_acc: 0.8374\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3580 - acc: 0.8409 - val_loss: 0.3849 - val_acc: 0.8407\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3744 - acc: 0.8356 - val_loss: 0.3813 - val_acc: 0.8506\n",
      "Training with parameters {'batch_size': 150, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_57 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_168 (Dense)            (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_112 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_169 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_170 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.6754 - acc: 0.5942 - val_loss: 0.6039 - val_acc: 0.7176\n",
      "Epoch 2/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.6083 - acc: 0.6889 - val_loss: 0.5423 - val_acc: 0.7701\n",
      "Epoch 3/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.5647 - acc: 0.7219 - val_loss: 0.4927 - val_acc: 0.8030\n",
      "Epoch 4/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.5276 - acc: 0.7581 - val_loss: 0.4609 - val_acc: 0.8062\n",
      "Epoch 5/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.5006 - acc: 0.7732 - val_loss: 0.4376 - val_acc: 0.8095\n",
      "Epoch 6/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4894 - acc: 0.7752 - val_loss: 0.4245 - val_acc: 0.8309\n",
      "Epoch 7/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4749 - acc: 0.7864 - val_loss: 0.4118 - val_acc: 0.8342\n",
      "Epoch 8/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4609 - acc: 0.7902 - val_loss: 0.4083 - val_acc: 0.8374\n",
      "Epoch 9/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4615 - acc: 0.7862 - val_loss: 0.4000 - val_acc: 0.8424\n",
      "Epoch 10/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4450 - acc: 0.8030 - val_loss: 0.3946 - val_acc: 0.8424\n",
      "Epoch 11/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4390 - acc: 0.8033 - val_loss: 0.3915 - val_acc: 0.8424\n",
      "Epoch 12/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4356 - acc: 0.8081 - val_loss: 0.3899 - val_acc: 0.8440\n",
      "Epoch 13/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4368 - acc: 0.8092 - val_loss: 0.4014 - val_acc: 0.8407\n",
      "Epoch 14/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4283 - acc: 0.8099 - val_loss: 0.3897 - val_acc: 0.8424\n",
      "Epoch 15/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4278 - acc: 0.8103 - val_loss: 0.3891 - val_acc: 0.8440\n",
      "Epoch 16/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4198 - acc: 0.8154 - val_loss: 0.3904 - val_acc: 0.8440\n",
      "Epoch 17/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4237 - acc: 0.8126 - val_loss: 0.3867 - val_acc: 0.8456\n",
      "Epoch 18/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4124 - acc: 0.8201 - val_loss: 0.3961 - val_acc: 0.8292\n",
      "Epoch 19/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4095 - acc: 0.8210 - val_loss: 0.3812 - val_acc: 0.8522\n",
      "Epoch 20/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4009 - acc: 0.8278 - val_loss: 0.3770 - val_acc: 0.8506\n",
      "Epoch 21/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4055 - acc: 0.8216 - val_loss: 0.3834 - val_acc: 0.8424\n",
      "Epoch 22/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4007 - acc: 0.8272 - val_loss: 0.3783 - val_acc: 0.8489\n",
      "Epoch 23/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4002 - acc: 0.8289 - val_loss: 0.3943 - val_acc: 0.8325\n",
      "Epoch 24/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3996 - acc: 0.8241 - val_loss: 0.3817 - val_acc: 0.8391\n",
      "Epoch 25/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3914 - acc: 0.8294 - val_loss: 0.3741 - val_acc: 0.8473\n",
      "Epoch 26/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3891 - acc: 0.8307 - val_loss: 0.3774 - val_acc: 0.8456\n",
      "Epoch 27/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3852 - acc: 0.8336 - val_loss: 0.3760 - val_acc: 0.8489\n",
      "Epoch 28/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3832 - acc: 0.8303 - val_loss: 0.3738 - val_acc: 0.8489\n",
      "Epoch 29/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3781 - acc: 0.8365 - val_loss: 0.3736 - val_acc: 0.8456\n",
      "Epoch 30/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3759 - acc: 0.8387 - val_loss: 0.3781 - val_acc: 0.8424\n",
      "Epoch 31/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3758 - acc: 0.8363 - val_loss: 0.3921 - val_acc: 0.8374\n",
      "Epoch 32/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3787 - acc: 0.8343 - val_loss: 0.4006 - val_acc: 0.8227\n",
      "Epoch 33/300\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.3751 - acc: 0.8378 - val_loss: 0.3724 - val_acc: 0.8489\n",
      "Epoch 34/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3701 - acc: 0.8420 - val_loss: 0.3715 - val_acc: 0.8440\n",
      "Epoch 35/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3647 - acc: 0.8418 - val_loss: 0.3880 - val_acc: 0.8325\n",
      "Epoch 36/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3667 - acc: 0.8404 - val_loss: 0.3714 - val_acc: 0.8489\n",
      "Epoch 37/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3685 - acc: 0.8391 - val_loss: 0.3800 - val_acc: 0.8473\n",
      "Epoch 38/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3599 - acc: 0.8446 - val_loss: 0.3742 - val_acc: 0.8473\n",
      "Epoch 39/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3562 - acc: 0.8508 - val_loss: 0.3744 - val_acc: 0.8456\n",
      "Epoch 40/300\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.3550 - acc: 0.8453 - val_loss: 0.3816 - val_acc: 0.8440\n",
      "Epoch 41/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3459 - acc: 0.8498 - val_loss: 0.3745 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 150, 'dropout': 0.1, 'lay_conf': [1024, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_58 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_171 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_114 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_172 (Dense)            (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_115 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_173 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "37/37 [==============================] - 1s 10ms/step - loss: 0.6834 - acc: 0.6023 - val_loss: 0.5619 - val_acc: 0.7406\n",
      "Epoch 2/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.5580 - acc: 0.7283 - val_loss: 0.4759 - val_acc: 0.8194\n",
      "Epoch 3/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.5043 - acc: 0.7668 - val_loss: 0.4496 - val_acc: 0.8177\n",
      "Epoch 4/300\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.4935 - acc: 0.7692 - val_loss: 0.4310 - val_acc: 0.8210\n",
      "Epoch 5/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4689 - acc: 0.7895 - val_loss: 0.4055 - val_acc: 0.8424\n",
      "Epoch 6/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4466 - acc: 0.8061 - val_loss: 0.3973 - val_acc: 0.8325\n",
      "Epoch 7/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4335 - acc: 0.8055 - val_loss: 0.3875 - val_acc: 0.8391\n",
      "Epoch 8/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4255 - acc: 0.8150 - val_loss: 0.3967 - val_acc: 0.8407\n",
      "Epoch 9/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4261 - acc: 0.8090 - val_loss: 0.3901 - val_acc: 0.8539\n",
      "Epoch 10/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4166 - acc: 0.8106 - val_loss: 0.3802 - val_acc: 0.8522\n",
      "Epoch 11/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4163 - acc: 0.8172 - val_loss: 0.3753 - val_acc: 0.8456\n",
      "Epoch 12/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4030 - acc: 0.8223 - val_loss: 0.3778 - val_acc: 0.8489\n",
      "Epoch 13/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3977 - acc: 0.8263 - val_loss: 0.3714 - val_acc: 0.8489\n",
      "Epoch 14/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3948 - acc: 0.8227 - val_loss: 0.3903 - val_acc: 0.8424\n",
      "Epoch 15/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3836 - acc: 0.8316 - val_loss: 0.3765 - val_acc: 0.8506\n",
      "Epoch 16/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3853 - acc: 0.8320 - val_loss: 0.3718 - val_acc: 0.8456\n",
      "Epoch 17/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3745 - acc: 0.8352 - val_loss: 0.3805 - val_acc: 0.8473\n",
      "Epoch 18/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3843 - acc: 0.8338 - val_loss: 0.3852 - val_acc: 0.8440\n",
      "Training with parameters {'batch_size': 150, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_59 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_174 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_116 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_175 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_117 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_176 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,985\n",
      "Trainable params: 409,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "37/37 [==============================] - 1s 7ms/step - loss: 0.7698 - acc: 0.5614 - val_loss: 0.6199 - val_acc: 0.6174\n",
      "Epoch 2/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.6161 - acc: 0.6714 - val_loss: 0.5499 - val_acc: 0.7488\n",
      "Epoch 3/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.5700 - acc: 0.7174 - val_loss: 0.5011 - val_acc: 0.8062\n",
      "Epoch 4/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.5447 - acc: 0.7515 - val_loss: 0.4773 - val_acc: 0.8013\n",
      "Epoch 5/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.5191 - acc: 0.7566 - val_loss: 0.4593 - val_acc: 0.8194\n",
      "Epoch 6/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4959 - acc: 0.7716 - val_loss: 0.4376 - val_acc: 0.8259\n",
      "Epoch 7/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4827 - acc: 0.7801 - val_loss: 0.4268 - val_acc: 0.8276\n",
      "Epoch 8/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4685 - acc: 0.7944 - val_loss: 0.4171 - val_acc: 0.8358\n",
      "Epoch 9/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4652 - acc: 0.7913 - val_loss: 0.4138 - val_acc: 0.8391\n",
      "Epoch 10/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4563 - acc: 0.7971 - val_loss: 0.4033 - val_acc: 0.8440\n",
      "Epoch 11/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4462 - acc: 0.7993 - val_loss: 0.3991 - val_acc: 0.8440\n",
      "Epoch 12/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4357 - acc: 0.8108 - val_loss: 0.3957 - val_acc: 0.8407\n",
      "Epoch 13/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4361 - acc: 0.8070 - val_loss: 0.3914 - val_acc: 0.8456\n",
      "Epoch 14/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4311 - acc: 0.8090 - val_loss: 0.3896 - val_acc: 0.8424\n",
      "Epoch 15/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4319 - acc: 0.8061 - val_loss: 0.3936 - val_acc: 0.8407\n",
      "Epoch 16/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4304 - acc: 0.8103 - val_loss: 0.3953 - val_acc: 0.8374\n",
      "Epoch 17/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4324 - acc: 0.8073 - val_loss: 0.3855 - val_acc: 0.8424\n",
      "Epoch 18/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4164 - acc: 0.8186 - val_loss: 0.3826 - val_acc: 0.8456\n",
      "Epoch 19/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4117 - acc: 0.8192 - val_loss: 0.3805 - val_acc: 0.8440\n",
      "Epoch 20/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4039 - acc: 0.8201 - val_loss: 0.3789 - val_acc: 0.8489\n",
      "Epoch 21/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4053 - acc: 0.8234 - val_loss: 0.3789 - val_acc: 0.8489\n",
      "Epoch 22/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4013 - acc: 0.8243 - val_loss: 0.3792 - val_acc: 0.8440\n",
      "Epoch 23/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3991 - acc: 0.8280 - val_loss: 0.3798 - val_acc: 0.8456\n",
      "Epoch 24/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3948 - acc: 0.8292 - val_loss: 0.3796 - val_acc: 0.8473\n",
      "Epoch 25/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3938 - acc: 0.8281 - val_loss: 0.3770 - val_acc: 0.8473\n",
      "Epoch 26/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3906 - acc: 0.8287 - val_loss: 0.3828 - val_acc: 0.8424\n",
      "Epoch 27/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3888 - acc: 0.8356 - val_loss: 0.3741 - val_acc: 0.8522\n",
      "Epoch 28/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3870 - acc: 0.8332 - val_loss: 0.3858 - val_acc: 0.8473\n",
      "Epoch 29/300\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.3859 - acc: 0.8318 - val_loss: 0.3780 - val_acc: 0.8489\n",
      "Epoch 30/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3814 - acc: 0.8363 - val_loss: 0.3767 - val_acc: 0.8456\n",
      "Epoch 31/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3762 - acc: 0.8365 - val_loss: 0.3750 - val_acc: 0.8473\n",
      "Epoch 32/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3761 - acc: 0.8378 - val_loss: 0.3728 - val_acc: 0.8489\n",
      "Epoch 33/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3713 - acc: 0.8402 - val_loss: 0.3735 - val_acc: 0.8489\n",
      "Epoch 34/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3727 - acc: 0.8374 - val_loss: 0.3845 - val_acc: 0.8424\n",
      "Epoch 35/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3757 - acc: 0.8393 - val_loss: 0.3832 - val_acc: 0.8424\n",
      "Epoch 36/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3691 - acc: 0.8444 - val_loss: 0.3738 - val_acc: 0.8489\n",
      "Epoch 37/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3590 - acc: 0.8460 - val_loss: 0.3766 - val_acc: 0.8440\n",
      "Training with parameters {'batch_size': 150, 'dropout': 0.1, 'lay_conf': [1024, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_60 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_177 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_118 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_178 (Dense)            (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_119 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_179 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,639,553\n",
      "Trainable params: 1,639,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "37/37 [==============================] - 1s 10ms/step - loss: 0.6450 - acc: 0.6358 - val_loss: 0.5267 - val_acc: 0.7734\n",
      "Epoch 2/300\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.5354 - acc: 0.7411 - val_loss: 0.4462 - val_acc: 0.8309\n",
      "Epoch 3/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4958 - acc: 0.7674 - val_loss: 0.4629 - val_acc: 0.7997\n",
      "Epoch 4/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4652 - acc: 0.7904 - val_loss: 0.4028 - val_acc: 0.8374\n",
      "Epoch 5/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4544 - acc: 0.7944 - val_loss: 0.4157 - val_acc: 0.8243\n",
      "Epoch 6/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4522 - acc: 0.7971 - val_loss: 0.4204 - val_acc: 0.8276\n",
      "Epoch 7/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4374 - acc: 0.8062 - val_loss: 0.4124 - val_acc: 0.8276\n",
      "Epoch 8/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4348 - acc: 0.8041 - val_loss: 0.3918 - val_acc: 0.8325\n",
      "Epoch 9/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4155 - acc: 0.8177 - val_loss: 0.3753 - val_acc: 0.8522\n",
      "Epoch 10/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4182 - acc: 0.8205 - val_loss: 0.3819 - val_acc: 0.8489\n",
      "Epoch 11/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4182 - acc: 0.8132 - val_loss: 0.3825 - val_acc: 0.8473\n",
      "Epoch 12/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4140 - acc: 0.8144 - val_loss: 0.3721 - val_acc: 0.8539\n",
      "Epoch 13/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4046 - acc: 0.8252 - val_loss: 0.4181 - val_acc: 0.8144\n",
      "Epoch 14/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4095 - acc: 0.8219 - val_loss: 0.3699 - val_acc: 0.8522\n",
      "Epoch 15/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3997 - acc: 0.8248 - val_loss: 0.3686 - val_acc: 0.8473\n",
      "Epoch 16/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3863 - acc: 0.8303 - val_loss: 0.3686 - val_acc: 0.8506\n",
      "Epoch 17/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3845 - acc: 0.8332 - val_loss: 0.3676 - val_acc: 0.8489\n",
      "Epoch 18/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3877 - acc: 0.8342 - val_loss: 0.3778 - val_acc: 0.8489\n",
      "Epoch 19/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3789 - acc: 0.8345 - val_loss: 0.3695 - val_acc: 0.8489\n",
      "Epoch 20/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3784 - acc: 0.8336 - val_loss: 0.3721 - val_acc: 0.8539\n",
      "Epoch 21/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3657 - acc: 0.8404 - val_loss: 0.3907 - val_acc: 0.8456\n",
      "Epoch 22/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3842 - acc: 0.8294 - val_loss: 0.3632 - val_acc: 0.8489\n",
      "Epoch 23/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3649 - acc: 0.8398 - val_loss: 0.4267 - val_acc: 0.8276\n",
      "Epoch 24/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3724 - acc: 0.8363 - val_loss: 0.3751 - val_acc: 0.8506\n",
      "Epoch 25/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3510 - acc: 0.8471 - val_loss: 0.3695 - val_acc: 0.8522\n",
      "Epoch 26/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3607 - acc: 0.8433 - val_loss: 0.3738 - val_acc: 0.8440\n",
      "Epoch 27/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3533 - acc: 0.8431 - val_loss: 0.3894 - val_acc: 0.8325\n",
      "Training with parameters {'batch_size': 150, 'dropout': 0.1, 'lay_conf': [1024, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_60\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_61 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_180 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_120 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_181 (Dense)            (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_121 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_182 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,606,721\n",
      "Trainable params: 1,606,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "37/37 [==============================] - 1s 9ms/step - loss: 0.6426 - acc: 0.6400 - val_loss: 0.4960 - val_acc: 0.8013\n",
      "Epoch 2/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.5268 - acc: 0.7510 - val_loss: 0.4338 - val_acc: 0.8259\n",
      "Epoch 3/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4955 - acc: 0.7665 - val_loss: 0.4139 - val_acc: 0.8325\n",
      "Epoch 4/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4667 - acc: 0.7904 - val_loss: 0.4024 - val_acc: 0.8325\n",
      "Epoch 5/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4594 - acc: 0.7916 - val_loss: 0.4048 - val_acc: 0.8309\n",
      "Epoch 6/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4570 - acc: 0.7940 - val_loss: 0.4101 - val_acc: 0.8325\n",
      "Epoch 7/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4463 - acc: 0.7988 - val_loss: 0.3912 - val_acc: 0.8473\n",
      "Epoch 8/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4258 - acc: 0.8161 - val_loss: 0.3771 - val_acc: 0.8489\n",
      "Epoch 9/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4207 - acc: 0.8157 - val_loss: 0.3738 - val_acc: 0.8571\n",
      "Epoch 10/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4141 - acc: 0.8217 - val_loss: 0.3693 - val_acc: 0.8506\n",
      "Epoch 11/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4077 - acc: 0.8177 - val_loss: 0.3720 - val_acc: 0.8506\n",
      "Epoch 12/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4084 - acc: 0.8223 - val_loss: 0.3779 - val_acc: 0.8506\n",
      "Epoch 13/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3998 - acc: 0.8234 - val_loss: 0.3658 - val_acc: 0.8539\n",
      "Epoch 14/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3948 - acc: 0.8301 - val_loss: 0.3674 - val_acc: 0.8489\n",
      "Epoch 15/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3897 - acc: 0.8320 - val_loss: 0.3663 - val_acc: 0.8506\n",
      "Epoch 16/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3904 - acc: 0.8331 - val_loss: 0.3721 - val_acc: 0.8489\n",
      "Epoch 17/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3868 - acc: 0.8327 - val_loss: 0.3755 - val_acc: 0.8555\n",
      "Epoch 18/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3804 - acc: 0.8351 - val_loss: 0.3637 - val_acc: 0.8473\n",
      "Epoch 19/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3724 - acc: 0.8396 - val_loss: 0.3666 - val_acc: 0.8489\n",
      "Epoch 20/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3684 - acc: 0.8455 - val_loss: 0.3667 - val_acc: 0.8522\n",
      "Epoch 21/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3692 - acc: 0.8360 - val_loss: 0.3826 - val_acc: 0.8473\n",
      "Epoch 22/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3684 - acc: 0.8389 - val_loss: 0.3734 - val_acc: 0.8489\n",
      "Epoch 23/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3671 - acc: 0.8396 - val_loss: 0.3692 - val_acc: 0.8506\n",
      "Training with parameters {'batch_size': 150, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_61\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_62 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_183 (Dense)            (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_122 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_184 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_123 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_185 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,050,113\n",
      "Trainable params: 1,050,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "37/37 [==============================] - 1s 9ms/step - loss: 0.6416 - acc: 0.6347 - val_loss: 0.5302 - val_acc: 0.7882\n",
      "Epoch 2/300\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.5411 - acc: 0.7398 - val_loss: 0.4578 - val_acc: 0.8177\n",
      "Epoch 3/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4967 - acc: 0.7719 - val_loss: 0.4236 - val_acc: 0.8292\n",
      "Epoch 4/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4814 - acc: 0.7792 - val_loss: 0.4242 - val_acc: 0.8210\n",
      "Epoch 5/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4604 - acc: 0.7955 - val_loss: 0.4029 - val_acc: 0.8424\n",
      "Epoch 6/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4485 - acc: 0.8019 - val_loss: 0.3957 - val_acc: 0.8358\n",
      "Epoch 7/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4321 - acc: 0.8082 - val_loss: 0.3954 - val_acc: 0.8456\n",
      "Epoch 8/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4205 - acc: 0.8168 - val_loss: 0.3921 - val_acc: 0.8440\n",
      "Epoch 9/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4180 - acc: 0.8172 - val_loss: 0.3802 - val_acc: 0.8522\n",
      "Epoch 10/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4101 - acc: 0.8166 - val_loss: 0.3830 - val_acc: 0.8456\n",
      "Epoch 11/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4135 - acc: 0.8201 - val_loss: 0.3816 - val_acc: 0.8539\n",
      "Epoch 12/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4058 - acc: 0.8230 - val_loss: 0.4038 - val_acc: 0.8227\n",
      "Epoch 13/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3935 - acc: 0.8272 - val_loss: 0.3782 - val_acc: 0.8571\n",
      "Epoch 14/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3901 - acc: 0.8305 - val_loss: 0.3710 - val_acc: 0.8539\n",
      "Epoch 15/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3802 - acc: 0.8340 - val_loss: 0.3735 - val_acc: 0.8456\n",
      "Epoch 16/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3770 - acc: 0.8356 - val_loss: 0.3950 - val_acc: 0.8325\n",
      "Epoch 17/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3819 - acc: 0.8307 - val_loss: 0.3729 - val_acc: 0.8456\n",
      "Epoch 18/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3642 - acc: 0.8400 - val_loss: 0.3736 - val_acc: 0.8473\n",
      "Epoch 19/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3758 - acc: 0.8358 - val_loss: 0.3701 - val_acc: 0.8588\n",
      "Epoch 20/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3614 - acc: 0.8431 - val_loss: 0.3737 - val_acc: 0.8588\n",
      "Epoch 21/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3573 - acc: 0.8456 - val_loss: 0.3716 - val_acc: 0.8522\n",
      "Epoch 22/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3489 - acc: 0.8467 - val_loss: 0.3955 - val_acc: 0.8506\n",
      "Epoch 23/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3440 - acc: 0.8517 - val_loss: 0.3779 - val_acc: 0.8424\n",
      "Epoch 24/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3390 - acc: 0.8564 - val_loss: 0.3823 - val_acc: 0.8473\n",
      "Training with parameters {'batch_size': 150, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_62\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_63 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_186 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_124 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_187 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_125 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_188 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 401,729\n",
      "Trainable params: 401,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "37/37 [==============================] - 1s 8ms/step - loss: 0.6974 - acc: 0.5703 - val_loss: 0.6163 - val_acc: 0.6962\n",
      "Epoch 2/300\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.6152 - acc: 0.6723 - val_loss: 0.5485 - val_acc: 0.7603\n",
      "Epoch 3/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.5632 - acc: 0.7323 - val_loss: 0.5098 - val_acc: 0.7750\n",
      "Epoch 4/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.5421 - acc: 0.7398 - val_loss: 0.4716 - val_acc: 0.8112\n",
      "Epoch 5/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.5107 - acc: 0.7661 - val_loss: 0.4512 - val_acc: 0.8177\n",
      "Epoch 6/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4917 - acc: 0.7749 - val_loss: 0.4297 - val_acc: 0.8292\n",
      "Epoch 7/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4786 - acc: 0.7851 - val_loss: 0.4318 - val_acc: 0.8259\n",
      "Epoch 8/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4651 - acc: 0.7920 - val_loss: 0.4126 - val_acc: 0.8309\n",
      "Epoch 9/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4636 - acc: 0.7958 - val_loss: 0.4201 - val_acc: 0.8227\n",
      "Epoch 10/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4505 - acc: 0.8002 - val_loss: 0.3982 - val_acc: 0.8424\n",
      "Epoch 11/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4466 - acc: 0.8057 - val_loss: 0.3964 - val_acc: 0.8391\n",
      "Epoch 12/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4386 - acc: 0.8064 - val_loss: 0.3940 - val_acc: 0.8456\n",
      "Epoch 13/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4286 - acc: 0.8123 - val_loss: 0.3970 - val_acc: 0.8374\n",
      "Epoch 14/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4297 - acc: 0.8103 - val_loss: 0.3948 - val_acc: 0.8456\n",
      "Epoch 15/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4419 - acc: 0.8055 - val_loss: 0.3879 - val_acc: 0.8407\n",
      "Epoch 16/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4302 - acc: 0.8055 - val_loss: 0.3882 - val_acc: 0.8539\n",
      "Epoch 17/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4163 - acc: 0.8201 - val_loss: 0.3816 - val_acc: 0.8440\n",
      "Epoch 18/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4135 - acc: 0.8201 - val_loss: 0.3826 - val_acc: 0.8456\n",
      "Epoch 19/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4085 - acc: 0.8221 - val_loss: 0.3857 - val_acc: 0.8407\n",
      "Epoch 20/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4076 - acc: 0.8207 - val_loss: 0.3832 - val_acc: 0.8456\n",
      "Epoch 21/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4016 - acc: 0.8283 - val_loss: 0.3782 - val_acc: 0.8473\n",
      "Epoch 22/300\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.4031 - acc: 0.8294 - val_loss: 0.3814 - val_acc: 0.8489\n",
      "Epoch 23/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3995 - acc: 0.8294 - val_loss: 0.3864 - val_acc: 0.8522\n",
      "Epoch 24/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3927 - acc: 0.8309 - val_loss: 0.3797 - val_acc: 0.8440\n",
      "Epoch 25/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3925 - acc: 0.8274 - val_loss: 0.3801 - val_acc: 0.8440\n",
      "Epoch 26/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3893 - acc: 0.8311 - val_loss: 0.3756 - val_acc: 0.8440\n",
      "Epoch 27/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3952 - acc: 0.8296 - val_loss: 0.3817 - val_acc: 0.8456\n",
      "Epoch 28/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3886 - acc: 0.8289 - val_loss: 0.3775 - val_acc: 0.8424\n",
      "Epoch 29/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3822 - acc: 0.8351 - val_loss: 0.3783 - val_acc: 0.8424\n",
      "Epoch 30/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3815 - acc: 0.8349 - val_loss: 0.3762 - val_acc: 0.8424\n",
      "Epoch 31/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.3754 - acc: 0.8349 - val_loss: 0.3760 - val_acc: 0.8440\n",
      "Training with parameters {'batch_size': 150, 'dropout': 0.1, 'lay_conf': [1024, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_64 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_189 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_126 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_190 (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_127 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_191 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,836,545\n",
      "Trainable params: 1,836,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "37/37 [==============================] - 1s 10ms/step - loss: 0.6503 - acc: 0.6439 - val_loss: 0.5487 - val_acc: 0.7291\n",
      "Epoch 2/300\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.5416 - acc: 0.7422 - val_loss: 0.4517 - val_acc: 0.8161\n",
      "Epoch 3/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4906 - acc: 0.7754 - val_loss: 0.4418 - val_acc: 0.8144\n",
      "Epoch 4/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4666 - acc: 0.7929 - val_loss: 0.4153 - val_acc: 0.8342\n",
      "Epoch 5/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4647 - acc: 0.7895 - val_loss: 0.3959 - val_acc: 0.8456\n",
      "Epoch 6/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4445 - acc: 0.7982 - val_loss: 0.4153 - val_acc: 0.8210\n",
      "Epoch 7/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4326 - acc: 0.8088 - val_loss: 0.3822 - val_acc: 0.8539\n",
      "Epoch 8/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4211 - acc: 0.8095 - val_loss: 0.3807 - val_acc: 0.8489\n",
      "Epoch 9/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4191 - acc: 0.8097 - val_loss: 0.4847 - val_acc: 0.7471\n",
      "Epoch 10/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4217 - acc: 0.8115 - val_loss: 0.3842 - val_acc: 0.8391\n",
      "Epoch 11/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4093 - acc: 0.8203 - val_loss: 0.3756 - val_acc: 0.8456\n",
      "Epoch 12/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4060 - acc: 0.8223 - val_loss: 0.3821 - val_acc: 0.8407\n",
      "Epoch 13/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3918 - acc: 0.8307 - val_loss: 0.3679 - val_acc: 0.8506\n",
      "Epoch 14/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3851 - acc: 0.8314 - val_loss: 0.3794 - val_acc: 0.8473\n",
      "Epoch 15/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3861 - acc: 0.8336 - val_loss: 0.4111 - val_acc: 0.8374\n",
      "Epoch 16/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.4009 - acc: 0.8208 - val_loss: 0.3826 - val_acc: 0.8374\n",
      "Epoch 17/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3778 - acc: 0.8374 - val_loss: 0.3757 - val_acc: 0.8473\n",
      "Epoch 18/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3694 - acc: 0.8398 - val_loss: 0.3678 - val_acc: 0.8456\n",
      "Epoch 19/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3599 - acc: 0.8449 - val_loss: 0.3695 - val_acc: 0.8440\n",
      "Epoch 20/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3580 - acc: 0.8427 - val_loss: 0.3694 - val_acc: 0.8473\n",
      "Epoch 21/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3614 - acc: 0.8447 - val_loss: 0.3704 - val_acc: 0.8407\n",
      "Epoch 22/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3540 - acc: 0.8471 - val_loss: 0.3720 - val_acc: 0.8456\n",
      "Epoch 23/300\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.3383 - acc: 0.8540 - val_loss: 0.3760 - val_acc: 0.8424\n",
      "Training with parameters {'batch_size': 200, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_64\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_65 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_192 (Dense)            (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_128 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_193 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_129 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_194 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.6824 - acc: 0.5789 - val_loss: 0.6191 - val_acc: 0.6929\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6201 - acc: 0.6694 - val_loss: 0.5589 - val_acc: 0.7553\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.5797 - acc: 0.7079 - val_loss: 0.5140 - val_acc: 0.7800\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5423 - acc: 0.7473 - val_loss: 0.4771 - val_acc: 0.8095\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.5157 - acc: 0.7606 - val_loss: 0.4583 - val_acc: 0.8194\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4965 - acc: 0.7760 - val_loss: 0.4449 - val_acc: 0.8144\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4902 - acc: 0.7772 - val_loss: 0.4246 - val_acc: 0.8276\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4689 - acc: 0.7907 - val_loss: 0.4149 - val_acc: 0.8292\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4638 - acc: 0.7962 - val_loss: 0.4103 - val_acc: 0.8424\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4533 - acc: 0.7993 - val_loss: 0.4052 - val_acc: 0.8440\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4482 - acc: 0.8017 - val_loss: 0.3979 - val_acc: 0.8391\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4394 - acc: 0.8121 - val_loss: 0.3958 - val_acc: 0.8440\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4394 - acc: 0.8130 - val_loss: 0.3931 - val_acc: 0.8424\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4321 - acc: 0.8099 - val_loss: 0.4026 - val_acc: 0.8374\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4288 - acc: 0.8132 - val_loss: 0.3907 - val_acc: 0.8424\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4222 - acc: 0.8132 - val_loss: 0.3993 - val_acc: 0.8325\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4233 - acc: 0.8157 - val_loss: 0.3847 - val_acc: 0.8473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4165 - acc: 0.8227 - val_loss: 0.3861 - val_acc: 0.8473\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4160 - acc: 0.8190 - val_loss: 0.3829 - val_acc: 0.8440\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4177 - acc: 0.8208 - val_loss: 0.3884 - val_acc: 0.8424\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4088 - acc: 0.8197 - val_loss: 0.3826 - val_acc: 0.8440\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4066 - acc: 0.8243 - val_loss: 0.3812 - val_acc: 0.8473\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4009 - acc: 0.8256 - val_loss: 0.3798 - val_acc: 0.8473\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3974 - acc: 0.8241 - val_loss: 0.3792 - val_acc: 0.8473\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3977 - acc: 0.8316 - val_loss: 0.3806 - val_acc: 0.8522\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3970 - acc: 0.8259 - val_loss: 0.3761 - val_acc: 0.8473\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3928 - acc: 0.8325 - val_loss: 0.3759 - val_acc: 0.8424\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3913 - acc: 0.8276 - val_loss: 0.3833 - val_acc: 0.8391\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3891 - acc: 0.8312 - val_loss: 0.3797 - val_acc: 0.8424\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3924 - acc: 0.8274 - val_loss: 0.3735 - val_acc: 0.8539\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3853 - acc: 0.8311 - val_loss: 0.3740 - val_acc: 0.8522\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3833 - acc: 0.8351 - val_loss: 0.3753 - val_acc: 0.8456\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3835 - acc: 0.8331 - val_loss: 0.3721 - val_acc: 0.8440\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3767 - acc: 0.8387 - val_loss: 0.3783 - val_acc: 0.8424\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3759 - acc: 0.8351 - val_loss: 0.3729 - val_acc: 0.8489\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3720 - acc: 0.8409 - val_loss: 0.3735 - val_acc: 0.8473\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3705 - acc: 0.8389 - val_loss: 0.3748 - val_acc: 0.8407\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3659 - acc: 0.8480 - val_loss: 0.3849 - val_acc: 0.8407\n",
      "Training with parameters {'batch_size': 200, 'dropout': 0.1, 'lay_conf': [1024, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_65\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_66 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_195 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_130 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_196 (Dense)            (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_131 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_197 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "28/28 [==============================] - 1s 12ms/step - loss: 0.6592 - acc: 0.6209 - val_loss: 0.5538 - val_acc: 0.7225\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.5493 - acc: 0.7340 - val_loss: 0.4614 - val_acc: 0.8227\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4969 - acc: 0.7688 - val_loss: 0.4250 - val_acc: 0.8374\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4711 - acc: 0.7836 - val_loss: 0.4112 - val_acc: 0.8309\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4636 - acc: 0.7889 - val_loss: 0.4922 - val_acc: 0.7488\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4573 - acc: 0.7935 - val_loss: 0.3897 - val_acc: 0.8473\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4452 - acc: 0.7986 - val_loss: 0.3950 - val_acc: 0.8342\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.4297 - acc: 0.8070 - val_loss: 0.3919 - val_acc: 0.8407\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4223 - acc: 0.8123 - val_loss: 0.3828 - val_acc: 0.8407\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4146 - acc: 0.8141 - val_loss: 0.3797 - val_acc: 0.8473\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.4105 - acc: 0.8201 - val_loss: 0.3764 - val_acc: 0.8407\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4063 - acc: 0.8225 - val_loss: 0.3748 - val_acc: 0.8489\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4090 - acc: 0.8223 - val_loss: 0.3919 - val_acc: 0.8407\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.3929 - acc: 0.8294 - val_loss: 0.3735 - val_acc: 0.8506\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3899 - acc: 0.8272 - val_loss: 0.3911 - val_acc: 0.8342\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3872 - acc: 0.8292 - val_loss: 0.3867 - val_acc: 0.8407\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3841 - acc: 0.8345 - val_loss: 0.3730 - val_acc: 0.8489\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3787 - acc: 0.8354 - val_loss: 0.3856 - val_acc: 0.8407\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3745 - acc: 0.8394 - val_loss: 0.3687 - val_acc: 0.8473\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.3639 - acc: 0.8411 - val_loss: 0.3692 - val_acc: 0.8456\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3594 - acc: 0.8415 - val_loss: 0.3704 - val_acc: 0.8473\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3571 - acc: 0.8473 - val_loss: 0.3915 - val_acc: 0.8358\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3580 - acc: 0.8475 - val_loss: 0.3640 - val_acc: 0.8506\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3435 - acc: 0.8519 - val_loss: 0.4217 - val_acc: 0.8013\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3456 - acc: 0.8553 - val_loss: 0.3742 - val_acc: 0.8456\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3428 - acc: 0.8551 - val_loss: 0.3690 - val_acc: 0.8489\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3411 - acc: 0.8502 - val_loss: 0.3794 - val_acc: 0.8473\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3254 - acc: 0.8604 - val_loss: 0.3762 - val_acc: 0.8440\n",
      "Training with parameters {'batch_size': 200, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_66\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_67 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_198 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_132 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_199 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_133 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_200 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,985\n",
      "Trainable params: 409,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "28/28 [==============================] - 1s 9ms/step - loss: 0.8163 - acc: 0.5408 - val_loss: 0.6451 - val_acc: 0.6995\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6457 - acc: 0.6331 - val_loss: 0.5828 - val_acc: 0.7586\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.6044 - acc: 0.6871 - val_loss: 0.5369 - val_acc: 0.7865\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.5726 - acc: 0.7121 - val_loss: 0.5072 - val_acc: 0.8079\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5424 - acc: 0.7460 - val_loss: 0.4828 - val_acc: 0.8030\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.5225 - acc: 0.7568 - val_loss: 0.4600 - val_acc: 0.8095\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.5033 - acc: 0.7696 - val_loss: 0.4443 - val_acc: 0.8276\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4950 - acc: 0.7760 - val_loss: 0.4335 - val_acc: 0.8276\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4738 - acc: 0.7818 - val_loss: 0.4248 - val_acc: 0.8259\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4757 - acc: 0.7849 - val_loss: 0.4230 - val_acc: 0.8227\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4681 - acc: 0.7885 - val_loss: 0.4135 - val_acc: 0.8342\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4536 - acc: 0.7953 - val_loss: 0.4051 - val_acc: 0.8407\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4481 - acc: 0.7958 - val_loss: 0.4078 - val_acc: 0.8325\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4485 - acc: 0.7975 - val_loss: 0.4013 - val_acc: 0.8407\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4403 - acc: 0.8039 - val_loss: 0.4075 - val_acc: 0.8309\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4367 - acc: 0.8081 - val_loss: 0.3976 - val_acc: 0.8391\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4334 - acc: 0.8106 - val_loss: 0.3993 - val_acc: 0.8391\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4291 - acc: 0.8117 - val_loss: 0.3983 - val_acc: 0.8407\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4231 - acc: 0.8163 - val_loss: 0.3947 - val_acc: 0.8473\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4192 - acc: 0.8159 - val_loss: 0.3906 - val_acc: 0.8440\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4177 - acc: 0.8152 - val_loss: 0.3878 - val_acc: 0.8440\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4216 - acc: 0.8174 - val_loss: 0.3900 - val_acc: 0.8473\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4133 - acc: 0.8230 - val_loss: 0.3873 - val_acc: 0.8440\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4110 - acc: 0.8227 - val_loss: 0.3857 - val_acc: 0.8456\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4047 - acc: 0.8252 - val_loss: 0.3848 - val_acc: 0.8456\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4006 - acc: 0.8280 - val_loss: 0.3836 - val_acc: 0.8456\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3978 - acc: 0.8248 - val_loss: 0.3816 - val_acc: 0.8456\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3971 - acc: 0.8298 - val_loss: 0.3872 - val_acc: 0.8440\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3929 - acc: 0.8311 - val_loss: 0.3872 - val_acc: 0.8489\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3926 - acc: 0.8292 - val_loss: 0.3811 - val_acc: 0.8473\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3924 - acc: 0.8289 - val_loss: 0.3834 - val_acc: 0.8440\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3867 - acc: 0.8343 - val_loss: 0.3809 - val_acc: 0.8489\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3867 - acc: 0.8323 - val_loss: 0.3809 - val_acc: 0.8489\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3843 - acc: 0.8343 - val_loss: 0.3799 - val_acc: 0.8522\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3766 - acc: 0.8396 - val_loss: 0.3981 - val_acc: 0.8456\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3834 - acc: 0.8320 - val_loss: 0.3804 - val_acc: 0.8522\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3799 - acc: 0.8354 - val_loss: 0.3813 - val_acc: 0.8489\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3719 - acc: 0.8422 - val_loss: 0.3814 - val_acc: 0.8489\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3698 - acc: 0.8438 - val_loss: 0.3903 - val_acc: 0.8325\n",
      "Training with parameters {'batch_size': 200, 'dropout': 0.1, 'lay_conf': [1024, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_67\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_68 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_201 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_134 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_202 (Dense)            (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_135 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_203 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,639,553\n",
      "Trainable params: 1,639,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "28/28 [==============================] - 1s 12ms/step - loss: 0.6427 - acc: 0.6415 - val_loss: 0.5270 - val_acc: 0.7931\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.5404 - acc: 0.7429 - val_loss: 0.4599 - val_acc: 0.8194\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.5030 - acc: 0.7672 - val_loss: 0.4301 - val_acc: 0.8325\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4735 - acc: 0.7862 - val_loss: 0.4165 - val_acc: 0.8342\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.4620 - acc: 0.7940 - val_loss: 0.4044 - val_acc: 0.8342\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4440 - acc: 0.8048 - val_loss: 0.3905 - val_acc: 0.8489\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4465 - acc: 0.8028 - val_loss: 0.4114 - val_acc: 0.8309\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4346 - acc: 0.8031 - val_loss: 0.3996 - val_acc: 0.8407\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.4275 - acc: 0.8115 - val_loss: 0.3774 - val_acc: 0.8506\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4244 - acc: 0.8088 - val_loss: 0.3858 - val_acc: 0.8522\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4137 - acc: 0.8141 - val_loss: 0.3787 - val_acc: 0.8571\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4107 - acc: 0.8176 - val_loss: 0.3730 - val_acc: 0.8539\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3996 - acc: 0.8227 - val_loss: 0.3803 - val_acc: 0.8489\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4028 - acc: 0.8276 - val_loss: 0.3903 - val_acc: 0.8374\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3990 - acc: 0.8250 - val_loss: 0.3679 - val_acc: 0.8539\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3944 - acc: 0.8259 - val_loss: 0.3656 - val_acc: 0.8522\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3874 - acc: 0.8342 - val_loss: 0.3701 - val_acc: 0.8522\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3852 - acc: 0.8301 - val_loss: 0.3949 - val_acc: 0.8259\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3771 - acc: 0.8374 - val_loss: 0.3763 - val_acc: 0.8456\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3790 - acc: 0.8376 - val_loss: 0.3712 - val_acc: 0.8506\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3716 - acc: 0.8374 - val_loss: 0.3786 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 200, 'dropout': 0.1, 'lay_conf': [1024, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_68\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_69 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_204 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_136 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_205 (Dense)            (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_137 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_206 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,606,721\n",
      "Trainable params: 1,606,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "28/28 [==============================] - 1s 11ms/step - loss: 0.6608 - acc: 0.6262 - val_loss: 0.5268 - val_acc: 0.7537\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.5444 - acc: 0.7360 - val_loss: 0.4530 - val_acc: 0.8144\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.5118 - acc: 0.7579 - val_loss: 0.4200 - val_acc: 0.8342\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4825 - acc: 0.7792 - val_loss: 0.4087 - val_acc: 0.8325\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4707 - acc: 0.7843 - val_loss: 0.4191 - val_acc: 0.8325\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4531 - acc: 0.7918 - val_loss: 0.4280 - val_acc: 0.8177\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4416 - acc: 0.8042 - val_loss: 0.3932 - val_acc: 0.8456\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4451 - acc: 0.8015 - val_loss: 0.3830 - val_acc: 0.8424\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4258 - acc: 0.8095 - val_loss: 0.3889 - val_acc: 0.8342\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4247 - acc: 0.8101 - val_loss: 0.3765 - val_acc: 0.8473\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4180 - acc: 0.8159 - val_loss: 0.3724 - val_acc: 0.8522\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4124 - acc: 0.8174 - val_loss: 0.3818 - val_acc: 0.8473\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4147 - acc: 0.8177 - val_loss: 0.3755 - val_acc: 0.8456\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4107 - acc: 0.8208 - val_loss: 0.3812 - val_acc: 0.8456\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4118 - acc: 0.8179 - val_loss: 0.3701 - val_acc: 0.8440\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4045 - acc: 0.8212 - val_loss: 0.3681 - val_acc: 0.8489\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3858 - acc: 0.8269 - val_loss: 0.3766 - val_acc: 0.8424\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3833 - acc: 0.8340 - val_loss: 0.3657 - val_acc: 0.8539\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3856 - acc: 0.8292 - val_loss: 0.3832 - val_acc: 0.8358\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3862 - acc: 0.8307 - val_loss: 0.3800 - val_acc: 0.8374\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3759 - acc: 0.8382 - val_loss: 0.3638 - val_acc: 0.8506\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3800 - acc: 0.8369 - val_loss: 0.3745 - val_acc: 0.8456\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3747 - acc: 0.8384 - val_loss: 0.3700 - val_acc: 0.8506\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3637 - acc: 0.8444 - val_loss: 0.3618 - val_acc: 0.8506\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3633 - acc: 0.8425 - val_loss: 0.3646 - val_acc: 0.8473\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3700 - acc: 0.8415 - val_loss: 0.3872 - val_acc: 0.8309\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3569 - acc: 0.8466 - val_loss: 0.3689 - val_acc: 0.8506\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.3530 - acc: 0.8493 - val_loss: 0.3649 - val_acc: 0.8555\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3471 - acc: 0.8477 - val_loss: 0.3694 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 200, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_69\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_70 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_207 (Dense)            (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_138 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_208 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_139 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_209 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,050,113\n",
      "Trainable params: 1,050,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "28/28 [==============================] - 1s 10ms/step - loss: 0.6492 - acc: 0.6287 - val_loss: 0.5654 - val_acc: 0.6929\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.5502 - acc: 0.7353 - val_loss: 0.4755 - val_acc: 0.8112\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.4956 - acc: 0.7734 - val_loss: 0.4519 - val_acc: 0.8013\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4804 - acc: 0.7827 - val_loss: 0.4422 - val_acc: 0.7964\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4617 - acc: 0.7898 - val_loss: 0.4510 - val_acc: 0.7865\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4491 - acc: 0.7962 - val_loss: 0.4213 - val_acc: 0.8144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4384 - acc: 0.8037 - val_loss: 0.3956 - val_acc: 0.8440\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4282 - acc: 0.8104 - val_loss: 0.4149 - val_acc: 0.8259\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4262 - acc: 0.8062 - val_loss: 0.3983 - val_acc: 0.8391\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4176 - acc: 0.8139 - val_loss: 0.3861 - val_acc: 0.8424\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4142 - acc: 0.8176 - val_loss: 0.3851 - val_acc: 0.8424\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4158 - acc: 0.8134 - val_loss: 0.3928 - val_acc: 0.8440\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4070 - acc: 0.8181 - val_loss: 0.3789 - val_acc: 0.8456\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3912 - acc: 0.8261 - val_loss: 0.3771 - val_acc: 0.8555\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3924 - acc: 0.8241 - val_loss: 0.4061 - val_acc: 0.8374\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3934 - acc: 0.8300 - val_loss: 0.4216 - val_acc: 0.8079\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3839 - acc: 0.8323 - val_loss: 0.3747 - val_acc: 0.8588\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3763 - acc: 0.8307 - val_loss: 0.3779 - val_acc: 0.8506\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3752 - acc: 0.8367 - val_loss: 0.3732 - val_acc: 0.8424\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3676 - acc: 0.8427 - val_loss: 0.3714 - val_acc: 0.8456\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3608 - acc: 0.8453 - val_loss: 0.3707 - val_acc: 0.8506\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3607 - acc: 0.8409 - val_loss: 0.3752 - val_acc: 0.8555\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3550 - acc: 0.8488 - val_loss: 0.3796 - val_acc: 0.8506\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3527 - acc: 0.8502 - val_loss: 0.3812 - val_acc: 0.8424\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3546 - acc: 0.8466 - val_loss: 0.3746 - val_acc: 0.8522\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3441 - acc: 0.8528 - val_loss: 0.3767 - val_acc: 0.8456\n",
      "Training with parameters {'batch_size': 200, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_70\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_71 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_210 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_140 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_211 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_141 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_212 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 401,729\n",
      "Trainable params: 401,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "28/28 [==============================] - 1s 9ms/step - loss: 0.7092 - acc: 0.5641 - val_loss: 0.6210 - val_acc: 0.7176\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.6273 - acc: 0.6583 - val_loss: 0.5647 - val_acc: 0.7488\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5862 - acc: 0.7150 - val_loss: 0.5237 - val_acc: 0.7915\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.5459 - acc: 0.7460 - val_loss: 0.4886 - val_acc: 0.8046\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.5240 - acc: 0.7546 - val_loss: 0.4677 - val_acc: 0.8046\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.5099 - acc: 0.7614 - val_loss: 0.4504 - val_acc: 0.8177\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4949 - acc: 0.7780 - val_loss: 0.4430 - val_acc: 0.8243\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4782 - acc: 0.7831 - val_loss: 0.4247 - val_acc: 0.8358\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4682 - acc: 0.7916 - val_loss: 0.4175 - val_acc: 0.8358\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4616 - acc: 0.7949 - val_loss: 0.4085 - val_acc: 0.8374\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4524 - acc: 0.7949 - val_loss: 0.4063 - val_acc: 0.8325\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4556 - acc: 0.8006 - val_loss: 0.4064 - val_acc: 0.8374\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4416 - acc: 0.8090 - val_loss: 0.3945 - val_acc: 0.8374\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4329 - acc: 0.8123 - val_loss: 0.4019 - val_acc: 0.8325\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4324 - acc: 0.8128 - val_loss: 0.3915 - val_acc: 0.8456\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4314 - acc: 0.8119 - val_loss: 0.3963 - val_acc: 0.8292\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4269 - acc: 0.8112 - val_loss: 0.3873 - val_acc: 0.8489\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4219 - acc: 0.8150 - val_loss: 0.3842 - val_acc: 0.8424\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4229 - acc: 0.8143 - val_loss: 0.3905 - val_acc: 0.8358\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4099 - acc: 0.8256 - val_loss: 0.3813 - val_acc: 0.8440\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4137 - acc: 0.8227 - val_loss: 0.3790 - val_acc: 0.8506\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4105 - acc: 0.8199 - val_loss: 0.3928 - val_acc: 0.8424\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.4059 - acc: 0.8254 - val_loss: 0.3812 - val_acc: 0.8440\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4034 - acc: 0.8227 - val_loss: 0.3786 - val_acc: 0.8522\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4037 - acc: 0.8267 - val_loss: 0.3868 - val_acc: 0.8407\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4046 - acc: 0.8254 - val_loss: 0.3753 - val_acc: 0.8440\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3964 - acc: 0.8283 - val_loss: 0.3756 - val_acc: 0.8506\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3921 - acc: 0.8287 - val_loss: 0.3771 - val_acc: 0.8506\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3854 - acc: 0.8327 - val_loss: 0.3790 - val_acc: 0.8456\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3873 - acc: 0.8367 - val_loss: 0.3802 - val_acc: 0.8440\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.3857 - acc: 0.8320 - val_loss: 0.3837 - val_acc: 0.8456\n",
      "Training with parameters {'batch_size': 200, 'dropout': 0.1, 'lay_conf': [1024, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_71\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_72 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_213 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_142 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_214 (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_143 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_215 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,836,545\n",
      "Trainable params: 1,836,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 1s 12ms/step - loss: 0.6783 - acc: 0.6178 - val_loss: 0.5563 - val_acc: 0.7734\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.5566 - acc: 0.7356 - val_loss: 0.4805 - val_acc: 0.7947\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.5105 - acc: 0.7586 - val_loss: 0.4423 - val_acc: 0.8112\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4816 - acc: 0.7770 - val_loss: 0.4174 - val_acc: 0.8227\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.4582 - acc: 0.7942 - val_loss: 0.4076 - val_acc: 0.8309\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.4437 - acc: 0.8015 - val_loss: 0.4025 - val_acc: 0.8391\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4326 - acc: 0.8104 - val_loss: 0.3926 - val_acc: 0.8391\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4268 - acc: 0.8086 - val_loss: 0.3810 - val_acc: 0.8489\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.4293 - acc: 0.8117 - val_loss: 0.3822 - val_acc: 0.8440\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.4113 - acc: 0.8207 - val_loss: 0.3811 - val_acc: 0.8440\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.4050 - acc: 0.8234 - val_loss: 0.3802 - val_acc: 0.8473\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.4003 - acc: 0.8186 - val_loss: 0.3723 - val_acc: 0.8555\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.4032 - acc: 0.8227 - val_loss: 0.3932 - val_acc: 0.8374\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3956 - acc: 0.8230 - val_loss: 0.3741 - val_acc: 0.8522\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.3818 - acc: 0.8276 - val_loss: 0.3788 - val_acc: 0.8424\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.3840 - acc: 0.8321 - val_loss: 0.3744 - val_acc: 0.8473\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.3771 - acc: 0.8362 - val_loss: 0.4117 - val_acc: 0.8374\n",
      "Training with parameters {'batch_size': 350, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_72\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_73 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_216 (Dense)            (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_144 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_217 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_145 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_218 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.7075 - acc: 0.5455 - val_loss: 0.6504 - val_acc: 0.6223\n",
      "Epoch 2/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6569 - acc: 0.6154 - val_loss: 0.6116 - val_acc: 0.7094\n",
      "Epoch 3/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6209 - acc: 0.6690 - val_loss: 0.5703 - val_acc: 0.7389\n",
      "Epoch 4/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5978 - acc: 0.6949 - val_loss: 0.5385 - val_acc: 0.7783\n",
      "Epoch 5/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5717 - acc: 0.7283 - val_loss: 0.5108 - val_acc: 0.7997\n",
      "Epoch 6/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5471 - acc: 0.7438 - val_loss: 0.4866 - val_acc: 0.8046\n",
      "Epoch 7/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5270 - acc: 0.7610 - val_loss: 0.4664 - val_acc: 0.8013\n",
      "Epoch 8/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5165 - acc: 0.7590 - val_loss: 0.4514 - val_acc: 0.8144\n",
      "Epoch 9/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4985 - acc: 0.7756 - val_loss: 0.4358 - val_acc: 0.8227\n",
      "Epoch 10/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4909 - acc: 0.7796 - val_loss: 0.4252 - val_acc: 0.8292\n",
      "Epoch 11/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4789 - acc: 0.7840 - val_loss: 0.4204 - val_acc: 0.8309\n",
      "Epoch 12/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4702 - acc: 0.7920 - val_loss: 0.4106 - val_acc: 0.8391\n",
      "Epoch 13/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4626 - acc: 0.7989 - val_loss: 0.4063 - val_acc: 0.8358\n",
      "Epoch 14/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4504 - acc: 0.8033 - val_loss: 0.4010 - val_acc: 0.8424\n",
      "Epoch 15/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4485 - acc: 0.8062 - val_loss: 0.4017 - val_acc: 0.8358\n",
      "Epoch 16/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4484 - acc: 0.8050 - val_loss: 0.3958 - val_acc: 0.8456\n",
      "Epoch 17/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4432 - acc: 0.8064 - val_loss: 0.3927 - val_acc: 0.8424\n",
      "Epoch 18/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4402 - acc: 0.8061 - val_loss: 0.3891 - val_acc: 0.8473\n",
      "Epoch 19/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4351 - acc: 0.8101 - val_loss: 0.3944 - val_acc: 0.8424\n",
      "Epoch 20/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4429 - acc: 0.8035 - val_loss: 0.3966 - val_acc: 0.8358\n",
      "Epoch 21/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4357 - acc: 0.8081 - val_loss: 0.3901 - val_acc: 0.8424\n",
      "Epoch 22/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4265 - acc: 0.8117 - val_loss: 0.3840 - val_acc: 0.8456\n",
      "Epoch 23/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4208 - acc: 0.8179 - val_loss: 0.3817 - val_acc: 0.8440\n",
      "Epoch 24/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4187 - acc: 0.8188 - val_loss: 0.3808 - val_acc: 0.8440\n",
      "Epoch 25/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4177 - acc: 0.8223 - val_loss: 0.3833 - val_acc: 0.8456\n",
      "Epoch 26/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4150 - acc: 0.8207 - val_loss: 0.3786 - val_acc: 0.8456\n",
      "Epoch 27/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4150 - acc: 0.8172 - val_loss: 0.3868 - val_acc: 0.8440\n",
      "Epoch 28/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4135 - acc: 0.8228 - val_loss: 0.3849 - val_acc: 0.8424\n",
      "Epoch 29/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4144 - acc: 0.8194 - val_loss: 0.3773 - val_acc: 0.8456\n",
      "Epoch 30/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4068 - acc: 0.8243 - val_loss: 0.3763 - val_acc: 0.8473\n",
      "Epoch 31/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4028 - acc: 0.8230 - val_loss: 0.3765 - val_acc: 0.8456\n",
      "Epoch 32/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4035 - acc: 0.8254 - val_loss: 0.3752 - val_acc: 0.8489\n",
      "Epoch 33/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4016 - acc: 0.8258 - val_loss: 0.3752 - val_acc: 0.8506\n",
      "Epoch 34/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3994 - acc: 0.8285 - val_loss: 0.3747 - val_acc: 0.8522\n",
      "Epoch 35/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3952 - acc: 0.8272 - val_loss: 0.3738 - val_acc: 0.8424\n",
      "Epoch 36/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3938 - acc: 0.8311 - val_loss: 0.3723 - val_acc: 0.8506\n",
      "Epoch 37/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3933 - acc: 0.8252 - val_loss: 0.3742 - val_acc: 0.8391\n",
      "Epoch 38/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3930 - acc: 0.8283 - val_loss: 0.3776 - val_acc: 0.8440\n",
      "Epoch 39/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3907 - acc: 0.8236 - val_loss: 0.3743 - val_acc: 0.8391\n",
      "Epoch 40/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3854 - acc: 0.8329 - val_loss: 0.3718 - val_acc: 0.8473\n",
      "Epoch 41/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3846 - acc: 0.8360 - val_loss: 0.3733 - val_acc: 0.8407\n",
      "Epoch 42/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3826 - acc: 0.8351 - val_loss: 0.3746 - val_acc: 0.8456\n",
      "Epoch 43/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3865 - acc: 0.8354 - val_loss: 0.3891 - val_acc: 0.8309\n",
      "Epoch 44/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3885 - acc: 0.8342 - val_loss: 0.3716 - val_acc: 0.8407\n",
      "Epoch 45/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3809 - acc: 0.8367 - val_loss: 0.3789 - val_acc: 0.8424\n",
      "Epoch 46/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3783 - acc: 0.8363 - val_loss: 0.3809 - val_acc: 0.8374\n",
      "Epoch 47/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3806 - acc: 0.8362 - val_loss: 0.3736 - val_acc: 0.8424\n",
      "Epoch 48/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3767 - acc: 0.8378 - val_loss: 0.3760 - val_acc: 0.8440\n",
      "Epoch 49/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3720 - acc: 0.8433 - val_loss: 0.3702 - val_acc: 0.8489\n",
      "Epoch 50/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3700 - acc: 0.8380 - val_loss: 0.3702 - val_acc: 0.8456\n",
      "Epoch 51/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3716 - acc: 0.8376 - val_loss: 0.3786 - val_acc: 0.8407\n",
      "Epoch 52/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3707 - acc: 0.8373 - val_loss: 0.3704 - val_acc: 0.8473\n",
      "Epoch 53/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3673 - acc: 0.8438 - val_loss: 0.3774 - val_acc: 0.8440\n",
      "Epoch 54/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3695 - acc: 0.8382 - val_loss: 0.3898 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 350, 'dropout': 0.1, 'lay_conf': [1024, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_73\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_74 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_219 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_146 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_220 (Dense)            (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_147 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_221 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "16/16 [==============================] - 1s 17ms/step - loss: 0.7275 - acc: 0.5557 - val_loss: 0.6334 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6214 - acc: 0.6563 - val_loss: 0.5510 - val_acc: 0.7570\n",
      "Epoch 3/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5712 - acc: 0.7157 - val_loss: 0.5021 - val_acc: 0.7964\n",
      "Epoch 4/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5334 - acc: 0.7495 - val_loss: 0.4675 - val_acc: 0.8194\n",
      "Epoch 5/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5026 - acc: 0.7645 - val_loss: 0.4418 - val_acc: 0.8276\n",
      "Epoch 6/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4842 - acc: 0.7772 - val_loss: 0.4269 - val_acc: 0.8243\n",
      "Epoch 7/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4666 - acc: 0.7864 - val_loss: 0.4276 - val_acc: 0.8161\n",
      "Epoch 8/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4587 - acc: 0.7920 - val_loss: 0.4055 - val_acc: 0.8325\n",
      "Epoch 9/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4475 - acc: 0.8002 - val_loss: 0.4121 - val_acc: 0.8309\n",
      "Epoch 10/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4441 - acc: 0.8013 - val_loss: 0.3983 - val_acc: 0.8358\n",
      "Epoch 11/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4301 - acc: 0.8113 - val_loss: 0.3896 - val_acc: 0.8407\n",
      "Epoch 12/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4253 - acc: 0.8132 - val_loss: 0.4007 - val_acc: 0.8358\n",
      "Epoch 13/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4240 - acc: 0.8137 - val_loss: 0.3885 - val_acc: 0.8407\n",
      "Epoch 14/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4160 - acc: 0.8176 - val_loss: 0.3822 - val_acc: 0.8424\n",
      "Epoch 15/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4111 - acc: 0.8210 - val_loss: 0.3819 - val_acc: 0.8440\n",
      "Epoch 16/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4062 - acc: 0.8234 - val_loss: 0.3771 - val_acc: 0.8440\n",
      "Epoch 17/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4023 - acc: 0.8247 - val_loss: 0.3758 - val_acc: 0.8506\n",
      "Epoch 18/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3974 - acc: 0.8223 - val_loss: 0.3786 - val_acc: 0.8473\n",
      "Epoch 19/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3969 - acc: 0.8230 - val_loss: 0.3723 - val_acc: 0.8489\n",
      "Epoch 20/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3870 - acc: 0.8290 - val_loss: 0.3774 - val_acc: 0.8473\n",
      "Epoch 21/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3861 - acc: 0.8316 - val_loss: 0.3769 - val_acc: 0.8571\n",
      "Epoch 22/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3778 - acc: 0.8327 - val_loss: 0.3707 - val_acc: 0.8440\n",
      "Epoch 23/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3772 - acc: 0.8352 - val_loss: 0.3717 - val_acc: 0.8456\n",
      "Epoch 24/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3713 - acc: 0.8398 - val_loss: 0.3699 - val_acc: 0.8440\n",
      "Epoch 25/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3730 - acc: 0.8352 - val_loss: 0.3774 - val_acc: 0.8473\n",
      "Epoch 26/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3667 - acc: 0.8416 - val_loss: 0.3695 - val_acc: 0.8506\n",
      "Epoch 27/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3568 - acc: 0.8444 - val_loss: 0.3716 - val_acc: 0.8473\n",
      "Epoch 28/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3591 - acc: 0.8456 - val_loss: 0.3683 - val_acc: 0.8456\n",
      "Epoch 29/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3582 - acc: 0.8435 - val_loss: 0.3707 - val_acc: 0.8456\n",
      "Epoch 30/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3492 - acc: 0.8520 - val_loss: 0.3772 - val_acc: 0.8456\n",
      "Epoch 31/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3480 - acc: 0.8513 - val_loss: 0.3812 - val_acc: 0.8440\n",
      "Epoch 32/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3453 - acc: 0.8531 - val_loss: 0.3933 - val_acc: 0.8292\n",
      "Epoch 33/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3487 - acc: 0.8488 - val_loss: 0.4101 - val_acc: 0.8456\n",
      "Training with parameters {'batch_size': 350, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_74\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_75 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_222 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_148 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_223 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_149 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_224 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,985\n",
      "Trainable params: 409,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9276 - acc: 0.5096 - val_loss: 0.6822 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6730 - acc: 0.5838 - val_loss: 0.6314 - val_acc: 0.6864\n",
      "Epoch 3/300\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6409 - acc: 0.6389 - val_loss: 0.5964 - val_acc: 0.7323\n",
      "Epoch 4/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6160 - acc: 0.6805 - val_loss: 0.5636 - val_acc: 0.7570\n",
      "Epoch 5/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5944 - acc: 0.6906 - val_loss: 0.5527 - val_acc: 0.7258\n",
      "Epoch 6/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5755 - acc: 0.7148 - val_loss: 0.5198 - val_acc: 0.7931\n",
      "Epoch 7/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5534 - acc: 0.7380 - val_loss: 0.5040 - val_acc: 0.8013\n",
      "Epoch 8/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5429 - acc: 0.7462 - val_loss: 0.4785 - val_acc: 0.8144\n",
      "Epoch 9/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5266 - acc: 0.7510 - val_loss: 0.4666 - val_acc: 0.8128\n",
      "Epoch 10/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5083 - acc: 0.7665 - val_loss: 0.4560 - val_acc: 0.8194\n",
      "Epoch 11/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5024 - acc: 0.7707 - val_loss: 0.4498 - val_acc: 0.8144\n",
      "Epoch 12/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4953 - acc: 0.7743 - val_loss: 0.4397 - val_acc: 0.8292\n",
      "Epoch 13/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4883 - acc: 0.7747 - val_loss: 0.4320 - val_acc: 0.8309\n",
      "Epoch 14/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4763 - acc: 0.7858 - val_loss: 0.4277 - val_acc: 0.8342\n",
      "Epoch 15/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4739 - acc: 0.7865 - val_loss: 0.4261 - val_acc: 0.8194\n",
      "Epoch 16/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4673 - acc: 0.7893 - val_loss: 0.4178 - val_acc: 0.8407\n",
      "Epoch 17/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4614 - acc: 0.7944 - val_loss: 0.4172 - val_acc: 0.8259\n",
      "Epoch 18/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4548 - acc: 0.7940 - val_loss: 0.4094 - val_acc: 0.8424\n",
      "Epoch 19/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4526 - acc: 0.7993 - val_loss: 0.4074 - val_acc: 0.8407\n",
      "Epoch 20/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4428 - acc: 0.8048 - val_loss: 0.4043 - val_acc: 0.8424\n",
      "Epoch 21/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4380 - acc: 0.8044 - val_loss: 0.4048 - val_acc: 0.8440\n",
      "Epoch 22/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4413 - acc: 0.8037 - val_loss: 0.4117 - val_acc: 0.8309\n",
      "Epoch 23/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4394 - acc: 0.8033 - val_loss: 0.4038 - val_acc: 0.8407\n",
      "Epoch 24/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4337 - acc: 0.8103 - val_loss: 0.3978 - val_acc: 0.8424\n",
      "Epoch 25/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4296 - acc: 0.8095 - val_loss: 0.3986 - val_acc: 0.8407\n",
      "Epoch 26/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4292 - acc: 0.8137 - val_loss: 0.3943 - val_acc: 0.8440\n",
      "Epoch 27/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4250 - acc: 0.8150 - val_loss: 0.3919 - val_acc: 0.8407\n",
      "Epoch 28/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4189 - acc: 0.8185 - val_loss: 0.3912 - val_acc: 0.8440\n",
      "Epoch 29/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4207 - acc: 0.8188 - val_loss: 0.3890 - val_acc: 0.8440\n",
      "Epoch 30/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4218 - acc: 0.8181 - val_loss: 0.3910 - val_acc: 0.8424\n",
      "Epoch 31/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4125 - acc: 0.8170 - val_loss: 0.3917 - val_acc: 0.8391\n",
      "Epoch 32/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4152 - acc: 0.8201 - val_loss: 0.3942 - val_acc: 0.8391\n",
      "Epoch 33/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4088 - acc: 0.8243 - val_loss: 0.3863 - val_acc: 0.8456\n",
      "Epoch 34/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4100 - acc: 0.8205 - val_loss: 0.3964 - val_acc: 0.8342\n",
      "Epoch 35/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4071 - acc: 0.8250 - val_loss: 0.3851 - val_acc: 0.8473\n",
      "Epoch 36/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4010 - acc: 0.8254 - val_loss: 0.3848 - val_acc: 0.8473\n",
      "Epoch 37/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4043 - acc: 0.8258 - val_loss: 0.3841 - val_acc: 0.8456\n",
      "Epoch 38/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3948 - acc: 0.8343 - val_loss: 0.3885 - val_acc: 0.8424\n",
      "Epoch 39/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3982 - acc: 0.8300 - val_loss: 0.3871 - val_acc: 0.8424\n",
      "Epoch 40/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3935 - acc: 0.8307 - val_loss: 0.3884 - val_acc: 0.8440\n",
      "Epoch 41/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3967 - acc: 0.8274 - val_loss: 0.3836 - val_acc: 0.8456\n",
      "Epoch 42/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3909 - acc: 0.8363 - val_loss: 0.3813 - val_acc: 0.8473\n",
      "Epoch 43/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4016 - acc: 0.8245 - val_loss: 0.3963 - val_acc: 0.8424\n",
      "Epoch 44/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3935 - acc: 0.8280 - val_loss: 0.3833 - val_acc: 0.8456\n",
      "Epoch 45/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3844 - acc: 0.8347 - val_loss: 0.3944 - val_acc: 0.8391\n",
      "Epoch 46/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3924 - acc: 0.8309 - val_loss: 0.4143 - val_acc: 0.8227\n",
      "Epoch 47/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3882 - acc: 0.8343 - val_loss: 0.3785 - val_acc: 0.8473\n",
      "Epoch 48/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3794 - acc: 0.8363 - val_loss: 0.3829 - val_acc: 0.8424\n",
      "Epoch 49/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3798 - acc: 0.8336 - val_loss: 0.3925 - val_acc: 0.8309\n",
      "Epoch 50/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3809 - acc: 0.8352 - val_loss: 0.3814 - val_acc: 0.8424\n",
      "Epoch 51/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3715 - acc: 0.8402 - val_loss: 0.3803 - val_acc: 0.8456\n",
      "Epoch 52/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3745 - acc: 0.8352 - val_loss: 0.3812 - val_acc: 0.8407\n",
      "Training with parameters {'batch_size': 350, 'dropout': 0.1, 'lay_conf': [1024, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_75\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_76 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_225 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_150 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_226 (Dense)            (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_151 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_227 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,639,553\n",
      "Trainable params: 1,639,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "16/16 [==============================] - 1s 16ms/step - loss: 0.6847 - acc: 0.5953 - val_loss: 0.5745 - val_acc: 0.7570\n",
      "Epoch 2/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5811 - acc: 0.7092 - val_loss: 0.5018 - val_acc: 0.8046\n",
      "Epoch 3/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5299 - acc: 0.7471 - val_loss: 0.4679 - val_acc: 0.8161\n",
      "Epoch 4/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5167 - acc: 0.7535 - val_loss: 0.4648 - val_acc: 0.7997\n",
      "Epoch 5/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4906 - acc: 0.7672 - val_loss: 0.4206 - val_acc: 0.8358\n",
      "Epoch 6/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4714 - acc: 0.7854 - val_loss: 0.4300 - val_acc: 0.8144\n",
      "Epoch 7/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4796 - acc: 0.7760 - val_loss: 0.4258 - val_acc: 0.8194\n",
      "Epoch 8/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4619 - acc: 0.7911 - val_loss: 0.4016 - val_acc: 0.8325\n",
      "Epoch 9/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4468 - acc: 0.7993 - val_loss: 0.3934 - val_acc: 0.8473\n",
      "Epoch 10/300\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.4405 - acc: 0.8046 - val_loss: 0.3885 - val_acc: 0.8506\n",
      "Epoch 11/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4345 - acc: 0.8072 - val_loss: 0.3873 - val_acc: 0.8506\n",
      "Epoch 12/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4330 - acc: 0.8084 - val_loss: 0.4033 - val_acc: 0.8227\n",
      "Epoch 13/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4273 - acc: 0.8165 - val_loss: 0.3799 - val_acc: 0.8555\n",
      "Epoch 14/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4254 - acc: 0.8077 - val_loss: 0.4021 - val_acc: 0.8210\n",
      "Epoch 15/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4183 - acc: 0.8119 - val_loss: 0.3864 - val_acc: 0.8424\n",
      "Epoch 16/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4168 - acc: 0.8166 - val_loss: 0.3743 - val_acc: 0.8522\n",
      "Epoch 17/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4080 - acc: 0.8221 - val_loss: 0.4026 - val_acc: 0.8391\n",
      "Epoch 18/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4150 - acc: 0.8183 - val_loss: 0.3912 - val_acc: 0.8407\n",
      "Epoch 19/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4011 - acc: 0.8227 - val_loss: 0.3881 - val_acc: 0.8440\n",
      "Epoch 20/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4036 - acc: 0.8228 - val_loss: 0.3701 - val_acc: 0.8522\n",
      "Epoch 21/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3920 - acc: 0.8305 - val_loss: 0.3731 - val_acc: 0.8522\n",
      "Epoch 22/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3942 - acc: 0.8263 - val_loss: 0.3683 - val_acc: 0.8539\n",
      "Epoch 23/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3917 - acc: 0.8305 - val_loss: 0.3896 - val_acc: 0.8342\n",
      "Epoch 24/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3992 - acc: 0.8243 - val_loss: 0.4311 - val_acc: 0.8128\n",
      "Epoch 25/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3956 - acc: 0.8234 - val_loss: 0.3660 - val_acc: 0.8539\n",
      "Epoch 26/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3776 - acc: 0.8342 - val_loss: 0.3636 - val_acc: 0.8506\n",
      "Epoch 27/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3752 - acc: 0.8362 - val_loss: 0.3660 - val_acc: 0.8489\n",
      "Epoch 28/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3691 - acc: 0.8433 - val_loss: 0.3661 - val_acc: 0.8489\n",
      "Epoch 29/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3666 - acc: 0.8427 - val_loss: 0.3672 - val_acc: 0.8506\n",
      "Epoch 30/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3677 - acc: 0.8393 - val_loss: 0.3679 - val_acc: 0.8489\n",
      "Epoch 31/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3617 - acc: 0.8420 - val_loss: 0.3854 - val_acc: 0.8342\n",
      "Training with parameters {'batch_size': 350, 'dropout': 0.1, 'lay_conf': [1024, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_76\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_77 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_228 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_152 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_229 (Dense)            (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_153 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_230 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,606,721\n",
      "Trainable params: 1,606,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "16/16 [==============================] - 1s 14ms/step - loss: 0.6981 - acc: 0.5769 - val_loss: 0.5879 - val_acc: 0.6995\n",
      "Epoch 2/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5928 - acc: 0.6960 - val_loss: 0.5003 - val_acc: 0.7931\n",
      "Epoch 3/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5456 - acc: 0.7345 - val_loss: 0.4603 - val_acc: 0.8243\n",
      "Epoch 4/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5110 - acc: 0.7614 - val_loss: 0.4302 - val_acc: 0.8227\n",
      "Epoch 5/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4893 - acc: 0.7719 - val_loss: 0.4443 - val_acc: 0.8079\n",
      "Epoch 6/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4777 - acc: 0.7807 - val_loss: 0.4105 - val_acc: 0.8374\n",
      "Epoch 7/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4616 - acc: 0.7926 - val_loss: 0.4005 - val_acc: 0.8374\n",
      "Epoch 8/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4553 - acc: 0.7964 - val_loss: 0.3924 - val_acc: 0.8473\n",
      "Epoch 9/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4497 - acc: 0.7962 - val_loss: 0.3933 - val_acc: 0.8407\n",
      "Epoch 10/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4402 - acc: 0.8053 - val_loss: 0.3840 - val_acc: 0.8489\n",
      "Epoch 11/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4388 - acc: 0.8041 - val_loss: 0.3838 - val_acc: 0.8506\n",
      "Epoch 12/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4333 - acc: 0.8090 - val_loss: 0.4031 - val_acc: 0.8342\n",
      "Epoch 13/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4338 - acc: 0.8064 - val_loss: 0.4003 - val_acc: 0.8374\n",
      "Epoch 14/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4247 - acc: 0.8112 - val_loss: 0.3767 - val_acc: 0.8522\n",
      "Epoch 15/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4139 - acc: 0.8144 - val_loss: 0.3741 - val_acc: 0.8522\n",
      "Epoch 16/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4085 - acc: 0.8203 - val_loss: 0.3703 - val_acc: 0.8539\n",
      "Epoch 17/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4090 - acc: 0.8228 - val_loss: 0.3691 - val_acc: 0.8506\n",
      "Epoch 18/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4035 - acc: 0.8241 - val_loss: 0.3833 - val_acc: 0.8506\n",
      "Epoch 19/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4049 - acc: 0.8227 - val_loss: 0.3730 - val_acc: 0.8506\n",
      "Epoch 20/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4011 - acc: 0.8238 - val_loss: 0.3938 - val_acc: 0.8276\n",
      "Epoch 21/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4200 - acc: 0.8079 - val_loss: 0.3870 - val_acc: 0.8407\n",
      "Epoch 22/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3963 - acc: 0.8265 - val_loss: 0.3679 - val_acc: 0.8522\n",
      "Epoch 23/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3915 - acc: 0.8276 - val_loss: 0.3756 - val_acc: 0.8473\n",
      "Epoch 24/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3925 - acc: 0.8285 - val_loss: 0.3778 - val_acc: 0.8440\n",
      "Epoch 25/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3851 - acc: 0.8347 - val_loss: 0.3696 - val_acc: 0.8473\n",
      "Epoch 26/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3789 - acc: 0.8354 - val_loss: 0.3666 - val_acc: 0.8489\n",
      "Epoch 27/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3764 - acc: 0.8349 - val_loss: 0.3696 - val_acc: 0.8539\n",
      "Epoch 28/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3753 - acc: 0.8351 - val_loss: 0.3763 - val_acc: 0.8440\n",
      "Epoch 29/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3808 - acc: 0.8378 - val_loss: 0.3794 - val_acc: 0.8440\n",
      "Epoch 30/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3803 - acc: 0.8349 - val_loss: 0.3698 - val_acc: 0.8473\n",
      "Epoch 31/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3754 - acc: 0.8394 - val_loss: 0.3631 - val_acc: 0.8539\n",
      "Epoch 32/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3630 - acc: 0.8440 - val_loss: 0.3618 - val_acc: 0.8506\n",
      "Epoch 33/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3626 - acc: 0.8447 - val_loss: 0.3633 - val_acc: 0.8473\n",
      "Epoch 34/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3557 - acc: 0.8469 - val_loss: 0.3724 - val_acc: 0.8522\n",
      "Epoch 35/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3584 - acc: 0.8416 - val_loss: 0.3657 - val_acc: 0.8555\n",
      "Epoch 36/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3564 - acc: 0.8484 - val_loss: 0.3658 - val_acc: 0.8456\n",
      "Epoch 37/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3580 - acc: 0.8438 - val_loss: 0.3730 - val_acc: 0.8506\n",
      "Training with parameters {'batch_size': 350, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_77\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_78 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_231 (Dense)            (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_154 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_232 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_155 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_233 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,050,113\n",
      "Trainable params: 1,050,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "16/16 [==============================] - 1s 16ms/step - loss: 0.6661 - acc: 0.6006 - val_loss: 0.5854 - val_acc: 0.6929\n",
      "Epoch 2/300\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5811 - acc: 0.7125 - val_loss: 0.5128 - val_acc: 0.7767\n",
      "Epoch 3/300\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5339 - acc: 0.7468 - val_loss: 0.4707 - val_acc: 0.8079\n",
      "Epoch 4/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5006 - acc: 0.7654 - val_loss: 0.4428 - val_acc: 0.8227\n",
      "Epoch 5/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4854 - acc: 0.7772 - val_loss: 0.4254 - val_acc: 0.8243\n",
      "Epoch 6/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4660 - acc: 0.7929 - val_loss: 0.4230 - val_acc: 0.8243\n",
      "Epoch 7/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4557 - acc: 0.7953 - val_loss: 0.4067 - val_acc: 0.8424\n",
      "Epoch 8/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4463 - acc: 0.8006 - val_loss: 0.4099 - val_acc: 0.8210\n",
      "Epoch 9/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4448 - acc: 0.7986 - val_loss: 0.3940 - val_acc: 0.8424\n",
      "Epoch 10/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4277 - acc: 0.8137 - val_loss: 0.3914 - val_acc: 0.8539\n",
      "Epoch 11/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4274 - acc: 0.8088 - val_loss: 0.3851 - val_acc: 0.8522\n",
      "Epoch 12/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4191 - acc: 0.8121 - val_loss: 0.3838 - val_acc: 0.8571\n",
      "Epoch 13/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4153 - acc: 0.8181 - val_loss: 0.3908 - val_acc: 0.8440\n",
      "Epoch 14/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4093 - acc: 0.8208 - val_loss: 0.3807 - val_acc: 0.8539\n",
      "Epoch 15/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4098 - acc: 0.8234 - val_loss: 0.3791 - val_acc: 0.8571\n",
      "Epoch 16/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4098 - acc: 0.8223 - val_loss: 0.3857 - val_acc: 0.8489\n",
      "Epoch 17/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4012 - acc: 0.8243 - val_loss: 0.3812 - val_acc: 0.8506\n",
      "Epoch 18/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3934 - acc: 0.8258 - val_loss: 0.3860 - val_acc: 0.8391\n",
      "Epoch 19/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3903 - acc: 0.8281 - val_loss: 0.3774 - val_acc: 0.8489\n",
      "Epoch 20/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3905 - acc: 0.8269 - val_loss: 0.3748 - val_acc: 0.8473\n",
      "Epoch 21/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3803 - acc: 0.8345 - val_loss: 0.3732 - val_acc: 0.8522\n",
      "Epoch 22/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3752 - acc: 0.8343 - val_loss: 0.3775 - val_acc: 0.8489\n",
      "Epoch 23/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3736 - acc: 0.8382 - val_loss: 0.3742 - val_acc: 0.8456\n",
      "Epoch 24/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3747 - acc: 0.8343 - val_loss: 0.4045 - val_acc: 0.8210\n",
      "Epoch 25/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3823 - acc: 0.8323 - val_loss: 0.3712 - val_acc: 0.8588\n",
      "Epoch 26/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3693 - acc: 0.8400 - val_loss: 0.3797 - val_acc: 0.8407\n",
      "Epoch 27/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3600 - acc: 0.8425 - val_loss: 0.3758 - val_acc: 0.8456\n",
      "Epoch 28/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3530 - acc: 0.8478 - val_loss: 0.3723 - val_acc: 0.8506\n",
      "Epoch 29/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3465 - acc: 0.8455 - val_loss: 0.3703 - val_acc: 0.8506\n",
      "Epoch 30/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3433 - acc: 0.8504 - val_loss: 0.3768 - val_acc: 0.8489\n",
      "Epoch 31/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3468 - acc: 0.8504 - val_loss: 0.3729 - val_acc: 0.8407\n",
      "Epoch 32/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3389 - acc: 0.8555 - val_loss: 0.4206 - val_acc: 0.8112\n",
      "Epoch 33/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3395 - acc: 0.8519 - val_loss: 0.3708 - val_acc: 0.8506\n",
      "Epoch 34/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3256 - acc: 0.8566 - val_loss: 0.3695 - val_acc: 0.8571\n",
      "Epoch 35/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3210 - acc: 0.8643 - val_loss: 0.3934 - val_acc: 0.8259\n",
      "Epoch 36/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3329 - acc: 0.8539 - val_loss: 0.3740 - val_acc: 0.8440\n",
      "Epoch 37/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3179 - acc: 0.8694 - val_loss: 0.3875 - val_acc: 0.8391\n",
      "Epoch 38/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3145 - acc: 0.8637 - val_loss: 0.4048 - val_acc: 0.8489\n",
      "Epoch 39/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3174 - acc: 0.8675 - val_loss: 0.3937 - val_acc: 0.8374\n",
      "Training with parameters {'batch_size': 350, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_78\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_79 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_234 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_156 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_235 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_157 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_236 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 401,729\n",
      "Trainable params: 401,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.7475 - acc: 0.5264 - val_loss: 0.6529 - val_acc: 0.6814\n",
      "Epoch 2/300\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6565 - acc: 0.6145 - val_loss: 0.6077 - val_acc: 0.7455\n",
      "Epoch 3/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6288 - acc: 0.6568 - val_loss: 0.5794 - val_acc: 0.7553\n",
      "Epoch 4/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6039 - acc: 0.6851 - val_loss: 0.5544 - val_acc: 0.7750\n",
      "Epoch 5/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5795 - acc: 0.7112 - val_loss: 0.5237 - val_acc: 0.7980\n",
      "Epoch 6/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5566 - acc: 0.7289 - val_loss: 0.4938 - val_acc: 0.7980\n",
      "Epoch 7/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5332 - acc: 0.7422 - val_loss: 0.4761 - val_acc: 0.8013\n",
      "Epoch 8/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5099 - acc: 0.7654 - val_loss: 0.4533 - val_acc: 0.8128\n",
      "Epoch 9/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5011 - acc: 0.7659 - val_loss: 0.4433 - val_acc: 0.8194\n",
      "Epoch 10/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4910 - acc: 0.7778 - val_loss: 0.4345 - val_acc: 0.8194\n",
      "Epoch 11/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4825 - acc: 0.7760 - val_loss: 0.4261 - val_acc: 0.8276\n",
      "Epoch 12/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4736 - acc: 0.7845 - val_loss: 0.4240 - val_acc: 0.8161\n",
      "Epoch 13/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4651 - acc: 0.7902 - val_loss: 0.4210 - val_acc: 0.8259\n",
      "Epoch 14/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4634 - acc: 0.7966 - val_loss: 0.4093 - val_acc: 0.8374\n",
      "Epoch 15/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4527 - acc: 0.8019 - val_loss: 0.4106 - val_acc: 0.8325\n",
      "Epoch 16/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4554 - acc: 0.7898 - val_loss: 0.4029 - val_acc: 0.8342\n",
      "Epoch 17/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4511 - acc: 0.8011 - val_loss: 0.4106 - val_acc: 0.8292\n",
      "Epoch 18/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4462 - acc: 0.7975 - val_loss: 0.4129 - val_acc: 0.8243\n",
      "Epoch 19/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4442 - acc: 0.8039 - val_loss: 0.4003 - val_acc: 0.8358\n",
      "Epoch 20/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4348 - acc: 0.8113 - val_loss: 0.3938 - val_acc: 0.8391\n",
      "Epoch 21/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4309 - acc: 0.8137 - val_loss: 0.3909 - val_acc: 0.8391\n",
      "Epoch 22/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4298 - acc: 0.8134 - val_loss: 0.3887 - val_acc: 0.8342\n",
      "Epoch 23/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4203 - acc: 0.8188 - val_loss: 0.3879 - val_acc: 0.8440\n",
      "Epoch 24/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4224 - acc: 0.8163 - val_loss: 0.3868 - val_acc: 0.8374\n",
      "Epoch 25/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4177 - acc: 0.8239 - val_loss: 0.3859 - val_acc: 0.8424\n",
      "Epoch 26/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4131 - acc: 0.8183 - val_loss: 0.3829 - val_acc: 0.8424\n",
      "Epoch 27/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4147 - acc: 0.8194 - val_loss: 0.3892 - val_acc: 0.8374\n",
      "Epoch 28/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4169 - acc: 0.8168 - val_loss: 0.3812 - val_acc: 0.8489\n",
      "Epoch 29/300\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4092 - acc: 0.8227 - val_loss: 0.3822 - val_acc: 0.8391\n",
      "Epoch 30/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4066 - acc: 0.8247 - val_loss: 0.3793 - val_acc: 0.8424\n",
      "Epoch 31/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4035 - acc: 0.8236 - val_loss: 0.3798 - val_acc: 0.8424\n",
      "Epoch 32/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4047 - acc: 0.8243 - val_loss: 0.3828 - val_acc: 0.8407\n",
      "Epoch 33/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4022 - acc: 0.8245 - val_loss: 0.3761 - val_acc: 0.8456\n",
      "Epoch 34/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3960 - acc: 0.8307 - val_loss: 0.3768 - val_acc: 0.8456\n",
      "Epoch 35/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3942 - acc: 0.8325 - val_loss: 0.3787 - val_acc: 0.8473\n",
      "Epoch 36/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3975 - acc: 0.8227 - val_loss: 0.3781 - val_acc: 0.8473\n",
      "Epoch 37/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3946 - acc: 0.8290 - val_loss: 0.3761 - val_acc: 0.8440\n",
      "Epoch 38/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3906 - acc: 0.8283 - val_loss: 0.3750 - val_acc: 0.8473\n",
      "Epoch 39/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3868 - acc: 0.8323 - val_loss: 0.3772 - val_acc: 0.8506\n",
      "Epoch 40/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3873 - acc: 0.8360 - val_loss: 0.3791 - val_acc: 0.8489\n",
      "Epoch 41/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3855 - acc: 0.8363 - val_loss: 0.3781 - val_acc: 0.8489\n",
      "Epoch 42/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3888 - acc: 0.8316 - val_loss: 0.3920 - val_acc: 0.8358\n",
      "Epoch 43/300\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3803 - acc: 0.8351 - val_loss: 0.3772 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 350, 'dropout': 0.1, 'lay_conf': [1024, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_79\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_80 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_237 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_158 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_238 (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_159 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_239 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,836,545\n",
      "Trainable params: 1,836,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "16/16 [==============================] - 1s 16ms/step - loss: 0.7083 - acc: 0.5807 - val_loss: 0.5960 - val_acc: 0.6962\n",
      "Epoch 2/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5923 - acc: 0.6929 - val_loss: 0.5258 - val_acc: 0.7915\n",
      "Epoch 3/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5417 - acc: 0.7407 - val_loss: 0.4713 - val_acc: 0.8128\n",
      "Epoch 4/300\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.5039 - acc: 0.7663 - val_loss: 0.4457 - val_acc: 0.8177\n",
      "Epoch 5/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4862 - acc: 0.7736 - val_loss: 0.4506 - val_acc: 0.8079\n",
      "Epoch 6/300\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.4694 - acc: 0.7867 - val_loss: 0.4105 - val_acc: 0.8325\n",
      "Epoch 7/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4541 - acc: 0.7973 - val_loss: 0.4016 - val_acc: 0.8407\n",
      "Epoch 8/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4444 - acc: 0.8024 - val_loss: 0.3952 - val_acc: 0.8374\n",
      "Epoch 9/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4347 - acc: 0.8088 - val_loss: 0.3953 - val_acc: 0.8407\n",
      "Epoch 10/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4309 - acc: 0.8106 - val_loss: 0.3923 - val_acc: 0.8374\n",
      "Epoch 11/300\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.4282 - acc: 0.8075 - val_loss: 0.3826 - val_acc: 0.8489\n",
      "Epoch 12/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4355 - acc: 0.8084 - val_loss: 0.4268 - val_acc: 0.8062\n",
      "Epoch 13/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4217 - acc: 0.8170 - val_loss: 0.3763 - val_acc: 0.8522\n",
      "Epoch 14/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4081 - acc: 0.8176 - val_loss: 0.3983 - val_acc: 0.8374\n",
      "Epoch 15/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4121 - acc: 0.8181 - val_loss: 0.3779 - val_acc: 0.8473\n",
      "Epoch 16/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3993 - acc: 0.8239 - val_loss: 0.3765 - val_acc: 0.8456\n",
      "Epoch 17/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4049 - acc: 0.8225 - val_loss: 0.3845 - val_acc: 0.8440\n",
      "Epoch 18/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3996 - acc: 0.8227 - val_loss: 0.3735 - val_acc: 0.8522\n",
      "Epoch 19/300\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.3844 - acc: 0.8311 - val_loss: 0.3738 - val_acc: 0.8489\n",
      "Epoch 20/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3860 - acc: 0.8305 - val_loss: 0.3728 - val_acc: 0.8456\n",
      "Epoch 21/300\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3873 - acc: 0.8325 - val_loss: 0.3823 - val_acc: 0.8489\n",
      "Epoch 22/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3811 - acc: 0.8320 - val_loss: 0.3697 - val_acc: 0.8506\n",
      "Epoch 23/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3698 - acc: 0.8404 - val_loss: 0.3761 - val_acc: 0.8473\n",
      "Epoch 24/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3683 - acc: 0.8418 - val_loss: 0.3733 - val_acc: 0.8456\n",
      "Epoch 25/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3737 - acc: 0.8413 - val_loss: 0.3741 - val_acc: 0.8522\n",
      "Epoch 26/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3631 - acc: 0.8405 - val_loss: 0.3690 - val_acc: 0.8522\n",
      "Epoch 27/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3574 - acc: 0.8453 - val_loss: 0.3800 - val_acc: 0.8440\n",
      "Epoch 28/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3545 - acc: 0.8482 - val_loss: 0.3781 - val_acc: 0.8407\n",
      "Epoch 29/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3489 - acc: 0.8517 - val_loss: 0.3689 - val_acc: 0.8604\n",
      "Epoch 30/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3386 - acc: 0.8555 - val_loss: 0.3697 - val_acc: 0.8506\n",
      "Epoch 31/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3462 - acc: 0.8495 - val_loss: 0.3680 - val_acc: 0.8555\n",
      "Epoch 32/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3495 - acc: 0.8482 - val_loss: 0.3722 - val_acc: 0.8473\n",
      "Epoch 33/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3268 - acc: 0.8591 - val_loss: 0.3722 - val_acc: 0.8473\n",
      "Epoch 34/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3252 - acc: 0.8582 - val_loss: 0.4034 - val_acc: 0.8424\n",
      "Epoch 35/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3409 - acc: 0.8511 - val_loss: 0.3722 - val_acc: 0.8489\n",
      "Epoch 36/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3156 - acc: 0.8643 - val_loss: 0.3823 - val_acc: 0.8456\n",
      "Training with parameters {'batch_size': 500, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_80\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_81 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_240 (Dense)            (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_160 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_241 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_161 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_242 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.7206 - acc: 0.5189 - val_loss: 0.6661 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6645 - acc: 0.6026 - val_loss: 0.6369 - val_acc: 0.6929\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.6459 - acc: 0.6324 - val_loss: 0.6062 - val_acc: 0.7176\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.6224 - acc: 0.6685 - val_loss: 0.5755 - val_acc: 0.7323\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.6009 - acc: 0.6875 - val_loss: 0.5493 - val_acc: 0.7521\n",
      "Epoch 6/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5824 - acc: 0.7145 - val_loss: 0.5269 - val_acc: 0.7898\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5621 - acc: 0.7307 - val_loss: 0.5075 - val_acc: 0.7947\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5441 - acc: 0.7480 - val_loss: 0.4930 - val_acc: 0.7882\n",
      "Epoch 9/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.5377 - acc: 0.7453 - val_loss: 0.4810 - val_acc: 0.8013\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.5196 - acc: 0.7626 - val_loss: 0.4659 - val_acc: 0.8079\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.5113 - acc: 0.7652 - val_loss: 0.4508 - val_acc: 0.8177\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4957 - acc: 0.7765 - val_loss: 0.4406 - val_acc: 0.8259\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4911 - acc: 0.7783 - val_loss: 0.4332 - val_acc: 0.8227\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4799 - acc: 0.7856 - val_loss: 0.4277 - val_acc: 0.8276\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4741 - acc: 0.7898 - val_loss: 0.4219 - val_acc: 0.8342\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4681 - acc: 0.7884 - val_loss: 0.4204 - val_acc: 0.8292\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4639 - acc: 0.7940 - val_loss: 0.4144 - val_acc: 0.8227\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4587 - acc: 0.7971 - val_loss: 0.4091 - val_acc: 0.8374\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4538 - acc: 0.7986 - val_loss: 0.4066 - val_acc: 0.8391\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4531 - acc: 0.8084 - val_loss: 0.4042 - val_acc: 0.8407\n",
      "Epoch 21/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4447 - acc: 0.8002 - val_loss: 0.4001 - val_acc: 0.8407\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.4422 - acc: 0.8057 - val_loss: 0.3981 - val_acc: 0.8424\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4411 - acc: 0.8088 - val_loss: 0.3976 - val_acc: 0.8424\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4384 - acc: 0.8099 - val_loss: 0.3996 - val_acc: 0.8391\n",
      "Epoch 25/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4353 - acc: 0.8113 - val_loss: 0.3970 - val_acc: 0.8407\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4307 - acc: 0.8082 - val_loss: 0.3928 - val_acc: 0.8473\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4261 - acc: 0.8134 - val_loss: 0.3922 - val_acc: 0.8456\n",
      "Epoch 28/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4272 - acc: 0.8166 - val_loss: 0.3903 - val_acc: 0.8456\n",
      "Epoch 29/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4273 - acc: 0.8119 - val_loss: 0.3913 - val_acc: 0.8424\n",
      "Epoch 30/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4221 - acc: 0.8176 - val_loss: 0.3872 - val_acc: 0.8473\n",
      "Epoch 31/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4194 - acc: 0.8179 - val_loss: 0.3880 - val_acc: 0.8473\n",
      "Epoch 32/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4233 - acc: 0.8115 - val_loss: 0.3867 - val_acc: 0.8489\n",
      "Epoch 33/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4152 - acc: 0.8205 - val_loss: 0.3860 - val_acc: 0.8473\n",
      "Epoch 34/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4117 - acc: 0.8232 - val_loss: 0.3854 - val_acc: 0.8456\n",
      "Epoch 35/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4121 - acc: 0.8201 - val_loss: 0.3854 - val_acc: 0.8440\n",
      "Epoch 36/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4140 - acc: 0.8141 - val_loss: 0.3933 - val_acc: 0.8391\n",
      "Epoch 37/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4137 - acc: 0.8214 - val_loss: 0.3924 - val_acc: 0.8407\n",
      "Epoch 38/300\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.4098 - acc: 0.8197 - val_loss: 0.3853 - val_acc: 0.8506\n",
      "Epoch 39/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4094 - acc: 0.8210 - val_loss: 0.3812 - val_acc: 0.8506\n",
      "Epoch 40/300\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.4075 - acc: 0.8216 - val_loss: 0.3814 - val_acc: 0.8489\n",
      "Epoch 41/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4022 - acc: 0.8283 - val_loss: 0.3826 - val_acc: 0.8506\n",
      "Epoch 42/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3998 - acc: 0.8298 - val_loss: 0.3869 - val_acc: 0.8440\n",
      "Epoch 43/300\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.4008 - acc: 0.8258 - val_loss: 0.3813 - val_acc: 0.8473\n",
      "Epoch 44/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4031 - acc: 0.8256 - val_loss: 0.3853 - val_acc: 0.8506\n",
      "Training with parameters {'batch_size': 500, 'dropout': 0.1, 'lay_conf': [1024, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_81\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_82 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_243 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_162 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_244 (Dense)            (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_163 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_245 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "11/11 [==============================] - 1s 25ms/step - loss: 0.7514 - acc: 0.5437 - val_loss: 0.6432 - val_acc: 0.6535\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.6399 - acc: 0.6305 - val_loss: 0.6078 - val_acc: 0.6831\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.6031 - acc: 0.6831 - val_loss: 0.5497 - val_acc: 0.7241\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.5616 - acc: 0.7276 - val_loss: 0.5058 - val_acc: 0.7849\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.5365 - acc: 0.7453 - val_loss: 0.4762 - val_acc: 0.8112\n",
      "Epoch 6/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.5141 - acc: 0.7670 - val_loss: 0.4554 - val_acc: 0.8227\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.5018 - acc: 0.7637 - val_loss: 0.4577 - val_acc: 0.8013\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4898 - acc: 0.7685 - val_loss: 0.4397 - val_acc: 0.8177\n",
      "Epoch 9/300\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.4747 - acc: 0.7833 - val_loss: 0.4162 - val_acc: 0.8342\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4639 - acc: 0.7909 - val_loss: 0.4088 - val_acc: 0.8325\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4546 - acc: 0.7944 - val_loss: 0.4040 - val_acc: 0.8440\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4402 - acc: 0.8072 - val_loss: 0.3982 - val_acc: 0.8374\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4360 - acc: 0.8042 - val_loss: 0.3959 - val_acc: 0.8391\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.4351 - acc: 0.8070 - val_loss: 0.3937 - val_acc: 0.8473\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4286 - acc: 0.8134 - val_loss: 0.3866 - val_acc: 0.8407\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4260 - acc: 0.8086 - val_loss: 0.4011 - val_acc: 0.8424\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4224 - acc: 0.8141 - val_loss: 0.3816 - val_acc: 0.8440\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4118 - acc: 0.8172 - val_loss: 0.3878 - val_acc: 0.8473\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4113 - acc: 0.8210 - val_loss: 0.3793 - val_acc: 0.8424\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4098 - acc: 0.8236 - val_loss: 0.3824 - val_acc: 0.8539\n",
      "Epoch 21/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4031 - acc: 0.8265 - val_loss: 0.3818 - val_acc: 0.8456\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3988 - acc: 0.8243 - val_loss: 0.3774 - val_acc: 0.8456\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4010 - acc: 0.8254 - val_loss: 0.3840 - val_acc: 0.8440\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3901 - acc: 0.8307 - val_loss: 0.3783 - val_acc: 0.8506\n",
      "Epoch 25/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3865 - acc: 0.8294 - val_loss: 0.3793 - val_acc: 0.8489\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3903 - acc: 0.8270 - val_loss: 0.3829 - val_acc: 0.8391\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3874 - acc: 0.8331 - val_loss: 0.3785 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 500, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_82\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_83 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_246 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_164 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_247 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_165 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_248 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,985\n",
      "Trainable params: 409,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.9836 - acc: 0.4966 - val_loss: 0.7757 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.7124 - acc: 0.5729 - val_loss: 0.6617 - val_acc: 0.6765\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6651 - acc: 0.5984 - val_loss: 0.6244 - val_acc: 0.5993\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.6401 - acc: 0.6315 - val_loss: 0.5948 - val_acc: 0.7422\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.6137 - acc: 0.6787 - val_loss: 0.5677 - val_acc: 0.7635\n",
      "Epoch 6/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.6002 - acc: 0.6918 - val_loss: 0.5480 - val_acc: 0.7833\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5885 - acc: 0.7072 - val_loss: 0.5318 - val_acc: 0.7947\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5697 - acc: 0.7247 - val_loss: 0.5178 - val_acc: 0.7915\n",
      "Epoch 9/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.5587 - acc: 0.7302 - val_loss: 0.5022 - val_acc: 0.8144\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5407 - acc: 0.7464 - val_loss: 0.4887 - val_acc: 0.8161\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5364 - acc: 0.7462 - val_loss: 0.4827 - val_acc: 0.8177\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.5246 - acc: 0.7535 - val_loss: 0.4764 - val_acc: 0.8128\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5185 - acc: 0.7550 - val_loss: 0.4686 - val_acc: 0.8161\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.5023 - acc: 0.7725 - val_loss: 0.4522 - val_acc: 0.8177\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5007 - acc: 0.7694 - val_loss: 0.4424 - val_acc: 0.8243\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4903 - acc: 0.7798 - val_loss: 0.4377 - val_acc: 0.8259\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4835 - acc: 0.7814 - val_loss: 0.4316 - val_acc: 0.8325\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4803 - acc: 0.7823 - val_loss: 0.4319 - val_acc: 0.8259\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4789 - acc: 0.7827 - val_loss: 0.4264 - val_acc: 0.8342\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4676 - acc: 0.7896 - val_loss: 0.4195 - val_acc: 0.8424\n",
      "Epoch 21/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4649 - acc: 0.7991 - val_loss: 0.4164 - val_acc: 0.8374\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4588 - acc: 0.7957 - val_loss: 0.4130 - val_acc: 0.8424\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4580 - acc: 0.7949 - val_loss: 0.4106 - val_acc: 0.8456\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4502 - acc: 0.7993 - val_loss: 0.4075 - val_acc: 0.8456\n",
      "Epoch 25/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4508 - acc: 0.8009 - val_loss: 0.4087 - val_acc: 0.8424\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4509 - acc: 0.7999 - val_loss: 0.4085 - val_acc: 0.8342\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4430 - acc: 0.8037 - val_loss: 0.4018 - val_acc: 0.8489\n",
      "Epoch 28/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4379 - acc: 0.8126 - val_loss: 0.4009 - val_acc: 0.8489\n",
      "Epoch 29/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4368 - acc: 0.8090 - val_loss: 0.3980 - val_acc: 0.8473\n",
      "Epoch 30/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4350 - acc: 0.8070 - val_loss: 0.4058 - val_acc: 0.8374\n",
      "Epoch 31/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4309 - acc: 0.8101 - val_loss: 0.3983 - val_acc: 0.8424\n",
      "Epoch 32/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4324 - acc: 0.8126 - val_loss: 0.3978 - val_acc: 0.8440\n",
      "Epoch 33/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4268 - acc: 0.8146 - val_loss: 0.3915 - val_acc: 0.8473\n",
      "Epoch 34/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4249 - acc: 0.8144 - val_loss: 0.3975 - val_acc: 0.8407\n",
      "Epoch 35/300\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.4238 - acc: 0.8134 - val_loss: 0.3918 - val_acc: 0.8473\n",
      "Epoch 36/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4258 - acc: 0.8135 - val_loss: 0.3939 - val_acc: 0.8440\n",
      "Epoch 37/300\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.4234 - acc: 0.8141 - val_loss: 0.3935 - val_acc: 0.8407\n",
      "Epoch 38/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4185 - acc: 0.8144 - val_loss: 0.3916 - val_acc: 0.8424\n",
      "Training with parameters {'batch_size': 500, 'dropout': 0.1, 'lay_conf': [1024, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_83\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_84 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_249 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_166 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_250 (Dense)            (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_167 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_251 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,639,553\n",
      "Trainable params: 1,639,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.7105 - acc: 0.5612 - val_loss: 0.6170 - val_acc: 0.6076\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.6216 - acc: 0.6533 - val_loss: 0.5601 - val_acc: 0.7455\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.5834 - acc: 0.6980 - val_loss: 0.5218 - val_acc: 0.7865\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.5467 - acc: 0.7327 - val_loss: 0.4893 - val_acc: 0.8013\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.5263 - acc: 0.7488 - val_loss: 0.4540 - val_acc: 0.8243\n",
      "Epoch 6/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4966 - acc: 0.7703 - val_loss: 0.4337 - val_acc: 0.8276\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4802 - acc: 0.7831 - val_loss: 0.4182 - val_acc: 0.8342\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4729 - acc: 0.7867 - val_loss: 0.4113 - val_acc: 0.8325\n",
      "Epoch 9/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4649 - acc: 0.7873 - val_loss: 0.4001 - val_acc: 0.8456\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4531 - acc: 0.7964 - val_loss: 0.3964 - val_acc: 0.8473\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4432 - acc: 0.8070 - val_loss: 0.3896 - val_acc: 0.8539\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4372 - acc: 0.8064 - val_loss: 0.3865 - val_acc: 0.8539\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4357 - acc: 0.8062 - val_loss: 0.3903 - val_acc: 0.8489\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 0.4283 - acc: 0.8143 - val_loss: 0.3894 - val_acc: 0.8440\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4247 - acc: 0.8190 - val_loss: 0.3836 - val_acc: 0.8489\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4299 - acc: 0.8095 - val_loss: 0.3764 - val_acc: 0.8604\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4194 - acc: 0.8168 - val_loss: 0.3762 - val_acc: 0.8571\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4120 - acc: 0.8196 - val_loss: 0.3724 - val_acc: 0.8571\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4061 - acc: 0.8250 - val_loss: 0.3720 - val_acc: 0.8588\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4049 - acc: 0.8243 - val_loss: 0.3840 - val_acc: 0.8506\n",
      "Epoch 21/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4052 - acc: 0.8230 - val_loss: 0.3732 - val_acc: 0.8539\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4020 - acc: 0.8207 - val_loss: 0.3684 - val_acc: 0.8571\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4039 - acc: 0.8234 - val_loss: 0.3946 - val_acc: 0.8424\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4089 - acc: 0.8212 - val_loss: 0.3896 - val_acc: 0.8407\n",
      "Epoch 25/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4032 - acc: 0.8207 - val_loss: 0.3994 - val_acc: 0.8325\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3978 - acc: 0.8230 - val_loss: 0.3853 - val_acc: 0.8374\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3910 - acc: 0.8292 - val_loss: 0.3677 - val_acc: 0.8473\n",
      "Epoch 28/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3840 - acc: 0.8318 - val_loss: 0.3645 - val_acc: 0.8539\n",
      "Epoch 29/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3790 - acc: 0.8356 - val_loss: 0.3674 - val_acc: 0.8539\n",
      "Epoch 30/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3773 - acc: 0.8342 - val_loss: 0.3663 - val_acc: 0.8539\n",
      "Epoch 31/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3737 - acc: 0.8391 - val_loss: 0.3667 - val_acc: 0.8522\n",
      "Epoch 32/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3729 - acc: 0.8384 - val_loss: 0.3684 - val_acc: 0.8539\n",
      "Epoch 33/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3687 - acc: 0.8374 - val_loss: 0.3661 - val_acc: 0.8506\n",
      "Training with parameters {'batch_size': 500, 'dropout': 0.1, 'lay_conf': [1024, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_84\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_85 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_252 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_168 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_253 (Dense)            (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_169 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_254 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,606,721\n",
      "Trainable params: 1,606,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.7297 - acc: 0.5448 - val_loss: 0.6510 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.6323 - acc: 0.6451 - val_loss: 0.5612 - val_acc: 0.7521\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.5798 - acc: 0.7115 - val_loss: 0.4966 - val_acc: 0.8046\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.5442 - acc: 0.7302 - val_loss: 0.4755 - val_acc: 0.7997\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.5234 - acc: 0.7510 - val_loss: 0.4780 - val_acc: 0.7915\n",
      "Epoch 6/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.5120 - acc: 0.7604 - val_loss: 0.4328 - val_acc: 0.8276\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4837 - acc: 0.7783 - val_loss: 0.4179 - val_acc: 0.8292\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4766 - acc: 0.7862 - val_loss: 0.4108 - val_acc: 0.8391\n",
      "Epoch 9/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4651 - acc: 0.7927 - val_loss: 0.4034 - val_acc: 0.8407\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4597 - acc: 0.7922 - val_loss: 0.4027 - val_acc: 0.8358\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4536 - acc: 0.7971 - val_loss: 0.3964 - val_acc: 0.8424\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4572 - acc: 0.7889 - val_loss: 0.4310 - val_acc: 0.8144\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4528 - acc: 0.7942 - val_loss: 0.3979 - val_acc: 0.8342\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4399 - acc: 0.8072 - val_loss: 0.3900 - val_acc: 0.8407\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4324 - acc: 0.8088 - val_loss: 0.3908 - val_acc: 0.8407\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4278 - acc: 0.8123 - val_loss: 0.3804 - val_acc: 0.8555\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4231 - acc: 0.8137 - val_loss: 0.3758 - val_acc: 0.8473\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4185 - acc: 0.8146 - val_loss: 0.3817 - val_acc: 0.8456\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4138 - acc: 0.8172 - val_loss: 0.3719 - val_acc: 0.8522\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4102 - acc: 0.8197 - val_loss: 0.3725 - val_acc: 0.8571\n",
      "Epoch 21/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4093 - acc: 0.8150 - val_loss: 0.3703 - val_acc: 0.8522\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4018 - acc: 0.8298 - val_loss: 0.3748 - val_acc: 0.8473\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4079 - acc: 0.8208 - val_loss: 0.3793 - val_acc: 0.8522\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4069 - acc: 0.8210 - val_loss: 0.3696 - val_acc: 0.8571\n",
      "Epoch 25/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4052 - acc: 0.8205 - val_loss: 0.3756 - val_acc: 0.8522\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4003 - acc: 0.8254 - val_loss: 0.3718 - val_acc: 0.8539\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3901 - acc: 0.8309 - val_loss: 0.3710 - val_acc: 0.8522\n",
      "Epoch 28/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3891 - acc: 0.8309 - val_loss: 0.3765 - val_acc: 0.8506\n",
      "Epoch 29/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3925 - acc: 0.8318 - val_loss: 0.3783 - val_acc: 0.8440\n",
      "Training with parameters {'batch_size': 500, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_85\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_86 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_255 (Dense)            (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_170 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_256 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_171 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_257 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,050,113\n",
      "Trainable params: 1,050,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.6771 - acc: 0.5857 - val_loss: 0.6034 - val_acc: 0.6929\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.6101 - acc: 0.6696 - val_loss: 0.5491 - val_acc: 0.7652\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.5672 - acc: 0.7216 - val_loss: 0.5162 - val_acc: 0.7767\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.5426 - acc: 0.7415 - val_loss: 0.4842 - val_acc: 0.8046\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.5196 - acc: 0.7577 - val_loss: 0.4561 - val_acc: 0.8177\n",
      "Epoch 6/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4936 - acc: 0.7763 - val_loss: 0.4418 - val_acc: 0.8259\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4844 - acc: 0.7794 - val_loss: 0.4264 - val_acc: 0.8325\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4688 - acc: 0.7920 - val_loss: 0.4192 - val_acc: 0.8243\n",
      "Epoch 9/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4585 - acc: 0.7920 - val_loss: 0.4094 - val_acc: 0.8342\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4487 - acc: 0.8031 - val_loss: 0.4039 - val_acc: 0.8391\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4424 - acc: 0.8004 - val_loss: 0.4013 - val_acc: 0.8358\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4387 - acc: 0.8070 - val_loss: 0.3954 - val_acc: 0.8424\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4339 - acc: 0.8117 - val_loss: 0.3960 - val_acc: 0.8456\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4312 - acc: 0.8031 - val_loss: 0.4087 - val_acc: 0.8342\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4309 - acc: 0.8081 - val_loss: 0.3895 - val_acc: 0.8473\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4197 - acc: 0.8166 - val_loss: 0.3911 - val_acc: 0.8473\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4158 - acc: 0.8223 - val_loss: 0.3880 - val_acc: 0.8506\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4104 - acc: 0.8194 - val_loss: 0.3829 - val_acc: 0.8522\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4037 - acc: 0.8247 - val_loss: 0.3810 - val_acc: 0.8489\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3997 - acc: 0.8276 - val_loss: 0.3792 - val_acc: 0.8506\n",
      "Epoch 21/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3998 - acc: 0.8241 - val_loss: 0.3857 - val_acc: 0.8489\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4045 - acc: 0.8212 - val_loss: 0.4139 - val_acc: 0.8374\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4125 - acc: 0.8192 - val_loss: 0.3899 - val_acc: 0.8456\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3968 - acc: 0.8292 - val_loss: 0.3820 - val_acc: 0.8424\n",
      "Epoch 25/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3831 - acc: 0.8351 - val_loss: 0.3780 - val_acc: 0.8571\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3832 - acc: 0.8362 - val_loss: 0.3767 - val_acc: 0.8440\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.3785 - acc: 0.8318 - val_loss: 0.3793 - val_acc: 0.8522\n",
      "Epoch 28/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3786 - acc: 0.8340 - val_loss: 0.3747 - val_acc: 0.8555\n",
      "Epoch 29/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3715 - acc: 0.8385 - val_loss: 0.3781 - val_acc: 0.8424\n",
      "Epoch 30/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3671 - acc: 0.8402 - val_loss: 0.3747 - val_acc: 0.8571\n",
      "Epoch 31/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3667 - acc: 0.8425 - val_loss: 0.3765 - val_acc: 0.8506\n",
      "Epoch 32/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3578 - acc: 0.8436 - val_loss: 0.3746 - val_acc: 0.8489\n",
      "Epoch 33/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3561 - acc: 0.8473 - val_loss: 0.3770 - val_acc: 0.8539\n",
      "Epoch 34/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3561 - acc: 0.8451 - val_loss: 0.3761 - val_acc: 0.8522\n",
      "Epoch 35/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3510 - acc: 0.8447 - val_loss: 0.3739 - val_acc: 0.8456\n",
      "Epoch 36/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3480 - acc: 0.8511 - val_loss: 0.3710 - val_acc: 0.8506\n",
      "Epoch 37/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.3425 - acc: 0.8531 - val_loss: 0.3732 - val_acc: 0.8473\n",
      "Epoch 38/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3402 - acc: 0.8546 - val_loss: 0.3750 - val_acc: 0.8407\n",
      "Epoch 39/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3318 - acc: 0.8599 - val_loss: 0.3711 - val_acc: 0.8456\n",
      "Epoch 40/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3289 - acc: 0.8535 - val_loss: 0.3734 - val_acc: 0.8473\n",
      "Epoch 41/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3323 - acc: 0.8542 - val_loss: 0.3727 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 500, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_86\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_87 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_258 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_172 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_259 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_173 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_260 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 401,729\n",
      "Trainable params: 401,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 17ms/step - loss: 0.7775 - acc: 0.5090 - val_loss: 0.6713 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.6752 - acc: 0.5880 - val_loss: 0.6352 - val_acc: 0.7044\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6480 - acc: 0.6243 - val_loss: 0.6050 - val_acc: 0.7307\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6281 - acc: 0.6572 - val_loss: 0.5815 - val_acc: 0.7455\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.6080 - acc: 0.6771 - val_loss: 0.5569 - val_acc: 0.7488\n",
      "Epoch 6/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5886 - acc: 0.6951 - val_loss: 0.5378 - val_acc: 0.7783\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5764 - acc: 0.7126 - val_loss: 0.5213 - val_acc: 0.7931\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5553 - acc: 0.7274 - val_loss: 0.5030 - val_acc: 0.8062\n",
      "Epoch 9/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5461 - acc: 0.7365 - val_loss: 0.4892 - val_acc: 0.8046\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5319 - acc: 0.7528 - val_loss: 0.4732 - val_acc: 0.8062\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.5148 - acc: 0.7561 - val_loss: 0.4610 - val_acc: 0.8095\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5045 - acc: 0.7654 - val_loss: 0.4506 - val_acc: 0.8161\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.4968 - acc: 0.7721 - val_loss: 0.4409 - val_acc: 0.8210\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.4885 - acc: 0.7785 - val_loss: 0.4366 - val_acc: 0.8144\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4821 - acc: 0.7820 - val_loss: 0.4281 - val_acc: 0.8342\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4701 - acc: 0.7882 - val_loss: 0.4210 - val_acc: 0.8342\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4669 - acc: 0.7940 - val_loss: 0.4177 - val_acc: 0.8292\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.4616 - acc: 0.7955 - val_loss: 0.4132 - val_acc: 0.8358\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.4609 - acc: 0.7966 - val_loss: 0.4228 - val_acc: 0.8276\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4563 - acc: 0.7986 - val_loss: 0.4055 - val_acc: 0.8407\n",
      "Epoch 21/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4507 - acc: 0.8019 - val_loss: 0.4074 - val_acc: 0.8292\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4510 - acc: 0.8002 - val_loss: 0.4089 - val_acc: 0.8292\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4429 - acc: 0.8061 - val_loss: 0.4000 - val_acc: 0.8342\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4400 - acc: 0.8035 - val_loss: 0.3966 - val_acc: 0.8391\n",
      "Epoch 25/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4376 - acc: 0.8101 - val_loss: 0.3947 - val_acc: 0.8358\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4304 - acc: 0.8117 - val_loss: 0.3976 - val_acc: 0.8342\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4354 - acc: 0.8061 - val_loss: 0.3961 - val_acc: 0.8407\n",
      "Epoch 28/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4239 - acc: 0.8168 - val_loss: 0.3900 - val_acc: 0.8424\n",
      "Epoch 29/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4249 - acc: 0.8154 - val_loss: 0.3890 - val_acc: 0.8456\n",
      "Epoch 30/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4242 - acc: 0.8134 - val_loss: 0.3910 - val_acc: 0.8489\n",
      "Epoch 31/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4174 - acc: 0.8197 - val_loss: 0.3871 - val_acc: 0.8456\n",
      "Epoch 32/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4182 - acc: 0.8134 - val_loss: 0.3963 - val_acc: 0.8342\n",
      "Epoch 33/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4232 - acc: 0.8141 - val_loss: 0.3861 - val_acc: 0.8489\n",
      "Epoch 34/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4106 - acc: 0.8208 - val_loss: 0.3844 - val_acc: 0.8456\n",
      "Epoch 35/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4121 - acc: 0.8188 - val_loss: 0.3876 - val_acc: 0.8473\n",
      "Epoch 36/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.4094 - acc: 0.8274 - val_loss: 0.3815 - val_acc: 0.8456\n",
      "Epoch 37/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4105 - acc: 0.8245 - val_loss: 0.3907 - val_acc: 0.8473\n",
      "Epoch 38/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4042 - acc: 0.8287 - val_loss: 0.3822 - val_acc: 0.8440\n",
      "Epoch 39/300\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.4061 - acc: 0.8258 - val_loss: 0.3889 - val_acc: 0.8473\n",
      "Epoch 40/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4048 - acc: 0.8232 - val_loss: 0.3803 - val_acc: 0.8506\n",
      "Epoch 41/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3985 - acc: 0.8276 - val_loss: 0.3811 - val_acc: 0.8456\n",
      "Epoch 42/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3952 - acc: 0.8287 - val_loss: 0.3778 - val_acc: 0.8456\n",
      "Epoch 43/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3953 - acc: 0.8311 - val_loss: 0.3807 - val_acc: 0.8456\n",
      "Epoch 44/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3998 - acc: 0.8274 - val_loss: 0.3929 - val_acc: 0.8374\n",
      "Epoch 45/300\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.3950 - acc: 0.8307 - val_loss: 0.3780 - val_acc: 0.8456\n",
      "Epoch 46/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3872 - acc: 0.8312 - val_loss: 0.3807 - val_acc: 0.8424\n",
      "Epoch 47/300\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.3908 - acc: 0.8325 - val_loss: 0.3771 - val_acc: 0.8473\n",
      "Epoch 48/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3927 - acc: 0.8332 - val_loss: 0.3822 - val_acc: 0.8473\n",
      "Epoch 49/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3889 - acc: 0.8338 - val_loss: 0.3782 - val_acc: 0.8391\n",
      "Epoch 50/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3865 - acc: 0.8331 - val_loss: 0.3764 - val_acc: 0.8473\n",
      "Epoch 51/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3861 - acc: 0.8367 - val_loss: 0.3784 - val_acc: 0.8440\n",
      "Epoch 52/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3806 - acc: 0.8356 - val_loss: 0.3781 - val_acc: 0.8456\n",
      "Epoch 53/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3793 - acc: 0.8382 - val_loss: 0.3755 - val_acc: 0.8440\n",
      "Epoch 54/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3819 - acc: 0.8343 - val_loss: 0.3771 - val_acc: 0.8440\n",
      "Epoch 55/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3792 - acc: 0.8354 - val_loss: 0.3794 - val_acc: 0.8424\n",
      "Epoch 56/300\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.3803 - acc: 0.8356 - val_loss: 0.3764 - val_acc: 0.8424\n",
      "Epoch 57/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3754 - acc: 0.8415 - val_loss: 0.3780 - val_acc: 0.8391\n",
      "Epoch 58/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3756 - acc: 0.8374 - val_loss: 0.3764 - val_acc: 0.8473\n",
      "Training with parameters {'batch_size': 500, 'dropout': 0.1, 'lay_conf': [1024, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_87\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_88 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_261 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_174 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_262 (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_175 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_263 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,836,545\n",
      "Trainable params: 1,836,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 20ms/step - loss: 0.7487 - acc: 0.5619 - val_loss: 0.6412 - val_acc: 0.6634\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.6284 - acc: 0.6462 - val_loss: 0.5760 - val_acc: 0.7241\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.5835 - acc: 0.7061 - val_loss: 0.5239 - val_acc: 0.7931\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.5497 - acc: 0.7369 - val_loss: 0.4953 - val_acc: 0.7849\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.5246 - acc: 0.7488 - val_loss: 0.4638 - val_acc: 0.8161\n",
      "Epoch 6/300\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.4961 - acc: 0.7747 - val_loss: 0.4388 - val_acc: 0.8194\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4847 - acc: 0.7778 - val_loss: 0.4250 - val_acc: 0.8227\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4726 - acc: 0.7853 - val_loss: 0.4187 - val_acc: 0.8227\n",
      "Epoch 9/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4591 - acc: 0.7922 - val_loss: 0.4110 - val_acc: 0.8243\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.4554 - acc: 0.7969 - val_loss: 0.4033 - val_acc: 0.8407\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4432 - acc: 0.8017 - val_loss: 0.3921 - val_acc: 0.8440\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4337 - acc: 0.8068 - val_loss: 0.3900 - val_acc: 0.8506\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4270 - acc: 0.8135 - val_loss: 0.3867 - val_acc: 0.8506\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4207 - acc: 0.8137 - val_loss: 0.3831 - val_acc: 0.8555\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4202 - acc: 0.8112 - val_loss: 0.3813 - val_acc: 0.8473\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4149 - acc: 0.8159 - val_loss: 0.3782 - val_acc: 0.8473\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4091 - acc: 0.8181 - val_loss: 0.3780 - val_acc: 0.8489\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4060 - acc: 0.8221 - val_loss: 0.3932 - val_acc: 0.8292\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4051 - acc: 0.8254 - val_loss: 0.4105 - val_acc: 0.8161\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4081 - acc: 0.8196 - val_loss: 0.3826 - val_acc: 0.8407\n",
      "Epoch 21/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.3957 - acc: 0.8265 - val_loss: 0.3731 - val_acc: 0.8506\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.3886 - acc: 0.8298 - val_loss: 0.3782 - val_acc: 0.8424\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.3927 - acc: 0.8250 - val_loss: 0.3697 - val_acc: 0.8555\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3889 - acc: 0.8311 - val_loss: 0.3833 - val_acc: 0.8473\n",
      "Epoch 25/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.3857 - acc: 0.8329 - val_loss: 0.3688 - val_acc: 0.8456\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.3815 - acc: 0.8352 - val_loss: 0.3751 - val_acc: 0.8489\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3747 - acc: 0.8373 - val_loss: 0.3791 - val_acc: 0.8391\n",
      "Epoch 28/300\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.3841 - acc: 0.8296 - val_loss: 0.3686 - val_acc: 0.8506\n",
      "Epoch 29/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3683 - acc: 0.8436 - val_loss: 0.3660 - val_acc: 0.8440\n",
      "Epoch 30/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.3668 - acc: 0.8429 - val_loss: 0.4158 - val_acc: 0.8112\n",
      "Epoch 31/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.3769 - acc: 0.8349 - val_loss: 0.3662 - val_acc: 0.8489\n",
      "Epoch 32/300\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.3562 - acc: 0.8464 - val_loss: 0.3810 - val_acc: 0.8374\n",
      "Epoch 33/300\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.3566 - acc: 0.8455 - val_loss: 0.3784 - val_acc: 0.8391\n",
      "Epoch 34/300\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.3577 - acc: 0.8460 - val_loss: 0.3686 - val_acc: 0.8539\n",
      "Training with parameters {'batch_size': 700, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_88\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_89 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_264 (Dense)            (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_176 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_265 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_177 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_266 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.7351 - acc: 0.4997 - val_loss: 0.6800 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.6760 - acc: 0.5833 - val_loss: 0.6500 - val_acc: 0.6273\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.6592 - acc: 0.6145 - val_loss: 0.6280 - val_acc: 0.6913\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.6417 - acc: 0.6395 - val_loss: 0.6025 - val_acc: 0.7061\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.6252 - acc: 0.6630 - val_loss: 0.5793 - val_acc: 0.7438\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.6110 - acc: 0.6756 - val_loss: 0.5582 - val_acc: 0.7553\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5930 - acc: 0.6971 - val_loss: 0.5386 - val_acc: 0.7718\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.5739 - acc: 0.7196 - val_loss: 0.5226 - val_acc: 0.7767\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5659 - acc: 0.7247 - val_loss: 0.5083 - val_acc: 0.7947\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.5506 - acc: 0.7365 - val_loss: 0.4914 - val_acc: 0.8046\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5359 - acc: 0.7541 - val_loss: 0.4789 - val_acc: 0.8095\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5249 - acc: 0.7617 - val_loss: 0.4678 - val_acc: 0.8177\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5149 - acc: 0.7668 - val_loss: 0.4597 - val_acc: 0.8128\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5067 - acc: 0.7725 - val_loss: 0.4500 - val_acc: 0.8227\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4963 - acc: 0.7756 - val_loss: 0.4416 - val_acc: 0.8243\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4911 - acc: 0.7820 - val_loss: 0.4350 - val_acc: 0.8161\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4808 - acc: 0.7885 - val_loss: 0.4294 - val_acc: 0.8177\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4853 - acc: 0.7763 - val_loss: 0.4239 - val_acc: 0.8210\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4794 - acc: 0.7820 - val_loss: 0.4196 - val_acc: 0.8309\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4696 - acc: 0.7922 - val_loss: 0.4167 - val_acc: 0.8309\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4662 - acc: 0.7905 - val_loss: 0.4160 - val_acc: 0.8259\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.4637 - acc: 0.7957 - val_loss: 0.4121 - val_acc: 0.8276\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4596 - acc: 0.7973 - val_loss: 0.4067 - val_acc: 0.8325\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4526 - acc: 0.8000 - val_loss: 0.4064 - val_acc: 0.8358\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4523 - acc: 0.8019 - val_loss: 0.4047 - val_acc: 0.8358\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4516 - acc: 0.7997 - val_loss: 0.4020 - val_acc: 0.8391\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4483 - acc: 0.8031 - val_loss: 0.3996 - val_acc: 0.8374\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4441 - acc: 0.8028 - val_loss: 0.3986 - val_acc: 0.8358\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4387 - acc: 0.8093 - val_loss: 0.3962 - val_acc: 0.8391\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4342 - acc: 0.8135 - val_loss: 0.3951 - val_acc: 0.8424\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4357 - acc: 0.8066 - val_loss: 0.3956 - val_acc: 0.8374\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4324 - acc: 0.8097 - val_loss: 0.3947 - val_acc: 0.8407\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4342 - acc: 0.8099 - val_loss: 0.3927 - val_acc: 0.8489\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4281 - acc: 0.8126 - val_loss: 0.3925 - val_acc: 0.8440\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4275 - acc: 0.8130 - val_loss: 0.3908 - val_acc: 0.8456\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4248 - acc: 0.8099 - val_loss: 0.3895 - val_acc: 0.8489\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4240 - acc: 0.8170 - val_loss: 0.3887 - val_acc: 0.8489\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4232 - acc: 0.8161 - val_loss: 0.3880 - val_acc: 0.8456\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4169 - acc: 0.8177 - val_loss: 0.3861 - val_acc: 0.8473\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4171 - acc: 0.8210 - val_loss: 0.3854 - val_acc: 0.8456\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4172 - acc: 0.8183 - val_loss: 0.3848 - val_acc: 0.8456\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4144 - acc: 0.8214 - val_loss: 0.3843 - val_acc: 0.8456\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4173 - acc: 0.8155 - val_loss: 0.3884 - val_acc: 0.8407\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4135 - acc: 0.8185 - val_loss: 0.3838 - val_acc: 0.8473\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4126 - acc: 0.8248 - val_loss: 0.3877 - val_acc: 0.8440\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4060 - acc: 0.8239 - val_loss: 0.3873 - val_acc: 0.8407\n",
      "Epoch 47/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4105 - acc: 0.8192 - val_loss: 0.3865 - val_acc: 0.8440\n",
      "Epoch 48/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4091 - acc: 0.8247 - val_loss: 0.3830 - val_acc: 0.8473\n",
      "Epoch 49/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4048 - acc: 0.8247 - val_loss: 0.3834 - val_acc: 0.8440\n",
      "Epoch 50/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4013 - acc: 0.8239 - val_loss: 0.3821 - val_acc: 0.8456\n",
      "Epoch 51/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4068 - acc: 0.8221 - val_loss: 0.3848 - val_acc: 0.8440\n",
      "Epoch 52/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4015 - acc: 0.8217 - val_loss: 0.3896 - val_acc: 0.8342\n",
      "Epoch 53/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4015 - acc: 0.8248 - val_loss: 0.3803 - val_acc: 0.8473\n",
      "Epoch 54/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3964 - acc: 0.8311 - val_loss: 0.3814 - val_acc: 0.8489\n",
      "Epoch 55/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3977 - acc: 0.8292 - val_loss: 0.3863 - val_acc: 0.8358\n",
      "Epoch 56/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3991 - acc: 0.8234 - val_loss: 0.3819 - val_acc: 0.8456\n",
      "Epoch 57/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3953 - acc: 0.8294 - val_loss: 0.3797 - val_acc: 0.8440\n",
      "Epoch 58/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3923 - acc: 0.8287 - val_loss: 0.3811 - val_acc: 0.8489\n",
      "Epoch 59/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3969 - acc: 0.8258 - val_loss: 0.3783 - val_acc: 0.8522\n",
      "Epoch 60/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3910 - acc: 0.8301 - val_loss: 0.3824 - val_acc: 0.8456\n",
      "Epoch 61/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3923 - acc: 0.8312 - val_loss: 0.3776 - val_acc: 0.8489\n",
      "Epoch 62/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3858 - acc: 0.8323 - val_loss: 0.3774 - val_acc: 0.8489\n",
      "Epoch 63/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3841 - acc: 0.8325 - val_loss: 0.3782 - val_acc: 0.8506\n",
      "Epoch 64/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3800 - acc: 0.8354 - val_loss: 0.3784 - val_acc: 0.8473\n",
      "Epoch 65/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3831 - acc: 0.8309 - val_loss: 0.3787 - val_acc: 0.8489\n",
      "Epoch 66/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3807 - acc: 0.8340 - val_loss: 0.3799 - val_acc: 0.8456\n",
      "Epoch 67/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3808 - acc: 0.8323 - val_loss: 0.3781 - val_acc: 0.8506\n",
      "Training with parameters {'batch_size': 700, 'dropout': 0.1, 'lay_conf': [1024, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_89\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_90 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_267 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_178 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_268 (Dense)            (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_179 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_269 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "8/8 [==============================] - 1s 35ms/step - loss: 0.7740 - acc: 0.5346 - val_loss: 0.6827 - val_acc: 0.5681\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.6656 - acc: 0.5961 - val_loss: 0.6101 - val_acc: 0.6256\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.6216 - acc: 0.6585 - val_loss: 0.5628 - val_acc: 0.7504\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.5883 - acc: 0.6970 - val_loss: 0.5501 - val_acc: 0.7291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.5710 - acc: 0.7126 - val_loss: 0.5068 - val_acc: 0.8013\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.5420 - acc: 0.7375 - val_loss: 0.4839 - val_acc: 0.7997\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.5242 - acc: 0.7475 - val_loss: 0.4677 - val_acc: 0.8144\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.5089 - acc: 0.7670 - val_loss: 0.4512 - val_acc: 0.8079\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.4969 - acc: 0.7703 - val_loss: 0.4351 - val_acc: 0.8292\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.4805 - acc: 0.7789 - val_loss: 0.4237 - val_acc: 0.8391\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4722 - acc: 0.7836 - val_loss: 0.4177 - val_acc: 0.8358\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4615 - acc: 0.7944 - val_loss: 0.4078 - val_acc: 0.8358\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.4540 - acc: 0.7975 - val_loss: 0.4040 - val_acc: 0.8391\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4505 - acc: 0.7991 - val_loss: 0.4105 - val_acc: 0.8292\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4441 - acc: 0.8000 - val_loss: 0.3936 - val_acc: 0.8424\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4353 - acc: 0.8086 - val_loss: 0.3901 - val_acc: 0.8456\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4298 - acc: 0.8097 - val_loss: 0.3903 - val_acc: 0.8391\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4254 - acc: 0.8093 - val_loss: 0.3891 - val_acc: 0.8440\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4234 - acc: 0.8144 - val_loss: 0.3826 - val_acc: 0.8440\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4196 - acc: 0.8185 - val_loss: 0.3804 - val_acc: 0.8440\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4200 - acc: 0.8126 - val_loss: 0.3904 - val_acc: 0.8407\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4109 - acc: 0.8163 - val_loss: 0.3809 - val_acc: 0.8440\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4095 - acc: 0.8179 - val_loss: 0.3782 - val_acc: 0.8506\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4044 - acc: 0.8212 - val_loss: 0.3794 - val_acc: 0.8407\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.4075 - acc: 0.8216 - val_loss: 0.3749 - val_acc: 0.8456\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4004 - acc: 0.8269 - val_loss: 0.3729 - val_acc: 0.8489\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3954 - acc: 0.8267 - val_loss: 0.3774 - val_acc: 0.8456\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3911 - acc: 0.8274 - val_loss: 0.3727 - val_acc: 0.8473\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3905 - acc: 0.8276 - val_loss: 0.3732 - val_acc: 0.8489\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.3826 - acc: 0.8332 - val_loss: 0.3700 - val_acc: 0.8489\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3855 - acc: 0.8320 - val_loss: 0.3705 - val_acc: 0.8456\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3783 - acc: 0.8362 - val_loss: 0.3723 - val_acc: 0.8489\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3764 - acc: 0.8345 - val_loss: 0.3707 - val_acc: 0.8473\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3739 - acc: 0.8358 - val_loss: 0.3770 - val_acc: 0.8424\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.3804 - acc: 0.8345 - val_loss: 0.3719 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 700, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_90\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_91 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_270 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_180 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_271 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_181 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_272 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,985\n",
      "Trainable params: 409,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 1.1005 - acc: 0.4534 - val_loss: 0.7522 - val_acc: 0.5731\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.7574 - acc: 0.5632 - val_loss: 0.6811 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.6825 - acc: 0.5700 - val_loss: 0.6602 - val_acc: 0.6617\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.6696 - acc: 0.5928 - val_loss: 0.6296 - val_acc: 0.6125\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.6498 - acc: 0.6200 - val_loss: 0.6100 - val_acc: 0.7143\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.6308 - acc: 0.6564 - val_loss: 0.5920 - val_acc: 0.7586\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.6160 - acc: 0.6731 - val_loss: 0.5730 - val_acc: 0.7701\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.6050 - acc: 0.6884 - val_loss: 0.5555 - val_acc: 0.7800\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5936 - acc: 0.6906 - val_loss: 0.5400 - val_acc: 0.7898\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5782 - acc: 0.7172 - val_loss: 0.5279 - val_acc: 0.7947\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5692 - acc: 0.7119 - val_loss: 0.5155 - val_acc: 0.7980\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5575 - acc: 0.7345 - val_loss: 0.5037 - val_acc: 0.8062\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5462 - acc: 0.7417 - val_loss: 0.4913 - val_acc: 0.8161\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.5365 - acc: 0.7455 - val_loss: 0.4841 - val_acc: 0.8161\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.5285 - acc: 0.7528 - val_loss: 0.4726 - val_acc: 0.8161\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.5160 - acc: 0.7584 - val_loss: 0.4649 - val_acc: 0.8095\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.5089 - acc: 0.7659 - val_loss: 0.4567 - val_acc: 0.8177\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.5039 - acc: 0.7666 - val_loss: 0.4498 - val_acc: 0.8210\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4971 - acc: 0.7714 - val_loss: 0.4451 - val_acc: 0.8243\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4921 - acc: 0.7776 - val_loss: 0.4386 - val_acc: 0.8259\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4867 - acc: 0.7791 - val_loss: 0.4338 - val_acc: 0.8243\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4785 - acc: 0.7873 - val_loss: 0.4286 - val_acc: 0.8391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4714 - acc: 0.7869 - val_loss: 0.4283 - val_acc: 0.8259\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4715 - acc: 0.7805 - val_loss: 0.4212 - val_acc: 0.8391\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4686 - acc: 0.7862 - val_loss: 0.4178 - val_acc: 0.8407\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4645 - acc: 0.7902 - val_loss: 0.4145 - val_acc: 0.8440\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4570 - acc: 0.7946 - val_loss: 0.4126 - val_acc: 0.8407\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4538 - acc: 0.7968 - val_loss: 0.4098 - val_acc: 0.8440\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4512 - acc: 0.7988 - val_loss: 0.4084 - val_acc: 0.8440\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4502 - acc: 0.8020 - val_loss: 0.4066 - val_acc: 0.8407\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4460 - acc: 0.8037 - val_loss: 0.4063 - val_acc: 0.8424\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4444 - acc: 0.8041 - val_loss: 0.4039 - val_acc: 0.8440\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4432 - acc: 0.8066 - val_loss: 0.4009 - val_acc: 0.8391\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4411 - acc: 0.8035 - val_loss: 0.3997 - val_acc: 0.8424\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4339 - acc: 0.8095 - val_loss: 0.3982 - val_acc: 0.8424\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4324 - acc: 0.8084 - val_loss: 0.3978 - val_acc: 0.8407\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4345 - acc: 0.8073 - val_loss: 0.3979 - val_acc: 0.8424\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4333 - acc: 0.8104 - val_loss: 0.3973 - val_acc: 0.8407\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4262 - acc: 0.8166 - val_loss: 0.3937 - val_acc: 0.8440\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4263 - acc: 0.8139 - val_loss: 0.3927 - val_acc: 0.8440\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4236 - acc: 0.8166 - val_loss: 0.3957 - val_acc: 0.8456\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4200 - acc: 0.8181 - val_loss: 0.3908 - val_acc: 0.8407\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4192 - acc: 0.8183 - val_loss: 0.3946 - val_acc: 0.8440\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4198 - acc: 0.8139 - val_loss: 0.3961 - val_acc: 0.8424\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4211 - acc: 0.8170 - val_loss: 0.3910 - val_acc: 0.8440\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4163 - acc: 0.8155 - val_loss: 0.3895 - val_acc: 0.8407\n",
      "Epoch 47/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4135 - acc: 0.8216 - val_loss: 0.3901 - val_acc: 0.8424\n",
      "Epoch 48/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4150 - acc: 0.8192 - val_loss: 0.3967 - val_acc: 0.8407\n",
      "Epoch 49/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4186 - acc: 0.8163 - val_loss: 0.4054 - val_acc: 0.8342\n",
      "Epoch 50/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4198 - acc: 0.8144 - val_loss: 0.3976 - val_acc: 0.8391\n",
      "Epoch 51/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4157 - acc: 0.8197 - val_loss: 0.3866 - val_acc: 0.8489\n",
      "Epoch 52/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4081 - acc: 0.8227 - val_loss: 0.3870 - val_acc: 0.8440\n",
      "Epoch 53/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4076 - acc: 0.8219 - val_loss: 0.3861 - val_acc: 0.8424\n",
      "Epoch 54/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4055 - acc: 0.8256 - val_loss: 0.3915 - val_acc: 0.8424\n",
      "Epoch 55/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4081 - acc: 0.8232 - val_loss: 0.3937 - val_acc: 0.8407\n",
      "Epoch 56/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4064 - acc: 0.8234 - val_loss: 0.3863 - val_acc: 0.8391\n",
      "Epoch 57/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4023 - acc: 0.8261 - val_loss: 0.3869 - val_acc: 0.8407\n",
      "Epoch 58/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3959 - acc: 0.8289 - val_loss: 0.3855 - val_acc: 0.8440\n",
      "Epoch 59/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3989 - acc: 0.8276 - val_loss: 0.3851 - val_acc: 0.8506\n",
      "Epoch 60/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3974 - acc: 0.8301 - val_loss: 0.3855 - val_acc: 0.8407\n",
      "Epoch 61/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3972 - acc: 0.8289 - val_loss: 0.3867 - val_acc: 0.8424\n",
      "Epoch 62/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3987 - acc: 0.8276 - val_loss: 0.3853 - val_acc: 0.8473\n",
      "Epoch 63/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3931 - acc: 0.8331 - val_loss: 0.3887 - val_acc: 0.8374\n",
      "Epoch 64/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3958 - acc: 0.8301 - val_loss: 0.3840 - val_acc: 0.8489\n",
      "Epoch 65/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3908 - acc: 0.8305 - val_loss: 0.3838 - val_acc: 0.8489\n",
      "Epoch 66/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3905 - acc: 0.8300 - val_loss: 0.3833 - val_acc: 0.8456\n",
      "Epoch 67/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3886 - acc: 0.8325 - val_loss: 0.3841 - val_acc: 0.8506\n",
      "Epoch 68/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3880 - acc: 0.8314 - val_loss: 0.3829 - val_acc: 0.8473\n",
      "Epoch 69/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3882 - acc: 0.8320 - val_loss: 0.3837 - val_acc: 0.8489\n",
      "Epoch 70/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3842 - acc: 0.8354 - val_loss: 0.3835 - val_acc: 0.8489\n",
      "Epoch 71/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3821 - acc: 0.8365 - val_loss: 0.3839 - val_acc: 0.8407\n",
      "Epoch 72/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3825 - acc: 0.8336 - val_loss: 0.3829 - val_acc: 0.8473\n",
      "Epoch 73/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3862 - acc: 0.8320 - val_loss: 0.3827 - val_acc: 0.8489\n",
      "Epoch 74/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3850 - acc: 0.8342 - val_loss: 0.3883 - val_acc: 0.8374\n",
      "Epoch 75/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3804 - acc: 0.8382 - val_loss: 0.3819 - val_acc: 0.8489\n",
      "Epoch 76/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3803 - acc: 0.8373 - val_loss: 0.3816 - val_acc: 0.8506\n",
      "Epoch 77/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3756 - acc: 0.8402 - val_loss: 0.3846 - val_acc: 0.8407\n",
      "Epoch 78/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3736 - acc: 0.8398 - val_loss: 0.3813 - val_acc: 0.8506\n",
      "Epoch 79/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3757 - acc: 0.8418 - val_loss: 0.3818 - val_acc: 0.8489\n",
      "Epoch 80/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3737 - acc: 0.8425 - val_loss: 0.3819 - val_acc: 0.8456\n",
      "Epoch 81/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3746 - acc: 0.8415 - val_loss: 0.3815 - val_acc: 0.8473\n",
      "Epoch 82/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3698 - acc: 0.8435 - val_loss: 0.3814 - val_acc: 0.8456\n",
      "Epoch 83/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3701 - acc: 0.8447 - val_loss: 0.3870 - val_acc: 0.8407\n",
      "Training with parameters {'batch_size': 700, 'dropout': 0.1, 'lay_conf': [1024, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_91\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_92 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_273 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_182 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_274 (Dense)            (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_183 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_275 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,639,553\n",
      "Trainable params: 1,639,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "8/8 [==============================] - 1s 32ms/step - loss: 0.7118 - acc: 0.5598 - val_loss: 0.6190 - val_acc: 0.7061\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.6223 - acc: 0.6625 - val_loss: 0.5681 - val_acc: 0.7077\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.5872 - acc: 0.6979 - val_loss: 0.5243 - val_acc: 0.7734\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.5547 - acc: 0.7389 - val_loss: 0.4890 - val_acc: 0.8013\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.5295 - acc: 0.7449 - val_loss: 0.4708 - val_acc: 0.8046\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.5122 - acc: 0.7568 - val_loss: 0.4446 - val_acc: 0.8276\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.5002 - acc: 0.7674 - val_loss: 0.4409 - val_acc: 0.8292\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4858 - acc: 0.7745 - val_loss: 0.4313 - val_acc: 0.8276\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4752 - acc: 0.7842 - val_loss: 0.4140 - val_acc: 0.8374\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4693 - acc: 0.7873 - val_loss: 0.4187 - val_acc: 0.8243\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4657 - acc: 0.7902 - val_loss: 0.4148 - val_acc: 0.8259\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4552 - acc: 0.7988 - val_loss: 0.4133 - val_acc: 0.8276\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.4494 - acc: 0.8037 - val_loss: 0.3950 - val_acc: 0.8506\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4435 - acc: 0.8002 - val_loss: 0.3921 - val_acc: 0.8506\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4411 - acc: 0.8055 - val_loss: 0.3898 - val_acc: 0.8473\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4325 - acc: 0.8061 - val_loss: 0.3864 - val_acc: 0.8539\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4293 - acc: 0.8126 - val_loss: 0.3817 - val_acc: 0.8539\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.4250 - acc: 0.8152 - val_loss: 0.3804 - val_acc: 0.8539\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4281 - acc: 0.8104 - val_loss: 0.3870 - val_acc: 0.8489\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4221 - acc: 0.8121 - val_loss: 0.3787 - val_acc: 0.8555\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4169 - acc: 0.8199 - val_loss: 0.3770 - val_acc: 0.8539\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4065 - acc: 0.8225 - val_loss: 0.3746 - val_acc: 0.8555\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4079 - acc: 0.8150 - val_loss: 0.3728 - val_acc: 0.8506\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4066 - acc: 0.8270 - val_loss: 0.3745 - val_acc: 0.8522\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4041 - acc: 0.8227 - val_loss: 0.3704 - val_acc: 0.8571\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3981 - acc: 0.8270 - val_loss: 0.3717 - val_acc: 0.8555\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3941 - acc: 0.8261 - val_loss: 0.3722 - val_acc: 0.8506\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3960 - acc: 0.8309 - val_loss: 0.3699 - val_acc: 0.8489\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3923 - acc: 0.8311 - val_loss: 0.3696 - val_acc: 0.8539\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3875 - acc: 0.8323 - val_loss: 0.3803 - val_acc: 0.8506\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3877 - acc: 0.8311 - val_loss: 0.3714 - val_acc: 0.8555\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3839 - acc: 0.8311 - val_loss: 0.3697 - val_acc: 0.8506\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3823 - acc: 0.8325 - val_loss: 0.3667 - val_acc: 0.8506\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3746 - acc: 0.8342 - val_loss: 0.3663 - val_acc: 0.8555\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3735 - acc: 0.8367 - val_loss: 0.3670 - val_acc: 0.8489\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3730 - acc: 0.8413 - val_loss: 0.3686 - val_acc: 0.8522\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3714 - acc: 0.8418 - val_loss: 0.3673 - val_acc: 0.8522\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3649 - acc: 0.8418 - val_loss: 0.3707 - val_acc: 0.8489\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3606 - acc: 0.8418 - val_loss: 0.3656 - val_acc: 0.8522\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3616 - acc: 0.8422 - val_loss: 0.3674 - val_acc: 0.8522\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3568 - acc: 0.8489 - val_loss: 0.3786 - val_acc: 0.8522\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3631 - acc: 0.8418 - val_loss: 0.3672 - val_acc: 0.8489\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3665 - acc: 0.8396 - val_loss: 0.3720 - val_acc: 0.8506\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3607 - acc: 0.8415 - val_loss: 0.3686 - val_acc: 0.8440\n",
      "Training with parameters {'batch_size': 700, 'dropout': 0.1, 'lay_conf': [1024, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_92\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_93 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_276 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_184 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_277 (Dense)            (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_185 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_278 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,606,721\n",
      "Trainable params: 1,606,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.7666 - acc: 0.5340 - val_loss: 0.6760 - val_acc: 0.5731\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.6647 - acc: 0.6015 - val_loss: 0.6107 - val_acc: 0.6913\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.6223 - acc: 0.6541 - val_loss: 0.5553 - val_acc: 0.7603\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.5823 - acc: 0.7006 - val_loss: 0.5078 - val_acc: 0.8030\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.5518 - acc: 0.7316 - val_loss: 0.4823 - val_acc: 0.8062\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.5313 - acc: 0.7522 - val_loss: 0.4634 - val_acc: 0.8144\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.5146 - acc: 0.7677 - val_loss: 0.4438 - val_acc: 0.8161\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4994 - acc: 0.7712 - val_loss: 0.4291 - val_acc: 0.8309\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4887 - acc: 0.7754 - val_loss: 0.4275 - val_acc: 0.8243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4816 - acc: 0.7794 - val_loss: 0.4113 - val_acc: 0.8325\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4743 - acc: 0.7860 - val_loss: 0.4086 - val_acc: 0.8407\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4657 - acc: 0.7871 - val_loss: 0.4062 - val_acc: 0.8342\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4604 - acc: 0.7947 - val_loss: 0.3996 - val_acc: 0.8325\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4494 - acc: 0.8000 - val_loss: 0.3931 - val_acc: 0.8424\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4468 - acc: 0.7997 - val_loss: 0.3925 - val_acc: 0.8440\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4453 - acc: 0.8030 - val_loss: 0.3909 - val_acc: 0.8473\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4371 - acc: 0.8030 - val_loss: 0.4050 - val_acc: 0.8309\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4438 - acc: 0.8026 - val_loss: 0.3929 - val_acc: 0.8440\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4322 - acc: 0.8135 - val_loss: 0.3800 - val_acc: 0.8440\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4281 - acc: 0.8130 - val_loss: 0.3802 - val_acc: 0.8555\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4227 - acc: 0.8168 - val_loss: 0.3790 - val_acc: 0.8489\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4224 - acc: 0.8154 - val_loss: 0.3760 - val_acc: 0.8473\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4206 - acc: 0.8139 - val_loss: 0.3862 - val_acc: 0.8456\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4119 - acc: 0.8179 - val_loss: 0.3728 - val_acc: 0.8506\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4127 - acc: 0.8177 - val_loss: 0.3762 - val_acc: 0.8522\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4174 - acc: 0.8174 - val_loss: 0.3885 - val_acc: 0.8424\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4163 - acc: 0.8161 - val_loss: 0.3795 - val_acc: 0.8473\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4116 - acc: 0.8203 - val_loss: 0.3696 - val_acc: 0.8555\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4025 - acc: 0.8223 - val_loss: 0.3693 - val_acc: 0.8555\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4027 - acc: 0.8227 - val_loss: 0.3713 - val_acc: 0.8456\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3958 - acc: 0.8318 - val_loss: 0.3749 - val_acc: 0.8506\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3936 - acc: 0.8250 - val_loss: 0.3722 - val_acc: 0.8489\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4023 - acc: 0.8207 - val_loss: 0.3684 - val_acc: 0.8539\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3885 - acc: 0.8354 - val_loss: 0.3684 - val_acc: 0.8588\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3935 - acc: 0.8278 - val_loss: 0.3695 - val_acc: 0.8555\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3903 - acc: 0.8292 - val_loss: 0.3662 - val_acc: 0.8539\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3821 - acc: 0.8331 - val_loss: 0.3667 - val_acc: 0.8473\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3849 - acc: 0.8323 - val_loss: 0.3706 - val_acc: 0.8522\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3805 - acc: 0.8332 - val_loss: 0.3656 - val_acc: 0.8555\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3818 - acc: 0.8349 - val_loss: 0.3656 - val_acc: 0.8473\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3759 - acc: 0.8354 - val_loss: 0.3661 - val_acc: 0.8555\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3715 - acc: 0.8396 - val_loss: 0.3648 - val_acc: 0.8621\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3735 - acc: 0.8389 - val_loss: 0.3673 - val_acc: 0.8571\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3685 - acc: 0.8385 - val_loss: 0.3637 - val_acc: 0.8506\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3674 - acc: 0.8407 - val_loss: 0.3658 - val_acc: 0.8539\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3732 - acc: 0.8425 - val_loss: 0.3664 - val_acc: 0.8571\n",
      "Epoch 47/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3713 - acc: 0.8394 - val_loss: 0.3730 - val_acc: 0.8506\n",
      "Epoch 48/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3644 - acc: 0.8418 - val_loss: 0.3645 - val_acc: 0.8555\n",
      "Epoch 49/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3568 - acc: 0.8486 - val_loss: 0.3661 - val_acc: 0.8456\n",
      "Training with parameters {'batch_size': 700, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_93\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_94 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_279 (Dense)            (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_186 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_280 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_187 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_281 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,050,113\n",
      "Trainable params: 1,050,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.7027 - acc: 0.5590 - val_loss: 0.6561 - val_acc: 0.6388\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.6365 - acc: 0.6419 - val_loss: 0.5839 - val_acc: 0.7011\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.6038 - acc: 0.6809 - val_loss: 0.5503 - val_acc: 0.7586\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.5753 - acc: 0.7101 - val_loss: 0.5235 - val_acc: 0.7931\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.5489 - acc: 0.7354 - val_loss: 0.4901 - val_acc: 0.8046\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.5260 - acc: 0.7592 - val_loss: 0.4702 - val_acc: 0.8128\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.5055 - acc: 0.7661 - val_loss: 0.4525 - val_acc: 0.8210\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4910 - acc: 0.7716 - val_loss: 0.4406 - val_acc: 0.8177\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4812 - acc: 0.7789 - val_loss: 0.4263 - val_acc: 0.8325\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4685 - acc: 0.7862 - val_loss: 0.4194 - val_acc: 0.8325\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4580 - acc: 0.7958 - val_loss: 0.4104 - val_acc: 0.8358\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4506 - acc: 0.7995 - val_loss: 0.4043 - val_acc: 0.8358\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4432 - acc: 0.8055 - val_loss: 0.4055 - val_acc: 0.8292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4388 - acc: 0.8070 - val_loss: 0.3954 - val_acc: 0.8456\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4320 - acc: 0.8106 - val_loss: 0.3928 - val_acc: 0.8456\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4250 - acc: 0.8137 - val_loss: 0.3944 - val_acc: 0.8391\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4256 - acc: 0.8113 - val_loss: 0.3879 - val_acc: 0.8489\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4210 - acc: 0.8123 - val_loss: 0.3852 - val_acc: 0.8473\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4199 - acc: 0.8161 - val_loss: 0.3867 - val_acc: 0.8424\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4077 - acc: 0.8234 - val_loss: 0.3803 - val_acc: 0.8473\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4119 - acc: 0.8157 - val_loss: 0.3839 - val_acc: 0.8424\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4036 - acc: 0.8217 - val_loss: 0.3825 - val_acc: 0.8555\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4010 - acc: 0.8258 - val_loss: 0.3776 - val_acc: 0.8506\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3990 - acc: 0.8243 - val_loss: 0.4049 - val_acc: 0.8227\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4054 - acc: 0.8225 - val_loss: 0.3758 - val_acc: 0.8473\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4000 - acc: 0.8252 - val_loss: 0.3825 - val_acc: 0.8424\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3876 - acc: 0.8318 - val_loss: 0.3737 - val_acc: 0.8489\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3859 - acc: 0.8305 - val_loss: 0.3767 - val_acc: 0.8539\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3847 - acc: 0.8305 - val_loss: 0.3733 - val_acc: 0.8539\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3790 - acc: 0.8351 - val_loss: 0.3821 - val_acc: 0.8440\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3771 - acc: 0.8363 - val_loss: 0.3736 - val_acc: 0.8539\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3708 - acc: 0.8416 - val_loss: 0.3809 - val_acc: 0.8407\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3737 - acc: 0.8369 - val_loss: 0.3751 - val_acc: 0.8440\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3680 - acc: 0.8391 - val_loss: 0.3714 - val_acc: 0.8555\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3652 - acc: 0.8405 - val_loss: 0.3709 - val_acc: 0.8522\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3661 - acc: 0.8398 - val_loss: 0.3752 - val_acc: 0.8555\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3681 - acc: 0.8405 - val_loss: 0.3704 - val_acc: 0.8489\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3574 - acc: 0.8438 - val_loss: 0.3846 - val_acc: 0.8407\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3555 - acc: 0.8462 - val_loss: 0.3753 - val_acc: 0.8456\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3540 - acc: 0.8486 - val_loss: 0.3723 - val_acc: 0.8539\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.3475 - acc: 0.8477 - val_loss: 0.3699 - val_acc: 0.8489\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3407 - acc: 0.8548 - val_loss: 0.3701 - val_acc: 0.8604\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3410 - acc: 0.8539 - val_loss: 0.3782 - val_acc: 0.8456\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3376 - acc: 0.8564 - val_loss: 0.3751 - val_acc: 0.8555\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3329 - acc: 0.8562 - val_loss: 0.3720 - val_acc: 0.8473\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3297 - acc: 0.8637 - val_loss: 0.3741 - val_acc: 0.8424\n",
      "Training with parameters {'batch_size': 700, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_94\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_95 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_282 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_188 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_283 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_189 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_284 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 401,729\n",
      "Trainable params: 401,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.8041 - acc: 0.4809 - val_loss: 0.6906 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.6847 - acc: 0.5749 - val_loss: 0.6550 - val_acc: 0.6486\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.6614 - acc: 0.6108 - val_loss: 0.6284 - val_acc: 0.6338\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.6458 - acc: 0.6320 - val_loss: 0.6080 - val_acc: 0.7110\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.6282 - acc: 0.6654 - val_loss: 0.5876 - val_acc: 0.7258\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.6126 - acc: 0.6809 - val_loss: 0.5664 - val_acc: 0.7504\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.5990 - acc: 0.6971 - val_loss: 0.5478 - val_acc: 0.7701\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.5797 - acc: 0.7143 - val_loss: 0.5316 - val_acc: 0.7800\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.5691 - acc: 0.7207 - val_loss: 0.5146 - val_acc: 0.7980\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5564 - acc: 0.7311 - val_loss: 0.5015 - val_acc: 0.8013\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.5417 - acc: 0.7482 - val_loss: 0.4937 - val_acc: 0.8013\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.5325 - acc: 0.7570 - val_loss: 0.4763 - val_acc: 0.8062\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.5231 - acc: 0.7528 - val_loss: 0.4648 - val_acc: 0.8062\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.5092 - acc: 0.7681 - val_loss: 0.4565 - val_acc: 0.8062\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.5053 - acc: 0.7663 - val_loss: 0.4478 - val_acc: 0.8079\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.5002 - acc: 0.7665 - val_loss: 0.4412 - val_acc: 0.8144\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4921 - acc: 0.7736 - val_loss: 0.4323 - val_acc: 0.8243\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4830 - acc: 0.7816 - val_loss: 0.4274 - val_acc: 0.8276\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4793 - acc: 0.7847 - val_loss: 0.4228 - val_acc: 0.8259\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4755 - acc: 0.7865 - val_loss: 0.4190 - val_acc: 0.8309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4679 - acc: 0.7873 - val_loss: 0.4149 - val_acc: 0.8325\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4647 - acc: 0.7949 - val_loss: 0.4123 - val_acc: 0.8342\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4597 - acc: 0.8008 - val_loss: 0.4163 - val_acc: 0.8259\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4591 - acc: 0.7978 - val_loss: 0.4178 - val_acc: 0.8227\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4515 - acc: 0.7999 - val_loss: 0.4024 - val_acc: 0.8309\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4448 - acc: 0.8048 - val_loss: 0.4036 - val_acc: 0.8391\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4445 - acc: 0.8035 - val_loss: 0.3981 - val_acc: 0.8342\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4393 - acc: 0.8051 - val_loss: 0.3970 - val_acc: 0.8342\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4412 - acc: 0.8092 - val_loss: 0.4009 - val_acc: 0.8342\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4366 - acc: 0.8113 - val_loss: 0.3944 - val_acc: 0.8358\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4359 - acc: 0.8115 - val_loss: 0.3927 - val_acc: 0.8374\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4331 - acc: 0.8097 - val_loss: 0.3913 - val_acc: 0.8391\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4319 - acc: 0.8112 - val_loss: 0.3905 - val_acc: 0.8407\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4282 - acc: 0.8139 - val_loss: 0.3889 - val_acc: 0.8391\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4266 - acc: 0.8144 - val_loss: 0.3882 - val_acc: 0.8407\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4230 - acc: 0.8177 - val_loss: 0.3869 - val_acc: 0.8440\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4187 - acc: 0.8163 - val_loss: 0.3859 - val_acc: 0.8407\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4201 - acc: 0.8183 - val_loss: 0.3859 - val_acc: 0.8440\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4154 - acc: 0.8181 - val_loss: 0.3860 - val_acc: 0.8456\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4115 - acc: 0.8228 - val_loss: 0.3836 - val_acc: 0.8391\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4132 - acc: 0.8192 - val_loss: 0.3834 - val_acc: 0.8391\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4135 - acc: 0.8183 - val_loss: 0.3862 - val_acc: 0.8506\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4136 - acc: 0.8197 - val_loss: 0.3865 - val_acc: 0.8522\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4115 - acc: 0.8219 - val_loss: 0.3830 - val_acc: 0.8489\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4076 - acc: 0.8208 - val_loss: 0.3801 - val_acc: 0.8391\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4037 - acc: 0.8228 - val_loss: 0.3797 - val_acc: 0.8440\n",
      "Epoch 47/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3995 - acc: 0.8301 - val_loss: 0.3798 - val_acc: 0.8440\n",
      "Epoch 48/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3989 - acc: 0.8267 - val_loss: 0.3794 - val_acc: 0.8456\n",
      "Epoch 49/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3986 - acc: 0.8289 - val_loss: 0.3773 - val_acc: 0.8407\n",
      "Epoch 50/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3982 - acc: 0.8332 - val_loss: 0.3776 - val_acc: 0.8391\n",
      "Epoch 51/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3924 - acc: 0.8318 - val_loss: 0.3806 - val_acc: 0.8440\n",
      "Epoch 52/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3941 - acc: 0.8311 - val_loss: 0.3777 - val_acc: 0.8440\n",
      "Epoch 53/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3930 - acc: 0.8300 - val_loss: 0.3757 - val_acc: 0.8440\n",
      "Epoch 54/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3903 - acc: 0.8334 - val_loss: 0.3771 - val_acc: 0.8473\n",
      "Epoch 55/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3899 - acc: 0.8314 - val_loss: 0.3839 - val_acc: 0.8424\n",
      "Epoch 56/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3928 - acc: 0.8289 - val_loss: 0.3768 - val_acc: 0.8489\n",
      "Epoch 57/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3891 - acc: 0.8294 - val_loss: 0.3775 - val_acc: 0.8456\n",
      "Epoch 58/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3891 - acc: 0.8312 - val_loss: 0.3754 - val_acc: 0.8473\n",
      "Epoch 59/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3886 - acc: 0.8325 - val_loss: 0.3768 - val_acc: 0.8489\n",
      "Epoch 60/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3907 - acc: 0.8307 - val_loss: 0.3778 - val_acc: 0.8473\n",
      "Epoch 61/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3795 - acc: 0.8343 - val_loss: 0.3765 - val_acc: 0.8424\n",
      "Epoch 62/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3791 - acc: 0.8331 - val_loss: 0.3764 - val_acc: 0.8456\n",
      "Epoch 63/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3778 - acc: 0.8391 - val_loss: 0.3783 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 700, 'dropout': 0.1, 'lay_conf': [1024, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_95\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_96 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_285 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_190 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_286 (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_191 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_287 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,836,545\n",
      "Trainable params: 1,836,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.7880 - acc: 0.5457 - val_loss: 0.6653 - val_acc: 0.6256\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.6623 - acc: 0.6030 - val_loss: 0.6150 - val_acc: 0.5895\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.6220 - acc: 0.6502 - val_loss: 0.5813 - val_acc: 0.7110\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.5922 - acc: 0.7008 - val_loss: 0.5372 - val_acc: 0.7816\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.5676 - acc: 0.7194 - val_loss: 0.5105 - val_acc: 0.8030\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.5435 - acc: 0.7448 - val_loss: 0.4851 - val_acc: 0.8030\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5188 - acc: 0.7608 - val_loss: 0.4641 - val_acc: 0.8128\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.5035 - acc: 0.7659 - val_loss: 0.4474 - val_acc: 0.8144\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.4868 - acc: 0.7763 - val_loss: 0.4363 - val_acc: 0.8259\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4807 - acc: 0.7791 - val_loss: 0.4213 - val_acc: 0.8342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.4670 - acc: 0.7884 - val_loss: 0.4127 - val_acc: 0.8309\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4618 - acc: 0.7889 - val_loss: 0.4045 - val_acc: 0.8342\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4489 - acc: 0.8004 - val_loss: 0.4014 - val_acc: 0.8391\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4431 - acc: 0.8048 - val_loss: 0.3942 - val_acc: 0.8424\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4407 - acc: 0.8061 - val_loss: 0.3964 - val_acc: 0.8407\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4388 - acc: 0.8064 - val_loss: 0.3888 - val_acc: 0.8456\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4300 - acc: 0.8072 - val_loss: 0.3928 - val_acc: 0.8456\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4282 - acc: 0.8075 - val_loss: 0.3837 - val_acc: 0.8473\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4197 - acc: 0.8117 - val_loss: 0.3884 - val_acc: 0.8456\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4178 - acc: 0.8155 - val_loss: 0.3806 - val_acc: 0.8539\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4139 - acc: 0.8183 - val_loss: 0.3795 - val_acc: 0.8489\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4090 - acc: 0.8174 - val_loss: 0.3801 - val_acc: 0.8506\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4030 - acc: 0.8221 - val_loss: 0.3779 - val_acc: 0.8522\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4044 - acc: 0.8223 - val_loss: 0.3766 - val_acc: 0.8522\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4004 - acc: 0.8259 - val_loss: 0.3755 - val_acc: 0.8522\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3933 - acc: 0.8261 - val_loss: 0.3745 - val_acc: 0.8506\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.3947 - acc: 0.8250 - val_loss: 0.3737 - val_acc: 0.8473\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3877 - acc: 0.8318 - val_loss: 0.3711 - val_acc: 0.8522\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3889 - acc: 0.8301 - val_loss: 0.3736 - val_acc: 0.8473\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3882 - acc: 0.8338 - val_loss: 0.4255 - val_acc: 0.7997\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3990 - acc: 0.8221 - val_loss: 0.3832 - val_acc: 0.8440\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3891 - acc: 0.8334 - val_loss: 0.3715 - val_acc: 0.8489\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3752 - acc: 0.8358 - val_loss: 0.3718 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 1000, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_96\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_97 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_288 (Dense)            (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_192 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_289 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_193 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_290 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.7521 - acc: 0.4782 - val_loss: 0.6804 - val_acc: 0.5714\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6859 - acc: 0.5656 - val_loss: 0.6596 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6692 - acc: 0.5878 - val_loss: 0.6440 - val_acc: 0.6913\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.6556 - acc: 0.6305 - val_loss: 0.6255 - val_acc: 0.6732\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.6419 - acc: 0.6377 - val_loss: 0.6050 - val_acc: 0.7110\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.6243 - acc: 0.6603 - val_loss: 0.5852 - val_acc: 0.7241\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6150 - acc: 0.6767 - val_loss: 0.5669 - val_acc: 0.7586\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.5969 - acc: 0.6917 - val_loss: 0.5504 - val_acc: 0.7537\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.5847 - acc: 0.7004 - val_loss: 0.5356 - val_acc: 0.7849\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.5769 - acc: 0.7178 - val_loss: 0.5229 - val_acc: 0.7849\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.5606 - acc: 0.7327 - val_loss: 0.5084 - val_acc: 0.8030\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.5516 - acc: 0.7396 - val_loss: 0.4953 - val_acc: 0.7865\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.5383 - acc: 0.7482 - val_loss: 0.4834 - val_acc: 0.7980\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.5338 - acc: 0.7526 - val_loss: 0.4758 - val_acc: 0.8144\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.5247 - acc: 0.7594 - val_loss: 0.4661 - val_acc: 0.8079\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.5170 - acc: 0.7630 - val_loss: 0.4579 - val_acc: 0.8095\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.5060 - acc: 0.7707 - val_loss: 0.4542 - val_acc: 0.8079\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.5026 - acc: 0.7743 - val_loss: 0.4453 - val_acc: 0.8161\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4959 - acc: 0.7791 - val_loss: 0.4396 - val_acc: 0.8194\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.4919 - acc: 0.7794 - val_loss: 0.4345 - val_acc: 0.8161\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4788 - acc: 0.7896 - val_loss: 0.4294 - val_acc: 0.8210\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4779 - acc: 0.7871 - val_loss: 0.4247 - val_acc: 0.8177\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4752 - acc: 0.7951 - val_loss: 0.4221 - val_acc: 0.8292\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4701 - acc: 0.7931 - val_loss: 0.4184 - val_acc: 0.8325\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4656 - acc: 0.7900 - val_loss: 0.4166 - val_acc: 0.8292\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4663 - acc: 0.7895 - val_loss: 0.4151 - val_acc: 0.8276\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.4618 - acc: 0.7940 - val_loss: 0.4113 - val_acc: 0.8309\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.4557 - acc: 0.7991 - val_loss: 0.4090 - val_acc: 0.8358\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4553 - acc: 0.7984 - val_loss: 0.4109 - val_acc: 0.8243\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4536 - acc: 0.7978 - val_loss: 0.4059 - val_acc: 0.8391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4486 - acc: 0.8057 - val_loss: 0.4033 - val_acc: 0.8374\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4490 - acc: 0.8033 - val_loss: 0.4059 - val_acc: 0.8259\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4475 - acc: 0.7977 - val_loss: 0.4009 - val_acc: 0.8407\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4420 - acc: 0.8075 - val_loss: 0.3985 - val_acc: 0.8391\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4398 - acc: 0.8062 - val_loss: 0.3975 - val_acc: 0.8424\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4369 - acc: 0.8055 - val_loss: 0.3978 - val_acc: 0.8424\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4381 - acc: 0.8072 - val_loss: 0.3948 - val_acc: 0.8440\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4332 - acc: 0.8135 - val_loss: 0.3939 - val_acc: 0.8440\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4340 - acc: 0.8084 - val_loss: 0.3948 - val_acc: 0.8440\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.4309 - acc: 0.8123 - val_loss: 0.3922 - val_acc: 0.8440\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4283 - acc: 0.8143 - val_loss: 0.4027 - val_acc: 0.8309\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4308 - acc: 0.8121 - val_loss: 0.3927 - val_acc: 0.8440\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4293 - acc: 0.8110 - val_loss: 0.3905 - val_acc: 0.8424\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4262 - acc: 0.8132 - val_loss: 0.3919 - val_acc: 0.8358\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4221 - acc: 0.8183 - val_loss: 0.3878 - val_acc: 0.8489\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4244 - acc: 0.8141 - val_loss: 0.3882 - val_acc: 0.8473\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4207 - acc: 0.8174 - val_loss: 0.3861 - val_acc: 0.8424\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4186 - acc: 0.8197 - val_loss: 0.3883 - val_acc: 0.8407\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4165 - acc: 0.8219 - val_loss: 0.3870 - val_acc: 0.8473\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4149 - acc: 0.8201 - val_loss: 0.3860 - val_acc: 0.8489\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4144 - acc: 0.8163 - val_loss: 0.3852 - val_acc: 0.8489\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4111 - acc: 0.8159 - val_loss: 0.3837 - val_acc: 0.8440\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.4099 - acc: 0.8241 - val_loss: 0.3834 - val_acc: 0.8424\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.4088 - acc: 0.8243 - val_loss: 0.3866 - val_acc: 0.8489\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.4090 - acc: 0.8196 - val_loss: 0.3814 - val_acc: 0.8473\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.4091 - acc: 0.8234 - val_loss: 0.3810 - val_acc: 0.8456\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4014 - acc: 0.8300 - val_loss: 0.3824 - val_acc: 0.8489\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4057 - acc: 0.8236 - val_loss: 0.3819 - val_acc: 0.8506\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4045 - acc: 0.8219 - val_loss: 0.3807 - val_acc: 0.8456\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4047 - acc: 0.8241 - val_loss: 0.3868 - val_acc: 0.8424\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4019 - acc: 0.8243 - val_loss: 0.3844 - val_acc: 0.8522\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4024 - acc: 0.8287 - val_loss: 0.3806 - val_acc: 0.8506\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3981 - acc: 0.8290 - val_loss: 0.3810 - val_acc: 0.8539\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.3963 - acc: 0.8265 - val_loss: 0.3822 - val_acc: 0.8424\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.3981 - acc: 0.8316 - val_loss: 0.3829 - val_acc: 0.8506\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.3924 - acc: 0.8320 - val_loss: 0.3791 - val_acc: 0.8440\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.3913 - acc: 0.8300 - val_loss: 0.3816 - val_acc: 0.8539\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.3918 - acc: 0.8311 - val_loss: 0.3817 - val_acc: 0.8555\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.3919 - acc: 0.8301 - val_loss: 0.3788 - val_acc: 0.8440\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3881 - acc: 0.8349 - val_loss: 0.3836 - val_acc: 0.8473\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.3913 - acc: 0.8305 - val_loss: 0.3793 - val_acc: 0.8456\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3877 - acc: 0.8316 - val_loss: 0.3793 - val_acc: 0.8456\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3896 - acc: 0.8312 - val_loss: 0.3777 - val_acc: 0.8440\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3833 - acc: 0.8334 - val_loss: 0.3778 - val_acc: 0.8555\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3848 - acc: 0.8380 - val_loss: 0.3789 - val_acc: 0.8539\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3826 - acc: 0.8356 - val_loss: 0.3775 - val_acc: 0.8473\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3792 - acc: 0.8358 - val_loss: 0.3783 - val_acc: 0.8424\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3794 - acc: 0.8398 - val_loss: 0.3801 - val_acc: 0.8522\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3789 - acc: 0.8360 - val_loss: 0.3788 - val_acc: 0.8440\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3787 - acc: 0.8391 - val_loss: 0.3780 - val_acc: 0.8456\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3737 - acc: 0.8371 - val_loss: 0.3793 - val_acc: 0.8539\n",
      "Training with parameters {'batch_size': 1000, 'dropout': 0.1, 'lay_conf': [1024, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_97\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_98 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_291 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_194 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_292 (Dense)            (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_195 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_293 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.8169 - acc: 0.5213 - val_loss: 0.6619 - val_acc: 0.5764\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.6793 - acc: 0.5714 - val_loss: 0.6232 - val_acc: 0.7028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.6438 - acc: 0.6139 - val_loss: 0.5895 - val_acc: 0.7323\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.6155 - acc: 0.6710 - val_loss: 0.5629 - val_acc: 0.7356\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.5924 - acc: 0.7072 - val_loss: 0.5352 - val_acc: 0.7652\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.5724 - acc: 0.7088 - val_loss: 0.5192 - val_acc: 0.7750\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.5513 - acc: 0.7353 - val_loss: 0.4988 - val_acc: 0.7947\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.5344 - acc: 0.7479 - val_loss: 0.4781 - val_acc: 0.8144\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.5203 - acc: 0.7510 - val_loss: 0.4632 - val_acc: 0.8128\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.5099 - acc: 0.7637 - val_loss: 0.4561 - val_acc: 0.8062\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.5031 - acc: 0.7608 - val_loss: 0.4418 - val_acc: 0.8276\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4824 - acc: 0.7776 - val_loss: 0.4459 - val_acc: 0.8144\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4852 - acc: 0.7745 - val_loss: 0.4282 - val_acc: 0.8276\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.4732 - acc: 0.7836 - val_loss: 0.4205 - val_acc: 0.8309\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4629 - acc: 0.7920 - val_loss: 0.4117 - val_acc: 0.8342\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4613 - acc: 0.7902 - val_loss: 0.4069 - val_acc: 0.8358\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4606 - acc: 0.7924 - val_loss: 0.4047 - val_acc: 0.8374\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4466 - acc: 0.7980 - val_loss: 0.4031 - val_acc: 0.8342\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4468 - acc: 0.8024 - val_loss: 0.4049 - val_acc: 0.8259\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4427 - acc: 0.8033 - val_loss: 0.3966 - val_acc: 0.8456\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4326 - acc: 0.8066 - val_loss: 0.3945 - val_acc: 0.8440\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4319 - acc: 0.8072 - val_loss: 0.3979 - val_acc: 0.8358\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4317 - acc: 0.8082 - val_loss: 0.3896 - val_acc: 0.8407\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4223 - acc: 0.8144 - val_loss: 0.3883 - val_acc: 0.8440\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4205 - acc: 0.8148 - val_loss: 0.3928 - val_acc: 0.8506\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4242 - acc: 0.8077 - val_loss: 0.3891 - val_acc: 0.8489\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.4163 - acc: 0.8174 - val_loss: 0.3886 - val_acc: 0.8506\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4170 - acc: 0.8137 - val_loss: 0.3945 - val_acc: 0.8522\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4188 - acc: 0.8214 - val_loss: 0.3832 - val_acc: 0.8522\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4079 - acc: 0.8186 - val_loss: 0.3850 - val_acc: 0.8473\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4061 - acc: 0.8225 - val_loss: 0.3785 - val_acc: 0.8456\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4019 - acc: 0.8207 - val_loss: 0.3780 - val_acc: 0.8473\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4007 - acc: 0.8243 - val_loss: 0.3783 - val_acc: 0.8489\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3963 - acc: 0.8276 - val_loss: 0.3763 - val_acc: 0.8456\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3965 - acc: 0.8265 - val_loss: 0.3748 - val_acc: 0.8424\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4034 - acc: 0.8201 - val_loss: 0.3861 - val_acc: 0.8407\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3996 - acc: 0.8254 - val_loss: 0.3792 - val_acc: 0.8473\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3849 - acc: 0.8352 - val_loss: 0.3743 - val_acc: 0.8456\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.3798 - acc: 0.8345 - val_loss: 0.3724 - val_acc: 0.8473\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.3811 - acc: 0.8296 - val_loss: 0.3768 - val_acc: 0.8506\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3850 - acc: 0.8298 - val_loss: 0.3742 - val_acc: 0.8473\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3749 - acc: 0.8385 - val_loss: 0.3722 - val_acc: 0.8473\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3734 - acc: 0.8378 - val_loss: 0.3762 - val_acc: 0.8489\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.3745 - acc: 0.8362 - val_loss: 0.3713 - val_acc: 0.8456\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3694 - acc: 0.8435 - val_loss: 0.3710 - val_acc: 0.8489\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3670 - acc: 0.8367 - val_loss: 0.3741 - val_acc: 0.8506\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3645 - acc: 0.8447 - val_loss: 0.3715 - val_acc: 0.8506\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3637 - acc: 0.8429 - val_loss: 0.3692 - val_acc: 0.8489\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3612 - acc: 0.8449 - val_loss: 0.3679 - val_acc: 0.8489\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.3557 - acc: 0.8449 - val_loss: 0.3734 - val_acc: 0.8473\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.3552 - acc: 0.8467 - val_loss: 0.3708 - val_acc: 0.8473\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3554 - acc: 0.8489 - val_loss: 0.3767 - val_acc: 0.8456\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3559 - acc: 0.8497 - val_loss: 0.3688 - val_acc: 0.8539\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3534 - acc: 0.8488 - val_loss: 0.3895 - val_acc: 0.8358\n",
      "Training with parameters {'batch_size': 1000, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_98\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_99 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_294 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_196 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_295 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_197 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_296 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,985\n",
      "Trainable params: 409,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 1.2293 - acc: 0.4368 - val_loss: 0.7064 - val_acc: 0.5731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7593 - acc: 0.5590 - val_loss: 0.7535 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.7305 - acc: 0.5649 - val_loss: 0.6635 - val_acc: 0.5796\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6831 - acc: 0.5627 - val_loss: 0.6571 - val_acc: 0.6864\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.6713 - acc: 0.5878 - val_loss: 0.6317 - val_acc: 0.6847\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.6475 - acc: 0.6309 - val_loss: 0.6173 - val_acc: 0.6420\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.6380 - acc: 0.6433 - val_loss: 0.6009 - val_acc: 0.7373\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.6278 - acc: 0.6636 - val_loss: 0.5850 - val_acc: 0.7586\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.6156 - acc: 0.6747 - val_loss: 0.5713 - val_acc: 0.7586\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.6048 - acc: 0.6864 - val_loss: 0.5592 - val_acc: 0.7718\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.5949 - acc: 0.6997 - val_loss: 0.5478 - val_acc: 0.7734\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.5849 - acc: 0.7077 - val_loss: 0.5384 - val_acc: 0.7882\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.5796 - acc: 0.7059 - val_loss: 0.5288 - val_acc: 0.7980\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.5702 - acc: 0.7243 - val_loss: 0.5198 - val_acc: 0.8013\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.5641 - acc: 0.7251 - val_loss: 0.5113 - val_acc: 0.8062\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.5567 - acc: 0.7294 - val_loss: 0.5007 - val_acc: 0.8079\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.5474 - acc: 0.7353 - val_loss: 0.4926 - val_acc: 0.8112\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.5424 - acc: 0.7422 - val_loss: 0.4844 - val_acc: 0.8112\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.5361 - acc: 0.7471 - val_loss: 0.4768 - val_acc: 0.8128\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.5275 - acc: 0.7533 - val_loss: 0.4706 - val_acc: 0.8144\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.5154 - acc: 0.7628 - val_loss: 0.4649 - val_acc: 0.8177\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.5092 - acc: 0.7668 - val_loss: 0.4597 - val_acc: 0.8177\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.5037 - acc: 0.7725 - val_loss: 0.4540 - val_acc: 0.8194\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5020 - acc: 0.7745 - val_loss: 0.4497 - val_acc: 0.8177\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4994 - acc: 0.7729 - val_loss: 0.4464 - val_acc: 0.8194\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4892 - acc: 0.7803 - val_loss: 0.4433 - val_acc: 0.8292\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4876 - acc: 0.7823 - val_loss: 0.4387 - val_acc: 0.8227\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4843 - acc: 0.7834 - val_loss: 0.4346 - val_acc: 0.8325\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.4832 - acc: 0.7827 - val_loss: 0.4314 - val_acc: 0.8325\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4769 - acc: 0.7900 - val_loss: 0.4290 - val_acc: 0.8391\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4782 - acc: 0.7878 - val_loss: 0.4266 - val_acc: 0.8259\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4713 - acc: 0.7869 - val_loss: 0.4257 - val_acc: 0.8391\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4657 - acc: 0.7918 - val_loss: 0.4210 - val_acc: 0.8424\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4642 - acc: 0.7938 - val_loss: 0.4201 - val_acc: 0.8309\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4621 - acc: 0.7949 - val_loss: 0.4168 - val_acc: 0.8440\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4607 - acc: 0.7940 - val_loss: 0.4162 - val_acc: 0.8325\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4587 - acc: 0.7949 - val_loss: 0.4129 - val_acc: 0.8456\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4563 - acc: 0.7982 - val_loss: 0.4129 - val_acc: 0.8424\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4558 - acc: 0.7989 - val_loss: 0.4134 - val_acc: 0.8342\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4515 - acc: 0.8000 - val_loss: 0.4084 - val_acc: 0.8456\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4468 - acc: 0.8042 - val_loss: 0.4084 - val_acc: 0.8440\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4516 - acc: 0.7982 - val_loss: 0.4055 - val_acc: 0.8456\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4497 - acc: 0.7978 - val_loss: 0.4082 - val_acc: 0.8374\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4511 - acc: 0.8011 - val_loss: 0.4083 - val_acc: 0.8358\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4431 - acc: 0.8039 - val_loss: 0.4044 - val_acc: 0.8456\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4419 - acc: 0.8035 - val_loss: 0.4022 - val_acc: 0.8473\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4387 - acc: 0.8130 - val_loss: 0.4008 - val_acc: 0.8456\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.4395 - acc: 0.8064 - val_loss: 0.3992 - val_acc: 0.8407\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4371 - acc: 0.8092 - val_loss: 0.3989 - val_acc: 0.8456\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4379 - acc: 0.8050 - val_loss: 0.3980 - val_acc: 0.8424\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4306 - acc: 0.8150 - val_loss: 0.3969 - val_acc: 0.8424\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4340 - acc: 0.8077 - val_loss: 0.3964 - val_acc: 0.8424\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4316 - acc: 0.8099 - val_loss: 0.3956 - val_acc: 0.8440\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4297 - acc: 0.8117 - val_loss: 0.3952 - val_acc: 0.8473\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4254 - acc: 0.8128 - val_loss: 0.3942 - val_acc: 0.8440\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.4271 - acc: 0.8126 - val_loss: 0.3935 - val_acc: 0.8473\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4269 - acc: 0.8139 - val_loss: 0.3943 - val_acc: 0.8489\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4209 - acc: 0.8163 - val_loss: 0.3937 - val_acc: 0.8424\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4244 - acc: 0.8154 - val_loss: 0.3918 - val_acc: 0.8440\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4241 - acc: 0.8130 - val_loss: 0.3949 - val_acc: 0.8440\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4246 - acc: 0.8144 - val_loss: 0.3907 - val_acc: 0.8440\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4187 - acc: 0.8183 - val_loss: 0.3944 - val_acc: 0.8456\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4192 - acc: 0.8183 - val_loss: 0.3931 - val_acc: 0.8456\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4202 - acc: 0.8157 - val_loss: 0.3890 - val_acc: 0.8440\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4120 - acc: 0.8207 - val_loss: 0.3897 - val_acc: 0.8440\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4129 - acc: 0.8194 - val_loss: 0.3905 - val_acc: 0.8440\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4122 - acc: 0.8212 - val_loss: 0.3884 - val_acc: 0.8473\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.4122 - acc: 0.8186 - val_loss: 0.3885 - val_acc: 0.8456\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.4108 - acc: 0.8183 - val_loss: 0.3880 - val_acc: 0.8456\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.4075 - acc: 0.8236 - val_loss: 0.3885 - val_acc: 0.8440\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4066 - acc: 0.8241 - val_loss: 0.3876 - val_acc: 0.8489\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4076 - acc: 0.8252 - val_loss: 0.3874 - val_acc: 0.8473\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4066 - acc: 0.8230 - val_loss: 0.3859 - val_acc: 0.8456\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4090 - acc: 0.8241 - val_loss: 0.3863 - val_acc: 0.8473\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4058 - acc: 0.8270 - val_loss: 0.3878 - val_acc: 0.8456\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4046 - acc: 0.8203 - val_loss: 0.3860 - val_acc: 0.8407\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4032 - acc: 0.8225 - val_loss: 0.3853 - val_acc: 0.8456\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3998 - acc: 0.8239 - val_loss: 0.3840 - val_acc: 0.8473\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3984 - acc: 0.8289 - val_loss: 0.3870 - val_acc: 0.8456\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3995 - acc: 0.8283 - val_loss: 0.3861 - val_acc: 0.8456\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3989 - acc: 0.8287 - val_loss: 0.3846 - val_acc: 0.8440\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3959 - acc: 0.8303 - val_loss: 0.3840 - val_acc: 0.8440\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3957 - acc: 0.8283 - val_loss: 0.3850 - val_acc: 0.8440\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.3996 - acc: 0.8256 - val_loss: 0.3836 - val_acc: 0.8424\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3907 - acc: 0.8323 - val_loss: 0.3845 - val_acc: 0.8440\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3954 - acc: 0.8296 - val_loss: 0.3856 - val_acc: 0.8440\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3947 - acc: 0.8318 - val_loss: 0.3834 - val_acc: 0.8456\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3926 - acc: 0.8356 - val_loss: 0.3977 - val_acc: 0.8292\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3951 - acc: 0.8270 - val_loss: 0.3816 - val_acc: 0.8473\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3933 - acc: 0.8300 - val_loss: 0.3897 - val_acc: 0.8374\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3919 - acc: 0.8283 - val_loss: 0.3824 - val_acc: 0.8456\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3863 - acc: 0.8365 - val_loss: 0.3856 - val_acc: 0.8407\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3836 - acc: 0.8369 - val_loss: 0.3871 - val_acc: 0.8391\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3870 - acc: 0.8347 - val_loss: 0.3820 - val_acc: 0.8473\n",
      "Training with parameters {'batch_size': 1000, 'dropout': 0.1, 'lay_conf': [1024, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_99\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_100 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_297 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_198 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_298 (Dense)            (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_199 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_299 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,639,553\n",
      "Trainable params: 1,639,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.7498 - acc: 0.5362 - val_loss: 0.6553 - val_acc: 0.6470\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6578 - acc: 0.6045 - val_loss: 0.6104 - val_acc: 0.6190\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.6183 - acc: 0.6751 - val_loss: 0.5638 - val_acc: 0.7241\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.5862 - acc: 0.7037 - val_loss: 0.5285 - val_acc: 0.7964\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.5643 - acc: 0.7141 - val_loss: 0.5118 - val_acc: 0.7898\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.5553 - acc: 0.7263 - val_loss: 0.4796 - val_acc: 0.8161\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.5355 - acc: 0.7486 - val_loss: 0.4815 - val_acc: 0.7931\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.5196 - acc: 0.7559 - val_loss: 0.4506 - val_acc: 0.8243\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.5034 - acc: 0.7703 - val_loss: 0.4368 - val_acc: 0.8276\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4899 - acc: 0.7743 - val_loss: 0.4270 - val_acc: 0.8342\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4847 - acc: 0.7765 - val_loss: 0.4264 - val_acc: 0.8374\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4758 - acc: 0.7845 - val_loss: 0.4160 - val_acc: 0.8440\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4648 - acc: 0.7907 - val_loss: 0.4149 - val_acc: 0.8358\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4688 - acc: 0.7878 - val_loss: 0.4026 - val_acc: 0.8407\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4598 - acc: 0.7977 - val_loss: 0.4016 - val_acc: 0.8391\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4535 - acc: 0.7955 - val_loss: 0.3971 - val_acc: 0.8391\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4484 - acc: 0.7971 - val_loss: 0.3939 - val_acc: 0.8456\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4446 - acc: 0.8024 - val_loss: 0.3897 - val_acc: 0.8555\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4460 - acc: 0.8022 - val_loss: 0.3876 - val_acc: 0.8555\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4407 - acc: 0.8031 - val_loss: 0.3858 - val_acc: 0.8522\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4337 - acc: 0.8084 - val_loss: 0.3891 - val_acc: 0.8473\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4312 - acc: 0.8046 - val_loss: 0.3813 - val_acc: 0.8522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4312 - acc: 0.8050 - val_loss: 0.3858 - val_acc: 0.8522\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4329 - acc: 0.8073 - val_loss: 0.3815 - val_acc: 0.8555\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4238 - acc: 0.8119 - val_loss: 0.3787 - val_acc: 0.8522\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4174 - acc: 0.8170 - val_loss: 0.3795 - val_acc: 0.8555\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4189 - acc: 0.8150 - val_loss: 0.3747 - val_acc: 0.8539\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4152 - acc: 0.8183 - val_loss: 0.3759 - val_acc: 0.8571\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4121 - acc: 0.8199 - val_loss: 0.3725 - val_acc: 0.8571\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4105 - acc: 0.8190 - val_loss: 0.3730 - val_acc: 0.8571\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4080 - acc: 0.8201 - val_loss: 0.3727 - val_acc: 0.8555\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4045 - acc: 0.8219 - val_loss: 0.3734 - val_acc: 0.8522\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4021 - acc: 0.8276 - val_loss: 0.3701 - val_acc: 0.8588\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3978 - acc: 0.8259 - val_loss: 0.3692 - val_acc: 0.8604\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3983 - acc: 0.8250 - val_loss: 0.3693 - val_acc: 0.8539\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3967 - acc: 0.8267 - val_loss: 0.3701 - val_acc: 0.8588\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3949 - acc: 0.8278 - val_loss: 0.3669 - val_acc: 0.8571\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3924 - acc: 0.8305 - val_loss: 0.3681 - val_acc: 0.8555\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.3882 - acc: 0.8340 - val_loss: 0.3681 - val_acc: 0.8588\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3862 - acc: 0.8318 - val_loss: 0.3667 - val_acc: 0.8539\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3861 - acc: 0.8329 - val_loss: 0.3693 - val_acc: 0.8539\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3849 - acc: 0.8343 - val_loss: 0.3650 - val_acc: 0.8506\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3797 - acc: 0.8320 - val_loss: 0.3655 - val_acc: 0.8506\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3779 - acc: 0.8349 - val_loss: 0.3671 - val_acc: 0.8506\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.3753 - acc: 0.8407 - val_loss: 0.3664 - val_acc: 0.8539\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3752 - acc: 0.8387 - val_loss: 0.3670 - val_acc: 0.8473\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3720 - acc: 0.8409 - val_loss: 0.3688 - val_acc: 0.8522\n",
      "Training with parameters {'batch_size': 1000, 'dropout': 0.1, 'lay_conf': [1024, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_100\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_101 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_300 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_200 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_301 (Dense)            (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_201 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_302 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,606,721\n",
      "Trainable params: 1,606,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 0.7649 - acc: 0.5402 - val_loss: 0.6638 - val_acc: 0.6043\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.6642 - acc: 0.5995 - val_loss: 0.6089 - val_acc: 0.6355\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.6269 - acc: 0.6526 - val_loss: 0.5620 - val_acc: 0.7553\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.5953 - acc: 0.7006 - val_loss: 0.5273 - val_acc: 0.7668\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.5677 - acc: 0.7194 - val_loss: 0.4979 - val_acc: 0.7980\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.5482 - acc: 0.7316 - val_loss: 0.4805 - val_acc: 0.8095\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.5316 - acc: 0.7477 - val_loss: 0.4613 - val_acc: 0.8210\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.5145 - acc: 0.7566 - val_loss: 0.4579 - val_acc: 0.8161\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.5062 - acc: 0.7656 - val_loss: 0.4453 - val_acc: 0.8177\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.4976 - acc: 0.7648 - val_loss: 0.4243 - val_acc: 0.8276\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.4819 - acc: 0.7820 - val_loss: 0.4295 - val_acc: 0.8276\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.4838 - acc: 0.7798 - val_loss: 0.4159 - val_acc: 0.8342\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.4738 - acc: 0.7791 - val_loss: 0.4090 - val_acc: 0.8309\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.4656 - acc: 0.7911 - val_loss: 0.4069 - val_acc: 0.8374\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.4561 - acc: 0.7933 - val_loss: 0.4056 - val_acc: 0.8358\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.4533 - acc: 0.7977 - val_loss: 0.4013 - val_acc: 0.8391\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.4565 - acc: 0.7935 - val_loss: 0.4085 - val_acc: 0.8325\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.4555 - acc: 0.8004 - val_loss: 0.3905 - val_acc: 0.8489\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.4424 - acc: 0.7986 - val_loss: 0.3946 - val_acc: 0.8407\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.4389 - acc: 0.8088 - val_loss: 0.3937 - val_acc: 0.8407\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.4371 - acc: 0.8061 - val_loss: 0.3856 - val_acc: 0.8440\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4319 - acc: 0.8061 - val_loss: 0.3831 - val_acc: 0.8456\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4296 - acc: 0.8068 - val_loss: 0.3811 - val_acc: 0.8473\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4273 - acc: 0.8141 - val_loss: 0.3844 - val_acc: 0.8489\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.4261 - acc: 0.8084 - val_loss: 0.3795 - val_acc: 0.8539\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4246 - acc: 0.8108 - val_loss: 0.3787 - val_acc: 0.8539\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4174 - acc: 0.8194 - val_loss: 0.3797 - val_acc: 0.8522\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4162 - acc: 0.8197 - val_loss: 0.3757 - val_acc: 0.8506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4137 - acc: 0.8177 - val_loss: 0.3743 - val_acc: 0.8522\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4145 - acc: 0.8190 - val_loss: 0.3747 - val_acc: 0.8522\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4122 - acc: 0.8241 - val_loss: 0.3740 - val_acc: 0.8522\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4093 - acc: 0.8265 - val_loss: 0.3723 - val_acc: 0.8555\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4037 - acc: 0.8236 - val_loss: 0.3749 - val_acc: 0.8539\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4050 - acc: 0.8236 - val_loss: 0.3773 - val_acc: 0.8522\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4075 - acc: 0.8225 - val_loss: 0.3774 - val_acc: 0.8506\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.3980 - acc: 0.8258 - val_loss: 0.3699 - val_acc: 0.8555\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.3954 - acc: 0.8278 - val_loss: 0.3695 - val_acc: 0.8571\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.3924 - acc: 0.8269 - val_loss: 0.3722 - val_acc: 0.8571\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3940 - acc: 0.8276 - val_loss: 0.3691 - val_acc: 0.8555\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3874 - acc: 0.8316 - val_loss: 0.3768 - val_acc: 0.8456\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.3932 - acc: 0.8281 - val_loss: 0.3709 - val_acc: 0.8506\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3910 - acc: 0.8294 - val_loss: 0.3674 - val_acc: 0.8588\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3834 - acc: 0.8300 - val_loss: 0.3700 - val_acc: 0.8506\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3853 - acc: 0.8307 - val_loss: 0.3707 - val_acc: 0.8555\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3813 - acc: 0.8332 - val_loss: 0.3682 - val_acc: 0.8588\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3878 - acc: 0.8327 - val_loss: 0.3666 - val_acc: 0.8588\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.3765 - acc: 0.8363 - val_loss: 0.3655 - val_acc: 0.8539\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3757 - acc: 0.8378 - val_loss: 0.3677 - val_acc: 0.8588\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3734 - acc: 0.8400 - val_loss: 0.3659 - val_acc: 0.8588\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3802 - acc: 0.8343 - val_loss: 0.3668 - val_acc: 0.8522\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3702 - acc: 0.8385 - val_loss: 0.3675 - val_acc: 0.8539\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.3684 - acc: 0.8447 - val_loss: 0.3678 - val_acc: 0.8588\n",
      "Training with parameters {'batch_size': 1000, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_101\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_102 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_303 (Dense)            (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_202 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_304 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_203 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_305 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,050,113\n",
      "Trainable params: 1,050,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.7021 - acc: 0.5548 - val_loss: 0.6470 - val_acc: 0.6568\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.6430 - acc: 0.6318 - val_loss: 0.5892 - val_acc: 0.7389\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.6099 - acc: 0.6749 - val_loss: 0.5552 - val_acc: 0.7668\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.5790 - acc: 0.7185 - val_loss: 0.5271 - val_acc: 0.7849\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.5552 - acc: 0.7336 - val_loss: 0.5009 - val_acc: 0.8013\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.5325 - acc: 0.7526 - val_loss: 0.4789 - val_acc: 0.8079\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.5153 - acc: 0.7606 - val_loss: 0.4609 - val_acc: 0.8227\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.5043 - acc: 0.7663 - val_loss: 0.4458 - val_acc: 0.8194\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4892 - acc: 0.7758 - val_loss: 0.4337 - val_acc: 0.8210\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4804 - acc: 0.7827 - val_loss: 0.4257 - val_acc: 0.8276\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4686 - acc: 0.7931 - val_loss: 0.4196 - val_acc: 0.8210\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4585 - acc: 0.7944 - val_loss: 0.4132 - val_acc: 0.8292\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4560 - acc: 0.7929 - val_loss: 0.4070 - val_acc: 0.8374\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4575 - acc: 0.7920 - val_loss: 0.4044 - val_acc: 0.8374\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4455 - acc: 0.8035 - val_loss: 0.4081 - val_acc: 0.8292\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4435 - acc: 0.8015 - val_loss: 0.4040 - val_acc: 0.8391\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4399 - acc: 0.7986 - val_loss: 0.3948 - val_acc: 0.8456\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4293 - acc: 0.8103 - val_loss: 0.3919 - val_acc: 0.8440\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4280 - acc: 0.8117 - val_loss: 0.3901 - val_acc: 0.8456\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4245 - acc: 0.8106 - val_loss: 0.3909 - val_acc: 0.8489\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.4273 - acc: 0.8104 - val_loss: 0.3889 - val_acc: 0.8473\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.4250 - acc: 0.8110 - val_loss: 0.3933 - val_acc: 0.8473\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4166 - acc: 0.8141 - val_loss: 0.3866 - val_acc: 0.8440\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4137 - acc: 0.8112 - val_loss: 0.3908 - val_acc: 0.8407\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4138 - acc: 0.8174 - val_loss: 0.3914 - val_acc: 0.8407\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.4128 - acc: 0.8161 - val_loss: 0.3859 - val_acc: 0.8424\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4056 - acc: 0.8205 - val_loss: 0.3816 - val_acc: 0.8506\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4022 - acc: 0.8239 - val_loss: 0.3798 - val_acc: 0.8539\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.4020 - acc: 0.8239 - val_loss: 0.3788 - val_acc: 0.8473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3960 - acc: 0.8256 - val_loss: 0.3782 - val_acc: 0.8456\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3937 - acc: 0.8278 - val_loss: 0.3767 - val_acc: 0.8506\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3926 - acc: 0.8280 - val_loss: 0.3845 - val_acc: 0.8374\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3953 - acc: 0.8311 - val_loss: 0.3841 - val_acc: 0.8407\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3924 - acc: 0.8316 - val_loss: 0.3807 - val_acc: 0.8440\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3894 - acc: 0.8287 - val_loss: 0.3874 - val_acc: 0.8407\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3921 - acc: 0.8296 - val_loss: 0.3823 - val_acc: 0.8407\n",
      "Training with parameters {'batch_size': 1000, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_102\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_103 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_306 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_204 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_307 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_205 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_308 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 401,729\n",
      "Trainable params: 401,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.8465 - acc: 0.4590 - val_loss: 0.6947 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7002 - acc: 0.5676 - val_loss: 0.6614 - val_acc: 0.5764\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6765 - acc: 0.5809 - val_loss: 0.6471 - val_acc: 0.6913\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.6618 - acc: 0.6141 - val_loss: 0.6251 - val_acc: 0.6831\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.6440 - acc: 0.6388 - val_loss: 0.6076 - val_acc: 0.7241\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.6324 - acc: 0.6533 - val_loss: 0.5922 - val_acc: 0.7422\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.6203 - acc: 0.6656 - val_loss: 0.5759 - val_acc: 0.7373\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.6088 - acc: 0.6745 - val_loss: 0.5635 - val_acc: 0.7455\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.6001 - acc: 0.6898 - val_loss: 0.5497 - val_acc: 0.7521\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.5870 - acc: 0.7068 - val_loss: 0.5376 - val_acc: 0.7586\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.5805 - acc: 0.7110 - val_loss: 0.5262 - val_acc: 0.7783\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.5672 - acc: 0.7252 - val_loss: 0.5153 - val_acc: 0.7898\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.5576 - acc: 0.7331 - val_loss: 0.5055 - val_acc: 0.7947\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.5493 - acc: 0.7378 - val_loss: 0.4974 - val_acc: 0.7964\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.5385 - acc: 0.7455 - val_loss: 0.4874 - val_acc: 0.8030\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.5314 - acc: 0.7513 - val_loss: 0.4799 - val_acc: 0.8030\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.5263 - acc: 0.7594 - val_loss: 0.4714 - val_acc: 0.8128\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.5153 - acc: 0.7577 - val_loss: 0.4636 - val_acc: 0.8144\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.5086 - acc: 0.7645 - val_loss: 0.4580 - val_acc: 0.8194\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.5041 - acc: 0.7712 - val_loss: 0.4515 - val_acc: 0.8144\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4987 - acc: 0.7694 - val_loss: 0.4463 - val_acc: 0.8227\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4951 - acc: 0.7734 - val_loss: 0.4403 - val_acc: 0.8243\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4917 - acc: 0.7749 - val_loss: 0.4403 - val_acc: 0.8194\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4841 - acc: 0.7770 - val_loss: 0.4332 - val_acc: 0.8227\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4786 - acc: 0.7878 - val_loss: 0.4266 - val_acc: 0.8325\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4760 - acc: 0.7814 - val_loss: 0.4276 - val_acc: 0.8210\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4769 - acc: 0.7854 - val_loss: 0.4210 - val_acc: 0.8391\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4710 - acc: 0.7895 - val_loss: 0.4179 - val_acc: 0.8374\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4665 - acc: 0.7926 - val_loss: 0.4133 - val_acc: 0.8358\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4615 - acc: 0.7968 - val_loss: 0.4136 - val_acc: 0.8374\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4584 - acc: 0.7966 - val_loss: 0.4101 - val_acc: 0.8407\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4532 - acc: 0.7997 - val_loss: 0.4074 - val_acc: 0.8391\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4549 - acc: 0.7988 - val_loss: 0.4059 - val_acc: 0.8391\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4523 - acc: 0.8009 - val_loss: 0.4057 - val_acc: 0.8325\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4472 - acc: 0.8008 - val_loss: 0.4036 - val_acc: 0.8391\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4431 - acc: 0.8101 - val_loss: 0.4010 - val_acc: 0.8391\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4420 - acc: 0.8117 - val_loss: 0.3992 - val_acc: 0.8391\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4416 - acc: 0.8095 - val_loss: 0.4017 - val_acc: 0.8407\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4384 - acc: 0.8072 - val_loss: 0.3972 - val_acc: 0.8391\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4357 - acc: 0.8112 - val_loss: 0.3975 - val_acc: 0.8374\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4355 - acc: 0.8135 - val_loss: 0.3940 - val_acc: 0.8407\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4313 - acc: 0.8141 - val_loss: 0.3918 - val_acc: 0.8424\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4278 - acc: 0.8090 - val_loss: 0.3911 - val_acc: 0.8407\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4249 - acc: 0.8150 - val_loss: 0.3915 - val_acc: 0.8358\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4257 - acc: 0.8188 - val_loss: 0.3914 - val_acc: 0.8391\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4181 - acc: 0.8199 - val_loss: 0.3893 - val_acc: 0.8358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4207 - acc: 0.8130 - val_loss: 0.3877 - val_acc: 0.8391\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.4201 - acc: 0.8176 - val_loss: 0.3896 - val_acc: 0.8325\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4179 - acc: 0.8155 - val_loss: 0.3868 - val_acc: 0.8342\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4170 - acc: 0.8205 - val_loss: 0.3889 - val_acc: 0.8522\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4194 - acc: 0.8179 - val_loss: 0.3863 - val_acc: 0.8489\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4171 - acc: 0.8176 - val_loss: 0.3844 - val_acc: 0.8358\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4129 - acc: 0.8201 - val_loss: 0.3832 - val_acc: 0.8424\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4101 - acc: 0.8212 - val_loss: 0.3816 - val_acc: 0.8424\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4084 - acc: 0.8194 - val_loss: 0.3854 - val_acc: 0.8407\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4054 - acc: 0.8199 - val_loss: 0.3811 - val_acc: 0.8440\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4032 - acc: 0.8263 - val_loss: 0.3885 - val_acc: 0.8506\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4109 - acc: 0.8183 - val_loss: 0.3810 - val_acc: 0.8456\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4067 - acc: 0.8221 - val_loss: 0.3796 - val_acc: 0.8424\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4041 - acc: 0.8270 - val_loss: 0.3826 - val_acc: 0.8424\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4048 - acc: 0.8214 - val_loss: 0.3823 - val_acc: 0.8374\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3976 - acc: 0.8296 - val_loss: 0.3790 - val_acc: 0.8489\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3961 - acc: 0.8287 - val_loss: 0.3788 - val_acc: 0.8407\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3982 - acc: 0.8325 - val_loss: 0.3783 - val_acc: 0.8407\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.3950 - acc: 0.8316 - val_loss: 0.3783 - val_acc: 0.8424\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3958 - acc: 0.8272 - val_loss: 0.3781 - val_acc: 0.8456\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3904 - acc: 0.8331 - val_loss: 0.3798 - val_acc: 0.8456\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3919 - acc: 0.8300 - val_loss: 0.3773 - val_acc: 0.8473\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3897 - acc: 0.8323 - val_loss: 0.3800 - val_acc: 0.8456\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.3839 - acc: 0.8384 - val_loss: 0.3790 - val_acc: 0.8473\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3892 - acc: 0.8352 - val_loss: 0.3853 - val_acc: 0.8407\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3925 - acc: 0.8300 - val_loss: 0.3763 - val_acc: 0.8440\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3841 - acc: 0.8369 - val_loss: 0.3765 - val_acc: 0.8440\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3845 - acc: 0.8351 - val_loss: 0.3852 - val_acc: 0.8456\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3882 - acc: 0.8325 - val_loss: 0.3764 - val_acc: 0.8424\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3825 - acc: 0.8351 - val_loss: 0.3820 - val_acc: 0.8522\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3822 - acc: 0.8352 - val_loss: 0.3768 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 1000, 'dropout': 0.1, 'lay_conf': [1024, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_103\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_104 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_309 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_206 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_310 (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_207 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_311 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,836,545\n",
      "Trainable params: 1,836,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.8251 - acc: 0.5251 - val_loss: 0.6499 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.6769 - acc: 0.5849 - val_loss: 0.6180 - val_acc: 0.6782\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.6393 - acc: 0.6274 - val_loss: 0.5945 - val_acc: 0.6880\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.6092 - acc: 0.6701 - val_loss: 0.5702 - val_acc: 0.7209\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.5862 - acc: 0.6991 - val_loss: 0.5384 - val_acc: 0.7767\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.5677 - acc: 0.7178 - val_loss: 0.5140 - val_acc: 0.7865\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.5516 - acc: 0.7353 - val_loss: 0.4998 - val_acc: 0.7783\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.5344 - acc: 0.7426 - val_loss: 0.4750 - val_acc: 0.8144\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.5158 - acc: 0.7570 - val_loss: 0.4601 - val_acc: 0.8177\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.5010 - acc: 0.7754 - val_loss: 0.4487 - val_acc: 0.8144\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4889 - acc: 0.7767 - val_loss: 0.4341 - val_acc: 0.8177\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4827 - acc: 0.7789 - val_loss: 0.4283 - val_acc: 0.8259\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4742 - acc: 0.7811 - val_loss: 0.4189 - val_acc: 0.8276\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4653 - acc: 0.7916 - val_loss: 0.4129 - val_acc: 0.8325\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4550 - acc: 0.7977 - val_loss: 0.4192 - val_acc: 0.8276\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4504 - acc: 0.7989 - val_loss: 0.4036 - val_acc: 0.8407\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4471 - acc: 0.8015 - val_loss: 0.3994 - val_acc: 0.8407\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.4445 - acc: 0.8002 - val_loss: 0.4008 - val_acc: 0.8374\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4365 - acc: 0.8057 - val_loss: 0.3921 - val_acc: 0.8424\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4348 - acc: 0.8051 - val_loss: 0.3887 - val_acc: 0.8440\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4283 - acc: 0.8077 - val_loss: 0.3883 - val_acc: 0.8473\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4251 - acc: 0.8152 - val_loss: 0.3871 - val_acc: 0.8473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4256 - acc: 0.8123 - val_loss: 0.3827 - val_acc: 0.8539\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4192 - acc: 0.8170 - val_loss: 0.3816 - val_acc: 0.8506\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4154 - acc: 0.8159 - val_loss: 0.3819 - val_acc: 0.8456\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4153 - acc: 0.8152 - val_loss: 0.3949 - val_acc: 0.8391\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4184 - acc: 0.8166 - val_loss: 0.4037 - val_acc: 0.8407\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4121 - acc: 0.8188 - val_loss: 0.3813 - val_acc: 0.8522\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4059 - acc: 0.8181 - val_loss: 0.3793 - val_acc: 0.8555\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4052 - acc: 0.8203 - val_loss: 0.3755 - val_acc: 0.8539\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4020 - acc: 0.8228 - val_loss: 0.3804 - val_acc: 0.8506\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4002 - acc: 0.8238 - val_loss: 0.3883 - val_acc: 0.8456\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.3957 - acc: 0.8267 - val_loss: 0.3898 - val_acc: 0.8424\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3933 - acc: 0.8259 - val_loss: 0.3967 - val_acc: 0.8325\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.3962 - acc: 0.8239 - val_loss: 0.4068 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': 1100, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_104\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_105 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_312 (Dense)            (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_208 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_313 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_209 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_314 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 1s 61ms/step - loss: 0.7550 - acc: 0.4758 - val_loss: 0.6802 - val_acc: 0.5764\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6911 - acc: 0.5670 - val_loss: 0.6708 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6740 - acc: 0.5829 - val_loss: 0.6493 - val_acc: 0.6125\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6622 - acc: 0.6123 - val_loss: 0.6384 - val_acc: 0.6979\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6479 - acc: 0.6320 - val_loss: 0.6181 - val_acc: 0.6864\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6343 - acc: 0.6479 - val_loss: 0.6014 - val_acc: 0.7307\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6288 - acc: 0.6561 - val_loss: 0.5859 - val_acc: 0.7307\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6162 - acc: 0.6774 - val_loss: 0.5719 - val_acc: 0.7406\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6042 - acc: 0.6836 - val_loss: 0.5583 - val_acc: 0.7537\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5964 - acc: 0.6917 - val_loss: 0.5457 - val_acc: 0.7619\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5865 - acc: 0.7039 - val_loss: 0.5327 - val_acc: 0.7849\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5762 - acc: 0.7178 - val_loss: 0.5218 - val_acc: 0.7816\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5639 - acc: 0.7285 - val_loss: 0.5118 - val_acc: 0.7833\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5573 - acc: 0.7422 - val_loss: 0.5016 - val_acc: 0.7980\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5474 - acc: 0.7356 - val_loss: 0.4925 - val_acc: 0.8030\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5419 - acc: 0.7515 - val_loss: 0.4844 - val_acc: 0.7997\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5353 - acc: 0.7502 - val_loss: 0.4773 - val_acc: 0.8128\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5256 - acc: 0.7546 - val_loss: 0.4724 - val_acc: 0.8079\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5201 - acc: 0.7625 - val_loss: 0.4636 - val_acc: 0.8144\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5155 - acc: 0.7645 - val_loss: 0.4570 - val_acc: 0.8062\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5097 - acc: 0.7676 - val_loss: 0.4508 - val_acc: 0.8144\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5053 - acc: 0.7692 - val_loss: 0.4463 - val_acc: 0.8128\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5004 - acc: 0.7760 - val_loss: 0.4416 - val_acc: 0.8112\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4936 - acc: 0.7796 - val_loss: 0.4378 - val_acc: 0.8243\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4865 - acc: 0.7860 - val_loss: 0.4334 - val_acc: 0.8259\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4826 - acc: 0.7860 - val_loss: 0.4292 - val_acc: 0.8292\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4793 - acc: 0.7924 - val_loss: 0.4264 - val_acc: 0.8276\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4803 - acc: 0.7864 - val_loss: 0.4238 - val_acc: 0.8276\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4748 - acc: 0.7904 - val_loss: 0.4229 - val_acc: 0.8259\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4712 - acc: 0.7926 - val_loss: 0.4167 - val_acc: 0.8342\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4685 - acc: 0.7900 - val_loss: 0.4145 - val_acc: 0.8342\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4636 - acc: 0.7960 - val_loss: 0.4123 - val_acc: 0.8374\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4640 - acc: 0.7937 - val_loss: 0.4092 - val_acc: 0.8342\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4576 - acc: 0.7933 - val_loss: 0.4089 - val_acc: 0.8325\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4561 - acc: 0.7991 - val_loss: 0.4074 - val_acc: 0.8407\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4551 - acc: 0.7975 - val_loss: 0.4036 - val_acc: 0.8407\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4520 - acc: 0.7975 - val_loss: 0.4027 - val_acc: 0.8440\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4493 - acc: 0.8022 - val_loss: 0.4011 - val_acc: 0.8424\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4454 - acc: 0.8033 - val_loss: 0.3995 - val_acc: 0.8456\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4445 - acc: 0.8062 - val_loss: 0.3979 - val_acc: 0.8456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4434 - acc: 0.8044 - val_loss: 0.3973 - val_acc: 0.8407\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4431 - acc: 0.8088 - val_loss: 0.3955 - val_acc: 0.8456\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4392 - acc: 0.8044 - val_loss: 0.3952 - val_acc: 0.8440\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4352 - acc: 0.8093 - val_loss: 0.3949 - val_acc: 0.8391\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4344 - acc: 0.8104 - val_loss: 0.3926 - val_acc: 0.8440\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4358 - acc: 0.8093 - val_loss: 0.3927 - val_acc: 0.8456\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4324 - acc: 0.8139 - val_loss: 0.3929 - val_acc: 0.8407\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4273 - acc: 0.8144 - val_loss: 0.3917 - val_acc: 0.8407\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4339 - acc: 0.8104 - val_loss: 0.3891 - val_acc: 0.8473\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4273 - acc: 0.8137 - val_loss: 0.3896 - val_acc: 0.8489\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4286 - acc: 0.8128 - val_loss: 0.3910 - val_acc: 0.8374\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4254 - acc: 0.8179 - val_loss: 0.3880 - val_acc: 0.8489\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4280 - acc: 0.8106 - val_loss: 0.3879 - val_acc: 0.8489\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4209 - acc: 0.8201 - val_loss: 0.3862 - val_acc: 0.8489\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4216 - acc: 0.8179 - val_loss: 0.3859 - val_acc: 0.8456\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4200 - acc: 0.8185 - val_loss: 0.3850 - val_acc: 0.8440\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4191 - acc: 0.8165 - val_loss: 0.3844 - val_acc: 0.8489\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4207 - acc: 0.8181 - val_loss: 0.3843 - val_acc: 0.8506\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4150 - acc: 0.8174 - val_loss: 0.3838 - val_acc: 0.8424\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4156 - acc: 0.8199 - val_loss: 0.3836 - val_acc: 0.8440\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4188 - acc: 0.8179 - val_loss: 0.3828 - val_acc: 0.8489\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4137 - acc: 0.8176 - val_loss: 0.3830 - val_acc: 0.8489\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4121 - acc: 0.8225 - val_loss: 0.3825 - val_acc: 0.8424\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4093 - acc: 0.8245 - val_loss: 0.3802 - val_acc: 0.8506\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4082 - acc: 0.8241 - val_loss: 0.3794 - val_acc: 0.8506\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4075 - acc: 0.8250 - val_loss: 0.3805 - val_acc: 0.8489\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4069 - acc: 0.8234 - val_loss: 0.3802 - val_acc: 0.8440\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4037 - acc: 0.8261 - val_loss: 0.3813 - val_acc: 0.8424\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4074 - acc: 0.8219 - val_loss: 0.3823 - val_acc: 0.8456\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4032 - acc: 0.8245 - val_loss: 0.3786 - val_acc: 0.8489\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4022 - acc: 0.8245 - val_loss: 0.3779 - val_acc: 0.8506\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4022 - acc: 0.8265 - val_loss: 0.3812 - val_acc: 0.8456\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4008 - acc: 0.8254 - val_loss: 0.3808 - val_acc: 0.8456\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4024 - acc: 0.8263 - val_loss: 0.3787 - val_acc: 0.8522\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4017 - acc: 0.8259 - val_loss: 0.3783 - val_acc: 0.8522\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3972 - acc: 0.8289 - val_loss: 0.3769 - val_acc: 0.8506\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3978 - acc: 0.8250 - val_loss: 0.3774 - val_acc: 0.8489\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3972 - acc: 0.8254 - val_loss: 0.3799 - val_acc: 0.8440\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3984 - acc: 0.8274 - val_loss: 0.3789 - val_acc: 0.8489\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3969 - acc: 0.8278 - val_loss: 0.3763 - val_acc: 0.8473\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3934 - acc: 0.8290 - val_loss: 0.3759 - val_acc: 0.8473\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3949 - acc: 0.8320 - val_loss: 0.3759 - val_acc: 0.8473\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3913 - acc: 0.8316 - val_loss: 0.3757 - val_acc: 0.8473\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3928 - acc: 0.8287 - val_loss: 0.3759 - val_acc: 0.8489\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3857 - acc: 0.8360 - val_loss: 0.3759 - val_acc: 0.8473\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3918 - acc: 0.8292 - val_loss: 0.3791 - val_acc: 0.8489\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3876 - acc: 0.8305 - val_loss: 0.3755 - val_acc: 0.8473\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3903 - acc: 0.8320 - val_loss: 0.3748 - val_acc: 0.8489\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3848 - acc: 0.8354 - val_loss: 0.3744 - val_acc: 0.8456\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3852 - acc: 0.8340 - val_loss: 0.3747 - val_acc: 0.8489\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3823 - acc: 0.8325 - val_loss: 0.3751 - val_acc: 0.8506\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3803 - acc: 0.8316 - val_loss: 0.3749 - val_acc: 0.8473\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3830 - acc: 0.8389 - val_loss: 0.3756 - val_acc: 0.8489\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3790 - acc: 0.8393 - val_loss: 0.3763 - val_acc: 0.8407\n",
      "Training with parameters {'batch_size': 1100, 'dropout': 0.1, 'lay_conf': [1024, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_105\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_106 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_315 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_210 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_316 (Dense)            (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_211 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_317 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "5/5 [==============================] - 1s 119ms/step - loss: 0.8127 - acc: 0.5238 - val_loss: 0.6907 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.6843 - acc: 0.5745 - val_loss: 0.6606 - val_acc: 0.6207\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.6507 - acc: 0.6103 - val_loss: 0.6164 - val_acc: 0.5944\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.6281 - acc: 0.6481 - val_loss: 0.5858 - val_acc: 0.6995\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.5998 - acc: 0.6835 - val_loss: 0.5550 - val_acc: 0.7603\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.5855 - acc: 0.7110 - val_loss: 0.5381 - val_acc: 0.7553\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.5686 - acc: 0.7154 - val_loss: 0.5164 - val_acc: 0.7898\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.5546 - acc: 0.7300 - val_loss: 0.4990 - val_acc: 0.8112\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.5364 - acc: 0.7473 - val_loss: 0.4823 - val_acc: 0.8161\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.5226 - acc: 0.7555 - val_loss: 0.4657 - val_acc: 0.8161\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.5096 - acc: 0.7646 - val_loss: 0.4518 - val_acc: 0.8243\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4978 - acc: 0.7679 - val_loss: 0.4455 - val_acc: 0.8243\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4891 - acc: 0.7692 - val_loss: 0.4386 - val_acc: 0.8194\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4811 - acc: 0.7761 - val_loss: 0.4241 - val_acc: 0.8309\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4709 - acc: 0.7854 - val_loss: 0.4182 - val_acc: 0.8292\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4629 - acc: 0.7900 - val_loss: 0.4115 - val_acc: 0.8325\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4577 - acc: 0.7898 - val_loss: 0.4081 - val_acc: 0.8309\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4551 - acc: 0.7971 - val_loss: 0.4039 - val_acc: 0.8325\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4531 - acc: 0.7964 - val_loss: 0.4102 - val_acc: 0.8227\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4471 - acc: 0.8020 - val_loss: 0.4057 - val_acc: 0.8292\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4446 - acc: 0.7975 - val_loss: 0.3971 - val_acc: 0.8374\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4385 - acc: 0.8059 - val_loss: 0.3940 - val_acc: 0.8374\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4388 - acc: 0.8068 - val_loss: 0.3945 - val_acc: 0.8391\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4328 - acc: 0.8073 - val_loss: 0.3908 - val_acc: 0.8424\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4308 - acc: 0.8077 - val_loss: 0.3876 - val_acc: 0.8407\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4256 - acc: 0.8137 - val_loss: 0.3857 - val_acc: 0.8424\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4211 - acc: 0.8152 - val_loss: 0.3865 - val_acc: 0.8440\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4160 - acc: 0.8181 - val_loss: 0.3885 - val_acc: 0.8473\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4177 - acc: 0.8148 - val_loss: 0.3823 - val_acc: 0.8440\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4134 - acc: 0.8217 - val_loss: 0.3881 - val_acc: 0.8358\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4148 - acc: 0.8150 - val_loss: 0.3930 - val_acc: 0.8407\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4166 - acc: 0.8148 - val_loss: 0.3998 - val_acc: 0.8342\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4175 - acc: 0.8188 - val_loss: 0.3797 - val_acc: 0.8424\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4105 - acc: 0.8174 - val_loss: 0.3801 - val_acc: 0.8489\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4013 - acc: 0.8232 - val_loss: 0.3784 - val_acc: 0.8456\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3982 - acc: 0.8250 - val_loss: 0.3771 - val_acc: 0.8424\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3990 - acc: 0.8261 - val_loss: 0.3777 - val_acc: 0.8424\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3951 - acc: 0.8294 - val_loss: 0.3773 - val_acc: 0.8456\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3979 - acc: 0.8258 - val_loss: 0.3763 - val_acc: 0.8473\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3904 - acc: 0.8274 - val_loss: 0.3747 - val_acc: 0.8424\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3889 - acc: 0.8314 - val_loss: 0.3734 - val_acc: 0.8407\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3870 - acc: 0.8265 - val_loss: 0.3730 - val_acc: 0.8424\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3827 - acc: 0.8290 - val_loss: 0.3726 - val_acc: 0.8407\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3787 - acc: 0.8354 - val_loss: 0.3733 - val_acc: 0.8424\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3763 - acc: 0.8338 - val_loss: 0.3707 - val_acc: 0.8456\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3793 - acc: 0.8356 - val_loss: 0.3701 - val_acc: 0.8456\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3793 - acc: 0.8343 - val_loss: 0.3722 - val_acc: 0.8456\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3770 - acc: 0.8352 - val_loss: 0.3715 - val_acc: 0.8473\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3742 - acc: 0.8405 - val_loss: 0.3733 - val_acc: 0.8473\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3678 - acc: 0.8424 - val_loss: 0.3703 - val_acc: 0.8473\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3646 - acc: 0.8409 - val_loss: 0.3714 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 1100, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_106\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_107 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_318 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_212 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_319 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_213 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_320 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,985\n",
      "Trainable params: 409,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 1.2758 - acc: 0.4382 - val_loss: 0.7122 - val_acc: 0.4663\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.7616 - acc: 0.5360 - val_loss: 0.7857 - val_acc: 0.5747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.7648 - acc: 0.5680 - val_loss: 0.7001 - val_acc: 0.5747\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6969 - acc: 0.5676 - val_loss: 0.6588 - val_acc: 0.6420\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6816 - acc: 0.5711 - val_loss: 0.6568 - val_acc: 0.6535\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6665 - acc: 0.5982 - val_loss: 0.6322 - val_acc: 0.6585\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6554 - acc: 0.6083 - val_loss: 0.6242 - val_acc: 0.6092\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6457 - acc: 0.6258 - val_loss: 0.6070 - val_acc: 0.7291\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6341 - acc: 0.6471 - val_loss: 0.5964 - val_acc: 0.7274\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6249 - acc: 0.6623 - val_loss: 0.5808 - val_acc: 0.7570\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6150 - acc: 0.6751 - val_loss: 0.5694 - val_acc: 0.7619\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6041 - acc: 0.6875 - val_loss: 0.5601 - val_acc: 0.7586\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5954 - acc: 0.6986 - val_loss: 0.5491 - val_acc: 0.7718\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5944 - acc: 0.7066 - val_loss: 0.5401 - val_acc: 0.7816\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5810 - acc: 0.7044 - val_loss: 0.5343 - val_acc: 0.7833\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5754 - acc: 0.7114 - val_loss: 0.5250 - val_acc: 0.7865\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5658 - acc: 0.7282 - val_loss: 0.5187 - val_acc: 0.7964\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5584 - acc: 0.7303 - val_loss: 0.5109 - val_acc: 0.8046\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5569 - acc: 0.7353 - val_loss: 0.5044 - val_acc: 0.8112\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5499 - acc: 0.7303 - val_loss: 0.4977 - val_acc: 0.8095\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5447 - acc: 0.7511 - val_loss: 0.4907 - val_acc: 0.8112\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5380 - acc: 0.7462 - val_loss: 0.4842 - val_acc: 0.8062\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5298 - acc: 0.7552 - val_loss: 0.4776 - val_acc: 0.8112\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5276 - acc: 0.7519 - val_loss: 0.4714 - val_acc: 0.8128\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5172 - acc: 0.7621 - val_loss: 0.4660 - val_acc: 0.8128\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5104 - acc: 0.7652 - val_loss: 0.4606 - val_acc: 0.8210\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5073 - acc: 0.7670 - val_loss: 0.4557 - val_acc: 0.8177\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5036 - acc: 0.7707 - val_loss: 0.4512 - val_acc: 0.8259\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5020 - acc: 0.7643 - val_loss: 0.4471 - val_acc: 0.8210\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4987 - acc: 0.7827 - val_loss: 0.4433 - val_acc: 0.8259\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4923 - acc: 0.7747 - val_loss: 0.4399 - val_acc: 0.8259\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4853 - acc: 0.7800 - val_loss: 0.4368 - val_acc: 0.8276\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4839 - acc: 0.7796 - val_loss: 0.4332 - val_acc: 0.8325\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4781 - acc: 0.7862 - val_loss: 0.4303 - val_acc: 0.8358\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4800 - acc: 0.7865 - val_loss: 0.4276 - val_acc: 0.8374\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4764 - acc: 0.7887 - val_loss: 0.4264 - val_acc: 0.8259\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4761 - acc: 0.7884 - val_loss: 0.4235 - val_acc: 0.8309\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4704 - acc: 0.7876 - val_loss: 0.4218 - val_acc: 0.8424\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4683 - acc: 0.7898 - val_loss: 0.4209 - val_acc: 0.8309\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4692 - acc: 0.7851 - val_loss: 0.4174 - val_acc: 0.8407\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4623 - acc: 0.7993 - val_loss: 0.4157 - val_acc: 0.8424\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4645 - acc: 0.7935 - val_loss: 0.4148 - val_acc: 0.8391\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4589 - acc: 0.8004 - val_loss: 0.4131 - val_acc: 0.8440\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4562 - acc: 0.7955 - val_loss: 0.4109 - val_acc: 0.8391\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4521 - acc: 0.7995 - val_loss: 0.4103 - val_acc: 0.8440\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4525 - acc: 0.8048 - val_loss: 0.4110 - val_acc: 0.8391\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4515 - acc: 0.8002 - val_loss: 0.4079 - val_acc: 0.8407\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4506 - acc: 0.7986 - val_loss: 0.4055 - val_acc: 0.8489\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4455 - acc: 0.8046 - val_loss: 0.4043 - val_acc: 0.8440\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4429 - acc: 0.8044 - val_loss: 0.4038 - val_acc: 0.8473\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4460 - acc: 0.8028 - val_loss: 0.4023 - val_acc: 0.8407\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4391 - acc: 0.8075 - val_loss: 0.4012 - val_acc: 0.8473\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4401 - acc: 0.8053 - val_loss: 0.4005 - val_acc: 0.8440\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4368 - acc: 0.8097 - val_loss: 0.3997 - val_acc: 0.8456\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4342 - acc: 0.8143 - val_loss: 0.3989 - val_acc: 0.8456\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4344 - acc: 0.8086 - val_loss: 0.3990 - val_acc: 0.8424\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4355 - acc: 0.8112 - val_loss: 0.3984 - val_acc: 0.8456\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4354 - acc: 0.8110 - val_loss: 0.3965 - val_acc: 0.8440\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4326 - acc: 0.8079 - val_loss: 0.3975 - val_acc: 0.8440\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4321 - acc: 0.8121 - val_loss: 0.3979 - val_acc: 0.8440\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4263 - acc: 0.8148 - val_loss: 0.3947 - val_acc: 0.8473\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4270 - acc: 0.8130 - val_loss: 0.3940 - val_acc: 0.8456\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4254 - acc: 0.8146 - val_loss: 0.3940 - val_acc: 0.8424\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4263 - acc: 0.8137 - val_loss: 0.3929 - val_acc: 0.8456\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4248 - acc: 0.8168 - val_loss: 0.3934 - val_acc: 0.8424\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4223 - acc: 0.8177 - val_loss: 0.3968 - val_acc: 0.8440\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4239 - acc: 0.8132 - val_loss: 0.3926 - val_acc: 0.8473\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4189 - acc: 0.8168 - val_loss: 0.3910 - val_acc: 0.8424\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4196 - acc: 0.8144 - val_loss: 0.3905 - val_acc: 0.8424\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4177 - acc: 0.8174 - val_loss: 0.3914 - val_acc: 0.8440\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4191 - acc: 0.8135 - val_loss: 0.3901 - val_acc: 0.8407\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4147 - acc: 0.8186 - val_loss: 0.3901 - val_acc: 0.8424\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4155 - acc: 0.8201 - val_loss: 0.3906 - val_acc: 0.8489\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4138 - acc: 0.8214 - val_loss: 0.3922 - val_acc: 0.8440\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4143 - acc: 0.8181 - val_loss: 0.3878 - val_acc: 0.8473\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4130 - acc: 0.8227 - val_loss: 0.3875 - val_acc: 0.8473\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4081 - acc: 0.8201 - val_loss: 0.3883 - val_acc: 0.8424\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4129 - acc: 0.8203 - val_loss: 0.3881 - val_acc: 0.8522\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4100 - acc: 0.8188 - val_loss: 0.3871 - val_acc: 0.8440\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4083 - acc: 0.8223 - val_loss: 0.3865 - val_acc: 0.8391\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4030 - acc: 0.8241 - val_loss: 0.3866 - val_acc: 0.8440\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4060 - acc: 0.8243 - val_loss: 0.3867 - val_acc: 0.8424\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4028 - acc: 0.8261 - val_loss: 0.3868 - val_acc: 0.8424\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4022 - acc: 0.8254 - val_loss: 0.3863 - val_acc: 0.8407\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4028 - acc: 0.8270 - val_loss: 0.3860 - val_acc: 0.8424\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4004 - acc: 0.8252 - val_loss: 0.3878 - val_acc: 0.8440\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3993 - acc: 0.8283 - val_loss: 0.3855 - val_acc: 0.8424\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3973 - acc: 0.8245 - val_loss: 0.3862 - val_acc: 0.8407\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4005 - acc: 0.8280 - val_loss: 0.3870 - val_acc: 0.8456\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3968 - acc: 0.8265 - val_loss: 0.3860 - val_acc: 0.8407\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3967 - acc: 0.8272 - val_loss: 0.3853 - val_acc: 0.8440\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3937 - acc: 0.8336 - val_loss: 0.3891 - val_acc: 0.8424\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3948 - acc: 0.8298 - val_loss: 0.3857 - val_acc: 0.8407\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3903 - acc: 0.8354 - val_loss: 0.3866 - val_acc: 0.8391\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3893 - acc: 0.8301 - val_loss: 0.3894 - val_acc: 0.8424\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3915 - acc: 0.8298 - val_loss: 0.3886 - val_acc: 0.8407\n",
      "Training with parameters {'batch_size': 1100, 'dropout': 0.1, 'lay_conf': [1024, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_107\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_108 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_321 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_214 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_322 (Dense)            (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_215 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_323 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,639,553\n",
      "Trainable params: 1,639,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.7596 - acc: 0.5342 - val_loss: 0.6481 - val_acc: 0.6223\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6690 - acc: 0.5888 - val_loss: 0.6120 - val_acc: 0.7258\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.6263 - acc: 0.6554 - val_loss: 0.5733 - val_acc: 0.7537\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.6009 - acc: 0.6818 - val_loss: 0.5448 - val_acc: 0.7701\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5803 - acc: 0.7112 - val_loss: 0.5266 - val_acc: 0.7504\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5619 - acc: 0.7178 - val_loss: 0.5039 - val_acc: 0.8013\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5492 - acc: 0.7285 - val_loss: 0.4783 - val_acc: 0.8194\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5300 - acc: 0.7460 - val_loss: 0.4639 - val_acc: 0.8177\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.5154 - acc: 0.7588 - val_loss: 0.4505 - val_acc: 0.8259\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4999 - acc: 0.7688 - val_loss: 0.4399 - val_acc: 0.8259\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4896 - acc: 0.7738 - val_loss: 0.4293 - val_acc: 0.8292\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4854 - acc: 0.7774 - val_loss: 0.4214 - val_acc: 0.8325\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4774 - acc: 0.7836 - val_loss: 0.4175 - val_acc: 0.8325\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4684 - acc: 0.7851 - val_loss: 0.4099 - val_acc: 0.8407\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4653 - acc: 0.7873 - val_loss: 0.4060 - val_acc: 0.8407\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4640 - acc: 0.7891 - val_loss: 0.4053 - val_acc: 0.8358\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4556 - acc: 0.7953 - val_loss: 0.3985 - val_acc: 0.8456\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4487 - acc: 0.7980 - val_loss: 0.3963 - val_acc: 0.8440\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4462 - acc: 0.7984 - val_loss: 0.3927 - val_acc: 0.8489\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4427 - acc: 0.8042 - val_loss: 0.3957 - val_acc: 0.8424\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4419 - acc: 0.8079 - val_loss: 0.3942 - val_acc: 0.8391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4391 - acc: 0.8039 - val_loss: 0.3879 - val_acc: 0.8522\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4354 - acc: 0.8064 - val_loss: 0.3841 - val_acc: 0.8555\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4296 - acc: 0.8163 - val_loss: 0.3839 - val_acc: 0.8539\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4269 - acc: 0.8123 - val_loss: 0.3819 - val_acc: 0.8571\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4213 - acc: 0.8121 - val_loss: 0.3802 - val_acc: 0.8571\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4182 - acc: 0.8177 - val_loss: 0.3801 - val_acc: 0.8506\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4174 - acc: 0.8165 - val_loss: 0.3821 - val_acc: 0.8456\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4157 - acc: 0.8157 - val_loss: 0.3764 - val_acc: 0.8588\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4134 - acc: 0.8214 - val_loss: 0.3780 - val_acc: 0.8539\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4119 - acc: 0.8176 - val_loss: 0.3951 - val_acc: 0.8374\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4142 - acc: 0.8126 - val_loss: 0.3791 - val_acc: 0.8489\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4088 - acc: 0.8221 - val_loss: 0.3725 - val_acc: 0.8571\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4041 - acc: 0.8234 - val_loss: 0.3763 - val_acc: 0.8456\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4043 - acc: 0.8201 - val_loss: 0.3725 - val_acc: 0.8522\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4038 - acc: 0.8221 - val_loss: 0.3752 - val_acc: 0.8489\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3999 - acc: 0.8238 - val_loss: 0.3716 - val_acc: 0.8522\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3982 - acc: 0.8258 - val_loss: 0.3693 - val_acc: 0.8522\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3973 - acc: 0.8294 - val_loss: 0.3724 - val_acc: 0.8489\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3937 - acc: 0.8318 - val_loss: 0.3695 - val_acc: 0.8489\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3936 - acc: 0.8272 - val_loss: 0.3675 - val_acc: 0.8506\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3913 - acc: 0.8289 - val_loss: 0.3692 - val_acc: 0.8473\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3874 - acc: 0.8334 - val_loss: 0.3688 - val_acc: 0.8473\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3878 - acc: 0.8318 - val_loss: 0.3697 - val_acc: 0.8506\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3851 - acc: 0.8309 - val_loss: 0.3680 - val_acc: 0.8456\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3813 - acc: 0.8340 - val_loss: 0.3660 - val_acc: 0.8506\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3778 - acc: 0.8374 - val_loss: 0.3665 - val_acc: 0.8522\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3798 - acc: 0.8316 - val_loss: 0.3664 - val_acc: 0.8522\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3736 - acc: 0.8385 - val_loss: 0.3673 - val_acc: 0.8489\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3757 - acc: 0.8393 - val_loss: 0.3698 - val_acc: 0.8522\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3706 - acc: 0.8402 - val_loss: 0.3668 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 1100, 'dropout': 0.1, 'lay_conf': [1024, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_108\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_109 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_324 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_216 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_325 (Dense)            (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_217 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_326 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,606,721\n",
      "Trainable params: 1,606,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.7940 - acc: 0.5327 - val_loss: 0.6583 - val_acc: 0.6059\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.6783 - acc: 0.5866 - val_loss: 0.6136 - val_acc: 0.7143\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.6330 - acc: 0.6451 - val_loss: 0.5817 - val_acc: 0.7504\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.6079 - acc: 0.6734 - val_loss: 0.5465 - val_acc: 0.7652\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.5836 - acc: 0.7081 - val_loss: 0.5165 - val_acc: 0.7816\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.5636 - acc: 0.7207 - val_loss: 0.4973 - val_acc: 0.8079\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.5403 - acc: 0.7407 - val_loss: 0.4777 - val_acc: 0.8144\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.5329 - acc: 0.7477 - val_loss: 0.4646 - val_acc: 0.8210\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.5217 - acc: 0.7572 - val_loss: 0.4556 - val_acc: 0.8144\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.5126 - acc: 0.7562 - val_loss: 0.4460 - val_acc: 0.8194\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.5070 - acc: 0.7650 - val_loss: 0.4401 - val_acc: 0.8177\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4933 - acc: 0.7741 - val_loss: 0.4237 - val_acc: 0.8292\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4889 - acc: 0.7750 - val_loss: 0.4292 - val_acc: 0.8292\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4862 - acc: 0.7754 - val_loss: 0.4257 - val_acc: 0.8227\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4764 - acc: 0.7792 - val_loss: 0.4084 - val_acc: 0.8424\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4710 - acc: 0.7880 - val_loss: 0.4081 - val_acc: 0.8342\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4608 - acc: 0.7905 - val_loss: 0.4068 - val_acc: 0.8424\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4601 - acc: 0.7915 - val_loss: 0.4001 - val_acc: 0.8456\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4554 - acc: 0.7960 - val_loss: 0.4005 - val_acc: 0.8374\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4566 - acc: 0.7940 - val_loss: 0.3935 - val_acc: 0.8424\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4513 - acc: 0.7989 - val_loss: 0.3935 - val_acc: 0.8473\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4416 - acc: 0.8031 - val_loss: 0.3899 - val_acc: 0.8456\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4409 - acc: 0.8024 - val_loss: 0.3886 - val_acc: 0.8440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4422 - acc: 0.8039 - val_loss: 0.3867 - val_acc: 0.8473\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4352 - acc: 0.8092 - val_loss: 0.3873 - val_acc: 0.8473\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4343 - acc: 0.8055 - val_loss: 0.3898 - val_acc: 0.8440\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4331 - acc: 0.8099 - val_loss: 0.3834 - val_acc: 0.8522\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4249 - acc: 0.8092 - val_loss: 0.3815 - val_acc: 0.8473\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4249 - acc: 0.8121 - val_loss: 0.3787 - val_acc: 0.8506\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4219 - acc: 0.8132 - val_loss: 0.3777 - val_acc: 0.8506\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4212 - acc: 0.8144 - val_loss: 0.3776 - val_acc: 0.8473\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4157 - acc: 0.8165 - val_loss: 0.3775 - val_acc: 0.8539\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4136 - acc: 0.8186 - val_loss: 0.3769 - val_acc: 0.8506\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4100 - acc: 0.8197 - val_loss: 0.3745 - val_acc: 0.8522\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4092 - acc: 0.8203 - val_loss: 0.3739 - val_acc: 0.8539\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4088 - acc: 0.8214 - val_loss: 0.3764 - val_acc: 0.8539\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4088 - acc: 0.8197 - val_loss: 0.3768 - val_acc: 0.8506\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4088 - acc: 0.8165 - val_loss: 0.3800 - val_acc: 0.8489\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4149 - acc: 0.8196 - val_loss: 0.3726 - val_acc: 0.8539\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4074 - acc: 0.8236 - val_loss: 0.3807 - val_acc: 0.8522\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4096 - acc: 0.8207 - val_loss: 0.3840 - val_acc: 0.8473\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4033 - acc: 0.8241 - val_loss: 0.3694 - val_acc: 0.8571\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4037 - acc: 0.8217 - val_loss: 0.3744 - val_acc: 0.8522\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3976 - acc: 0.8263 - val_loss: 0.3727 - val_acc: 0.8473\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3984 - acc: 0.8265 - val_loss: 0.3689 - val_acc: 0.8555\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3936 - acc: 0.8292 - val_loss: 0.3724 - val_acc: 0.8539\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3922 - acc: 0.8292 - val_loss: 0.3671 - val_acc: 0.8588\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3901 - acc: 0.8301 - val_loss: 0.3668 - val_acc: 0.8555\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3863 - acc: 0.8320 - val_loss: 0.3679 - val_acc: 0.8522\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3890 - acc: 0.8331 - val_loss: 0.3691 - val_acc: 0.8506\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3874 - acc: 0.8294 - val_loss: 0.3703 - val_acc: 0.8588\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3846 - acc: 0.8323 - val_loss: 0.3667 - val_acc: 0.8571\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3833 - acc: 0.8307 - val_loss: 0.3662 - val_acc: 0.8571\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3807 - acc: 0.8369 - val_loss: 0.3658 - val_acc: 0.8522\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3795 - acc: 0.8356 - val_loss: 0.3664 - val_acc: 0.8489\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3777 - acc: 0.8352 - val_loss: 0.3706 - val_acc: 0.8456\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3776 - acc: 0.8382 - val_loss: 0.3655 - val_acc: 0.8522\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3770 - acc: 0.8365 - val_loss: 0.3745 - val_acc: 0.8539\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3727 - acc: 0.8378 - val_loss: 0.3709 - val_acc: 0.8539\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3758 - acc: 0.8402 - val_loss: 0.3656 - val_acc: 0.8571\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3727 - acc: 0.8382 - val_loss: 0.3657 - val_acc: 0.8571\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3665 - acc: 0.8385 - val_loss: 0.3667 - val_acc: 0.8571\n",
      "Training with parameters {'batch_size': 1100, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_109\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_110 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_327 (Dense)            (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_218 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_328 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_219 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_329 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,050,113\n",
      "Trainable params: 1,050,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.7143 - acc: 0.5337 - val_loss: 0.6488 - val_acc: 0.6223\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6513 - acc: 0.6269 - val_loss: 0.6037 - val_acc: 0.7011\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6168 - acc: 0.6690 - val_loss: 0.5720 - val_acc: 0.7110\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.5908 - acc: 0.6893 - val_loss: 0.5474 - val_acc: 0.7701\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.5755 - acc: 0.7132 - val_loss: 0.5253 - val_acc: 0.7635\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5516 - acc: 0.7353 - val_loss: 0.5016 - val_acc: 0.7980\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5342 - acc: 0.7477 - val_loss: 0.4835 - val_acc: 0.8079\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.5232 - acc: 0.7599 - val_loss: 0.4690 - val_acc: 0.8128\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.5146 - acc: 0.7603 - val_loss: 0.4717 - val_acc: 0.8013\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.5074 - acc: 0.7615 - val_loss: 0.4451 - val_acc: 0.8227\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4967 - acc: 0.7679 - val_loss: 0.4501 - val_acc: 0.8128\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4871 - acc: 0.7772 - val_loss: 0.4341 - val_acc: 0.8276\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4755 - acc: 0.7834 - val_loss: 0.4273 - val_acc: 0.8292\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4670 - acc: 0.7924 - val_loss: 0.4229 - val_acc: 0.8276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4653 - acc: 0.7907 - val_loss: 0.4152 - val_acc: 0.8342\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4648 - acc: 0.7918 - val_loss: 0.4184 - val_acc: 0.8276\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4581 - acc: 0.7918 - val_loss: 0.4088 - val_acc: 0.8358\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4532 - acc: 0.7957 - val_loss: 0.4139 - val_acc: 0.8243\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4458 - acc: 0.8004 - val_loss: 0.4022 - val_acc: 0.8374\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4410 - acc: 0.8082 - val_loss: 0.4076 - val_acc: 0.8342\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4429 - acc: 0.8062 - val_loss: 0.3974 - val_acc: 0.8424\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4389 - acc: 0.8039 - val_loss: 0.4011 - val_acc: 0.8374\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4394 - acc: 0.8055 - val_loss: 0.4132 - val_acc: 0.8210\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4388 - acc: 0.8077 - val_loss: 0.3940 - val_acc: 0.8407\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4283 - acc: 0.8123 - val_loss: 0.3949 - val_acc: 0.8456\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4262 - acc: 0.8144 - val_loss: 0.3927 - val_acc: 0.8440\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4191 - acc: 0.8150 - val_loss: 0.3889 - val_acc: 0.8489\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4167 - acc: 0.8157 - val_loss: 0.3879 - val_acc: 0.8506\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4138 - acc: 0.8199 - val_loss: 0.3872 - val_acc: 0.8506\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4139 - acc: 0.8185 - val_loss: 0.3870 - val_acc: 0.8506\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4093 - acc: 0.8221 - val_loss: 0.3846 - val_acc: 0.8555\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4052 - acc: 0.8203 - val_loss: 0.3873 - val_acc: 0.8489\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4047 - acc: 0.8225 - val_loss: 0.3828 - val_acc: 0.8539\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4029 - acc: 0.8239 - val_loss: 0.3821 - val_acc: 0.8522\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3992 - acc: 0.8278 - val_loss: 0.3809 - val_acc: 0.8555\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3966 - acc: 0.8259 - val_loss: 0.3837 - val_acc: 0.8539\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3954 - acc: 0.8248 - val_loss: 0.3854 - val_acc: 0.8506\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3937 - acc: 0.8267 - val_loss: 0.3790 - val_acc: 0.8539\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3879 - acc: 0.8320 - val_loss: 0.3783 - val_acc: 0.8555\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3850 - acc: 0.8349 - val_loss: 0.3787 - val_acc: 0.8539\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3842 - acc: 0.8318 - val_loss: 0.3783 - val_acc: 0.8539\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3804 - acc: 0.8354 - val_loss: 0.3787 - val_acc: 0.8555\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3839 - acc: 0.8327 - val_loss: 0.3765 - val_acc: 0.8555\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3780 - acc: 0.8338 - val_loss: 0.3763 - val_acc: 0.8522\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3763 - acc: 0.8329 - val_loss: 0.3769 - val_acc: 0.8489\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3774 - acc: 0.8325 - val_loss: 0.3775 - val_acc: 0.8539\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3750 - acc: 0.8329 - val_loss: 0.3818 - val_acc: 0.8539\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3757 - acc: 0.8378 - val_loss: 0.3769 - val_acc: 0.8539\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3712 - acc: 0.8371 - val_loss: 0.3798 - val_acc: 0.8522\n",
      "Training with parameters {'batch_size': 1100, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_110\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_111 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_330 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_220 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_331 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_221 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_332 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 401,729\n",
      "Trainable params: 401,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.8535 - acc: 0.4541 - val_loss: 0.6996 - val_acc: 0.5714\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.7098 - acc: 0.5570 - val_loss: 0.6788 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6802 - acc: 0.5731 - val_loss: 0.6572 - val_acc: 0.6634\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6660 - acc: 0.5981 - val_loss: 0.6479 - val_acc: 0.6749\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6592 - acc: 0.6161 - val_loss: 0.6282 - val_acc: 0.6749\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6498 - acc: 0.6207 - val_loss: 0.6158 - val_acc: 0.7028\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6398 - acc: 0.6395 - val_loss: 0.6075 - val_acc: 0.7126\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6288 - acc: 0.6583 - val_loss: 0.5920 - val_acc: 0.7274\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6187 - acc: 0.6747 - val_loss: 0.5808 - val_acc: 0.7307\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6112 - acc: 0.6811 - val_loss: 0.5705 - val_acc: 0.7274\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6022 - acc: 0.6858 - val_loss: 0.5594 - val_acc: 0.7504\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5922 - acc: 0.7001 - val_loss: 0.5494 - val_acc: 0.7553\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5824 - acc: 0.7148 - val_loss: 0.5371 - val_acc: 0.7767\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5773 - acc: 0.7141 - val_loss: 0.5257 - val_acc: 0.7800\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5621 - acc: 0.7276 - val_loss: 0.5158 - val_acc: 0.7915\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5567 - acc: 0.7333 - val_loss: 0.5063 - val_acc: 0.8046\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5506 - acc: 0.7292 - val_loss: 0.4983 - val_acc: 0.7964\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5443 - acc: 0.7424 - val_loss: 0.4891 - val_acc: 0.8128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5315 - acc: 0.7559 - val_loss: 0.4816 - val_acc: 0.8112\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5258 - acc: 0.7508 - val_loss: 0.4745 - val_acc: 0.8161\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5193 - acc: 0.7579 - val_loss: 0.4690 - val_acc: 0.8112\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5127 - acc: 0.7688 - val_loss: 0.4634 - val_acc: 0.8194\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5086 - acc: 0.7705 - val_loss: 0.4574 - val_acc: 0.8177\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5024 - acc: 0.7696 - val_loss: 0.4525 - val_acc: 0.8112\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5016 - acc: 0.7716 - val_loss: 0.4476 - val_acc: 0.8292\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4958 - acc: 0.7767 - val_loss: 0.4436 - val_acc: 0.8161\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4904 - acc: 0.7812 - val_loss: 0.4399 - val_acc: 0.8259\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4883 - acc: 0.7814 - val_loss: 0.4361 - val_acc: 0.8276\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4829 - acc: 0.7829 - val_loss: 0.4324 - val_acc: 0.8292\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4806 - acc: 0.7836 - val_loss: 0.4298 - val_acc: 0.8243\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4740 - acc: 0.7944 - val_loss: 0.4285 - val_acc: 0.8325\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4749 - acc: 0.7867 - val_loss: 0.4250 - val_acc: 0.8292\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4707 - acc: 0.7922 - val_loss: 0.4218 - val_acc: 0.8325\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4625 - acc: 0.7986 - val_loss: 0.4190 - val_acc: 0.8358\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4620 - acc: 0.7951 - val_loss: 0.4163 - val_acc: 0.8342\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4607 - acc: 0.7958 - val_loss: 0.4197 - val_acc: 0.8342\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4613 - acc: 0.7931 - val_loss: 0.4187 - val_acc: 0.8309\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4576 - acc: 0.7957 - val_loss: 0.4104 - val_acc: 0.8309\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4535 - acc: 0.7991 - val_loss: 0.4076 - val_acc: 0.8358\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4507 - acc: 0.7993 - val_loss: 0.4107 - val_acc: 0.8309\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4517 - acc: 0.8030 - val_loss: 0.4076 - val_acc: 0.8292\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4458 - acc: 0.8051 - val_loss: 0.4042 - val_acc: 0.8358\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4461 - acc: 0.8057 - val_loss: 0.4020 - val_acc: 0.8391\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4456 - acc: 0.8062 - val_loss: 0.4018 - val_acc: 0.8358\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4371 - acc: 0.8124 - val_loss: 0.4001 - val_acc: 0.8391\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4398 - acc: 0.8084 - val_loss: 0.3997 - val_acc: 0.8407\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4410 - acc: 0.8044 - val_loss: 0.3996 - val_acc: 0.8374\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4394 - acc: 0.8119 - val_loss: 0.3981 - val_acc: 0.8440\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4348 - acc: 0.8057 - val_loss: 0.3977 - val_acc: 0.8424\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4355 - acc: 0.8095 - val_loss: 0.3956 - val_acc: 0.8440\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4301 - acc: 0.8150 - val_loss: 0.3949 - val_acc: 0.8424\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4295 - acc: 0.8121 - val_loss: 0.3930 - val_acc: 0.8456\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4260 - acc: 0.8146 - val_loss: 0.3909 - val_acc: 0.8407\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4292 - acc: 0.8132 - val_loss: 0.3909 - val_acc: 0.8407\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4253 - acc: 0.8141 - val_loss: 0.3908 - val_acc: 0.8407\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4195 - acc: 0.8203 - val_loss: 0.3924 - val_acc: 0.8424\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4232 - acc: 0.8117 - val_loss: 0.3898 - val_acc: 0.8407\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4201 - acc: 0.8179 - val_loss: 0.3896 - val_acc: 0.8456\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4176 - acc: 0.8174 - val_loss: 0.3876 - val_acc: 0.8424\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4181 - acc: 0.8199 - val_loss: 0.3858 - val_acc: 0.8407\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4129 - acc: 0.8192 - val_loss: 0.3891 - val_acc: 0.8456\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4133 - acc: 0.8228 - val_loss: 0.3859 - val_acc: 0.8342\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4104 - acc: 0.8203 - val_loss: 0.3845 - val_acc: 0.8424\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4100 - acc: 0.8227 - val_loss: 0.3833 - val_acc: 0.8391\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4116 - acc: 0.8190 - val_loss: 0.3830 - val_acc: 0.8391\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4045 - acc: 0.8272 - val_loss: 0.3834 - val_acc: 0.8358\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4074 - acc: 0.8232 - val_loss: 0.3819 - val_acc: 0.8440\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4016 - acc: 0.8261 - val_loss: 0.3810 - val_acc: 0.8407\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4001 - acc: 0.8290 - val_loss: 0.3852 - val_acc: 0.8440\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4028 - acc: 0.8294 - val_loss: 0.3790 - val_acc: 0.8374\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3996 - acc: 0.8285 - val_loss: 0.3797 - val_acc: 0.8456\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3962 - acc: 0.8281 - val_loss: 0.3793 - val_acc: 0.8407\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3931 - acc: 0.8269 - val_loss: 0.3816 - val_acc: 0.8473\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3992 - acc: 0.8238 - val_loss: 0.3778 - val_acc: 0.8424\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3928 - acc: 0.8312 - val_loss: 0.3856 - val_acc: 0.8489\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3944 - acc: 0.8285 - val_loss: 0.3786 - val_acc: 0.8440\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3898 - acc: 0.8316 - val_loss: 0.3775 - val_acc: 0.8473\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3885 - acc: 0.8309 - val_loss: 0.3775 - val_acc: 0.8440\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3870 - acc: 0.8338 - val_loss: 0.3767 - val_acc: 0.8489\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3881 - acc: 0.8301 - val_loss: 0.3761 - val_acc: 0.8473\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3856 - acc: 0.8327 - val_loss: 0.3793 - val_acc: 0.8440\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3824 - acc: 0.8378 - val_loss: 0.3767 - val_acc: 0.8522\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3827 - acc: 0.8369 - val_loss: 0.3762 - val_acc: 0.8456\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3829 - acc: 0.8358 - val_loss: 0.3754 - val_acc: 0.8522\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3787 - acc: 0.8376 - val_loss: 0.3751 - val_acc: 0.8522\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3781 - acc: 0.8347 - val_loss: 0.3756 - val_acc: 0.8473\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3773 - acc: 0.8380 - val_loss: 0.3799 - val_acc: 0.8473\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3750 - acc: 0.8431 - val_loss: 0.3767 - val_acc: 0.8489\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3776 - acc: 0.8396 - val_loss: 0.3749 - val_acc: 0.8489\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3731 - acc: 0.8416 - val_loss: 0.3765 - val_acc: 0.8506\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3718 - acc: 0.8404 - val_loss: 0.3759 - val_acc: 0.8424\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3719 - acc: 0.8402 - val_loss: 0.3766 - val_acc: 0.8440\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3744 - acc: 0.8416 - val_loss: 0.3763 - val_acc: 0.8473\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3717 - acc: 0.8409 - val_loss: 0.3782 - val_acc: 0.8456\n",
      "Training with parameters {'batch_size': 1100, 'dropout': 0.1, 'lay_conf': [1024, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_111\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_112 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_333 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_222 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_334 (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_223 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_335 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,836,545\n",
      "Trainable params: 1,836,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.8258 - acc: 0.5331 - val_loss: 0.7091 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6825 - acc: 0.5853 - val_loss: 0.6723 - val_acc: 0.5977\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.6482 - acc: 0.6249 - val_loss: 0.6109 - val_acc: 0.6010\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.6291 - acc: 0.6408 - val_loss: 0.5743 - val_acc: 0.7553\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.6024 - acc: 0.6796 - val_loss: 0.5579 - val_acc: 0.7455\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5873 - acc: 0.7035 - val_loss: 0.5360 - val_acc: 0.7849\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5680 - acc: 0.7201 - val_loss: 0.5207 - val_acc: 0.7718\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5505 - acc: 0.7395 - val_loss: 0.5001 - val_acc: 0.8062\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5331 - acc: 0.7499 - val_loss: 0.4830 - val_acc: 0.8062\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5202 - acc: 0.7542 - val_loss: 0.4682 - val_acc: 0.8046\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5118 - acc: 0.7572 - val_loss: 0.4579 - val_acc: 0.8128\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4991 - acc: 0.7688 - val_loss: 0.4463 - val_acc: 0.8210\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4880 - acc: 0.7792 - val_loss: 0.4370 - val_acc: 0.8259\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4794 - acc: 0.7791 - val_loss: 0.4266 - val_acc: 0.8194\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4714 - acc: 0.7865 - val_loss: 0.4188 - val_acc: 0.8276\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4655 - acc: 0.7887 - val_loss: 0.4125 - val_acc: 0.8325\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4592 - acc: 0.7916 - val_loss: 0.4100 - val_acc: 0.8325\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4535 - acc: 0.7951 - val_loss: 0.4047 - val_acc: 0.8309\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4465 - acc: 0.8028 - val_loss: 0.3994 - val_acc: 0.8358\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4429 - acc: 0.8053 - val_loss: 0.3971 - val_acc: 0.8424\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4544 - acc: 0.7944 - val_loss: 0.3995 - val_acc: 0.8456\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4502 - acc: 0.7942 - val_loss: 0.4034 - val_acc: 0.8374\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4428 - acc: 0.8006 - val_loss: 0.3963 - val_acc: 0.8456\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4341 - acc: 0.8068 - val_loss: 0.3888 - val_acc: 0.8407\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4295 - acc: 0.8055 - val_loss: 0.4004 - val_acc: 0.8374\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4273 - acc: 0.8104 - val_loss: 0.3861 - val_acc: 0.8473\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4218 - acc: 0.8139 - val_loss: 0.3858 - val_acc: 0.8456\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4203 - acc: 0.8137 - val_loss: 0.3837 - val_acc: 0.8456\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4171 - acc: 0.8152 - val_loss: 0.3823 - val_acc: 0.8440\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4106 - acc: 0.8223 - val_loss: 0.3834 - val_acc: 0.8473\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4082 - acc: 0.8197 - val_loss: 0.3886 - val_acc: 0.8473\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4105 - acc: 0.8176 - val_loss: 0.3802 - val_acc: 0.8489\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4079 - acc: 0.8197 - val_loss: 0.3787 - val_acc: 0.8522\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4054 - acc: 0.8203 - val_loss: 0.3784 - val_acc: 0.8489\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4016 - acc: 0.8241 - val_loss: 0.3783 - val_acc: 0.8473\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4002 - acc: 0.8265 - val_loss: 0.3760 - val_acc: 0.8506\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3996 - acc: 0.8256 - val_loss: 0.3770 - val_acc: 0.8506\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3946 - acc: 0.8265 - val_loss: 0.3760 - val_acc: 0.8473\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3919 - acc: 0.8281 - val_loss: 0.3736 - val_acc: 0.8506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3907 - acc: 0.8265 - val_loss: 0.3740 - val_acc: 0.8506\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3893 - acc: 0.8292 - val_loss: 0.3744 - val_acc: 0.8440\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3857 - acc: 0.8340 - val_loss: 0.3750 - val_acc: 0.8489\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3824 - acc: 0.8318 - val_loss: 0.3734 - val_acc: 0.8473\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3780 - acc: 0.8382 - val_loss: 0.3740 - val_acc: 0.8522\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3784 - acc: 0.8363 - val_loss: 0.3748 - val_acc: 0.8506\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3756 - acc: 0.8354 - val_loss: 0.3784 - val_acc: 0.8391\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - acc: 0.8382 - val_loss: 0.3720 - val_acc: 0.8522\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3695 - acc: 0.8405 - val_loss: 0.3845 - val_acc: 0.8424\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3720 - acc: 0.8396 - val_loss: 0.3819 - val_acc: 0.8407\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3691 - acc: 0.8400 - val_loss: 0.3851 - val_acc: 0.8391\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3704 - acc: 0.8394 - val_loss: 0.3897 - val_acc: 0.8325\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3674 - acc: 0.8422 - val_loss: 0.3762 - val_acc: 0.8440\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_112\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_113 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_336 (Dense)            (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_224 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_337 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_225 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_338 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 1s 32ms/step - loss: 0.7696 - acc: 0.4643 - val_loss: 0.6791 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6893 - acc: 0.5629 - val_loss: 0.6716 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6768 - acc: 0.5769 - val_loss: 0.6491 - val_acc: 0.6092\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6630 - acc: 0.6086 - val_loss: 0.6363 - val_acc: 0.7011\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6518 - acc: 0.6362 - val_loss: 0.6163 - val_acc: 0.6995\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6395 - acc: 0.6437 - val_loss: 0.5979 - val_acc: 0.7241\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6242 - acc: 0.6643 - val_loss: 0.5839 - val_acc: 0.7209\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6155 - acc: 0.6778 - val_loss: 0.5705 - val_acc: 0.7537\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6034 - acc: 0.6917 - val_loss: 0.5596 - val_acc: 0.7455\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5958 - acc: 0.7006 - val_loss: 0.5462 - val_acc: 0.7767\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5900 - acc: 0.6984 - val_loss: 0.5355 - val_acc: 0.7635\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5770 - acc: 0.7110 - val_loss: 0.5242 - val_acc: 0.7931\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5715 - acc: 0.7289 - val_loss: 0.5135 - val_acc: 0.7964\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5600 - acc: 0.7309 - val_loss: 0.5046 - val_acc: 0.7997\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5491 - acc: 0.7438 - val_loss: 0.4944 - val_acc: 0.7980\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5443 - acc: 0.7398 - val_loss: 0.4858 - val_acc: 0.7997\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5354 - acc: 0.7513 - val_loss: 0.4776 - val_acc: 0.8062\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5309 - acc: 0.7599 - val_loss: 0.4705 - val_acc: 0.8030\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5178 - acc: 0.7725 - val_loss: 0.4631 - val_acc: 0.8095\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5129 - acc: 0.7688 - val_loss: 0.4566 - val_acc: 0.8161\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5102 - acc: 0.7677 - val_loss: 0.4509 - val_acc: 0.8177\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5041 - acc: 0.7723 - val_loss: 0.4456 - val_acc: 0.8194\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5016 - acc: 0.7747 - val_loss: 0.4408 - val_acc: 0.8210\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4944 - acc: 0.7783 - val_loss: 0.4361 - val_acc: 0.8210\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4923 - acc: 0.7820 - val_loss: 0.4319 - val_acc: 0.8210\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4818 - acc: 0.7854 - val_loss: 0.4272 - val_acc: 0.8276\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4794 - acc: 0.7847 - val_loss: 0.4233 - val_acc: 0.8309\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4772 - acc: 0.7873 - val_loss: 0.4191 - val_acc: 0.8358\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4689 - acc: 0.7904 - val_loss: 0.4156 - val_acc: 0.8358\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4680 - acc: 0.7893 - val_loss: 0.4138 - val_acc: 0.8374\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4641 - acc: 0.7946 - val_loss: 0.4109 - val_acc: 0.8374\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4627 - acc: 0.7957 - val_loss: 0.4093 - val_acc: 0.8342\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4604 - acc: 0.7916 - val_loss: 0.4068 - val_acc: 0.8374\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4581 - acc: 0.8015 - val_loss: 0.4057 - val_acc: 0.8358\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4553 - acc: 0.7973 - val_loss: 0.4025 - val_acc: 0.8358\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4520 - acc: 0.7980 - val_loss: 0.4005 - val_acc: 0.8374\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4501 - acc: 0.8055 - val_loss: 0.4031 - val_acc: 0.8407\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4497 - acc: 0.8019 - val_loss: 0.3982 - val_acc: 0.8391\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4458 - acc: 0.8035 - val_loss: 0.3949 - val_acc: 0.8391\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4431 - acc: 0.8026 - val_loss: 0.3975 - val_acc: 0.8391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4429 - acc: 0.8006 - val_loss: 0.4021 - val_acc: 0.8325\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4483 - acc: 0.8006 - val_loss: 0.3946 - val_acc: 0.8407\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4384 - acc: 0.8119 - val_loss: 0.3911 - val_acc: 0.8456\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4363 - acc: 0.8044 - val_loss: 0.3900 - val_acc: 0.8456\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4321 - acc: 0.8113 - val_loss: 0.3943 - val_acc: 0.8391\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4388 - acc: 0.8062 - val_loss: 0.3959 - val_acc: 0.8358\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4324 - acc: 0.8099 - val_loss: 0.3889 - val_acc: 0.8424\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4310 - acc: 0.8137 - val_loss: 0.3864 - val_acc: 0.8473\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4256 - acc: 0.8117 - val_loss: 0.3856 - val_acc: 0.8456\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4302 - acc: 0.8152 - val_loss: 0.3846 - val_acc: 0.8473\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4254 - acc: 0.8144 - val_loss: 0.3842 - val_acc: 0.8489\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4273 - acc: 0.8113 - val_loss: 0.3858 - val_acc: 0.8440\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4264 - acc: 0.8117 - val_loss: 0.3842 - val_acc: 0.8489\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4228 - acc: 0.8196 - val_loss: 0.3849 - val_acc: 0.8440\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4214 - acc: 0.8143 - val_loss: 0.3822 - val_acc: 0.8473\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4199 - acc: 0.8110 - val_loss: 0.3822 - val_acc: 0.8489\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4195 - acc: 0.8194 - val_loss: 0.3823 - val_acc: 0.8424\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4188 - acc: 0.8196 - val_loss: 0.3824 - val_acc: 0.8489\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4161 - acc: 0.8163 - val_loss: 0.3832 - val_acc: 0.8407\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4180 - acc: 0.8168 - val_loss: 0.3809 - val_acc: 0.8489\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4137 - acc: 0.8208 - val_loss: 0.3803 - val_acc: 0.8456\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4174 - acc: 0.8181 - val_loss: 0.3830 - val_acc: 0.8440\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4153 - acc: 0.8183 - val_loss: 0.3819 - val_acc: 0.8440\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4102 - acc: 0.8194 - val_loss: 0.3787 - val_acc: 0.8473\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4098 - acc: 0.8223 - val_loss: 0.3806 - val_acc: 0.8424\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4067 - acc: 0.8236 - val_loss: 0.3796 - val_acc: 0.8489\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4105 - acc: 0.8228 - val_loss: 0.3787 - val_acc: 0.8407\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4069 - acc: 0.8239 - val_loss: 0.3778 - val_acc: 0.8522\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4046 - acc: 0.8228 - val_loss: 0.3779 - val_acc: 0.8473\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3994 - acc: 0.8269 - val_loss: 0.3774 - val_acc: 0.8489\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4029 - acc: 0.8221 - val_loss: 0.3781 - val_acc: 0.8473\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4036 - acc: 0.8263 - val_loss: 0.3779 - val_acc: 0.8456\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3995 - acc: 0.8289 - val_loss: 0.3766 - val_acc: 0.8473\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3958 - acc: 0.8245 - val_loss: 0.3782 - val_acc: 0.8456\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3966 - acc: 0.8285 - val_loss: 0.3877 - val_acc: 0.8325\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4063 - acc: 0.8212 - val_loss: 0.3780 - val_acc: 0.8440\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3996 - acc: 0.8254 - val_loss: 0.3757 - val_acc: 0.8489\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4024 - acc: 0.8272 - val_loss: 0.3817 - val_acc: 0.8407\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3986 - acc: 0.8281 - val_loss: 0.3758 - val_acc: 0.8506\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3932 - acc: 0.8305 - val_loss: 0.3759 - val_acc: 0.8489\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3928 - acc: 0.8290 - val_loss: 0.3825 - val_acc: 0.8391\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3973 - acc: 0.8254 - val_loss: 0.3763 - val_acc: 0.8456\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.1, 'lay_conf': [1024, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_113\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_114 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_339 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_226 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_340 (Dense)            (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_227 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_341 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.8365 - acc: 0.5054 - val_loss: 0.6922 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6967 - acc: 0.5483 - val_loss: 0.6679 - val_acc: 0.6108\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.6586 - acc: 0.5977 - val_loss: 0.6218 - val_acc: 0.5878\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.6336 - acc: 0.6300 - val_loss: 0.5910 - val_acc: 0.6897\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.6110 - acc: 0.6712 - val_loss: 0.5600 - val_acc: 0.7553\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5886 - acc: 0.7004 - val_loss: 0.5427 - val_acc: 0.7553\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5711 - acc: 0.7115 - val_loss: 0.5235 - val_acc: 0.7833\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5603 - acc: 0.7345 - val_loss: 0.5123 - val_acc: 0.7734\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5488 - acc: 0.7254 - val_loss: 0.4990 - val_acc: 0.7931\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5376 - acc: 0.7444 - val_loss: 0.4830 - val_acc: 0.7964\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5225 - acc: 0.7508 - val_loss: 0.4641 - val_acc: 0.8194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5142 - acc: 0.7577 - val_loss: 0.4557 - val_acc: 0.8144\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5006 - acc: 0.7677 - val_loss: 0.4467 - val_acc: 0.8276\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4946 - acc: 0.7725 - val_loss: 0.4467 - val_acc: 0.8112\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4864 - acc: 0.7734 - val_loss: 0.4305 - val_acc: 0.8342\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4795 - acc: 0.7820 - val_loss: 0.4311 - val_acc: 0.8243\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4747 - acc: 0.7787 - val_loss: 0.4184 - val_acc: 0.8309\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4652 - acc: 0.7916 - val_loss: 0.4238 - val_acc: 0.8161\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4625 - acc: 0.7876 - val_loss: 0.4082 - val_acc: 0.8358\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4537 - acc: 0.7929 - val_loss: 0.4091 - val_acc: 0.8325\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4538 - acc: 0.7955 - val_loss: 0.4011 - val_acc: 0.8424\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4471 - acc: 0.8002 - val_loss: 0.4046 - val_acc: 0.8292\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4439 - acc: 0.8022 - val_loss: 0.3962 - val_acc: 0.8456\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4384 - acc: 0.8073 - val_loss: 0.3977 - val_acc: 0.8407\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4386 - acc: 0.8041 - val_loss: 0.4003 - val_acc: 0.8407\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4357 - acc: 0.8046 - val_loss: 0.3914 - val_acc: 0.8456\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4283 - acc: 0.8088 - val_loss: 0.3886 - val_acc: 0.8424\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4265 - acc: 0.8079 - val_loss: 0.3878 - val_acc: 0.8456\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4268 - acc: 0.8088 - val_loss: 0.3900 - val_acc: 0.8440\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4279 - acc: 0.8097 - val_loss: 0.3858 - val_acc: 0.8424\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4163 - acc: 0.8155 - val_loss: 0.3857 - val_acc: 0.8391\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4163 - acc: 0.8150 - val_loss: 0.3884 - val_acc: 0.8407\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4185 - acc: 0.8139 - val_loss: 0.3890 - val_acc: 0.8440\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4113 - acc: 0.8188 - val_loss: 0.3817 - val_acc: 0.8456\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4093 - acc: 0.8188 - val_loss: 0.3818 - val_acc: 0.8440\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4086 - acc: 0.8181 - val_loss: 0.3780 - val_acc: 0.8440\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4022 - acc: 0.8263 - val_loss: 0.3863 - val_acc: 0.8522\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4041 - acc: 0.8243 - val_loss: 0.3810 - val_acc: 0.8571\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4062 - acc: 0.8221 - val_loss: 0.3787 - val_acc: 0.8539\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3990 - acc: 0.8238 - val_loss: 0.3780 - val_acc: 0.8506\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3953 - acc: 0.8283 - val_loss: 0.3778 - val_acc: 0.8489\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3933 - acc: 0.8281 - val_loss: 0.3784 - val_acc: 0.8473\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3891 - acc: 0.8294 - val_loss: 0.3765 - val_acc: 0.8456\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3866 - acc: 0.8303 - val_loss: 0.3749 - val_acc: 0.8456\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3865 - acc: 0.8318 - val_loss: 0.3755 - val_acc: 0.8489\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3832 - acc: 0.8340 - val_loss: 0.3771 - val_acc: 0.8506\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3826 - acc: 0.8332 - val_loss: 0.3744 - val_acc: 0.8473\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3776 - acc: 0.8382 - val_loss: 0.3762 - val_acc: 0.8522\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3778 - acc: 0.8374 - val_loss: 0.3726 - val_acc: 0.8489\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3748 - acc: 0.8349 - val_loss: 0.3732 - val_acc: 0.8506\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3717 - acc: 0.8409 - val_loss: 0.3914 - val_acc: 0.8374\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3817 - acc: 0.8351 - val_loss: 0.3745 - val_acc: 0.8489\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3691 - acc: 0.8435 - val_loss: 0.3804 - val_acc: 0.8407\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3716 - acc: 0.8389 - val_loss: 0.3856 - val_acc: 0.8374\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_114\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_115 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_342 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_228 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_343 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_229 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_344 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,985\n",
      "Trainable params: 409,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 1.3586 - acc: 0.4328 - val_loss: 0.7137 - val_acc: 0.4417\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.7396 - acc: 0.5357 - val_loss: 0.7722 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.7589 - acc: 0.5674 - val_loss: 0.6936 - val_acc: 0.5747\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6936 - acc: 0.5694 - val_loss: 0.6568 - val_acc: 0.6700\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6806 - acc: 0.5645 - val_loss: 0.6540 - val_acc: 0.6831\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6659 - acc: 0.5957 - val_loss: 0.6319 - val_acc: 0.6749\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6537 - acc: 0.6161 - val_loss: 0.6223 - val_acc: 0.6158\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6457 - acc: 0.6263 - val_loss: 0.6071 - val_acc: 0.7340\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6335 - acc: 0.6552 - val_loss: 0.5974 - val_acc: 0.7373\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6227 - acc: 0.6647 - val_loss: 0.5842 - val_acc: 0.7553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6165 - acc: 0.6701 - val_loss: 0.5734 - val_acc: 0.7570\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6039 - acc: 0.6847 - val_loss: 0.5631 - val_acc: 0.7701\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5985 - acc: 0.6971 - val_loss: 0.5525 - val_acc: 0.7750\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5901 - acc: 0.7035 - val_loss: 0.5428 - val_acc: 0.7783\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5855 - acc: 0.7095 - val_loss: 0.5345 - val_acc: 0.7882\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5741 - acc: 0.7170 - val_loss: 0.5257 - val_acc: 0.7915\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5718 - acc: 0.7219 - val_loss: 0.5165 - val_acc: 0.7997\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5613 - acc: 0.7313 - val_loss: 0.5077 - val_acc: 0.8046\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5538 - acc: 0.7354 - val_loss: 0.5001 - val_acc: 0.8062\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5454 - acc: 0.7446 - val_loss: 0.4929 - val_acc: 0.8079\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5366 - acc: 0.7455 - val_loss: 0.4854 - val_acc: 0.8095\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5292 - acc: 0.7541 - val_loss: 0.4781 - val_acc: 0.8112\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5251 - acc: 0.7542 - val_loss: 0.4714 - val_acc: 0.8128\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5198 - acc: 0.7592 - val_loss: 0.4659 - val_acc: 0.8161\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5100 - acc: 0.7670 - val_loss: 0.4603 - val_acc: 0.8177\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5051 - acc: 0.7676 - val_loss: 0.4566 - val_acc: 0.8128\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5077 - acc: 0.7683 - val_loss: 0.4518 - val_acc: 0.8210\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4992 - acc: 0.7727 - val_loss: 0.4487 - val_acc: 0.8161\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4949 - acc: 0.7760 - val_loss: 0.4451 - val_acc: 0.8243\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4903 - acc: 0.7707 - val_loss: 0.4405 - val_acc: 0.8194\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4826 - acc: 0.7811 - val_loss: 0.4369 - val_acc: 0.8243\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4869 - acc: 0.7800 - val_loss: 0.4336 - val_acc: 0.8292\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4811 - acc: 0.7812 - val_loss: 0.4347 - val_acc: 0.8210\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4800 - acc: 0.7811 - val_loss: 0.4396 - val_acc: 0.8177\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4817 - acc: 0.7750 - val_loss: 0.4358 - val_acc: 0.8194\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4797 - acc: 0.7847 - val_loss: 0.4356 - val_acc: 0.8161\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4773 - acc: 0.7789 - val_loss: 0.4309 - val_acc: 0.8210\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4752 - acc: 0.7880 - val_loss: 0.4266 - val_acc: 0.8243\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4708 - acc: 0.7834 - val_loss: 0.4218 - val_acc: 0.8276\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4632 - acc: 0.7947 - val_loss: 0.4199 - val_acc: 0.8391\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4635 - acc: 0.7876 - val_loss: 0.4172 - val_acc: 0.8309\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4610 - acc: 0.7951 - val_loss: 0.4153 - val_acc: 0.8424\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4544 - acc: 0.7957 - val_loss: 0.4135 - val_acc: 0.8407\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4564 - acc: 0.8006 - val_loss: 0.4120 - val_acc: 0.8440\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4534 - acc: 0.7969 - val_loss: 0.4118 - val_acc: 0.8391\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4512 - acc: 0.8072 - val_loss: 0.4100 - val_acc: 0.8424\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4520 - acc: 0.8013 - val_loss: 0.4086 - val_acc: 0.8456\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4467 - acc: 0.8033 - val_loss: 0.4083 - val_acc: 0.8440\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4456 - acc: 0.8059 - val_loss: 0.4077 - val_acc: 0.8456\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4487 - acc: 0.7962 - val_loss: 0.4070 - val_acc: 0.8440\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4421 - acc: 0.8113 - val_loss: 0.4045 - val_acc: 0.8473\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4417 - acc: 0.8072 - val_loss: 0.4034 - val_acc: 0.8456\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4386 - acc: 0.8079 - val_loss: 0.4028 - val_acc: 0.8473\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4391 - acc: 0.8041 - val_loss: 0.4025 - val_acc: 0.8440\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4381 - acc: 0.8090 - val_loss: 0.4011 - val_acc: 0.8473\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4351 - acc: 0.8112 - val_loss: 0.4004 - val_acc: 0.8440\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4328 - acc: 0.8112 - val_loss: 0.3996 - val_acc: 0.8424\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4338 - acc: 0.8123 - val_loss: 0.3992 - val_acc: 0.8440\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4342 - acc: 0.8070 - val_loss: 0.3996 - val_acc: 0.8424\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4307 - acc: 0.8084 - val_loss: 0.3980 - val_acc: 0.8440\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4280 - acc: 0.8154 - val_loss: 0.3966 - val_acc: 0.8489\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4272 - acc: 0.8139 - val_loss: 0.3958 - val_acc: 0.8440\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4284 - acc: 0.8139 - val_loss: 0.3960 - val_acc: 0.8473\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4294 - acc: 0.8152 - val_loss: 0.3954 - val_acc: 0.8424\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4258 - acc: 0.8152 - val_loss: 0.3949 - val_acc: 0.8407\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4245 - acc: 0.8170 - val_loss: 0.3941 - val_acc: 0.8456\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4246 - acc: 0.8137 - val_loss: 0.3926 - val_acc: 0.8407\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4209 - acc: 0.8179 - val_loss: 0.3921 - val_acc: 0.8407\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4210 - acc: 0.8144 - val_loss: 0.3928 - val_acc: 0.8440\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4213 - acc: 0.8174 - val_loss: 0.3918 - val_acc: 0.8440\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4178 - acc: 0.8174 - val_loss: 0.3926 - val_acc: 0.8424\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4187 - acc: 0.8196 - val_loss: 0.3979 - val_acc: 0.8407\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4219 - acc: 0.8154 - val_loss: 0.3921 - val_acc: 0.8424\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4138 - acc: 0.8197 - val_loss: 0.3912 - val_acc: 0.8424\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4127 - acc: 0.8181 - val_loss: 0.3913 - val_acc: 0.8424\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4137 - acc: 0.8219 - val_loss: 0.3916 - val_acc: 0.8440\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4109 - acc: 0.8219 - val_loss: 0.3936 - val_acc: 0.8424\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4117 - acc: 0.8197 - val_loss: 0.3888 - val_acc: 0.8407\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4106 - acc: 0.8203 - val_loss: 0.3885 - val_acc: 0.8440\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4128 - acc: 0.8216 - val_loss: 0.3892 - val_acc: 0.8440\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4095 - acc: 0.8203 - val_loss: 0.3887 - val_acc: 0.8407\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4078 - acc: 0.8212 - val_loss: 0.3874 - val_acc: 0.8456\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4064 - acc: 0.8227 - val_loss: 0.3871 - val_acc: 0.8456\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4035 - acc: 0.8232 - val_loss: 0.3875 - val_acc: 0.8407\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4062 - acc: 0.8256 - val_loss: 0.3868 - val_acc: 0.8456\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4053 - acc: 0.8234 - val_loss: 0.3874 - val_acc: 0.8424\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4012 - acc: 0.8296 - val_loss: 0.3859 - val_acc: 0.8424\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4029 - acc: 0.8267 - val_loss: 0.3900 - val_acc: 0.8407\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4014 - acc: 0.8236 - val_loss: 0.3903 - val_acc: 0.8424\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4019 - acc: 0.8272 - val_loss: 0.3873 - val_acc: 0.8374\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4027 - acc: 0.8243 - val_loss: 0.3868 - val_acc: 0.8456\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4002 - acc: 0.8290 - val_loss: 0.3846 - val_acc: 0.8424\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3965 - acc: 0.8265 - val_loss: 0.3845 - val_acc: 0.8424\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3975 - acc: 0.8267 - val_loss: 0.3874 - val_acc: 0.8391\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3917 - acc: 0.8327 - val_loss: 0.3870 - val_acc: 0.8407\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3994 - acc: 0.8261 - val_loss: 0.3843 - val_acc: 0.8407\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3955 - acc: 0.8270 - val_loss: 0.3839 - val_acc: 0.8456\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3934 - acc: 0.8311 - val_loss: 0.3839 - val_acc: 0.8407\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3937 - acc: 0.8285 - val_loss: 0.3843 - val_acc: 0.8440\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3909 - acc: 0.8338 - val_loss: 0.3831 - val_acc: 0.8440\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3885 - acc: 0.8323 - val_loss: 0.3833 - val_acc: 0.8407\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3877 - acc: 0.8343 - val_loss: 0.3856 - val_acc: 0.8374\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3873 - acc: 0.8311 - val_loss: 0.3855 - val_acc: 0.8407\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3820 - acc: 0.8394 - val_loss: 0.3829 - val_acc: 0.8424\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3860 - acc: 0.8360 - val_loss: 0.3814 - val_acc: 0.8440\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3811 - acc: 0.8360 - val_loss: 0.3818 - val_acc: 0.8424\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3824 - acc: 0.8354 - val_loss: 0.3825 - val_acc: 0.8407\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3843 - acc: 0.8327 - val_loss: 0.3822 - val_acc: 0.8424\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3808 - acc: 0.8354 - val_loss: 0.3820 - val_acc: 0.8440\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3829 - acc: 0.8358 - val_loss: 0.3830 - val_acc: 0.8407\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.1, 'lay_conf': [1024, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_115\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_116 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_345 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_230 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_346 (Dense)            (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_231 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_347 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,639,553\n",
      "Trainable params: 1,639,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.7626 - acc: 0.5380 - val_loss: 0.6526 - val_acc: 0.6404\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6784 - acc: 0.5714 - val_loss: 0.6167 - val_acc: 0.7110\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.6338 - acc: 0.6276 - val_loss: 0.5824 - val_acc: 0.7521\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.6125 - acc: 0.6729 - val_loss: 0.5565 - val_acc: 0.7422\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5838 - acc: 0.7123 - val_loss: 0.5309 - val_acc: 0.7947\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.5678 - acc: 0.7139 - val_loss: 0.5109 - val_acc: 0.7947\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5471 - acc: 0.7418 - val_loss: 0.4893 - val_acc: 0.8030\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5342 - acc: 0.7484 - val_loss: 0.4710 - val_acc: 0.8259\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5183 - acc: 0.7583 - val_loss: 0.4584 - val_acc: 0.8292\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5091 - acc: 0.7699 - val_loss: 0.4448 - val_acc: 0.8342\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4952 - acc: 0.7718 - val_loss: 0.4316 - val_acc: 0.8358\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4879 - acc: 0.7736 - val_loss: 0.4228 - val_acc: 0.8374\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4793 - acc: 0.7800 - val_loss: 0.4178 - val_acc: 0.8374\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4728 - acc: 0.7862 - val_loss: 0.4317 - val_acc: 0.8161\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4754 - acc: 0.7847 - val_loss: 0.4091 - val_acc: 0.8374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4631 - acc: 0.7884 - val_loss: 0.4051 - val_acc: 0.8407\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4567 - acc: 0.7944 - val_loss: 0.4083 - val_acc: 0.8342\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4545 - acc: 0.7982 - val_loss: 0.3990 - val_acc: 0.8391\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4475 - acc: 0.7997 - val_loss: 0.3935 - val_acc: 0.8473\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4423 - acc: 0.8020 - val_loss: 0.3905 - val_acc: 0.8539\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4408 - acc: 0.8019 - val_loss: 0.3903 - val_acc: 0.8440\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4388 - acc: 0.8070 - val_loss: 0.3876 - val_acc: 0.8555\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4352 - acc: 0.8106 - val_loss: 0.3952 - val_acc: 0.8456\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4415 - acc: 0.8042 - val_loss: 0.3839 - val_acc: 0.8539\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4303 - acc: 0.8090 - val_loss: 0.3829 - val_acc: 0.8539\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4269 - acc: 0.8099 - val_loss: 0.3804 - val_acc: 0.8539\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4232 - acc: 0.8124 - val_loss: 0.3800 - val_acc: 0.8539\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4208 - acc: 0.8110 - val_loss: 0.3811 - val_acc: 0.8506\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4251 - acc: 0.8097 - val_loss: 0.3765 - val_acc: 0.8555\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4194 - acc: 0.8155 - val_loss: 0.3804 - val_acc: 0.8506\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4159 - acc: 0.8157 - val_loss: 0.3778 - val_acc: 0.8555\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4140 - acc: 0.8221 - val_loss: 0.3770 - val_acc: 0.8555\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4108 - acc: 0.8185 - val_loss: 0.3798 - val_acc: 0.8522\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4102 - acc: 0.8197 - val_loss: 0.3792 - val_acc: 0.8522\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.1, 'lay_conf': [1024, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_116\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_117 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_348 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_232 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_349 (Dense)            (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_233 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_350 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,606,721\n",
      "Trainable params: 1,606,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.7998 - acc: 0.5306 - val_loss: 0.6546 - val_acc: 0.6683\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6808 - acc: 0.5738 - val_loss: 0.6159 - val_acc: 0.6470\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.6441 - acc: 0.6247 - val_loss: 0.5857 - val_acc: 0.7274\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.6182 - acc: 0.6628 - val_loss: 0.5550 - val_acc: 0.7685\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5884 - acc: 0.7004 - val_loss: 0.5217 - val_acc: 0.7652\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5687 - acc: 0.7068 - val_loss: 0.5009 - val_acc: 0.8030\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5503 - acc: 0.7333 - val_loss: 0.4807 - val_acc: 0.8161\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5343 - acc: 0.7477 - val_loss: 0.4676 - val_acc: 0.8128\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5226 - acc: 0.7562 - val_loss: 0.4605 - val_acc: 0.8144\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5111 - acc: 0.7632 - val_loss: 0.4508 - val_acc: 0.8144\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5090 - acc: 0.7601 - val_loss: 0.4411 - val_acc: 0.8194\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5001 - acc: 0.7714 - val_loss: 0.4249 - val_acc: 0.8276\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4907 - acc: 0.7752 - val_loss: 0.4287 - val_acc: 0.8243\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4841 - acc: 0.7794 - val_loss: 0.4183 - val_acc: 0.8309\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4791 - acc: 0.7774 - val_loss: 0.4103 - val_acc: 0.8325\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4703 - acc: 0.7895 - val_loss: 0.4075 - val_acc: 0.8407\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4616 - acc: 0.7949 - val_loss: 0.4029 - val_acc: 0.8440\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4578 - acc: 0.7929 - val_loss: 0.4006 - val_acc: 0.8407\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4589 - acc: 0.7949 - val_loss: 0.3981 - val_acc: 0.8407\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4586 - acc: 0.7982 - val_loss: 0.4007 - val_acc: 0.8407\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4543 - acc: 0.7993 - val_loss: 0.3930 - val_acc: 0.8489\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4461 - acc: 0.8039 - val_loss: 0.3900 - val_acc: 0.8506\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4463 - acc: 0.7986 - val_loss: 0.3889 - val_acc: 0.8489\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4415 - acc: 0.8050 - val_loss: 0.3880 - val_acc: 0.8555\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4336 - acc: 0.8104 - val_loss: 0.3867 - val_acc: 0.8506\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4300 - acc: 0.8130 - val_loss: 0.3867 - val_acc: 0.8456\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4334 - acc: 0.8115 - val_loss: 0.3819 - val_acc: 0.8489\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4367 - acc: 0.8066 - val_loss: 0.3877 - val_acc: 0.8489\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4276 - acc: 0.8137 - val_loss: 0.3798 - val_acc: 0.8473\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4231 - acc: 0.8141 - val_loss: 0.3787 - val_acc: 0.8522\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4216 - acc: 0.8161 - val_loss: 0.3803 - val_acc: 0.8506\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4201 - acc: 0.8154 - val_loss: 0.3852 - val_acc: 0.8489\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4249 - acc: 0.8126 - val_loss: 0.3839 - val_acc: 0.8506\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4212 - acc: 0.8137 - val_loss: 0.3757 - val_acc: 0.8506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4180 - acc: 0.8216 - val_loss: 0.3762 - val_acc: 0.8522\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4108 - acc: 0.8241 - val_loss: 0.3746 - val_acc: 0.8555\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4108 - acc: 0.8217 - val_loss: 0.3768 - val_acc: 0.8506\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4084 - acc: 0.8227 - val_loss: 0.3733 - val_acc: 0.8555\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4061 - acc: 0.8232 - val_loss: 0.3733 - val_acc: 0.8522\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4053 - acc: 0.8243 - val_loss: 0.3787 - val_acc: 0.8571\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4069 - acc: 0.8245 - val_loss: 0.3731 - val_acc: 0.8588\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4021 - acc: 0.8205 - val_loss: 0.3700 - val_acc: 0.8571\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3996 - acc: 0.8259 - val_loss: 0.3704 - val_acc: 0.8506\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3961 - acc: 0.8261 - val_loss: 0.3710 - val_acc: 0.8489\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3965 - acc: 0.8287 - val_loss: 0.3695 - val_acc: 0.8522\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3912 - acc: 0.8300 - val_loss: 0.3701 - val_acc: 0.8539\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3936 - acc: 0.8272 - val_loss: 0.3692 - val_acc: 0.8489\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3908 - acc: 0.8300 - val_loss: 0.3682 - val_acc: 0.8522\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3862 - acc: 0.8371 - val_loss: 0.3735 - val_acc: 0.8473\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3919 - acc: 0.8323 - val_loss: 0.3685 - val_acc: 0.8456\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3858 - acc: 0.8354 - val_loss: 0.3680 - val_acc: 0.8489\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3821 - acc: 0.8338 - val_loss: 0.3664 - val_acc: 0.8571\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3796 - acc: 0.8369 - val_loss: 0.3700 - val_acc: 0.8555\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3802 - acc: 0.8362 - val_loss: 0.3663 - val_acc: 0.8539\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.3752 - acc: 0.8398 - val_loss: 0.3652 - val_acc: 0.8522\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3766 - acc: 0.8389 - val_loss: 0.3655 - val_acc: 0.8555\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3733 - acc: 0.8374 - val_loss: 0.3750 - val_acc: 0.8489\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3779 - acc: 0.8338 - val_loss: 0.3678 - val_acc: 0.8555\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3738 - acc: 0.8396 - val_loss: 0.3817 - val_acc: 0.8440\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3797 - acc: 0.8369 - val_loss: 0.3676 - val_acc: 0.8539\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_117\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_118 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_351 (Dense)            (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_234 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_352 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_235 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_353 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,050,113\n",
      "Trainable params: 1,050,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.7290 - acc: 0.5338 - val_loss: 0.6522 - val_acc: 0.6158\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6699 - acc: 0.5908 - val_loss: 0.6126 - val_acc: 0.7110\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6350 - acc: 0.6315 - val_loss: 0.5846 - val_acc: 0.7455\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.6075 - acc: 0.6794 - val_loss: 0.5616 - val_acc: 0.7356\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5851 - acc: 0.7052 - val_loss: 0.5371 - val_acc: 0.7849\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.5688 - acc: 0.7163 - val_loss: 0.5157 - val_acc: 0.7898\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5515 - acc: 0.7422 - val_loss: 0.5050 - val_acc: 0.7915\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.5370 - acc: 0.7440 - val_loss: 0.4839 - val_acc: 0.8079\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5193 - acc: 0.7639 - val_loss: 0.4692 - val_acc: 0.8144\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.5074 - acc: 0.7612 - val_loss: 0.4579 - val_acc: 0.8161\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.5010 - acc: 0.7699 - val_loss: 0.4547 - val_acc: 0.8128\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4924 - acc: 0.7727 - val_loss: 0.4368 - val_acc: 0.8325\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4783 - acc: 0.7820 - val_loss: 0.4296 - val_acc: 0.8292\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4723 - acc: 0.7922 - val_loss: 0.4250 - val_acc: 0.8276\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4660 - acc: 0.7920 - val_loss: 0.4192 - val_acc: 0.8374\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4577 - acc: 0.7964 - val_loss: 0.4129 - val_acc: 0.8358\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4524 - acc: 0.8028 - val_loss: 0.4076 - val_acc: 0.8358\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4498 - acc: 0.7984 - val_loss: 0.4030 - val_acc: 0.8374\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4430 - acc: 0.8051 - val_loss: 0.4003 - val_acc: 0.8440\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4429 - acc: 0.8022 - val_loss: 0.3982 - val_acc: 0.8473\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4344 - acc: 0.8104 - val_loss: 0.3983 - val_acc: 0.8374\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4319 - acc: 0.8104 - val_loss: 0.3961 - val_acc: 0.8407\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4291 - acc: 0.8086 - val_loss: 0.3926 - val_acc: 0.8424\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4263 - acc: 0.8123 - val_loss: 0.3907 - val_acc: 0.8456\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4191 - acc: 0.8166 - val_loss: 0.3881 - val_acc: 0.8604\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4143 - acc: 0.8139 - val_loss: 0.3870 - val_acc: 0.8473\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4130 - acc: 0.8190 - val_loss: 0.3853 - val_acc: 0.8555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4128 - acc: 0.8170 - val_loss: 0.3852 - val_acc: 0.8506\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4092 - acc: 0.8265 - val_loss: 0.3833 - val_acc: 0.8522\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4062 - acc: 0.8212 - val_loss: 0.3853 - val_acc: 0.8391\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4027 - acc: 0.8245 - val_loss: 0.3840 - val_acc: 0.8604\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4029 - acc: 0.8234 - val_loss: 0.3841 - val_acc: 0.8588\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3992 - acc: 0.8274 - val_loss: 0.3793 - val_acc: 0.8571\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4004 - acc: 0.8256 - val_loss: 0.3874 - val_acc: 0.8424\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3976 - acc: 0.8287 - val_loss: 0.3776 - val_acc: 0.8539\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3887 - acc: 0.8283 - val_loss: 0.3892 - val_acc: 0.8440\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3983 - acc: 0.8238 - val_loss: 0.3883 - val_acc: 0.8424\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3932 - acc: 0.8265 - val_loss: 0.3858 - val_acc: 0.8473\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3923 - acc: 0.8283 - val_loss: 0.3770 - val_acc: 0.8555\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3813 - acc: 0.8374 - val_loss: 0.3854 - val_acc: 0.8489\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3858 - acc: 0.8300 - val_loss: 0.3763 - val_acc: 0.8522\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3808 - acc: 0.8303 - val_loss: 0.3755 - val_acc: 0.8506\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3746 - acc: 0.8345 - val_loss: 0.3761 - val_acc: 0.8506\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3717 - acc: 0.8396 - val_loss: 0.3754 - val_acc: 0.8555\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3702 - acc: 0.8371 - val_loss: 0.3774 - val_acc: 0.8588\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3717 - acc: 0.8404 - val_loss: 0.3841 - val_acc: 0.8424\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3714 - acc: 0.8325 - val_loss: 0.3800 - val_acc: 0.8506\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3681 - acc: 0.8373 - val_loss: 0.3768 - val_acc: 0.8506\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3619 - acc: 0.8431 - val_loss: 0.3717 - val_acc: 0.8506\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3584 - acc: 0.8418 - val_loss: 0.3734 - val_acc: 0.8456\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3571 - acc: 0.8462 - val_loss: 0.3786 - val_acc: 0.8440\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3547 - acc: 0.8449 - val_loss: 0.3742 - val_acc: 0.8539\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3550 - acc: 0.8444 - val_loss: 0.3890 - val_acc: 0.8292\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3582 - acc: 0.8456 - val_loss: 0.3992 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_118\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_119 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_354 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_236 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_355 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_237 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_356 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 401,729\n",
      "Trainable params: 401,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.8716 - acc: 0.4501 - val_loss: 0.6980 - val_acc: 0.5731\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.7063 - acc: 0.5603 - val_loss: 0.6759 - val_acc: 0.5764\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6830 - acc: 0.5661 - val_loss: 0.6549 - val_acc: 0.6585\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6693 - acc: 0.5997 - val_loss: 0.6431 - val_acc: 0.6880\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6552 - acc: 0.6200 - val_loss: 0.6256 - val_acc: 0.6355\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6471 - acc: 0.6253 - val_loss: 0.6110 - val_acc: 0.6979\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6340 - acc: 0.6528 - val_loss: 0.6011 - val_acc: 0.7192\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6245 - acc: 0.6661 - val_loss: 0.5897 - val_acc: 0.7176\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6193 - acc: 0.6681 - val_loss: 0.5776 - val_acc: 0.7422\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6090 - acc: 0.6772 - val_loss: 0.5684 - val_acc: 0.7373\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6015 - acc: 0.6935 - val_loss: 0.5586 - val_acc: 0.7586\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5943 - acc: 0.6966 - val_loss: 0.5479 - val_acc: 0.7668\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5855 - acc: 0.6959 - val_loss: 0.5361 - val_acc: 0.7619\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5780 - acc: 0.7117 - val_loss: 0.5253 - val_acc: 0.7800\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5694 - acc: 0.7218 - val_loss: 0.5160 - val_acc: 0.7898\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5568 - acc: 0.7263 - val_loss: 0.5076 - val_acc: 0.7898\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5505 - acc: 0.7382 - val_loss: 0.5007 - val_acc: 0.8013\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5459 - acc: 0.7396 - val_loss: 0.4929 - val_acc: 0.7964\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5340 - acc: 0.7539 - val_loss: 0.4850 - val_acc: 0.8112\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5319 - acc: 0.7500 - val_loss: 0.4776 - val_acc: 0.8095\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5228 - acc: 0.7530 - val_loss: 0.4706 - val_acc: 0.8144\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5194 - acc: 0.7606 - val_loss: 0.4652 - val_acc: 0.8144\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5112 - acc: 0.7656 - val_loss: 0.4603 - val_acc: 0.8161\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5064 - acc: 0.7708 - val_loss: 0.4557 - val_acc: 0.8112\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4989 - acc: 0.7738 - val_loss: 0.4500 - val_acc: 0.8210\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4928 - acc: 0.7765 - val_loss: 0.4461 - val_acc: 0.8227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4888 - acc: 0.7769 - val_loss: 0.4404 - val_acc: 0.8259\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4921 - acc: 0.7754 - val_loss: 0.4346 - val_acc: 0.8227\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4842 - acc: 0.7807 - val_loss: 0.4312 - val_acc: 0.8276\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4774 - acc: 0.7885 - val_loss: 0.4330 - val_acc: 0.8210\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4786 - acc: 0.7836 - val_loss: 0.4291 - val_acc: 0.8210\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4759 - acc: 0.7865 - val_loss: 0.4217 - val_acc: 0.8309\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4697 - acc: 0.7909 - val_loss: 0.4190 - val_acc: 0.8309\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4734 - acc: 0.7895 - val_loss: 0.4207 - val_acc: 0.8309\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4637 - acc: 0.7971 - val_loss: 0.4155 - val_acc: 0.8292\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4603 - acc: 0.7977 - val_loss: 0.4112 - val_acc: 0.8342\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4553 - acc: 0.8000 - val_loss: 0.4089 - val_acc: 0.8325\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4563 - acc: 0.7944 - val_loss: 0.4102 - val_acc: 0.8309\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4526 - acc: 0.8000 - val_loss: 0.4124 - val_acc: 0.8210\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4560 - acc: 0.7986 - val_loss: 0.4086 - val_acc: 0.8325\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4494 - acc: 0.8061 - val_loss: 0.4037 - val_acc: 0.8374\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4464 - acc: 0.8073 - val_loss: 0.4062 - val_acc: 0.8276\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4441 - acc: 0.8002 - val_loss: 0.4138 - val_acc: 0.8243\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4448 - acc: 0.8051 - val_loss: 0.4088 - val_acc: 0.8227\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4430 - acc: 0.8059 - val_loss: 0.4068 - val_acc: 0.8358\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4427 - acc: 0.8009 - val_loss: 0.3994 - val_acc: 0.8358\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4410 - acc: 0.8079 - val_loss: 0.3972 - val_acc: 0.8424\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4377 - acc: 0.8103 - val_loss: 0.3969 - val_acc: 0.8391\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4327 - acc: 0.8115 - val_loss: 0.3951 - val_acc: 0.8391\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4312 - acc: 0.8190 - val_loss: 0.3949 - val_acc: 0.8391\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4340 - acc: 0.8086 - val_loss: 0.3947 - val_acc: 0.8407\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4324 - acc: 0.8139 - val_loss: 0.4013 - val_acc: 0.8292\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4340 - acc: 0.8075 - val_loss: 0.3954 - val_acc: 0.8473\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4236 - acc: 0.8161 - val_loss: 0.3919 - val_acc: 0.8374\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4260 - acc: 0.8135 - val_loss: 0.3905 - val_acc: 0.8407\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4227 - acc: 0.8141 - val_loss: 0.3907 - val_acc: 0.8391\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4216 - acc: 0.8183 - val_loss: 0.3905 - val_acc: 0.8358\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4241 - acc: 0.8188 - val_loss: 0.3900 - val_acc: 0.8407\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4202 - acc: 0.8188 - val_loss: 0.3895 - val_acc: 0.8374\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4222 - acc: 0.8159 - val_loss: 0.3902 - val_acc: 0.8407\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4150 - acc: 0.8148 - val_loss: 0.3876 - val_acc: 0.8407\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4165 - acc: 0.8243 - val_loss: 0.3879 - val_acc: 0.8391\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4170 - acc: 0.8183 - val_loss: 0.3909 - val_acc: 0.8391\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4163 - acc: 0.8199 - val_loss: 0.3895 - val_acc: 0.8424\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4126 - acc: 0.8181 - val_loss: 0.3854 - val_acc: 0.8407\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4132 - acc: 0.8219 - val_loss: 0.3846 - val_acc: 0.8407\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4098 - acc: 0.8261 - val_loss: 0.3855 - val_acc: 0.8407\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4112 - acc: 0.8239 - val_loss: 0.3856 - val_acc: 0.8374\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4078 - acc: 0.8265 - val_loss: 0.3881 - val_acc: 0.8440\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4065 - acc: 0.8272 - val_loss: 0.3849 - val_acc: 0.8391\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4017 - acc: 0.8241 - val_loss: 0.3830 - val_acc: 0.8391\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4064 - acc: 0.8241 - val_loss: 0.3838 - val_acc: 0.8424\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4032 - acc: 0.8280 - val_loss: 0.3812 - val_acc: 0.8407\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4047 - acc: 0.8258 - val_loss: 0.3821 - val_acc: 0.8424\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4028 - acc: 0.8285 - val_loss: 0.3823 - val_acc: 0.8407\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4003 - acc: 0.8258 - val_loss: 0.3950 - val_acc: 0.8456\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4078 - acc: 0.8207 - val_loss: 0.3820 - val_acc: 0.8407\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3980 - acc: 0.8287 - val_loss: 0.3813 - val_acc: 0.8424\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.1, 'lay_conf': [1024, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_119\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_120 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_357 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_238 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_358 (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_239 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_359 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,836,545\n",
      "Trainable params: 1,836,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 1s 57ms/step - loss: 0.8373 - acc: 0.5202 - val_loss: 0.7043 - val_acc: 0.5747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6830 - acc: 0.5880 - val_loss: 0.6621 - val_acc: 0.6273\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.6483 - acc: 0.6344 - val_loss: 0.6109 - val_acc: 0.5977\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.6346 - acc: 0.6322 - val_loss: 0.5740 - val_acc: 0.7373\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.6048 - acc: 0.6789 - val_loss: 0.5581 - val_acc: 0.7356\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5806 - acc: 0.7095 - val_loss: 0.5369 - val_acc: 0.7816\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5688 - acc: 0.7174 - val_loss: 0.5219 - val_acc: 0.7685\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5496 - acc: 0.7314 - val_loss: 0.4967 - val_acc: 0.8079\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5339 - acc: 0.7493 - val_loss: 0.4818 - val_acc: 0.8030\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5201 - acc: 0.7583 - val_loss: 0.4646 - val_acc: 0.8194\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5099 - acc: 0.7628 - val_loss: 0.4589 - val_acc: 0.8144\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5008 - acc: 0.7666 - val_loss: 0.4432 - val_acc: 0.8227\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4917 - acc: 0.7783 - val_loss: 0.4502 - val_acc: 0.8095\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4834 - acc: 0.7780 - val_loss: 0.4298 - val_acc: 0.8309\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4759 - acc: 0.7812 - val_loss: 0.4215 - val_acc: 0.8210\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4702 - acc: 0.7869 - val_loss: 0.4200 - val_acc: 0.8342\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4636 - acc: 0.7907 - val_loss: 0.4133 - val_acc: 0.8325\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4566 - acc: 0.7933 - val_loss: 0.4131 - val_acc: 0.8309\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4536 - acc: 0.8033 - val_loss: 0.4036 - val_acc: 0.8358\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4530 - acc: 0.7978 - val_loss: 0.4048 - val_acc: 0.8374\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4409 - acc: 0.8030 - val_loss: 0.3983 - val_acc: 0.8391\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4447 - acc: 0.8028 - val_loss: 0.4151 - val_acc: 0.8358\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4487 - acc: 0.7999 - val_loss: 0.4116 - val_acc: 0.8342\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4463 - acc: 0.8024 - val_loss: 0.4048 - val_acc: 0.8358\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4398 - acc: 0.8035 - val_loss: 0.3988 - val_acc: 0.8391\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4309 - acc: 0.8066 - val_loss: 0.3887 - val_acc: 0.8424\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4274 - acc: 0.8086 - val_loss: 0.3968 - val_acc: 0.8407\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4306 - acc: 0.8097 - val_loss: 0.3859 - val_acc: 0.8473\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4308 - acc: 0.8106 - val_loss: 0.3979 - val_acc: 0.8391\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4241 - acc: 0.8101 - val_loss: 0.3850 - val_acc: 0.8456\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4193 - acc: 0.8172 - val_loss: 0.4001 - val_acc: 0.8358\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4178 - acc: 0.8152 - val_loss: 0.3892 - val_acc: 0.8456\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4174 - acc: 0.8148 - val_loss: 0.3828 - val_acc: 0.8506\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4072 - acc: 0.8208 - val_loss: 0.3805 - val_acc: 0.8506\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4053 - acc: 0.8236 - val_loss: 0.3834 - val_acc: 0.8456\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4089 - acc: 0.8252 - val_loss: 0.3803 - val_acc: 0.8522\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4028 - acc: 0.8197 - val_loss: 0.3782 - val_acc: 0.8473\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4019 - acc: 0.8258 - val_loss: 0.3787 - val_acc: 0.8456\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3977 - acc: 0.8267 - val_loss: 0.3874 - val_acc: 0.8407\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4033 - acc: 0.8252 - val_loss: 0.3914 - val_acc: 0.8391\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4033 - acc: 0.8258 - val_loss: 0.3770 - val_acc: 0.8506\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3998 - acc: 0.8219 - val_loss: 0.3876 - val_acc: 0.8424\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3950 - acc: 0.8265 - val_loss: 0.3896 - val_acc: 0.8391\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4018 - acc: 0.8212 - val_loss: 0.3754 - val_acc: 0.8506\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3926 - acc: 0.8276 - val_loss: 0.3810 - val_acc: 0.8522\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3868 - acc: 0.8309 - val_loss: 0.3801 - val_acc: 0.8522\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3856 - acc: 0.8314 - val_loss: 0.3775 - val_acc: 0.8473\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3804 - acc: 0.8329 - val_loss: 0.3735 - val_acc: 0.8539\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3776 - acc: 0.8342 - val_loss: 0.3755 - val_acc: 0.8440\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3753 - acc: 0.8342 - val_loss: 0.3729 - val_acc: 0.8489\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3739 - acc: 0.8400 - val_loss: 0.3724 - val_acc: 0.8506\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3701 - acc: 0.8393 - val_loss: 0.3727 - val_acc: 0.8555\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3678 - acc: 0.8407 - val_loss: 0.3730 - val_acc: 0.8539\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3657 - acc: 0.8396 - val_loss: 0.3714 - val_acc: 0.8539\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3612 - acc: 0.8424 - val_loss: 0.3726 - val_acc: 0.8489\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3615 - acc: 0.8425 - val_loss: 0.3713 - val_acc: 0.8571\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3561 - acc: 0.8484 - val_loss: 0.3804 - val_acc: 0.8374\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3561 - acc: 0.8433 - val_loss: 0.3947 - val_acc: 0.8259\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3569 - acc: 0.8433 - val_loss: 0.3760 - val_acc: 0.8555\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3539 - acc: 0.8480 - val_loss: 0.3728 - val_acc: 0.8588\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3498 - acc: 0.8447 - val_loss: 0.3744 - val_acc: 0.8555\n",
      "Training with parameters {'batch_size': 1500, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_120\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_121 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_360 (Dense)            (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_240 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_361 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_241 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_362 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.7827 - acc: 0.4483 - val_loss: 0.6811 - val_acc: 0.5649\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.6946 - acc: 0.5483 - val_loss: 0.6787 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6848 - acc: 0.5694 - val_loss: 0.6591 - val_acc: 0.5747\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6713 - acc: 0.5857 - val_loss: 0.6483 - val_acc: 0.6519\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6643 - acc: 0.6015 - val_loss: 0.6400 - val_acc: 0.6929\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.6524 - acc: 0.6313 - val_loss: 0.6251 - val_acc: 0.7011\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.6450 - acc: 0.6344 - val_loss: 0.6126 - val_acc: 0.6913\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.6335 - acc: 0.6468 - val_loss: 0.5974 - val_acc: 0.7209\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.6230 - acc: 0.6672 - val_loss: 0.5848 - val_acc: 0.7225\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.6161 - acc: 0.6701 - val_loss: 0.5720 - val_acc: 0.7537\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.6076 - acc: 0.6849 - val_loss: 0.5604 - val_acc: 0.7521\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.5958 - acc: 0.6944 - val_loss: 0.5500 - val_acc: 0.7603\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.5885 - acc: 0.7057 - val_loss: 0.5400 - val_acc: 0.7865\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.5811 - acc: 0.7117 - val_loss: 0.5306 - val_acc: 0.7734\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5740 - acc: 0.7230 - val_loss: 0.5208 - val_acc: 0.7964\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5629 - acc: 0.7338 - val_loss: 0.5120 - val_acc: 0.8030\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5578 - acc: 0.7345 - val_loss: 0.5034 - val_acc: 0.7997\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5455 - acc: 0.7457 - val_loss: 0.4946 - val_acc: 0.7964\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5415 - acc: 0.7406 - val_loss: 0.4866 - val_acc: 0.7964\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5353 - acc: 0.7559 - val_loss: 0.4792 - val_acc: 0.8046\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5273 - acc: 0.7559 - val_loss: 0.4721 - val_acc: 0.8046\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5186 - acc: 0.7595 - val_loss: 0.4661 - val_acc: 0.8030\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5153 - acc: 0.7665 - val_loss: 0.4606 - val_acc: 0.8128\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5122 - acc: 0.7643 - val_loss: 0.4554 - val_acc: 0.8161\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5054 - acc: 0.7716 - val_loss: 0.4507 - val_acc: 0.8128\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5033 - acc: 0.7723 - val_loss: 0.4457 - val_acc: 0.8210\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4997 - acc: 0.7738 - val_loss: 0.4408 - val_acc: 0.8194\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4964 - acc: 0.7781 - val_loss: 0.4371 - val_acc: 0.8177\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4920 - acc: 0.7756 - val_loss: 0.4335 - val_acc: 0.8243\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4893 - acc: 0.7769 - val_loss: 0.4301 - val_acc: 0.8243\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4869 - acc: 0.7796 - val_loss: 0.4270 - val_acc: 0.8259\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4813 - acc: 0.7867 - val_loss: 0.4249 - val_acc: 0.8259\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4748 - acc: 0.7871 - val_loss: 0.4218 - val_acc: 0.8309\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4742 - acc: 0.7905 - val_loss: 0.4192 - val_acc: 0.8309\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4730 - acc: 0.7880 - val_loss: 0.4170 - val_acc: 0.8309\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4668 - acc: 0.7905 - val_loss: 0.4150 - val_acc: 0.8342\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4601 - acc: 0.8004 - val_loss: 0.4126 - val_acc: 0.8342\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4645 - acc: 0.7947 - val_loss: 0.4104 - val_acc: 0.8325\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.4589 - acc: 0.7973 - val_loss: 0.4084 - val_acc: 0.8391\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4567 - acc: 0.7971 - val_loss: 0.4064 - val_acc: 0.8374\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4566 - acc: 0.8020 - val_loss: 0.4050 - val_acc: 0.8391\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4557 - acc: 0.7993 - val_loss: 0.4033 - val_acc: 0.8358\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4534 - acc: 0.8044 - val_loss: 0.4020 - val_acc: 0.8391\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4515 - acc: 0.8041 - val_loss: 0.4008 - val_acc: 0.8407\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4480 - acc: 0.8035 - val_loss: 0.3996 - val_acc: 0.8407\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.4501 - acc: 0.8009 - val_loss: 0.3986 - val_acc: 0.8391\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4460 - acc: 0.8022 - val_loss: 0.3980 - val_acc: 0.8424\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4434 - acc: 0.8084 - val_loss: 0.3975 - val_acc: 0.8407\n",
      "Epoch 49/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4432 - acc: 0.8042 - val_loss: 0.3956 - val_acc: 0.8424\n",
      "Epoch 50/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4417 - acc: 0.8084 - val_loss: 0.3946 - val_acc: 0.8424\n",
      "Epoch 51/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4375 - acc: 0.8072 - val_loss: 0.3937 - val_acc: 0.8407\n",
      "Epoch 52/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4341 - acc: 0.8088 - val_loss: 0.3940 - val_acc: 0.8407\n",
      "Epoch 53/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4376 - acc: 0.8112 - val_loss: 0.3931 - val_acc: 0.8424\n",
      "Epoch 54/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4336 - acc: 0.8101 - val_loss: 0.3922 - val_acc: 0.8391\n",
      "Epoch 55/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4349 - acc: 0.8130 - val_loss: 0.3919 - val_acc: 0.8424\n",
      "Epoch 56/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4285 - acc: 0.8077 - val_loss: 0.3913 - val_acc: 0.8424\n",
      "Epoch 57/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4320 - acc: 0.8121 - val_loss: 0.3916 - val_acc: 0.8456\n",
      "Epoch 58/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4283 - acc: 0.8117 - val_loss: 0.3914 - val_acc: 0.8391\n",
      "Epoch 59/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4278 - acc: 0.8143 - val_loss: 0.3911 - val_acc: 0.8489\n",
      "Epoch 60/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4274 - acc: 0.8093 - val_loss: 0.3893 - val_acc: 0.8358\n",
      "Epoch 61/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4291 - acc: 0.8123 - val_loss: 0.3864 - val_acc: 0.8473\n",
      "Epoch 62/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4233 - acc: 0.8177 - val_loss: 0.3865 - val_acc: 0.8489\n",
      "Epoch 63/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4222 - acc: 0.8188 - val_loss: 0.3874 - val_acc: 0.8473\n",
      "Epoch 64/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4226 - acc: 0.8108 - val_loss: 0.3879 - val_acc: 0.8358\n",
      "Epoch 65/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4205 - acc: 0.8161 - val_loss: 0.3866 - val_acc: 0.8473\n",
      "Epoch 66/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4169 - acc: 0.8190 - val_loss: 0.3854 - val_acc: 0.8407\n",
      "Epoch 67/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4205 - acc: 0.8183 - val_loss: 0.3847 - val_acc: 0.8473\n",
      "Epoch 68/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4143 - acc: 0.8177 - val_loss: 0.3835 - val_acc: 0.8489\n",
      "Epoch 69/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4133 - acc: 0.8199 - val_loss: 0.3829 - val_acc: 0.8506\n",
      "Epoch 70/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4150 - acc: 0.8188 - val_loss: 0.3823 - val_acc: 0.8489\n",
      "Epoch 71/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4127 - acc: 0.8228 - val_loss: 0.3837 - val_acc: 0.8440\n",
      "Epoch 72/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4145 - acc: 0.8267 - val_loss: 0.3854 - val_acc: 0.8473\n",
      "Epoch 73/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4123 - acc: 0.8208 - val_loss: 0.3827 - val_acc: 0.8440\n",
      "Epoch 74/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4100 - acc: 0.8243 - val_loss: 0.3852 - val_acc: 0.8440\n",
      "Epoch 75/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4104 - acc: 0.8219 - val_loss: 0.3873 - val_acc: 0.8473\n",
      "Training with parameters {'batch_size': 1500, 'dropout': 0.1, 'lay_conf': [1024, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_121\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_122 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_363 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_242 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_364 (Dense)            (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_243 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_365 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.8466 - acc: 0.4970 - val_loss: 0.7574 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.7084 - acc: 0.5738 - val_loss: 0.6852 - val_acc: 0.5369\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.6858 - acc: 0.5692 - val_loss: 0.6230 - val_acc: 0.7011\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.6457 - acc: 0.6178 - val_loss: 0.6171 - val_acc: 0.5944\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.6290 - acc: 0.6450 - val_loss: 0.5906 - val_acc: 0.6929\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.6160 - acc: 0.6665 - val_loss: 0.5620 - val_acc: 0.7241\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5979 - acc: 0.6895 - val_loss: 0.5493 - val_acc: 0.7619\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.5815 - acc: 0.7106 - val_loss: 0.5378 - val_acc: 0.7504\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5717 - acc: 0.7068 - val_loss: 0.5162 - val_acc: 0.7882\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5566 - acc: 0.7367 - val_loss: 0.5040 - val_acc: 0.8013\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5448 - acc: 0.7380 - val_loss: 0.4899 - val_acc: 0.8095\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5340 - acc: 0.7515 - val_loss: 0.4787 - val_acc: 0.8128\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5199 - acc: 0.7564 - val_loss: 0.4666 - val_acc: 0.8128\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5120 - acc: 0.7635 - val_loss: 0.4592 - val_acc: 0.8194\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5075 - acc: 0.7608 - val_loss: 0.4474 - val_acc: 0.8243\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4920 - acc: 0.7719 - val_loss: 0.4391 - val_acc: 0.8259\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4871 - acc: 0.7761 - val_loss: 0.4326 - val_acc: 0.8276\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4791 - acc: 0.7809 - val_loss: 0.4259 - val_acc: 0.8276\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4729 - acc: 0.7880 - val_loss: 0.4249 - val_acc: 0.8276\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4666 - acc: 0.7871 - val_loss: 0.4184 - val_acc: 0.8259\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4623 - acc: 0.7920 - val_loss: 0.4127 - val_acc: 0.8325\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4595 - acc: 0.7922 - val_loss: 0.4160 - val_acc: 0.8243\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4545 - acc: 0.7926 - val_loss: 0.4063 - val_acc: 0.8342\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.4484 - acc: 0.7984 - val_loss: 0.4026 - val_acc: 0.8391\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.4471 - acc: 0.7988 - val_loss: 0.4003 - val_acc: 0.8342\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.4427 - acc: 0.8046 - val_loss: 0.3977 - val_acc: 0.8391\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.4388 - acc: 0.8037 - val_loss: 0.3953 - val_acc: 0.8407\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.4391 - acc: 0.8035 - val_loss: 0.3940 - val_acc: 0.8407\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.4318 - acc: 0.8119 - val_loss: 0.3927 - val_acc: 0.8407\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.4295 - acc: 0.8121 - val_loss: 0.3897 - val_acc: 0.8424\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.4246 - acc: 0.8103 - val_loss: 0.3907 - val_acc: 0.8407\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4219 - acc: 0.8121 - val_loss: 0.3860 - val_acc: 0.8424\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.4203 - acc: 0.8124 - val_loss: 0.3843 - val_acc: 0.8374\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.4188 - acc: 0.8112 - val_loss: 0.3844 - val_acc: 0.8407\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.4157 - acc: 0.8174 - val_loss: 0.3860 - val_acc: 0.8391\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4150 - acc: 0.8161 - val_loss: 0.3826 - val_acc: 0.8391\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.4119 - acc: 0.8207 - val_loss: 0.3830 - val_acc: 0.8391\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.4104 - acc: 0.8212 - val_loss: 0.3823 - val_acc: 0.8374\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.4040 - acc: 0.8179 - val_loss: 0.3793 - val_acc: 0.8391\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.4063 - acc: 0.8207 - val_loss: 0.3850 - val_acc: 0.8539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4057 - acc: 0.8214 - val_loss: 0.3779 - val_acc: 0.8424\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.4051 - acc: 0.8196 - val_loss: 0.3817 - val_acc: 0.8456\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.4040 - acc: 0.8223 - val_loss: 0.3984 - val_acc: 0.8358\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.4044 - acc: 0.8192 - val_loss: 0.3768 - val_acc: 0.8440\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3956 - acc: 0.8254 - val_loss: 0.3764 - val_acc: 0.8440\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3922 - acc: 0.8281 - val_loss: 0.3759 - val_acc: 0.8407\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3871 - acc: 0.8321 - val_loss: 0.3749 - val_acc: 0.8407\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3861 - acc: 0.8309 - val_loss: 0.3759 - val_acc: 0.8456\n",
      "Epoch 49/300\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.3885 - acc: 0.8305 - val_loss: 0.3772 - val_acc: 0.8440\n",
      "Epoch 50/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3857 - acc: 0.8325 - val_loss: 0.3740 - val_acc: 0.8407\n",
      "Epoch 51/300\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.3825 - acc: 0.8331 - val_loss: 0.3730 - val_acc: 0.8407\n",
      "Epoch 52/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3807 - acc: 0.8371 - val_loss: 0.3805 - val_acc: 0.8440\n",
      "Epoch 53/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3801 - acc: 0.8374 - val_loss: 0.3739 - val_acc: 0.8407\n",
      "Epoch 54/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3751 - acc: 0.8373 - val_loss: 0.3727 - val_acc: 0.8440\n",
      "Epoch 55/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3716 - acc: 0.8409 - val_loss: 0.3797 - val_acc: 0.8506\n",
      "Epoch 56/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3755 - acc: 0.8376 - val_loss: 0.3737 - val_acc: 0.8489\n",
      "Epoch 57/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3743 - acc: 0.8378 - val_loss: 0.3772 - val_acc: 0.8424\n",
      "Epoch 58/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3707 - acc: 0.8391 - val_loss: 0.3886 - val_acc: 0.8309\n",
      "Epoch 59/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3703 - acc: 0.8389 - val_loss: 0.3727 - val_acc: 0.8473\n",
      "Training with parameters {'batch_size': 1500, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_122\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_123 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_366 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_244 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_367 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_245 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_368 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,985\n",
      "Trainable params: 409,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 1.4429 - acc: 0.4335 - val_loss: 0.7786 - val_acc: 0.4269\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.7578 - acc: 0.4983 - val_loss: 0.7565 - val_acc: 0.5731\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.7815 - acc: 0.5616 - val_loss: 0.7535 - val_acc: 0.5747\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.7429 - acc: 0.5667 - val_loss: 0.6802 - val_acc: 0.5747\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6866 - acc: 0.5780 - val_loss: 0.6587 - val_acc: 0.6585\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.6799 - acc: 0.5718 - val_loss: 0.6574 - val_acc: 0.6732\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.6725 - acc: 0.5806 - val_loss: 0.6358 - val_acc: 0.7126\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.6589 - acc: 0.6014 - val_loss: 0.6255 - val_acc: 0.6076\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.6505 - acc: 0.6243 - val_loss: 0.6139 - val_acc: 0.6470\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.6388 - acc: 0.6440 - val_loss: 0.6019 - val_acc: 0.7373\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6245 - acc: 0.6689 - val_loss: 0.5913 - val_acc: 0.7438\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6212 - acc: 0.6709 - val_loss: 0.5792 - val_acc: 0.7553\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.6148 - acc: 0.6689 - val_loss: 0.5700 - val_acc: 0.7635\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6056 - acc: 0.6906 - val_loss: 0.5609 - val_acc: 0.7685\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5963 - acc: 0.7001 - val_loss: 0.5541 - val_acc: 0.7635\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5919 - acc: 0.7006 - val_loss: 0.5447 - val_acc: 0.7767\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5888 - acc: 0.7064 - val_loss: 0.5375 - val_acc: 0.7882\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5805 - acc: 0.7156 - val_loss: 0.5314 - val_acc: 0.7947\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.5737 - acc: 0.7165 - val_loss: 0.5240 - val_acc: 0.7997\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5638 - acc: 0.7287 - val_loss: 0.5179 - val_acc: 0.7997\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5639 - acc: 0.7289 - val_loss: 0.5120 - val_acc: 0.8030\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5585 - acc: 0.7336 - val_loss: 0.5058 - val_acc: 0.8079\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.5508 - acc: 0.7378 - val_loss: 0.5002 - val_acc: 0.8128\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5502 - acc: 0.7369 - val_loss: 0.4942 - val_acc: 0.8128\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.5395 - acc: 0.7460 - val_loss: 0.4887 - val_acc: 0.8128\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.5369 - acc: 0.7460 - val_loss: 0.4843 - val_acc: 0.8079\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.5325 - acc: 0.7552 - val_loss: 0.4787 - val_acc: 0.8128\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5224 - acc: 0.7612 - val_loss: 0.4729 - val_acc: 0.8161\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5226 - acc: 0.7586 - val_loss: 0.4677 - val_acc: 0.8210\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5176 - acc: 0.7572 - val_loss: 0.4639 - val_acc: 0.8144\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5144 - acc: 0.7612 - val_loss: 0.4587 - val_acc: 0.8243\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5059 - acc: 0.7712 - val_loss: 0.4552 - val_acc: 0.8161\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5062 - acc: 0.7730 - val_loss: 0.4509 - val_acc: 0.8276\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5012 - acc: 0.7727 - val_loss: 0.4480 - val_acc: 0.8177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4993 - acc: 0.7756 - val_loss: 0.4439 - val_acc: 0.8243\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4964 - acc: 0.7805 - val_loss: 0.4411 - val_acc: 0.8276\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4895 - acc: 0.7818 - val_loss: 0.4379 - val_acc: 0.8259\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4864 - acc: 0.7809 - val_loss: 0.4350 - val_acc: 0.8276\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4888 - acc: 0.7752 - val_loss: 0.4326 - val_acc: 0.8325\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4835 - acc: 0.7853 - val_loss: 0.4302 - val_acc: 0.8325\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4784 - acc: 0.7856 - val_loss: 0.4282 - val_acc: 0.8358\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4766 - acc: 0.7836 - val_loss: 0.4261 - val_acc: 0.8374\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4755 - acc: 0.7882 - val_loss: 0.4245 - val_acc: 0.8391\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4738 - acc: 0.7874 - val_loss: 0.4227 - val_acc: 0.8374\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4708 - acc: 0.7878 - val_loss: 0.4209 - val_acc: 0.8456\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4682 - acc: 0.7933 - val_loss: 0.4193 - val_acc: 0.8407\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4677 - acc: 0.7935 - val_loss: 0.4179 - val_acc: 0.8424\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4617 - acc: 0.7953 - val_loss: 0.4164 - val_acc: 0.8440\n",
      "Epoch 49/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4594 - acc: 0.7926 - val_loss: 0.4145 - val_acc: 0.8440\n",
      "Epoch 50/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4576 - acc: 0.7969 - val_loss: 0.4133 - val_acc: 0.8440\n",
      "Epoch 51/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4575 - acc: 0.7971 - val_loss: 0.4123 - val_acc: 0.8440\n",
      "Epoch 52/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4536 - acc: 0.7964 - val_loss: 0.4112 - val_acc: 0.8440\n",
      "Epoch 53/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4539 - acc: 0.7978 - val_loss: 0.4103 - val_acc: 0.8424\n",
      "Epoch 54/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4527 - acc: 0.8020 - val_loss: 0.4080 - val_acc: 0.8440\n",
      "Epoch 55/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4485 - acc: 0.7988 - val_loss: 0.4067 - val_acc: 0.8440\n",
      "Epoch 56/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4488 - acc: 0.7977 - val_loss: 0.4059 - val_acc: 0.8489\n",
      "Epoch 57/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4424 - acc: 0.8077 - val_loss: 0.4050 - val_acc: 0.8456\n",
      "Epoch 58/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4453 - acc: 0.8057 - val_loss: 0.4041 - val_acc: 0.8456\n",
      "Epoch 59/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4433 - acc: 0.8030 - val_loss: 0.4044 - val_acc: 0.8456\n",
      "Epoch 60/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4380 - acc: 0.8081 - val_loss: 0.4022 - val_acc: 0.8456\n",
      "Epoch 61/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4406 - acc: 0.8072 - val_loss: 0.4014 - val_acc: 0.8440\n",
      "Epoch 62/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4374 - acc: 0.8106 - val_loss: 0.3999 - val_acc: 0.8424\n",
      "Epoch 63/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4399 - acc: 0.8077 - val_loss: 0.3993 - val_acc: 0.8440\n",
      "Epoch 64/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4380 - acc: 0.8039 - val_loss: 0.3990 - val_acc: 0.8440\n",
      "Epoch 65/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4351 - acc: 0.8055 - val_loss: 0.3997 - val_acc: 0.8440\n",
      "Epoch 66/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4344 - acc: 0.8050 - val_loss: 0.3991 - val_acc: 0.8424\n",
      "Epoch 67/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4333 - acc: 0.8115 - val_loss: 0.3965 - val_acc: 0.8407\n",
      "Epoch 68/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4332 - acc: 0.8101 - val_loss: 0.3960 - val_acc: 0.8407\n",
      "Epoch 69/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4292 - acc: 0.8119 - val_loss: 0.3951 - val_acc: 0.8424\n",
      "Epoch 70/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4286 - acc: 0.8124 - val_loss: 0.3938 - val_acc: 0.8407\n",
      "Epoch 71/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4277 - acc: 0.8126 - val_loss: 0.3929 - val_acc: 0.8424\n",
      "Epoch 72/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4257 - acc: 0.8113 - val_loss: 0.3917 - val_acc: 0.8456\n",
      "Epoch 73/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4224 - acc: 0.8157 - val_loss: 0.3911 - val_acc: 0.8424\n",
      "Epoch 74/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4279 - acc: 0.8119 - val_loss: 0.3930 - val_acc: 0.8473\n",
      "Epoch 75/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4279 - acc: 0.8139 - val_loss: 0.3922 - val_acc: 0.8440\n",
      "Epoch 76/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4187 - acc: 0.8183 - val_loss: 0.3913 - val_acc: 0.8456\n",
      "Epoch 77/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4242 - acc: 0.8159 - val_loss: 0.3903 - val_acc: 0.8489\n",
      "Epoch 78/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4159 - acc: 0.8239 - val_loss: 0.3895 - val_acc: 0.8424\n",
      "Epoch 79/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4180 - acc: 0.8228 - val_loss: 0.3894 - val_acc: 0.8456\n",
      "Epoch 80/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4143 - acc: 0.8185 - val_loss: 0.3912 - val_acc: 0.8440\n",
      "Epoch 81/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4192 - acc: 0.8192 - val_loss: 0.3896 - val_acc: 0.8407\n",
      "Epoch 82/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4182 - acc: 0.8185 - val_loss: 0.3883 - val_acc: 0.8456\n",
      "Epoch 83/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4189 - acc: 0.8205 - val_loss: 0.3882 - val_acc: 0.8506\n",
      "Epoch 84/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4139 - acc: 0.8227 - val_loss: 0.3879 - val_acc: 0.8506\n",
      "Epoch 85/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4111 - acc: 0.8199 - val_loss: 0.3875 - val_acc: 0.8473\n",
      "Epoch 86/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4117 - acc: 0.8179 - val_loss: 0.3875 - val_acc: 0.8440\n",
      "Epoch 87/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4078 - acc: 0.8241 - val_loss: 0.3881 - val_acc: 0.8473\n",
      "Epoch 88/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4110 - acc: 0.8238 - val_loss: 0.3881 - val_acc: 0.8456\n",
      "Epoch 89/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4089 - acc: 0.8247 - val_loss: 0.3904 - val_acc: 0.8440\n",
      "Epoch 90/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4128 - acc: 0.8230 - val_loss: 0.3900 - val_acc: 0.8424\n",
      "Training with parameters {'batch_size': 1500, 'dropout': 0.1, 'lay_conf': [1024, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_123\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_124 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_369 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_246 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_370 (Dense)            (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_247 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_371 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,639,553\n",
      "Trainable params: 1,639,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "4/4 [==============================] - 1s 133ms/step - loss: 0.7704 - acc: 0.5220 - val_loss: 0.6720 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.6770 - acc: 0.5765 - val_loss: 0.6404 - val_acc: 0.6716\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.6426 - acc: 0.6369 - val_loss: 0.6064 - val_acc: 0.6240\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.6189 - acc: 0.6597 - val_loss: 0.5671 - val_acc: 0.7159\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.6002 - acc: 0.6798 - val_loss: 0.5464 - val_acc: 0.7685\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.5802 - acc: 0.7145 - val_loss: 0.5250 - val_acc: 0.7619\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.5638 - acc: 0.7092 - val_loss: 0.5021 - val_acc: 0.7980\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.5457 - acc: 0.7402 - val_loss: 0.4877 - val_acc: 0.7947\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.5321 - acc: 0.7460 - val_loss: 0.4715 - val_acc: 0.8243\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.5191 - acc: 0.7564 - val_loss: 0.4573 - val_acc: 0.8259\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.5117 - acc: 0.7643 - val_loss: 0.4473 - val_acc: 0.8292\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4978 - acc: 0.7727 - val_loss: 0.4423 - val_acc: 0.8325\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4953 - acc: 0.7676 - val_loss: 0.4347 - val_acc: 0.8177\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4860 - acc: 0.7741 - val_loss: 0.4242 - val_acc: 0.8309\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.4803 - acc: 0.7778 - val_loss: 0.4227 - val_acc: 0.8407\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.4722 - acc: 0.7833 - val_loss: 0.4176 - val_acc: 0.8292\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.4718 - acc: 0.7893 - val_loss: 0.4096 - val_acc: 0.8407\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.4655 - acc: 0.7915 - val_loss: 0.4205 - val_acc: 0.8325\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.4657 - acc: 0.7915 - val_loss: 0.4163 - val_acc: 0.8325\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.4588 - acc: 0.7938 - val_loss: 0.4023 - val_acc: 0.8440\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4574 - acc: 0.7946 - val_loss: 0.3998 - val_acc: 0.8407\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4487 - acc: 0.8030 - val_loss: 0.3983 - val_acc: 0.8391\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4491 - acc: 0.7984 - val_loss: 0.3934 - val_acc: 0.8506\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4449 - acc: 0.8026 - val_loss: 0.3924 - val_acc: 0.8473\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.4395 - acc: 0.8057 - val_loss: 0.3904 - val_acc: 0.8522\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 55ms/step - loss: 0.4387 - acc: 0.8042 - val_loss: 0.3876 - val_acc: 0.8539\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 55ms/step - loss: 0.4333 - acc: 0.8093 - val_loss: 0.3863 - val_acc: 0.8522\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.4297 - acc: 0.8121 - val_loss: 0.3858 - val_acc: 0.8522\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.4288 - acc: 0.8124 - val_loss: 0.3859 - val_acc: 0.8489\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.4270 - acc: 0.8117 - val_loss: 0.3822 - val_acc: 0.8571\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4239 - acc: 0.8115 - val_loss: 0.3801 - val_acc: 0.8539\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.4230 - acc: 0.8108 - val_loss: 0.3861 - val_acc: 0.8473\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.4246 - acc: 0.8154 - val_loss: 0.3775 - val_acc: 0.8555\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.4198 - acc: 0.8134 - val_loss: 0.3860 - val_acc: 0.8473\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4202 - acc: 0.8176 - val_loss: 0.3811 - val_acc: 0.8506\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.4180 - acc: 0.8132 - val_loss: 0.3767 - val_acc: 0.8555\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4164 - acc: 0.8163 - val_loss: 0.3804 - val_acc: 0.8571\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.4097 - acc: 0.8243 - val_loss: 0.3752 - val_acc: 0.8588\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.4115 - acc: 0.8181 - val_loss: 0.3745 - val_acc: 0.8555\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4074 - acc: 0.8190 - val_loss: 0.3744 - val_acc: 0.8555\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.4079 - acc: 0.8201 - val_loss: 0.3727 - val_acc: 0.8555\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.4009 - acc: 0.8289 - val_loss: 0.3745 - val_acc: 0.8571\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.4044 - acc: 0.8261 - val_loss: 0.3713 - val_acc: 0.8571\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3982 - acc: 0.8283 - val_loss: 0.3747 - val_acc: 0.8522\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3992 - acc: 0.8267 - val_loss: 0.3711 - val_acc: 0.8604\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3962 - acc: 0.8272 - val_loss: 0.3701 - val_acc: 0.8571\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3962 - acc: 0.8280 - val_loss: 0.3719 - val_acc: 0.8506\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3948 - acc: 0.8305 - val_loss: 0.3704 - val_acc: 0.8621\n",
      "Epoch 49/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3944 - acc: 0.8254 - val_loss: 0.3771 - val_acc: 0.8456\n",
      "Epoch 50/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3931 - acc: 0.8305 - val_loss: 0.3707 - val_acc: 0.8588\n",
      "Epoch 51/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3901 - acc: 0.8311 - val_loss: 0.3727 - val_acc: 0.8456\n",
      "Training with parameters {'batch_size': 1500, 'dropout': 0.1, 'lay_conf': [1024, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_124\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_125 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_372 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_248 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_373 (Dense)            (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_249 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_374 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,606,721\n",
      "Trainable params: 1,606,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "4/4 [==============================] - 1s 127ms/step - loss: 0.7963 - acc: 0.5203 - val_loss: 0.6646 - val_acc: 0.5764\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.6912 - acc: 0.5592 - val_loss: 0.6301 - val_acc: 0.6929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.6485 - acc: 0.6178 - val_loss: 0.6143 - val_acc: 0.6174\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.6338 - acc: 0.6504 - val_loss: 0.5844 - val_acc: 0.7126\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.6094 - acc: 0.6718 - val_loss: 0.5498 - val_acc: 0.7570\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.5875 - acc: 0.7002 - val_loss: 0.5156 - val_acc: 0.7800\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.5613 - acc: 0.7203 - val_loss: 0.4964 - val_acc: 0.8095\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 58ms/step - loss: 0.5508 - acc: 0.7305 - val_loss: 0.4792 - val_acc: 0.8112\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.5320 - acc: 0.7506 - val_loss: 0.4674 - val_acc: 0.8177\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.5222 - acc: 0.7482 - val_loss: 0.4556 - val_acc: 0.8177\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.5144 - acc: 0.7590 - val_loss: 0.4444 - val_acc: 0.8161\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.5013 - acc: 0.7656 - val_loss: 0.4380 - val_acc: 0.8243\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4921 - acc: 0.7734 - val_loss: 0.4275 - val_acc: 0.8259\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.4867 - acc: 0.7716 - val_loss: 0.4204 - val_acc: 0.8325\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4825 - acc: 0.7831 - val_loss: 0.4151 - val_acc: 0.8325\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4787 - acc: 0.7778 - val_loss: 0.4112 - val_acc: 0.8358\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.4697 - acc: 0.7867 - val_loss: 0.4067 - val_acc: 0.8407\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4615 - acc: 0.7933 - val_loss: 0.4021 - val_acc: 0.8407\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4583 - acc: 0.7938 - val_loss: 0.4008 - val_acc: 0.8424\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4601 - acc: 0.7913 - val_loss: 0.4048 - val_acc: 0.8374\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4569 - acc: 0.7962 - val_loss: 0.3968 - val_acc: 0.8440\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4504 - acc: 0.8000 - val_loss: 0.3926 - val_acc: 0.8473\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4517 - acc: 0.7991 - val_loss: 0.4009 - val_acc: 0.8374\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4529 - acc: 0.7942 - val_loss: 0.3941 - val_acc: 0.8424\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4431 - acc: 0.8061 - val_loss: 0.3909 - val_acc: 0.8440\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4409 - acc: 0.8051 - val_loss: 0.3931 - val_acc: 0.8456\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4372 - acc: 0.8055 - val_loss: 0.3988 - val_acc: 0.8391\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4381 - acc: 0.8057 - val_loss: 0.3828 - val_acc: 0.8489\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4311 - acc: 0.8068 - val_loss: 0.3816 - val_acc: 0.8522\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4334 - acc: 0.8051 - val_loss: 0.3836 - val_acc: 0.8489\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4296 - acc: 0.8119 - val_loss: 0.3809 - val_acc: 0.8522\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4246 - acc: 0.8139 - val_loss: 0.3803 - val_acc: 0.8571\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4243 - acc: 0.8119 - val_loss: 0.3814 - val_acc: 0.8506\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4227 - acc: 0.8108 - val_loss: 0.3816 - val_acc: 0.8489\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4252 - acc: 0.8168 - val_loss: 0.3771 - val_acc: 0.8489\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4169 - acc: 0.8119 - val_loss: 0.3766 - val_acc: 0.8506\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4152 - acc: 0.8199 - val_loss: 0.3777 - val_acc: 0.8489\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4105 - acc: 0.8219 - val_loss: 0.3779 - val_acc: 0.8539\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4120 - acc: 0.8186 - val_loss: 0.3764 - val_acc: 0.8489\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4074 - acc: 0.8247 - val_loss: 0.3749 - val_acc: 0.8539\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4097 - acc: 0.8199 - val_loss: 0.3734 - val_acc: 0.8571\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4067 - acc: 0.8186 - val_loss: 0.3723 - val_acc: 0.8539\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4042 - acc: 0.8243 - val_loss: 0.3719 - val_acc: 0.8555\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4010 - acc: 0.8239 - val_loss: 0.3733 - val_acc: 0.8555\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4011 - acc: 0.8221 - val_loss: 0.3723 - val_acc: 0.8539\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3996 - acc: 0.8250 - val_loss: 0.3693 - val_acc: 0.8539\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3975 - acc: 0.8254 - val_loss: 0.3700 - val_acc: 0.8539\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3991 - acc: 0.8278 - val_loss: 0.3734 - val_acc: 0.8539\n",
      "Epoch 49/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3935 - acc: 0.8323 - val_loss: 0.3741 - val_acc: 0.8522\n",
      "Epoch 50/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3957 - acc: 0.8314 - val_loss: 0.3703 - val_acc: 0.8522\n",
      "Epoch 51/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3954 - acc: 0.8287 - val_loss: 0.3698 - val_acc: 0.8506\n",
      "Training with parameters {'batch_size': 1500, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_125\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_126 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_375 (Dense)            (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_250 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_376 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_251 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_377 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,050,113\n",
      "Trainable params: 1,050,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "4/4 [==============================] - 0s 63ms/step - loss: 0.7252 - acc: 0.5344 - val_loss: 0.6613 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.6701 - acc: 0.5906 - val_loss: 0.6365 - val_acc: 0.6634\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.6408 - acc: 0.6342 - val_loss: 0.6004 - val_acc: 0.6585\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.6191 - acc: 0.6592 - val_loss: 0.5794 - val_acc: 0.6913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5987 - acc: 0.6845 - val_loss: 0.5514 - val_acc: 0.7668\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5818 - acc: 0.7074 - val_loss: 0.5354 - val_acc: 0.7586\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.5614 - acc: 0.7282 - val_loss: 0.5146 - val_acc: 0.7931\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.5483 - acc: 0.7409 - val_loss: 0.5006 - val_acc: 0.8046\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5331 - acc: 0.7477 - val_loss: 0.4835 - val_acc: 0.8046\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5257 - acc: 0.7552 - val_loss: 0.4735 - val_acc: 0.8144\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.5082 - acc: 0.7608 - val_loss: 0.4577 - val_acc: 0.8144\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4988 - acc: 0.7710 - val_loss: 0.4469 - val_acc: 0.8210\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4907 - acc: 0.7749 - val_loss: 0.4383 - val_acc: 0.8227\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4787 - acc: 0.7825 - val_loss: 0.4396 - val_acc: 0.8194\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4818 - acc: 0.7796 - val_loss: 0.4256 - val_acc: 0.8342\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4683 - acc: 0.7878 - val_loss: 0.4329 - val_acc: 0.8161\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4744 - acc: 0.7803 - val_loss: 0.4208 - val_acc: 0.8276\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4641 - acc: 0.7902 - val_loss: 0.4166 - val_acc: 0.8309\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4618 - acc: 0.7885 - val_loss: 0.4323 - val_acc: 0.8194\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4627 - acc: 0.7880 - val_loss: 0.4247 - val_acc: 0.8227\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4563 - acc: 0.7915 - val_loss: 0.4097 - val_acc: 0.8276\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4451 - acc: 0.8006 - val_loss: 0.4045 - val_acc: 0.8391\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4426 - acc: 0.8073 - val_loss: 0.4020 - val_acc: 0.8407\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4407 - acc: 0.8011 - val_loss: 0.4014 - val_acc: 0.8424\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4400 - acc: 0.8061 - val_loss: 0.4046 - val_acc: 0.8407\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4368 - acc: 0.8008 - val_loss: 0.3993 - val_acc: 0.8391\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4318 - acc: 0.8130 - val_loss: 0.3979 - val_acc: 0.8407\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4281 - acc: 0.8082 - val_loss: 0.3958 - val_acc: 0.8374\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4266 - acc: 0.8106 - val_loss: 0.3945 - val_acc: 0.8391\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4218 - acc: 0.8186 - val_loss: 0.3975 - val_acc: 0.8456\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4240 - acc: 0.8155 - val_loss: 0.3937 - val_acc: 0.8407\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4199 - acc: 0.8190 - val_loss: 0.3895 - val_acc: 0.8522\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4192 - acc: 0.8155 - val_loss: 0.3931 - val_acc: 0.8489\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4206 - acc: 0.8139 - val_loss: 0.3880 - val_acc: 0.8522\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4129 - acc: 0.8150 - val_loss: 0.3919 - val_acc: 0.8440\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4144 - acc: 0.8159 - val_loss: 0.3896 - val_acc: 0.8522\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4105 - acc: 0.8176 - val_loss: 0.3854 - val_acc: 0.8473\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4076 - acc: 0.8188 - val_loss: 0.3842 - val_acc: 0.8489\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4040 - acc: 0.8230 - val_loss: 0.3904 - val_acc: 0.8489\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4065 - acc: 0.8223 - val_loss: 0.3838 - val_acc: 0.8440\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4001 - acc: 0.8208 - val_loss: 0.3828 - val_acc: 0.8473\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.3991 - acc: 0.8236 - val_loss: 0.3816 - val_acc: 0.8506\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3968 - acc: 0.8252 - val_loss: 0.3809 - val_acc: 0.8473\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3953 - acc: 0.8270 - val_loss: 0.3800 - val_acc: 0.8506\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3914 - acc: 0.8259 - val_loss: 0.3788 - val_acc: 0.8489\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3882 - acc: 0.8290 - val_loss: 0.3847 - val_acc: 0.8539\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3894 - acc: 0.8278 - val_loss: 0.3778 - val_acc: 0.8588\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.3835 - acc: 0.8338 - val_loss: 0.3778 - val_acc: 0.8506\n",
      "Epoch 49/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3819 - acc: 0.8305 - val_loss: 0.3798 - val_acc: 0.8473\n",
      "Epoch 50/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.3842 - acc: 0.8320 - val_loss: 0.3789 - val_acc: 0.8555\n",
      "Epoch 51/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.3816 - acc: 0.8334 - val_loss: 0.3773 - val_acc: 0.8588\n",
      "Epoch 52/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.3764 - acc: 0.8345 - val_loss: 0.3779 - val_acc: 0.8473\n",
      "Epoch 53/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3754 - acc: 0.8347 - val_loss: 0.3757 - val_acc: 0.8539\n",
      "Epoch 54/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.3717 - acc: 0.8398 - val_loss: 0.3764 - val_acc: 0.8555\n",
      "Epoch 55/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3701 - acc: 0.8402 - val_loss: 0.3748 - val_acc: 0.8489\n",
      "Epoch 56/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.3682 - acc: 0.8391 - val_loss: 0.3749 - val_acc: 0.8539\n",
      "Epoch 57/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3637 - acc: 0.8424 - val_loss: 0.3746 - val_acc: 0.8489\n",
      "Epoch 58/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3636 - acc: 0.8442 - val_loss: 0.3737 - val_acc: 0.8506\n",
      "Epoch 59/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3622 - acc: 0.8431 - val_loss: 0.3738 - val_acc: 0.8489\n",
      "Epoch 60/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3561 - acc: 0.8413 - val_loss: 0.3755 - val_acc: 0.8522\n",
      "Epoch 61/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3587 - acc: 0.8433 - val_loss: 0.3737 - val_acc: 0.8489\n",
      "Epoch 62/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3552 - acc: 0.8456 - val_loss: 0.3756 - val_acc: 0.8522\n",
      "Epoch 63/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3537 - acc: 0.8462 - val_loss: 0.3730 - val_acc: 0.8506\n",
      "Epoch 64/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3472 - acc: 0.8500 - val_loss: 0.3723 - val_acc: 0.8456\n",
      "Epoch 65/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3470 - acc: 0.8526 - val_loss: 0.3746 - val_acc: 0.8456\n",
      "Epoch 66/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3461 - acc: 0.8498 - val_loss: 0.3726 - val_acc: 0.8506\n",
      "Epoch 67/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3462 - acc: 0.8508 - val_loss: 0.3776 - val_acc: 0.8539\n",
      "Epoch 68/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3456 - acc: 0.8508 - val_loss: 0.4185 - val_acc: 0.8079\n",
      "Epoch 69/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3604 - acc: 0.8416 - val_loss: 0.3767 - val_acc: 0.8522\n",
      "Training with parameters {'batch_size': 1500, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_126\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_127 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_378 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_252 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_379 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_253 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_380 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 401,729\n",
      "Trainable params: 401,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.9066 - acc: 0.4393 - val_loss: 0.6977 - val_acc: 0.5567\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.7132 - acc: 0.5411 - val_loss: 0.6915 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.6965 - acc: 0.5641 - val_loss: 0.6626 - val_acc: 0.5764\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.6750 - acc: 0.5791 - val_loss: 0.6517 - val_acc: 0.6765\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6634 - acc: 0.6039 - val_loss: 0.6384 - val_acc: 0.7028\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.6528 - acc: 0.6223 - val_loss: 0.6191 - val_acc: 0.7044\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.6464 - acc: 0.6336 - val_loss: 0.6059 - val_acc: 0.7192\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.6348 - acc: 0.6446 - val_loss: 0.5938 - val_acc: 0.7291\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.6278 - acc: 0.6561 - val_loss: 0.5819 - val_acc: 0.7340\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6162 - acc: 0.6712 - val_loss: 0.5708 - val_acc: 0.7389\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.6104 - acc: 0.6780 - val_loss: 0.5609 - val_acc: 0.7537\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5994 - acc: 0.6913 - val_loss: 0.5510 - val_acc: 0.7488\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.5961 - acc: 0.6948 - val_loss: 0.5415 - val_acc: 0.7570\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5828 - acc: 0.7033 - val_loss: 0.5326 - val_acc: 0.7603\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5728 - acc: 0.7267 - val_loss: 0.5248 - val_acc: 0.7734\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5652 - acc: 0.7210 - val_loss: 0.5166 - val_acc: 0.7816\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5624 - acc: 0.7278 - val_loss: 0.5092 - val_acc: 0.7898\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5526 - acc: 0.7334 - val_loss: 0.5029 - val_acc: 0.7964\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5480 - acc: 0.7375 - val_loss: 0.4960 - val_acc: 0.8030\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5407 - acc: 0.7427 - val_loss: 0.4898 - val_acc: 0.8046\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5319 - acc: 0.7553 - val_loss: 0.4831 - val_acc: 0.8079\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5295 - acc: 0.7513 - val_loss: 0.4773 - val_acc: 0.8062\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5220 - acc: 0.7570 - val_loss: 0.4717 - val_acc: 0.8079\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.5191 - acc: 0.7621 - val_loss: 0.4653 - val_acc: 0.8144\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.5126 - acc: 0.7652 - val_loss: 0.4598 - val_acc: 0.8112\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.5020 - acc: 0.7696 - val_loss: 0.4539 - val_acc: 0.8144\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.4986 - acc: 0.7705 - val_loss: 0.4496 - val_acc: 0.8210\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.4960 - acc: 0.7761 - val_loss: 0.4449 - val_acc: 0.8161\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.4891 - acc: 0.7785 - val_loss: 0.4404 - val_acc: 0.8161\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.4919 - acc: 0.7770 - val_loss: 0.4359 - val_acc: 0.8177\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4859 - acc: 0.7791 - val_loss: 0.4324 - val_acc: 0.8325\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4795 - acc: 0.7867 - val_loss: 0.4308 - val_acc: 0.8227\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4747 - acc: 0.7873 - val_loss: 0.4266 - val_acc: 0.8342\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4741 - acc: 0.7865 - val_loss: 0.4238 - val_acc: 0.8309\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4694 - acc: 0.7918 - val_loss: 0.4231 - val_acc: 0.8259\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4693 - acc: 0.7896 - val_loss: 0.4194 - val_acc: 0.8292\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4673 - acc: 0.7900 - val_loss: 0.4168 - val_acc: 0.8358\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4627 - acc: 0.7942 - val_loss: 0.4148 - val_acc: 0.8342\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4602 - acc: 0.7927 - val_loss: 0.4125 - val_acc: 0.8342\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4576 - acc: 0.8002 - val_loss: 0.4105 - val_acc: 0.8358\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4545 - acc: 0.8020 - val_loss: 0.4086 - val_acc: 0.8342\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4518 - acc: 0.8048 - val_loss: 0.4075 - val_acc: 0.8342\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4531 - acc: 0.8030 - val_loss: 0.4085 - val_acc: 0.8309\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4482 - acc: 0.7997 - val_loss: 0.4078 - val_acc: 0.8342\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4464 - acc: 0.8048 - val_loss: 0.4048 - val_acc: 0.8309\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4499 - acc: 0.8061 - val_loss: 0.4015 - val_acc: 0.8374\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4435 - acc: 0.8013 - val_loss: 0.4012 - val_acc: 0.8374\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4408 - acc: 0.8072 - val_loss: 0.4070 - val_acc: 0.8259\n",
      "Epoch 49/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4438 - acc: 0.8050 - val_loss: 0.4039 - val_acc: 0.8391\n",
      "Epoch 50/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4417 - acc: 0.8039 - val_loss: 0.4007 - val_acc: 0.8342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4393 - acc: 0.8082 - val_loss: 0.3986 - val_acc: 0.8374\n",
      "Epoch 52/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4395 - acc: 0.8070 - val_loss: 0.3969 - val_acc: 0.8325\n",
      "Epoch 53/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4324 - acc: 0.8115 - val_loss: 0.3945 - val_acc: 0.8424\n",
      "Epoch 54/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4333 - acc: 0.8128 - val_loss: 0.3940 - val_acc: 0.8358\n",
      "Epoch 55/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4294 - acc: 0.8117 - val_loss: 0.3933 - val_acc: 0.8424\n",
      "Epoch 56/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4300 - acc: 0.8128 - val_loss: 0.3932 - val_acc: 0.8342\n",
      "Epoch 57/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4304 - acc: 0.8155 - val_loss: 0.3935 - val_acc: 0.8342\n",
      "Epoch 58/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4268 - acc: 0.8144 - val_loss: 0.3925 - val_acc: 0.8358\n",
      "Epoch 59/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4277 - acc: 0.8121 - val_loss: 0.3916 - val_acc: 0.8358\n",
      "Epoch 60/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4237 - acc: 0.8163 - val_loss: 0.3902 - val_acc: 0.8342\n",
      "Epoch 61/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4230 - acc: 0.8186 - val_loss: 0.3892 - val_acc: 0.8391\n",
      "Epoch 62/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4197 - acc: 0.8139 - val_loss: 0.3905 - val_acc: 0.8325\n",
      "Epoch 63/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4196 - acc: 0.8172 - val_loss: 0.3885 - val_acc: 0.8374\n",
      "Epoch 64/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4185 - acc: 0.8234 - val_loss: 0.3881 - val_acc: 0.8358\n",
      "Epoch 65/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4164 - acc: 0.8181 - val_loss: 0.3870 - val_acc: 0.8391\n",
      "Epoch 66/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4188 - acc: 0.8192 - val_loss: 0.3881 - val_acc: 0.8342\n",
      "Epoch 67/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4167 - acc: 0.8161 - val_loss: 0.3871 - val_acc: 0.8374\n",
      "Epoch 68/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4159 - acc: 0.8179 - val_loss: 0.3871 - val_acc: 0.8456\n",
      "Epoch 69/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4162 - acc: 0.8210 - val_loss: 0.3919 - val_acc: 0.8407\n",
      "Epoch 70/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4172 - acc: 0.8165 - val_loss: 0.3914 - val_acc: 0.8522\n",
      "Training with parameters {'batch_size': 1500, 'dropout': 0.1, 'lay_conf': [1024, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_127\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_128 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_381 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_254 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_382 (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_255 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_383 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,836,545\n",
      "Trainable params: 1,836,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "4/4 [==============================] - 1s 56ms/step - loss: 0.8448 - acc: 0.5140 - val_loss: 0.7810 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.7126 - acc: 0.5846 - val_loss: 0.6613 - val_acc: 0.6404\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.6808 - acc: 0.5791 - val_loss: 0.6189 - val_acc: 0.6749\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.6398 - acc: 0.6302 - val_loss: 0.6184 - val_acc: 0.5813\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.6294 - acc: 0.6327 - val_loss: 0.5774 - val_acc: 0.7406\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.6054 - acc: 0.6752 - val_loss: 0.5752 - val_acc: 0.7126\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.5985 - acc: 0.6847 - val_loss: 0.5495 - val_acc: 0.7718\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.5819 - acc: 0.7165 - val_loss: 0.5329 - val_acc: 0.7800\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.5691 - acc: 0.7119 - val_loss: 0.5196 - val_acc: 0.7816\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5598 - acc: 0.7283 - val_loss: 0.5029 - val_acc: 0.8030\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.5436 - acc: 0.7391 - val_loss: 0.4927 - val_acc: 0.7964\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.5286 - acc: 0.7508 - val_loss: 0.4784 - val_acc: 0.8112\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.5194 - acc: 0.7548 - val_loss: 0.4699 - val_acc: 0.8079\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.5123 - acc: 0.7603 - val_loss: 0.4570 - val_acc: 0.8227\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.5008 - acc: 0.7677 - val_loss: 0.4482 - val_acc: 0.8227\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4922 - acc: 0.7749 - val_loss: 0.4417 - val_acc: 0.8243\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4841 - acc: 0.7789 - val_loss: 0.4332 - val_acc: 0.8259\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4785 - acc: 0.7792 - val_loss: 0.4297 - val_acc: 0.8243\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4758 - acc: 0.7800 - val_loss: 0.4215 - val_acc: 0.8276\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4660 - acc: 0.7938 - val_loss: 0.4172 - val_acc: 0.8243\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4636 - acc: 0.7904 - val_loss: 0.4139 - val_acc: 0.8342\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4611 - acc: 0.7922 - val_loss: 0.4113 - val_acc: 0.8309\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4532 - acc: 0.7982 - val_loss: 0.4049 - val_acc: 0.8391\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4461 - acc: 0.8006 - val_loss: 0.4019 - val_acc: 0.8407\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4443 - acc: 0.8006 - val_loss: 0.3994 - val_acc: 0.8342\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4430 - acc: 0.8048 - val_loss: 0.3978 - val_acc: 0.8374\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4370 - acc: 0.8048 - val_loss: 0.3957 - val_acc: 0.8407\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4344 - acc: 0.8068 - val_loss: 0.3920 - val_acc: 0.8424\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4282 - acc: 0.8101 - val_loss: 0.3898 - val_acc: 0.8456\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4279 - acc: 0.8124 - val_loss: 0.3900 - val_acc: 0.8424\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4263 - acc: 0.8112 - val_loss: 0.3871 - val_acc: 0.8440\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4186 - acc: 0.8165 - val_loss: 0.3857 - val_acc: 0.8440\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4203 - acc: 0.8168 - val_loss: 0.3866 - val_acc: 0.8456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4221 - acc: 0.8103 - val_loss: 0.3926 - val_acc: 0.8391\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4146 - acc: 0.8157 - val_loss: 0.3814 - val_acc: 0.8506\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4102 - acc: 0.8199 - val_loss: 0.3820 - val_acc: 0.8522\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4085 - acc: 0.8217 - val_loss: 0.3807 - val_acc: 0.8539\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4049 - acc: 0.8241 - val_loss: 0.3796 - val_acc: 0.8440\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4020 - acc: 0.8259 - val_loss: 0.3788 - val_acc: 0.8489\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3972 - acc: 0.8274 - val_loss: 0.3785 - val_acc: 0.8506\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3963 - acc: 0.8276 - val_loss: 0.3773 - val_acc: 0.8489\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3941 - acc: 0.8289 - val_loss: 0.3782 - val_acc: 0.8489\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3925 - acc: 0.8300 - val_loss: 0.3817 - val_acc: 0.8489\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3934 - acc: 0.8261 - val_loss: 0.3761 - val_acc: 0.8506\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3886 - acc: 0.8363 - val_loss: 0.3745 - val_acc: 0.8522\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3845 - acc: 0.8327 - val_loss: 0.3736 - val_acc: 0.8522\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.3862 - acc: 0.8307 - val_loss: 0.3752 - val_acc: 0.8489\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3870 - acc: 0.8331 - val_loss: 0.3796 - val_acc: 0.8539\n",
      "Epoch 49/300\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.3819 - acc: 0.8316 - val_loss: 0.3814 - val_acc: 0.8522\n",
      "Epoch 50/300\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.3791 - acc: 0.8349 - val_loss: 0.3731 - val_acc: 0.8506\n",
      "Epoch 51/300\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3797 - acc: 0.8331 - val_loss: 0.3768 - val_acc: 0.8456\n",
      "Epoch 52/300\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3769 - acc: 0.8363 - val_loss: 0.3785 - val_acc: 0.8473\n",
      "Epoch 53/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3735 - acc: 0.8362 - val_loss: 0.3716 - val_acc: 0.8571\n",
      "Epoch 54/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3660 - acc: 0.8422 - val_loss: 0.3722 - val_acc: 0.8473\n",
      "Epoch 55/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3647 - acc: 0.8436 - val_loss: 0.3775 - val_acc: 0.8456\n",
      "Epoch 56/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3683 - acc: 0.8402 - val_loss: 0.3765 - val_acc: 0.8473\n",
      "Epoch 57/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3715 - acc: 0.8384 - val_loss: 0.3747 - val_acc: 0.8539\n",
      "Epoch 58/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3673 - acc: 0.8444 - val_loss: 0.3802 - val_acc: 0.8391\n",
      "Training with parameters {'batch_size': 2000, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_128\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_129 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_384 (Dense)            (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_256 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_385 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_257 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_386 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.8033 - acc: 0.4430 - val_loss: 0.6903 - val_acc: 0.5369\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6967 - acc: 0.5286 - val_loss: 0.6809 - val_acc: 0.5731\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6919 - acc: 0.5641 - val_loss: 0.6756 - val_acc: 0.5747\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6831 - acc: 0.5687 - val_loss: 0.6588 - val_acc: 0.5747\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6691 - acc: 0.5773 - val_loss: 0.6495 - val_acc: 0.6240\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6647 - acc: 0.6017 - val_loss: 0.6455 - val_acc: 0.6897\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6614 - acc: 0.6147 - val_loss: 0.6350 - val_acc: 0.6913\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6511 - acc: 0.6307 - val_loss: 0.6229 - val_acc: 0.6864\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.6444 - acc: 0.6384 - val_loss: 0.6131 - val_acc: 0.6946\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.6344 - acc: 0.6428 - val_loss: 0.6013 - val_acc: 0.7126\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.6264 - acc: 0.6561 - val_loss: 0.5920 - val_acc: 0.7143\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6207 - acc: 0.6714 - val_loss: 0.5822 - val_acc: 0.7241\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6146 - acc: 0.6718 - val_loss: 0.5724 - val_acc: 0.7373\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.6106 - acc: 0.6849 - val_loss: 0.5632 - val_acc: 0.7488\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5987 - acc: 0.6907 - val_loss: 0.5539 - val_acc: 0.7586\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5918 - acc: 0.7010 - val_loss: 0.5449 - val_acc: 0.7750\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5881 - acc: 0.7043 - val_loss: 0.5367 - val_acc: 0.7783\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5779 - acc: 0.7125 - val_loss: 0.5288 - val_acc: 0.7833\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5692 - acc: 0.7221 - val_loss: 0.5204 - val_acc: 0.7964\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5675 - acc: 0.7240 - val_loss: 0.5127 - val_acc: 0.8013\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5579 - acc: 0.7422 - val_loss: 0.5058 - val_acc: 0.7997\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5542 - acc: 0.7395 - val_loss: 0.4992 - val_acc: 0.7997\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5478 - acc: 0.7438 - val_loss: 0.4929 - val_acc: 0.8030\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5414 - acc: 0.7508 - val_loss: 0.4870 - val_acc: 0.7997\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5332 - acc: 0.7592 - val_loss: 0.4811 - val_acc: 0.7980\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5318 - acc: 0.7526 - val_loss: 0.4749 - val_acc: 0.8030\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5284 - acc: 0.7548 - val_loss: 0.4696 - val_acc: 0.8046\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5215 - acc: 0.7603 - val_loss: 0.4645 - val_acc: 0.8013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5152 - acc: 0.7639 - val_loss: 0.4600 - val_acc: 0.8079\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5132 - acc: 0.7698 - val_loss: 0.4555 - val_acc: 0.8112\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5045 - acc: 0.7741 - val_loss: 0.4512 - val_acc: 0.8177\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5031 - acc: 0.7739 - val_loss: 0.4470 - val_acc: 0.8177\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5022 - acc: 0.7710 - val_loss: 0.4425 - val_acc: 0.8243\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4962 - acc: 0.7772 - val_loss: 0.4383 - val_acc: 0.8227\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4923 - acc: 0.7732 - val_loss: 0.4347 - val_acc: 0.8227\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4884 - acc: 0.7803 - val_loss: 0.4316 - val_acc: 0.8259\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4880 - acc: 0.7770 - val_loss: 0.4291 - val_acc: 0.8259\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4814 - acc: 0.7800 - val_loss: 0.4272 - val_acc: 0.8210\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4809 - acc: 0.7845 - val_loss: 0.4239 - val_acc: 0.8259\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4732 - acc: 0.7900 - val_loss: 0.4231 - val_acc: 0.8309\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4754 - acc: 0.7909 - val_loss: 0.4209 - val_acc: 0.8325\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4707 - acc: 0.7856 - val_loss: 0.4180 - val_acc: 0.8292\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4724 - acc: 0.7895 - val_loss: 0.4152 - val_acc: 0.8309\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4681 - acc: 0.7915 - val_loss: 0.4137 - val_acc: 0.8342\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4681 - acc: 0.7915 - val_loss: 0.4129 - val_acc: 0.8292\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4660 - acc: 0.7942 - val_loss: 0.4100 - val_acc: 0.8309\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4586 - acc: 0.7995 - val_loss: 0.4086 - val_acc: 0.8358\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4583 - acc: 0.7973 - val_loss: 0.4069 - val_acc: 0.8342\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4588 - acc: 0.8002 - val_loss: 0.4054 - val_acc: 0.8391\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4551 - acc: 0.8006 - val_loss: 0.4040 - val_acc: 0.8424\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4505 - acc: 0.8026 - val_loss: 0.4032 - val_acc: 0.8391\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4555 - acc: 0.8013 - val_loss: 0.4014 - val_acc: 0.8391\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4528 - acc: 0.8004 - val_loss: 0.4005 - val_acc: 0.8424\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4475 - acc: 0.8086 - val_loss: 0.4003 - val_acc: 0.8325\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4445 - acc: 0.8026 - val_loss: 0.3994 - val_acc: 0.8407\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4445 - acc: 0.8068 - val_loss: 0.3981 - val_acc: 0.8358\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4452 - acc: 0.8108 - val_loss: 0.3968 - val_acc: 0.8407\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4445 - acc: 0.8046 - val_loss: 0.3950 - val_acc: 0.8424\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4413 - acc: 0.8082 - val_loss: 0.3944 - val_acc: 0.8424\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4381 - acc: 0.8066 - val_loss: 0.3940 - val_acc: 0.8407\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4391 - acc: 0.8101 - val_loss: 0.3938 - val_acc: 0.8424\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4390 - acc: 0.8121 - val_loss: 0.3919 - val_acc: 0.8440\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4334 - acc: 0.8088 - val_loss: 0.3912 - val_acc: 0.8473\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4328 - acc: 0.8106 - val_loss: 0.3907 - val_acc: 0.8440\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4339 - acc: 0.8070 - val_loss: 0.3901 - val_acc: 0.8440\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4336 - acc: 0.8064 - val_loss: 0.3896 - val_acc: 0.8440\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4308 - acc: 0.8084 - val_loss: 0.3890 - val_acc: 0.8456\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4304 - acc: 0.8093 - val_loss: 0.3884 - val_acc: 0.8506\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4309 - acc: 0.8155 - val_loss: 0.3877 - val_acc: 0.8506\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4276 - acc: 0.8126 - val_loss: 0.3868 - val_acc: 0.8489\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4272 - acc: 0.8104 - val_loss: 0.3873 - val_acc: 0.8424\n",
      "Epoch 72/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4266 - acc: 0.8148 - val_loss: 0.3861 - val_acc: 0.8456\n",
      "Epoch 73/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4272 - acc: 0.8126 - val_loss: 0.3859 - val_acc: 0.8473\n",
      "Epoch 74/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4254 - acc: 0.8115 - val_loss: 0.3852 - val_acc: 0.8489\n",
      "Epoch 75/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4232 - acc: 0.8165 - val_loss: 0.3849 - val_acc: 0.8473\n",
      "Epoch 76/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4217 - acc: 0.8139 - val_loss: 0.3859 - val_acc: 0.8456\n",
      "Epoch 77/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4256 - acc: 0.8132 - val_loss: 0.3849 - val_acc: 0.8407\n",
      "Epoch 78/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4183 - acc: 0.8152 - val_loss: 0.3848 - val_acc: 0.8456\n",
      "Epoch 79/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4232 - acc: 0.8123 - val_loss: 0.3838 - val_acc: 0.8424\n",
      "Epoch 80/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4198 - acc: 0.8194 - val_loss: 0.3827 - val_acc: 0.8489\n",
      "Epoch 81/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4184 - acc: 0.8134 - val_loss: 0.3822 - val_acc: 0.8489\n",
      "Epoch 82/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4199 - acc: 0.8165 - val_loss: 0.3820 - val_acc: 0.8489\n",
      "Epoch 83/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4169 - acc: 0.8181 - val_loss: 0.3836 - val_acc: 0.8489\n",
      "Epoch 84/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4129 - acc: 0.8207 - val_loss: 0.3819 - val_acc: 0.8456\n",
      "Epoch 85/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4154 - acc: 0.8194 - val_loss: 0.3820 - val_acc: 0.8489\n",
      "Epoch 86/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4166 - acc: 0.8159 - val_loss: 0.3800 - val_acc: 0.8489\n",
      "Epoch 87/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4131 - acc: 0.8203 - val_loss: 0.3803 - val_acc: 0.8456\n",
      "Epoch 88/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4113 - acc: 0.8214 - val_loss: 0.3809 - val_acc: 0.8489\n",
      "Epoch 89/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4151 - acc: 0.8205 - val_loss: 0.3797 - val_acc: 0.8489\n",
      "Epoch 90/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4118 - acc: 0.8192 - val_loss: 0.3799 - val_acc: 0.8456\n",
      "Epoch 91/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4081 - acc: 0.8228 - val_loss: 0.3794 - val_acc: 0.8489\n",
      "Epoch 92/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4089 - acc: 0.8239 - val_loss: 0.3792 - val_acc: 0.8473\n",
      "Epoch 93/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4103 - acc: 0.8258 - val_loss: 0.3791 - val_acc: 0.8473\n",
      "Epoch 94/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4061 - acc: 0.8238 - val_loss: 0.3797 - val_acc: 0.8440\n",
      "Epoch 95/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4068 - acc: 0.8239 - val_loss: 0.3800 - val_acc: 0.8506\n",
      "Epoch 96/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4099 - acc: 0.8208 - val_loss: 0.3786 - val_acc: 0.8522\n",
      "Epoch 97/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4044 - acc: 0.8245 - val_loss: 0.3780 - val_acc: 0.8473\n",
      "Epoch 98/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4068 - acc: 0.8223 - val_loss: 0.3782 - val_acc: 0.8489\n",
      "Epoch 99/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4037 - acc: 0.8236 - val_loss: 0.3784 - val_acc: 0.8456\n",
      "Epoch 100/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4042 - acc: 0.8256 - val_loss: 0.3780 - val_acc: 0.8522\n",
      "Epoch 101/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4050 - acc: 0.8228 - val_loss: 0.3780 - val_acc: 0.8473\n",
      "Epoch 102/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4011 - acc: 0.8252 - val_loss: 0.3776 - val_acc: 0.8506\n",
      "Epoch 103/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.3993 - acc: 0.8269 - val_loss: 0.3773 - val_acc: 0.8506\n",
      "Epoch 104/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4017 - acc: 0.8247 - val_loss: 0.3772 - val_acc: 0.8506\n",
      "Epoch 105/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4005 - acc: 0.8285 - val_loss: 0.3767 - val_acc: 0.8506\n",
      "Epoch 106/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.3991 - acc: 0.8254 - val_loss: 0.3765 - val_acc: 0.8522\n",
      "Epoch 107/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.3967 - acc: 0.8281 - val_loss: 0.3761 - val_acc: 0.8522\n",
      "Epoch 108/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.3957 - acc: 0.8283 - val_loss: 0.3760 - val_acc: 0.8522\n",
      "Epoch 109/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.3943 - acc: 0.8269 - val_loss: 0.3752 - val_acc: 0.8506\n",
      "Epoch 110/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.3970 - acc: 0.8280 - val_loss: 0.3759 - val_acc: 0.8555\n",
      "Epoch 111/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.3943 - acc: 0.8298 - val_loss: 0.3757 - val_acc: 0.8506\n",
      "Epoch 112/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.3924 - acc: 0.8309 - val_loss: 0.3759 - val_acc: 0.8506\n",
      "Epoch 113/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.3932 - acc: 0.8336 - val_loss: 0.3777 - val_acc: 0.8473\n",
      "Epoch 114/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.3931 - acc: 0.8311 - val_loss: 0.3762 - val_acc: 0.8456\n",
      "Training with parameters {'batch_size': 2000, 'dropout': 0.1, 'lay_conf': [1024, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_129\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_130 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_387 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_258 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_388 (Dense)            (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_259 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_389 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 1s 208ms/step - loss: 0.8418 - acc: 0.4837 - val_loss: 0.7898 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.7634 - acc: 0.5672 - val_loss: 0.6576 - val_acc: 0.5829\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 0.6845 - acc: 0.5598 - val_loss: 0.6890 - val_acc: 0.5304\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.6804 - acc: 0.5796 - val_loss: 0.6158 - val_acc: 0.7061\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.6386 - acc: 0.6196 - val_loss: 0.6205 - val_acc: 0.5846\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.6387 - acc: 0.6218 - val_loss: 0.5833 - val_acc: 0.7307\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.6125 - acc: 0.6663 - val_loss: 0.5881 - val_acc: 0.6847\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.6060 - acc: 0.6740 - val_loss: 0.5558 - val_acc: 0.7504\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.5956 - acc: 0.7024 - val_loss: 0.5515 - val_acc: 0.7553\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.5842 - acc: 0.7070 - val_loss: 0.5385 - val_acc: 0.7537\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.5759 - acc: 0.7030 - val_loss: 0.5274 - val_acc: 0.7668\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.5616 - acc: 0.7254 - val_loss: 0.5159 - val_acc: 0.7882\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.5537 - acc: 0.7378 - val_loss: 0.5043 - val_acc: 0.8112\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.5460 - acc: 0.7354 - val_loss: 0.4944 - val_acc: 0.8079\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.5356 - acc: 0.7497 - val_loss: 0.4844 - val_acc: 0.8095\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.5261 - acc: 0.7542 - val_loss: 0.4772 - val_acc: 0.8144\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5202 - acc: 0.7575 - val_loss: 0.4666 - val_acc: 0.8128\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5101 - acc: 0.7617 - val_loss: 0.4605 - val_acc: 0.8144\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5087 - acc: 0.7601 - val_loss: 0.4522 - val_acc: 0.8259\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4982 - acc: 0.7699 - val_loss: 0.4462 - val_acc: 0.8227\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4932 - acc: 0.7727 - val_loss: 0.4394 - val_acc: 0.8292\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4837 - acc: 0.7774 - val_loss: 0.4342 - val_acc: 0.8243\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4802 - acc: 0.7809 - val_loss: 0.4270 - val_acc: 0.8276\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4748 - acc: 0.7847 - val_loss: 0.4214 - val_acc: 0.8259\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4710 - acc: 0.7843 - val_loss: 0.4193 - val_acc: 0.8325\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4685 - acc: 0.7849 - val_loss: 0.4171 - val_acc: 0.8276\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4658 - acc: 0.7907 - val_loss: 0.4194 - val_acc: 0.8259\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4623 - acc: 0.7946 - val_loss: 0.4112 - val_acc: 0.8358\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4566 - acc: 0.7953 - val_loss: 0.4115 - val_acc: 0.8259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4519 - acc: 0.7915 - val_loss: 0.4022 - val_acc: 0.8391\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4483 - acc: 0.8009 - val_loss: 0.4030 - val_acc: 0.8407\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4477 - acc: 0.8013 - val_loss: 0.3970 - val_acc: 0.8358\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4411 - acc: 0.8061 - val_loss: 0.3955 - val_acc: 0.8342\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4424 - acc: 0.8037 - val_loss: 0.3986 - val_acc: 0.8374\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4408 - acc: 0.7953 - val_loss: 0.3936 - val_acc: 0.8325\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4376 - acc: 0.8066 - val_loss: 0.3913 - val_acc: 0.8440\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4331 - acc: 0.8088 - val_loss: 0.3914 - val_acc: 0.8456\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.4320 - acc: 0.8055 - val_loss: 0.3887 - val_acc: 0.8407\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4291 - acc: 0.8113 - val_loss: 0.3886 - val_acc: 0.8440\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4253 - acc: 0.8117 - val_loss: 0.3858 - val_acc: 0.8440\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4232 - acc: 0.8174 - val_loss: 0.3896 - val_acc: 0.8407\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4208 - acc: 0.8113 - val_loss: 0.3838 - val_acc: 0.8424\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4187 - acc: 0.8150 - val_loss: 0.3831 - val_acc: 0.8407\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4179 - acc: 0.8148 - val_loss: 0.3821 - val_acc: 0.8391\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4171 - acc: 0.8161 - val_loss: 0.3831 - val_acc: 0.8358\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4132 - acc: 0.8183 - val_loss: 0.3807 - val_acc: 0.8374\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4112 - acc: 0.8210 - val_loss: 0.3804 - val_acc: 0.8391\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4102 - acc: 0.8214 - val_loss: 0.3826 - val_acc: 0.8440\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4058 - acc: 0.8208 - val_loss: 0.3781 - val_acc: 0.8424\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4071 - acc: 0.8163 - val_loss: 0.3773 - val_acc: 0.8456\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4042 - acc: 0.8186 - val_loss: 0.3769 - val_acc: 0.8407\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4028 - acc: 0.8241 - val_loss: 0.3772 - val_acc: 0.8424\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3964 - acc: 0.8272 - val_loss: 0.3792 - val_acc: 0.8456\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3953 - acc: 0.8261 - val_loss: 0.3755 - val_acc: 0.8440\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3951 - acc: 0.8248 - val_loss: 0.3742 - val_acc: 0.8424\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.3933 - acc: 0.8287 - val_loss: 0.3738 - val_acc: 0.8407\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.3920 - acc: 0.8281 - val_loss: 0.3729 - val_acc: 0.8391\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3876 - acc: 0.8272 - val_loss: 0.3754 - val_acc: 0.8506\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3897 - acc: 0.8298 - val_loss: 0.3724 - val_acc: 0.8440\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.3870 - acc: 0.8303 - val_loss: 0.3779 - val_acc: 0.8522\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3867 - acc: 0.8329 - val_loss: 0.3847 - val_acc: 0.8424\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3889 - acc: 0.8305 - val_loss: 0.3706 - val_acc: 0.8473\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3843 - acc: 0.8331 - val_loss: 0.3820 - val_acc: 0.8440\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3963 - acc: 0.8290 - val_loss: 0.3870 - val_acc: 0.8391\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3858 - acc: 0.8331 - val_loss: 0.3705 - val_acc: 0.8473\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3797 - acc: 0.8382 - val_loss: 0.3689 - val_acc: 0.8489\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3772 - acc: 0.8391 - val_loss: 0.3711 - val_acc: 0.8522\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3762 - acc: 0.8367 - val_loss: 0.3729 - val_acc: 0.8506\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3765 - acc: 0.8345 - val_loss: 0.3735 - val_acc: 0.8489\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3746 - acc: 0.8387 - val_loss: 0.3681 - val_acc: 0.8473\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3705 - acc: 0.8402 - val_loss: 0.3681 - val_acc: 0.8473\n",
      "Epoch 72/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3695 - acc: 0.8374 - val_loss: 0.3748 - val_acc: 0.8473\n",
      "Epoch 73/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3680 - acc: 0.8425 - val_loss: 0.3691 - val_acc: 0.8440\n",
      "Epoch 74/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3662 - acc: 0.8415 - val_loss: 0.3689 - val_acc: 0.8539\n",
      "Epoch 75/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3616 - acc: 0.8429 - val_loss: 0.3718 - val_acc: 0.8522\n",
      "Training with parameters {'batch_size': 2000, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_130\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_131 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_390 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_260 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_391 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_261 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_392 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,985\n",
      "Trainable params: 409,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 1.5620 - acc: 0.4324 - val_loss: 0.9310 - val_acc: 0.4253\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.8527 - acc: 0.4454 - val_loss: 0.7069 - val_acc: 0.5764\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.7486 - acc: 0.5413 - val_loss: 0.7731 - val_acc: 0.5747\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.7773 - acc: 0.5678 - val_loss: 0.7526 - val_acc: 0.5747\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.7481 - acc: 0.5667 - val_loss: 0.6964 - val_acc: 0.5747\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.7019 - acc: 0.5711 - val_loss: 0.6619 - val_acc: 0.5846\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6817 - acc: 0.5733 - val_loss: 0.6586 - val_acc: 0.6765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6771 - acc: 0.5733 - val_loss: 0.6568 - val_acc: 0.6782\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.6735 - acc: 0.5723 - val_loss: 0.6431 - val_acc: 0.6962\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.6572 - acc: 0.6105 - val_loss: 0.6302 - val_acc: 0.6519\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.6525 - acc: 0.6221 - val_loss: 0.6249 - val_acc: 0.6059\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6497 - acc: 0.6214 - val_loss: 0.6162 - val_acc: 0.6388\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6407 - acc: 0.6320 - val_loss: 0.6058 - val_acc: 0.7274\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6312 - acc: 0.6561 - val_loss: 0.6002 - val_acc: 0.7307\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6327 - acc: 0.6506 - val_loss: 0.5927 - val_acc: 0.7291\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6246 - acc: 0.6557 - val_loss: 0.5829 - val_acc: 0.7603\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6144 - acc: 0.6794 - val_loss: 0.5754 - val_acc: 0.7488\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.6115 - acc: 0.6738 - val_loss: 0.5683 - val_acc: 0.7635\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6071 - acc: 0.6878 - val_loss: 0.5612 - val_acc: 0.7668\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5982 - acc: 0.6953 - val_loss: 0.5556 - val_acc: 0.7652\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5953 - acc: 0.6866 - val_loss: 0.5490 - val_acc: 0.7767\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5889 - acc: 0.7028 - val_loss: 0.5433 - val_acc: 0.7800\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5850 - acc: 0.7105 - val_loss: 0.5380 - val_acc: 0.7865\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5803 - acc: 0.7090 - val_loss: 0.5325 - val_acc: 0.7947\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5742 - acc: 0.7207 - val_loss: 0.5274 - val_acc: 0.7964\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5723 - acc: 0.7212 - val_loss: 0.5232 - val_acc: 0.7947\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5633 - acc: 0.7294 - val_loss: 0.5179 - val_acc: 0.8062\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5628 - acc: 0.7342 - val_loss: 0.5132 - val_acc: 0.8013\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5637 - acc: 0.7234 - val_loss: 0.5088 - val_acc: 0.8128\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5535 - acc: 0.7327 - val_loss: 0.5035 - val_acc: 0.8112\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5438 - acc: 0.7479 - val_loss: 0.4988 - val_acc: 0.8128\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5456 - acc: 0.7431 - val_loss: 0.4941 - val_acc: 0.8161\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5425 - acc: 0.7502 - val_loss: 0.4887 - val_acc: 0.8161\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5363 - acc: 0.7473 - val_loss: 0.4837 - val_acc: 0.8210\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5255 - acc: 0.7566 - val_loss: 0.4780 - val_acc: 0.8128\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5308 - acc: 0.7491 - val_loss: 0.4735 - val_acc: 0.8095\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5201 - acc: 0.7579 - val_loss: 0.4697 - val_acc: 0.8161\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5222 - acc: 0.7535 - val_loss: 0.4656 - val_acc: 0.8144\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.5189 - acc: 0.7559 - val_loss: 0.4621 - val_acc: 0.8177\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.5138 - acc: 0.7537 - val_loss: 0.4594 - val_acc: 0.8144\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5124 - acc: 0.7592 - val_loss: 0.4556 - val_acc: 0.8210\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5073 - acc: 0.7650 - val_loss: 0.4531 - val_acc: 0.8144\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5048 - acc: 0.7625 - val_loss: 0.4497 - val_acc: 0.8243\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5020 - acc: 0.7663 - val_loss: 0.4470 - val_acc: 0.8194\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4985 - acc: 0.7739 - val_loss: 0.4444 - val_acc: 0.8177\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.4959 - acc: 0.7781 - val_loss: 0.4415 - val_acc: 0.8259\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4904 - acc: 0.7758 - val_loss: 0.4397 - val_acc: 0.8210\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4888 - acc: 0.7772 - val_loss: 0.4366 - val_acc: 0.8276\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4863 - acc: 0.7829 - val_loss: 0.4342 - val_acc: 0.8325\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4787 - acc: 0.7889 - val_loss: 0.4319 - val_acc: 0.8342\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4832 - acc: 0.7769 - val_loss: 0.4297 - val_acc: 0.8358\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4779 - acc: 0.7856 - val_loss: 0.4284 - val_acc: 0.8325\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4807 - acc: 0.7865 - val_loss: 0.4266 - val_acc: 0.8407\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.4743 - acc: 0.7820 - val_loss: 0.4252 - val_acc: 0.8342\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4740 - acc: 0.7891 - val_loss: 0.4226 - val_acc: 0.8424\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4715 - acc: 0.7867 - val_loss: 0.4211 - val_acc: 0.8407\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4637 - acc: 0.7964 - val_loss: 0.4197 - val_acc: 0.8342\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4663 - acc: 0.7902 - val_loss: 0.4182 - val_acc: 0.8391\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4663 - acc: 0.7958 - val_loss: 0.4167 - val_acc: 0.8391\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4611 - acc: 0.7933 - val_loss: 0.4152 - val_acc: 0.8391\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4620 - acc: 0.7922 - val_loss: 0.4139 - val_acc: 0.8391\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4602 - acc: 0.7922 - val_loss: 0.4126 - val_acc: 0.8391\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4590 - acc: 0.7947 - val_loss: 0.4113 - val_acc: 0.8391\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4545 - acc: 0.7977 - val_loss: 0.4103 - val_acc: 0.8391\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4566 - acc: 0.8004 - val_loss: 0.4092 - val_acc: 0.8456\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4524 - acc: 0.7964 - val_loss: 0.4082 - val_acc: 0.8456\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4529 - acc: 0.8017 - val_loss: 0.4079 - val_acc: 0.8440\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4517 - acc: 0.7968 - val_loss: 0.4069 - val_acc: 0.8440\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4499 - acc: 0.7991 - val_loss: 0.4059 - val_acc: 0.8473\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4462 - acc: 0.8042 - val_loss: 0.4051 - val_acc: 0.8440\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4481 - acc: 0.8022 - val_loss: 0.4043 - val_acc: 0.8456\n",
      "Epoch 72/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4482 - acc: 0.8031 - val_loss: 0.4037 - val_acc: 0.8407\n",
      "Epoch 73/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4433 - acc: 0.8048 - val_loss: 0.4030 - val_acc: 0.8407\n",
      "Epoch 74/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4433 - acc: 0.7982 - val_loss: 0.4023 - val_acc: 0.8489\n",
      "Epoch 75/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4429 - acc: 0.8059 - val_loss: 0.4020 - val_acc: 0.8424\n",
      "Epoch 76/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4389 - acc: 0.8070 - val_loss: 0.4014 - val_acc: 0.8506\n",
      "Epoch 77/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4375 - acc: 0.8048 - val_loss: 0.4011 - val_acc: 0.8424\n",
      "Epoch 78/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4349 - acc: 0.8064 - val_loss: 0.4003 - val_acc: 0.8456\n",
      "Epoch 79/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4350 - acc: 0.8099 - val_loss: 0.3995 - val_acc: 0.8424\n",
      "Epoch 80/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4339 - acc: 0.8082 - val_loss: 0.3993 - val_acc: 0.8440\n",
      "Epoch 81/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4330 - acc: 0.8124 - val_loss: 0.3997 - val_acc: 0.8456\n",
      "Epoch 82/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4326 - acc: 0.8070 - val_loss: 0.3984 - val_acc: 0.8440\n",
      "Epoch 83/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4321 - acc: 0.8119 - val_loss: 0.3978 - val_acc: 0.8358\n",
      "Epoch 84/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4302 - acc: 0.8088 - val_loss: 0.3968 - val_acc: 0.8456\n",
      "Epoch 85/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4316 - acc: 0.8101 - val_loss: 0.3970 - val_acc: 0.8440\n",
      "Epoch 86/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4291 - acc: 0.8090 - val_loss: 0.3975 - val_acc: 0.8456\n",
      "Epoch 87/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4257 - acc: 0.8124 - val_loss: 0.3956 - val_acc: 0.8473\n",
      "Epoch 88/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4281 - acc: 0.8115 - val_loss: 0.3953 - val_acc: 0.8424\n",
      "Epoch 89/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.4265 - acc: 0.8104 - val_loss: 0.3940 - val_acc: 0.8440\n",
      "Epoch 90/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.4257 - acc: 0.8123 - val_loss: 0.3937 - val_acc: 0.8456\n",
      "Epoch 91/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4253 - acc: 0.8155 - val_loss: 0.3941 - val_acc: 0.8407\n",
      "Epoch 92/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4249 - acc: 0.8143 - val_loss: 0.3936 - val_acc: 0.8424\n",
      "Epoch 93/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4212 - acc: 0.8159 - val_loss: 0.3935 - val_acc: 0.8424\n",
      "Epoch 94/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4236 - acc: 0.8166 - val_loss: 0.3938 - val_acc: 0.8407\n",
      "Epoch 95/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4190 - acc: 0.8148 - val_loss: 0.3931 - val_acc: 0.8456\n",
      "Epoch 96/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4213 - acc: 0.8168 - val_loss: 0.3925 - val_acc: 0.8407\n",
      "Epoch 97/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4203 - acc: 0.8150 - val_loss: 0.3923 - val_acc: 0.8456\n",
      "Epoch 98/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4180 - acc: 0.8201 - val_loss: 0.3917 - val_acc: 0.8424\n",
      "Epoch 99/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4225 - acc: 0.8157 - val_loss: 0.3921 - val_acc: 0.8391\n",
      "Epoch 100/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4189 - acc: 0.8152 - val_loss: 0.3913 - val_acc: 0.8440\n",
      "Epoch 101/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4155 - acc: 0.8205 - val_loss: 0.3911 - val_acc: 0.8456\n",
      "Epoch 102/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4158 - acc: 0.8177 - val_loss: 0.3910 - val_acc: 0.8456\n",
      "Epoch 103/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4143 - acc: 0.8212 - val_loss: 0.3905 - val_acc: 0.8440\n",
      "Epoch 104/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4172 - acc: 0.8183 - val_loss: 0.3908 - val_acc: 0.8407\n",
      "Epoch 105/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4166 - acc: 0.8177 - val_loss: 0.3899 - val_acc: 0.8456\n",
      "Epoch 106/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4157 - acc: 0.8217 - val_loss: 0.3899 - val_acc: 0.8424\n",
      "Epoch 107/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4154 - acc: 0.8174 - val_loss: 0.3892 - val_acc: 0.8456\n",
      "Epoch 108/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4111 - acc: 0.8225 - val_loss: 0.3892 - val_acc: 0.8440\n",
      "Epoch 109/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4116 - acc: 0.8227 - val_loss: 0.3885 - val_acc: 0.8456\n",
      "Epoch 110/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4107 - acc: 0.8274 - val_loss: 0.3884 - val_acc: 0.8440\n",
      "Epoch 111/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4092 - acc: 0.8256 - val_loss: 0.3882 - val_acc: 0.8456\n",
      "Epoch 112/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4107 - acc: 0.8238 - val_loss: 0.3885 - val_acc: 0.8440\n",
      "Epoch 113/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4099 - acc: 0.8207 - val_loss: 0.3884 - val_acc: 0.8456\n",
      "Epoch 114/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4116 - acc: 0.8203 - val_loss: 0.3884 - val_acc: 0.8456\n",
      "Epoch 115/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4090 - acc: 0.8228 - val_loss: 0.3885 - val_acc: 0.8440\n",
      "Epoch 116/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4082 - acc: 0.8241 - val_loss: 0.3880 - val_acc: 0.8473\n",
      "Epoch 117/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4055 - acc: 0.8254 - val_loss: 0.3877 - val_acc: 0.8456\n",
      "Epoch 118/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4037 - acc: 0.8267 - val_loss: 0.3882 - val_acc: 0.8391\n",
      "Epoch 119/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4066 - acc: 0.8216 - val_loss: 0.3868 - val_acc: 0.8440\n",
      "Epoch 120/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4046 - acc: 0.8245 - val_loss: 0.3869 - val_acc: 0.8424\n",
      "Epoch 121/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4046 - acc: 0.8241 - val_loss: 0.3861 - val_acc: 0.8424\n",
      "Epoch 122/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4054 - acc: 0.8248 - val_loss: 0.3860 - val_acc: 0.8424\n",
      "Epoch 123/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4050 - acc: 0.8269 - val_loss: 0.3861 - val_acc: 0.8424\n",
      "Epoch 124/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4009 - acc: 0.8241 - val_loss: 0.3860 - val_acc: 0.8424\n",
      "Epoch 125/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4033 - acc: 0.8259 - val_loss: 0.3866 - val_acc: 0.8391\n",
      "Epoch 126/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.3978 - acc: 0.8259 - val_loss: 0.3856 - val_acc: 0.8456\n",
      "Epoch 127/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4019 - acc: 0.8256 - val_loss: 0.3858 - val_acc: 0.8391\n",
      "Epoch 128/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.3988 - acc: 0.8307 - val_loss: 0.3855 - val_acc: 0.8440\n",
      "Epoch 129/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4028 - acc: 0.8245 - val_loss: 0.3860 - val_acc: 0.8374\n",
      "Epoch 130/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4012 - acc: 0.8252 - val_loss: 0.3856 - val_acc: 0.8424\n",
      "Epoch 131/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.3982 - acc: 0.8254 - val_loss: 0.3855 - val_acc: 0.8456\n",
      "Epoch 132/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 32ms/step - loss: 0.3959 - acc: 0.8332 - val_loss: 0.3866 - val_acc: 0.8374\n",
      "Epoch 133/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.3955 - acc: 0.8307 - val_loss: 0.3853 - val_acc: 0.8456\n",
      "Epoch 134/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.3986 - acc: 0.8287 - val_loss: 0.3852 - val_acc: 0.8374\n",
      "Epoch 135/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.3947 - acc: 0.8305 - val_loss: 0.3845 - val_acc: 0.8473\n",
      "Epoch 136/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.3968 - acc: 0.8276 - val_loss: 0.3841 - val_acc: 0.8407\n",
      "Epoch 137/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.3936 - acc: 0.8309 - val_loss: 0.3836 - val_acc: 0.8473\n",
      "Epoch 138/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.3942 - acc: 0.8285 - val_loss: 0.3840 - val_acc: 0.8391\n",
      "Epoch 139/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.3919 - acc: 0.8331 - val_loss: 0.3836 - val_acc: 0.8473\n",
      "Epoch 140/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.3952 - acc: 0.8321 - val_loss: 0.3835 - val_acc: 0.8473\n",
      "Epoch 141/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.3916 - acc: 0.8316 - val_loss: 0.3835 - val_acc: 0.8440\n",
      "Epoch 142/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.3857 - acc: 0.8321 - val_loss: 0.3837 - val_acc: 0.8440\n",
      "Epoch 143/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.3885 - acc: 0.8303 - val_loss: 0.3838 - val_acc: 0.8440\n",
      "Epoch 144/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.3851 - acc: 0.8345 - val_loss: 0.3834 - val_acc: 0.8424\n",
      "Epoch 145/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.3871 - acc: 0.8356 - val_loss: 0.3835 - val_acc: 0.8407\n",
      "Epoch 146/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.3873 - acc: 0.8331 - val_loss: 0.3832 - val_acc: 0.8473\n",
      "Epoch 147/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.3887 - acc: 0.8305 - val_loss: 0.3833 - val_acc: 0.8424\n",
      "Epoch 148/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.3866 - acc: 0.8334 - val_loss: 0.3829 - val_acc: 0.8473\n",
      "Epoch 149/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.3859 - acc: 0.8342 - val_loss: 0.3830 - val_acc: 0.8424\n",
      "Epoch 150/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.3855 - acc: 0.8325 - val_loss: 0.3827 - val_acc: 0.8440\n",
      "Epoch 151/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.3862 - acc: 0.8300 - val_loss: 0.3829 - val_acc: 0.8424\n",
      "Epoch 152/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.3876 - acc: 0.8352 - val_loss: 0.3833 - val_acc: 0.8456\n",
      "Epoch 153/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.3828 - acc: 0.8369 - val_loss: 0.3829 - val_acc: 0.8407\n",
      "Epoch 154/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.3850 - acc: 0.8300 - val_loss: 0.3825 - val_acc: 0.8440\n",
      "Epoch 155/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.3831 - acc: 0.8385 - val_loss: 0.3821 - val_acc: 0.8407\n",
      "Epoch 156/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.3853 - acc: 0.8331 - val_loss: 0.3824 - val_acc: 0.8440\n",
      "Epoch 157/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.3843 - acc: 0.8342 - val_loss: 0.3829 - val_acc: 0.8440\n",
      "Epoch 158/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.3791 - acc: 0.8391 - val_loss: 0.3826 - val_acc: 0.8440\n",
      "Epoch 159/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.3821 - acc: 0.8363 - val_loss: 0.3821 - val_acc: 0.8407\n",
      "Epoch 160/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.3798 - acc: 0.8363 - val_loss: 0.3823 - val_acc: 0.8440\n",
      "Epoch 161/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.3814 - acc: 0.8354 - val_loss: 0.3827 - val_acc: 0.8424\n",
      "Epoch 162/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.3791 - acc: 0.8369 - val_loss: 0.3842 - val_acc: 0.8440\n",
      "Epoch 163/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.3785 - acc: 0.8363 - val_loss: 0.3827 - val_acc: 0.8424\n",
      "Epoch 164/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.3766 - acc: 0.8409 - val_loss: 0.3830 - val_acc: 0.8424\n",
      "Training with parameters {'batch_size': 2000, 'dropout': 0.1, 'lay_conf': [1024, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_131\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_132 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_393 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_262 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_394 (Dense)            (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_263 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_395 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,639,553\n",
      "Trainable params: 1,639,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 0.7864 - acc: 0.4990 - val_loss: 0.7077 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6927 - acc: 0.5743 - val_loss: 0.6529 - val_acc: 0.6535\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.6750 - acc: 0.5847 - val_loss: 0.6149 - val_acc: 0.7077\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.6338 - acc: 0.6448 - val_loss: 0.6070 - val_acc: 0.6223\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.6227 - acc: 0.6586 - val_loss: 0.5731 - val_acc: 0.7291\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.6032 - acc: 0.6827 - val_loss: 0.5600 - val_acc: 0.7209\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.5914 - acc: 0.6979 - val_loss: 0.5465 - val_acc: 0.7734\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.5779 - acc: 0.7210 - val_loss: 0.5265 - val_acc: 0.7570\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.5659 - acc: 0.7103 - val_loss: 0.5097 - val_acc: 0.7833\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.5517 - acc: 0.7344 - val_loss: 0.4964 - val_acc: 0.8079\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.5440 - acc: 0.7417 - val_loss: 0.4847 - val_acc: 0.8144\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.5353 - acc: 0.7486 - val_loss: 0.4751 - val_acc: 0.8194\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.5247 - acc: 0.7493 - val_loss: 0.4664 - val_acc: 0.8227\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.5118 - acc: 0.7635 - val_loss: 0.4594 - val_acc: 0.8243\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.5077 - acc: 0.7666 - val_loss: 0.4515 - val_acc: 0.8243\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.5012 - acc: 0.7701 - val_loss: 0.4433 - val_acc: 0.8243\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.4991 - acc: 0.7652 - val_loss: 0.4372 - val_acc: 0.8243\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.4921 - acc: 0.7723 - val_loss: 0.4314 - val_acc: 0.8292\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.4874 - acc: 0.7772 - val_loss: 0.4277 - val_acc: 0.8342\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.4823 - acc: 0.7763 - val_loss: 0.4227 - val_acc: 0.8292\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.4764 - acc: 0.7794 - val_loss: 0.4184 - val_acc: 0.8309\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.4705 - acc: 0.7905 - val_loss: 0.4148 - val_acc: 0.8342\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.4681 - acc: 0.7867 - val_loss: 0.4152 - val_acc: 0.8374\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.4621 - acc: 0.7924 - val_loss: 0.4125 - val_acc: 0.8292\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.4613 - acc: 0.7880 - val_loss: 0.4122 - val_acc: 0.8358\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4600 - acc: 0.7955 - val_loss: 0.4069 - val_acc: 0.8309\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4587 - acc: 0.7929 - val_loss: 0.4074 - val_acc: 0.8342\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4519 - acc: 0.8028 - val_loss: 0.4063 - val_acc: 0.8342\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4499 - acc: 0.7935 - val_loss: 0.4100 - val_acc: 0.8407\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4533 - acc: 0.7935 - val_loss: 0.4077 - val_acc: 0.8309\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4523 - acc: 0.7937 - val_loss: 0.4016 - val_acc: 0.8407\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4434 - acc: 0.8046 - val_loss: 0.3975 - val_acc: 0.8424\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4380 - acc: 0.8061 - val_loss: 0.3968 - val_acc: 0.8424\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4406 - acc: 0.8055 - val_loss: 0.3914 - val_acc: 0.8506\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4335 - acc: 0.8073 - val_loss: 0.3935 - val_acc: 0.8440\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4394 - acc: 0.8015 - val_loss: 0.3885 - val_acc: 0.8539\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4290 - acc: 0.8077 - val_loss: 0.3873 - val_acc: 0.8506\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4325 - acc: 0.8141 - val_loss: 0.3869 - val_acc: 0.8506\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4310 - acc: 0.8070 - val_loss: 0.3853 - val_acc: 0.8489\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4263 - acc: 0.8126 - val_loss: 0.3837 - val_acc: 0.8522\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4256 - acc: 0.8128 - val_loss: 0.3854 - val_acc: 0.8456\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4239 - acc: 0.8099 - val_loss: 0.3840 - val_acc: 0.8539\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4213 - acc: 0.8132 - val_loss: 0.3827 - val_acc: 0.8539\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4229 - acc: 0.8113 - val_loss: 0.3841 - val_acc: 0.8522\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4183 - acc: 0.8139 - val_loss: 0.3863 - val_acc: 0.8489\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4195 - acc: 0.8137 - val_loss: 0.3830 - val_acc: 0.8539\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4189 - acc: 0.8139 - val_loss: 0.3817 - val_acc: 0.8506\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4120 - acc: 0.8186 - val_loss: 0.3821 - val_acc: 0.8539\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4169 - acc: 0.8143 - val_loss: 0.3793 - val_acc: 0.8506\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4092 - acc: 0.8210 - val_loss: 0.3777 - val_acc: 0.8539\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4062 - acc: 0.8256 - val_loss: 0.3788 - val_acc: 0.8522\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4081 - acc: 0.8194 - val_loss: 0.3789 - val_acc: 0.8489\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4073 - acc: 0.8172 - val_loss: 0.3769 - val_acc: 0.8555\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4068 - acc: 0.8219 - val_loss: 0.3740 - val_acc: 0.8522\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3999 - acc: 0.8238 - val_loss: 0.3741 - val_acc: 0.8555\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4041 - acc: 0.8245 - val_loss: 0.3734 - val_acc: 0.8522\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4048 - acc: 0.8267 - val_loss: 0.3758 - val_acc: 0.8489\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3992 - acc: 0.8261 - val_loss: 0.3857 - val_acc: 0.8489\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4071 - acc: 0.8261 - val_loss: 0.3878 - val_acc: 0.8391\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4034 - acc: 0.8212 - val_loss: 0.3803 - val_acc: 0.8473\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.3983 - acc: 0.8261 - val_loss: 0.3797 - val_acc: 0.8407\n",
      "Training with parameters {'batch_size': 2000, 'dropout': 0.1, 'lay_conf': [1024, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_132\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_133 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_396 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_264 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_397 (Dense)            (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_265 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_398 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,606,721\n",
      "Trainable params: 1,606,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.8045 - acc: 0.5132 - val_loss: 0.7278 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.7057 - acc: 0.5774 - val_loss: 0.6635 - val_acc: 0.6059\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.6887 - acc: 0.5669 - val_loss: 0.6156 - val_acc: 0.7028\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.6436 - acc: 0.6315 - val_loss: 0.6078 - val_acc: 0.6486\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.6287 - acc: 0.6557 - val_loss: 0.5798 - val_acc: 0.7176\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.6132 - acc: 0.6752 - val_loss: 0.5568 - val_acc: 0.7570\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.5967 - acc: 0.6866 - val_loss: 0.5364 - val_acc: 0.7734\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.5769 - acc: 0.7063 - val_loss: 0.5154 - val_acc: 0.7800\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.5665 - acc: 0.7156 - val_loss: 0.5039 - val_acc: 0.8046\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.5562 - acc: 0.7294 - val_loss: 0.4856 - val_acc: 0.8095\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.5454 - acc: 0.7303 - val_loss: 0.4804 - val_acc: 0.7997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.5348 - acc: 0.7521 - val_loss: 0.4657 - val_acc: 0.8128\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.5238 - acc: 0.7528 - val_loss: 0.4577 - val_acc: 0.8177\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.5170 - acc: 0.7623 - val_loss: 0.4484 - val_acc: 0.8194\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.5034 - acc: 0.7657 - val_loss: 0.4404 - val_acc: 0.8177\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.5036 - acc: 0.7681 - val_loss: 0.4323 - val_acc: 0.8210\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.4899 - acc: 0.7723 - val_loss: 0.4258 - val_acc: 0.8309\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.4880 - acc: 0.7725 - val_loss: 0.4204 - val_acc: 0.8276\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4823 - acc: 0.7783 - val_loss: 0.4174 - val_acc: 0.8391\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4771 - acc: 0.7803 - val_loss: 0.4123 - val_acc: 0.8342\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4750 - acc: 0.7833 - val_loss: 0.4116 - val_acc: 0.8374\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4687 - acc: 0.7864 - val_loss: 0.4073 - val_acc: 0.8325\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4684 - acc: 0.7860 - val_loss: 0.4055 - val_acc: 0.8391\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4660 - acc: 0.7858 - val_loss: 0.3997 - val_acc: 0.8391\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4579 - acc: 0.7969 - val_loss: 0.3977 - val_acc: 0.8342\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4555 - acc: 0.7942 - val_loss: 0.3951 - val_acc: 0.8407\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4504 - acc: 0.7975 - val_loss: 0.3932 - val_acc: 0.8407\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4466 - acc: 0.8017 - val_loss: 0.3915 - val_acc: 0.8424\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4433 - acc: 0.8035 - val_loss: 0.3892 - val_acc: 0.8506\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4434 - acc: 0.8022 - val_loss: 0.3930 - val_acc: 0.8407\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4424 - acc: 0.7986 - val_loss: 0.3908 - val_acc: 0.8456\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4376 - acc: 0.8051 - val_loss: 0.3973 - val_acc: 0.8325\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4350 - acc: 0.8072 - val_loss: 0.3862 - val_acc: 0.8407\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4343 - acc: 0.8086 - val_loss: 0.3863 - val_acc: 0.8456\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4347 - acc: 0.8090 - val_loss: 0.3814 - val_acc: 0.8473\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4242 - acc: 0.8108 - val_loss: 0.3823 - val_acc: 0.8522\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4260 - acc: 0.8095 - val_loss: 0.3796 - val_acc: 0.8506\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4223 - acc: 0.8163 - val_loss: 0.3787 - val_acc: 0.8506\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4197 - acc: 0.8172 - val_loss: 0.3777 - val_acc: 0.8506\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4222 - acc: 0.8135 - val_loss: 0.3775 - val_acc: 0.8506\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4176 - acc: 0.8208 - val_loss: 0.3767 - val_acc: 0.8522\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4197 - acc: 0.8144 - val_loss: 0.3756 - val_acc: 0.8506\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4176 - acc: 0.8196 - val_loss: 0.3756 - val_acc: 0.8522\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4091 - acc: 0.8217 - val_loss: 0.3737 - val_acc: 0.8506\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4162 - acc: 0.8172 - val_loss: 0.3723 - val_acc: 0.8506\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4109 - acc: 0.8168 - val_loss: 0.3746 - val_acc: 0.8539\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4080 - acc: 0.8217 - val_loss: 0.3758 - val_acc: 0.8506\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4057 - acc: 0.8203 - val_loss: 0.3730 - val_acc: 0.8506\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4056 - acc: 0.8236 - val_loss: 0.3711 - val_acc: 0.8506\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4052 - acc: 0.8236 - val_loss: 0.3715 - val_acc: 0.8506\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4063 - acc: 0.8225 - val_loss: 0.3753 - val_acc: 0.8506\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4044 - acc: 0.8230 - val_loss: 0.3733 - val_acc: 0.8555\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4069 - acc: 0.8203 - val_loss: 0.3696 - val_acc: 0.8506\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.3992 - acc: 0.8216 - val_loss: 0.3699 - val_acc: 0.8539\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.3968 - acc: 0.8243 - val_loss: 0.3721 - val_acc: 0.8539\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.3968 - acc: 0.8280 - val_loss: 0.3718 - val_acc: 0.8506\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.3946 - acc: 0.8248 - val_loss: 0.3691 - val_acc: 0.8539\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.3947 - acc: 0.8245 - val_loss: 0.3698 - val_acc: 0.8555\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3907 - acc: 0.8309 - val_loss: 0.3813 - val_acc: 0.8456\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.3987 - acc: 0.8265 - val_loss: 0.3751 - val_acc: 0.8489\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.3921 - acc: 0.8305 - val_loss: 0.3693 - val_acc: 0.8506\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.3913 - acc: 0.8334 - val_loss: 0.3687 - val_acc: 0.8522\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.3911 - acc: 0.8298 - val_loss: 0.3730 - val_acc: 0.8522\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.3909 - acc: 0.8276 - val_loss: 0.3696 - val_acc: 0.8489\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3866 - acc: 0.8303 - val_loss: 0.3678 - val_acc: 0.8588\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.3863 - acc: 0.8316 - val_loss: 0.3669 - val_acc: 0.8571\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3816 - acc: 0.8336 - val_loss: 0.3675 - val_acc: 0.8506\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.3801 - acc: 0.8323 - val_loss: 0.3671 - val_acc: 0.8571\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.3809 - acc: 0.8367 - val_loss: 0.3675 - val_acc: 0.8588\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.3772 - acc: 0.8380 - val_loss: 0.3671 - val_acc: 0.8539\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.3774 - acc: 0.8387 - val_loss: 0.3682 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 2000, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_133\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_134 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_399 (Dense)            (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_266 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_400 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_267 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_401 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,050,113\n",
      "Trainable params: 1,050,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "3/3 [==============================] - 2s 226ms/step - loss: 0.7320 - acc: 0.5056 - val_loss: 0.6910 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.6841 - acc: 0.5760 - val_loss: 0.6507 - val_acc: 0.6535\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.6601 - acc: 0.6158 - val_loss: 0.6161 - val_acc: 0.6683\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.6312 - acc: 0.6475 - val_loss: 0.5956 - val_acc: 0.6831\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6170 - acc: 0.6634 - val_loss: 0.5729 - val_acc: 0.7209\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6025 - acc: 0.6738 - val_loss: 0.5574 - val_acc: 0.7356\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.5844 - acc: 0.7081 - val_loss: 0.5416 - val_acc: 0.7783\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.5725 - acc: 0.7214 - val_loss: 0.5302 - val_acc: 0.7586\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.5590 - acc: 0.7249 - val_loss: 0.5126 - val_acc: 0.7915\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5488 - acc: 0.7469 - val_loss: 0.5005 - val_acc: 0.7931\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.5401 - acc: 0.7473 - val_loss: 0.4912 - val_acc: 0.8062\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.5266 - acc: 0.7539 - val_loss: 0.4790 - val_acc: 0.8161\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5164 - acc: 0.7637 - val_loss: 0.4706 - val_acc: 0.8095\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5043 - acc: 0.7750 - val_loss: 0.4596 - val_acc: 0.8194\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4979 - acc: 0.7690 - val_loss: 0.4518 - val_acc: 0.8144\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4942 - acc: 0.7681 - val_loss: 0.4437 - val_acc: 0.8259\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4882 - acc: 0.7765 - val_loss: 0.4406 - val_acc: 0.8227\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4782 - acc: 0.7831 - val_loss: 0.4337 - val_acc: 0.8227\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4789 - acc: 0.7838 - val_loss: 0.4246 - val_acc: 0.8309\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4705 - acc: 0.7885 - val_loss: 0.4197 - val_acc: 0.8374\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4673 - acc: 0.7876 - val_loss: 0.4175 - val_acc: 0.8309\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4606 - acc: 0.7953 - val_loss: 0.4138 - val_acc: 0.8424\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4603 - acc: 0.7911 - val_loss: 0.4097 - val_acc: 0.8391\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4528 - acc: 0.8009 - val_loss: 0.4098 - val_acc: 0.8391\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4501 - acc: 0.7986 - val_loss: 0.4046 - val_acc: 0.8407\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4478 - acc: 0.8013 - val_loss: 0.4037 - val_acc: 0.8407\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4410 - acc: 0.8048 - val_loss: 0.4000 - val_acc: 0.8440\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4429 - acc: 0.8019 - val_loss: 0.3979 - val_acc: 0.8456\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4382 - acc: 0.8093 - val_loss: 0.3962 - val_acc: 0.8374\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4368 - acc: 0.8042 - val_loss: 0.3993 - val_acc: 0.8358\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4364 - acc: 0.8061 - val_loss: 0.3940 - val_acc: 0.8424\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4307 - acc: 0.8079 - val_loss: 0.3919 - val_acc: 0.8440\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4245 - acc: 0.8163 - val_loss: 0.3914 - val_acc: 0.8473\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4265 - acc: 0.8141 - val_loss: 0.3892 - val_acc: 0.8522\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4274 - acc: 0.8108 - val_loss: 0.3900 - val_acc: 0.8440\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4216 - acc: 0.8143 - val_loss: 0.3878 - val_acc: 0.8489\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4178 - acc: 0.8186 - val_loss: 0.3864 - val_acc: 0.8506\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4187 - acc: 0.8148 - val_loss: 0.3922 - val_acc: 0.8407\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4185 - acc: 0.8157 - val_loss: 0.3987 - val_acc: 0.8424\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4194 - acc: 0.8143 - val_loss: 0.3866 - val_acc: 0.8440\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4105 - acc: 0.8217 - val_loss: 0.3851 - val_acc: 0.8522\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4088 - acc: 0.8217 - val_loss: 0.3831 - val_acc: 0.8522\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4066 - acc: 0.8212 - val_loss: 0.3826 - val_acc: 0.8539\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4057 - acc: 0.8258 - val_loss: 0.3826 - val_acc: 0.8621\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4017 - acc: 0.8205 - val_loss: 0.3831 - val_acc: 0.8440\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.3999 - acc: 0.8285 - val_loss: 0.3874 - val_acc: 0.8555\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4037 - acc: 0.8230 - val_loss: 0.3861 - val_acc: 0.8440\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.3992 - acc: 0.8223 - val_loss: 0.3933 - val_acc: 0.8407\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4083 - acc: 0.8210 - val_loss: 0.3807 - val_acc: 0.8473\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.3985 - acc: 0.8252 - val_loss: 0.3799 - val_acc: 0.8670\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3976 - acc: 0.8290 - val_loss: 0.3775 - val_acc: 0.8489\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.3945 - acc: 0.8272 - val_loss: 0.3772 - val_acc: 0.8522\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.3883 - acc: 0.8289 - val_loss: 0.3771 - val_acc: 0.8555\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.3880 - acc: 0.8298 - val_loss: 0.3759 - val_acc: 0.8506\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.3851 - acc: 0.8340 - val_loss: 0.3765 - val_acc: 0.8604\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.3846 - acc: 0.8311 - val_loss: 0.3756 - val_acc: 0.8539\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.3810 - acc: 0.8351 - val_loss: 0.3827 - val_acc: 0.8506\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.3829 - acc: 0.8305 - val_loss: 0.3774 - val_acc: 0.8456\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.3795 - acc: 0.8384 - val_loss: 0.3792 - val_acc: 0.8588\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.3806 - acc: 0.8358 - val_loss: 0.3744 - val_acc: 0.8522\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.3796 - acc: 0.8351 - val_loss: 0.3735 - val_acc: 0.8539\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.3762 - acc: 0.8391 - val_loss: 0.3795 - val_acc: 0.8571\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.3740 - acc: 0.8345 - val_loss: 0.3742 - val_acc: 0.8489\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.3739 - acc: 0.8396 - val_loss: 0.3737 - val_acc: 0.8506\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.3721 - acc: 0.8387 - val_loss: 0.3818 - val_acc: 0.8522\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.3703 - acc: 0.8354 - val_loss: 0.3778 - val_acc: 0.8440\n",
      "Training with parameters {'batch_size': 2000, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_134\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_135 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_402 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_268 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_403 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_269 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_404 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 401,729\n",
      "Trainable params: 401,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 1s 60ms/step - loss: 0.9606 - acc: 0.4288 - val_loss: 0.7142 - val_acc: 0.4384\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.7192 - acc: 0.5063 - val_loss: 0.7012 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.7130 - acc: 0.5546 - val_loss: 0.6845 - val_acc: 0.5747\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6936 - acc: 0.5638 - val_loss: 0.6636 - val_acc: 0.5764\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6769 - acc: 0.5751 - val_loss: 0.6569 - val_acc: 0.6437\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6707 - acc: 0.5919 - val_loss: 0.6510 - val_acc: 0.6814\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6657 - acc: 0.6121 - val_loss: 0.6379 - val_acc: 0.7077\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.6532 - acc: 0.6218 - val_loss: 0.6264 - val_acc: 0.6798\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6491 - acc: 0.6300 - val_loss: 0.6177 - val_acc: 0.6798\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.6447 - acc: 0.6311 - val_loss: 0.6085 - val_acc: 0.7258\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6337 - acc: 0.6554 - val_loss: 0.6011 - val_acc: 0.7307\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6280 - acc: 0.6577 - val_loss: 0.5921 - val_acc: 0.7307\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6223 - acc: 0.6676 - val_loss: 0.5836 - val_acc: 0.7274\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6144 - acc: 0.6763 - val_loss: 0.5759 - val_acc: 0.7291\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6080 - acc: 0.6804 - val_loss: 0.5685 - val_acc: 0.7356\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6049 - acc: 0.6840 - val_loss: 0.5613 - val_acc: 0.7471\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.5995 - acc: 0.6931 - val_loss: 0.5533 - val_acc: 0.7488\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.5934 - acc: 0.7011 - val_loss: 0.5458 - val_acc: 0.7553\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5866 - acc: 0.7094 - val_loss: 0.5384 - val_acc: 0.7570\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5798 - acc: 0.7033 - val_loss: 0.5315 - val_acc: 0.7537\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5753 - acc: 0.7112 - val_loss: 0.5252 - val_acc: 0.7750\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5626 - acc: 0.7289 - val_loss: 0.5187 - val_acc: 0.7882\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5634 - acc: 0.7241 - val_loss: 0.5125 - val_acc: 0.7915\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5571 - acc: 0.7287 - val_loss: 0.5066 - val_acc: 0.8013\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5527 - acc: 0.7353 - val_loss: 0.5012 - val_acc: 0.7947\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5439 - acc: 0.7400 - val_loss: 0.4961 - val_acc: 0.8030\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5438 - acc: 0.7460 - val_loss: 0.4913 - val_acc: 0.8062\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5345 - acc: 0.7521 - val_loss: 0.4867 - val_acc: 0.7980\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5337 - acc: 0.7475 - val_loss: 0.4808 - val_acc: 0.8079\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5249 - acc: 0.7590 - val_loss: 0.4763 - val_acc: 0.8079\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5258 - acc: 0.7555 - val_loss: 0.4724 - val_acc: 0.8079\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5189 - acc: 0.7597 - val_loss: 0.4682 - val_acc: 0.8079\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5170 - acc: 0.7628 - val_loss: 0.4645 - val_acc: 0.8144\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5122 - acc: 0.7681 - val_loss: 0.4603 - val_acc: 0.8112\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5049 - acc: 0.7723 - val_loss: 0.4555 - val_acc: 0.8128\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5065 - acc: 0.7718 - val_loss: 0.4520 - val_acc: 0.8161\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5020 - acc: 0.7648 - val_loss: 0.4478 - val_acc: 0.8161\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5004 - acc: 0.7716 - val_loss: 0.4451 - val_acc: 0.8210\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4950 - acc: 0.7776 - val_loss: 0.4418 - val_acc: 0.8210\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4875 - acc: 0.7805 - val_loss: 0.4389 - val_acc: 0.8243\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4858 - acc: 0.7800 - val_loss: 0.4370 - val_acc: 0.8342\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4828 - acc: 0.7754 - val_loss: 0.4337 - val_acc: 0.8259\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4825 - acc: 0.7851 - val_loss: 0.4316 - val_acc: 0.8342\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4768 - acc: 0.7882 - val_loss: 0.4287 - val_acc: 0.8259\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4756 - acc: 0.7860 - val_loss: 0.4268 - val_acc: 0.8358\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4740 - acc: 0.7833 - val_loss: 0.4246 - val_acc: 0.8358\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4707 - acc: 0.7889 - val_loss: 0.4228 - val_acc: 0.8276\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4688 - acc: 0.7957 - val_loss: 0.4222 - val_acc: 0.8325\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4691 - acc: 0.7940 - val_loss: 0.4196 - val_acc: 0.8325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4680 - acc: 0.7969 - val_loss: 0.4180 - val_acc: 0.8342\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4615 - acc: 0.7971 - val_loss: 0.4169 - val_acc: 0.8309\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4643 - acc: 0.7955 - val_loss: 0.4155 - val_acc: 0.8292\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4579 - acc: 0.7966 - val_loss: 0.4148 - val_acc: 0.8292\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4600 - acc: 0.7991 - val_loss: 0.4128 - val_acc: 0.8325\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4549 - acc: 0.8019 - val_loss: 0.4119 - val_acc: 0.8342\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4551 - acc: 0.7980 - val_loss: 0.4108 - val_acc: 0.8358\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4531 - acc: 0.8008 - val_loss: 0.4093 - val_acc: 0.8325\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4526 - acc: 0.7999 - val_loss: 0.4081 - val_acc: 0.8374\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4440 - acc: 0.8093 - val_loss: 0.4067 - val_acc: 0.8374\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4468 - acc: 0.8030 - val_loss: 0.4062 - val_acc: 0.8309\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4433 - acc: 0.8061 - val_loss: 0.4042 - val_acc: 0.8358\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4412 - acc: 0.8050 - val_loss: 0.4050 - val_acc: 0.8325\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4479 - acc: 0.8019 - val_loss: 0.4025 - val_acc: 0.8342\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4470 - acc: 0.8042 - val_loss: 0.4018 - val_acc: 0.8358\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4442 - acc: 0.8030 - val_loss: 0.4021 - val_acc: 0.8358\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4360 - acc: 0.8079 - val_loss: 0.4010 - val_acc: 0.8358\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4406 - acc: 0.8042 - val_loss: 0.4011 - val_acc: 0.8325\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4375 - acc: 0.8079 - val_loss: 0.3988 - val_acc: 0.8407\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4351 - acc: 0.8084 - val_loss: 0.4000 - val_acc: 0.8276\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4355 - acc: 0.8062 - val_loss: 0.3973 - val_acc: 0.8407\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4376 - acc: 0.8082 - val_loss: 0.3968 - val_acc: 0.8325\n",
      "Epoch 72/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4334 - acc: 0.8099 - val_loss: 0.3951 - val_acc: 0.8407\n",
      "Epoch 73/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4323 - acc: 0.8097 - val_loss: 0.3950 - val_acc: 0.8342\n",
      "Epoch 74/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4317 - acc: 0.8073 - val_loss: 0.3950 - val_acc: 0.8374\n",
      "Epoch 75/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4247 - acc: 0.8174 - val_loss: 0.3953 - val_acc: 0.8407\n",
      "Epoch 76/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4267 - acc: 0.8166 - val_loss: 0.3965 - val_acc: 0.8342\n",
      "Epoch 77/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4275 - acc: 0.8134 - val_loss: 0.3926 - val_acc: 0.8440\n",
      "Epoch 78/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4237 - acc: 0.8154 - val_loss: 0.3923 - val_acc: 0.8358\n",
      "Epoch 79/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4260 - acc: 0.8092 - val_loss: 0.3901 - val_acc: 0.8424\n",
      "Epoch 80/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4211 - acc: 0.8155 - val_loss: 0.3912 - val_acc: 0.8374\n",
      "Epoch 81/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4247 - acc: 0.8185 - val_loss: 0.3891 - val_acc: 0.8358\n",
      "Epoch 82/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4209 - acc: 0.8177 - val_loss: 0.3898 - val_acc: 0.8358\n",
      "Epoch 83/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4197 - acc: 0.8183 - val_loss: 0.3885 - val_acc: 0.8358\n",
      "Epoch 84/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4188 - acc: 0.8245 - val_loss: 0.3881 - val_acc: 0.8358\n",
      "Epoch 85/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4170 - acc: 0.8216 - val_loss: 0.3864 - val_acc: 0.8342\n",
      "Epoch 86/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4143 - acc: 0.8217 - val_loss: 0.3856 - val_acc: 0.8358\n",
      "Epoch 87/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4127 - acc: 0.8241 - val_loss: 0.3854 - val_acc: 0.8374\n",
      "Epoch 88/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4126 - acc: 0.8197 - val_loss: 0.3853 - val_acc: 0.8391\n",
      "Epoch 89/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4097 - acc: 0.8217 - val_loss: 0.3851 - val_acc: 0.8407\n",
      "Epoch 90/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4141 - acc: 0.8225 - val_loss: 0.3844 - val_acc: 0.8374\n",
      "Epoch 91/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4132 - acc: 0.8210 - val_loss: 0.3850 - val_acc: 0.8407\n",
      "Epoch 92/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4096 - acc: 0.8261 - val_loss: 0.3843 - val_acc: 0.8424\n",
      "Epoch 93/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4068 - acc: 0.8252 - val_loss: 0.3838 - val_acc: 0.8358\n",
      "Epoch 94/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4081 - acc: 0.8234 - val_loss: 0.3836 - val_acc: 0.8424\n",
      "Epoch 95/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4107 - acc: 0.8232 - val_loss: 0.3842 - val_acc: 0.8391\n",
      "Epoch 96/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4065 - acc: 0.8217 - val_loss: 0.3841 - val_acc: 0.8407\n",
      "Epoch 97/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4081 - acc: 0.8199 - val_loss: 0.3836 - val_acc: 0.8424\n",
      "Epoch 98/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4054 - acc: 0.8274 - val_loss: 0.3827 - val_acc: 0.8374\n",
      "Epoch 99/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4029 - acc: 0.8248 - val_loss: 0.3820 - val_acc: 0.8440\n",
      "Epoch 100/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4031 - acc: 0.8241 - val_loss: 0.3819 - val_acc: 0.8424\n",
      "Epoch 101/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4009 - acc: 0.8296 - val_loss: 0.3825 - val_acc: 0.8407\n",
      "Epoch 102/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4025 - acc: 0.8294 - val_loss: 0.3829 - val_acc: 0.8440\n",
      "Epoch 103/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4064 - acc: 0.8232 - val_loss: 0.3846 - val_acc: 0.8424\n",
      "Epoch 104/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4028 - acc: 0.8267 - val_loss: 0.3817 - val_acc: 0.8424\n",
      "Epoch 105/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.3968 - acc: 0.8281 - val_loss: 0.3816 - val_acc: 0.8391\n",
      "Epoch 106/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4008 - acc: 0.8314 - val_loss: 0.3804 - val_acc: 0.8440\n",
      "Epoch 107/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.3987 - acc: 0.8311 - val_loss: 0.3804 - val_acc: 0.8440\n",
      "Epoch 108/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.3977 - acc: 0.8318 - val_loss: 0.3803 - val_acc: 0.8456\n",
      "Epoch 109/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.3953 - acc: 0.8301 - val_loss: 0.3812 - val_acc: 0.8407\n",
      "Epoch 110/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.3908 - acc: 0.8325 - val_loss: 0.3811 - val_acc: 0.8424\n",
      "Epoch 111/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.3953 - acc: 0.8278 - val_loss: 0.3809 - val_acc: 0.8440\n",
      "Epoch 112/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.3954 - acc: 0.8256 - val_loss: 0.3788 - val_acc: 0.8456\n",
      "Epoch 113/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.3892 - acc: 0.8320 - val_loss: 0.3792 - val_acc: 0.8456\n",
      "Epoch 114/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.3932 - acc: 0.8269 - val_loss: 0.3789 - val_acc: 0.8456\n",
      "Epoch 115/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.3917 - acc: 0.8338 - val_loss: 0.3788 - val_acc: 0.8456\n",
      "Epoch 116/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.3912 - acc: 0.8303 - val_loss: 0.3788 - val_acc: 0.8456\n",
      "Epoch 117/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.3901 - acc: 0.8327 - val_loss: 0.3790 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 2000, 'dropout': 0.1, 'lay_conf': [1024, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_135\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_136 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_405 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_270 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_406 (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_271 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_407 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,836,545\n",
      "Trainable params: 1,836,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.8566 - acc: 0.4970 - val_loss: 0.8215 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.7823 - acc: 0.5680 - val_loss: 0.6483 - val_acc: 0.5764\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.6689 - acc: 0.5953 - val_loss: 0.6809 - val_acc: 0.5665\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.6879 - acc: 0.5718 - val_loss: 0.6191 - val_acc: 0.6716\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.6358 - acc: 0.6504 - val_loss: 0.6121 - val_acc: 0.5977\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6395 - acc: 0.6196 - val_loss: 0.5940 - val_acc: 0.6782\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6121 - acc: 0.6714 - val_loss: 0.5784 - val_acc: 0.7225\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.6072 - acc: 0.6703 - val_loss: 0.5741 - val_acc: 0.7159\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5951 - acc: 0.6856 - val_loss: 0.5496 - val_acc: 0.7701\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5829 - acc: 0.7176 - val_loss: 0.5411 - val_acc: 0.7718\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.5744 - acc: 0.7227 - val_loss: 0.5312 - val_acc: 0.7668\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5639 - acc: 0.7141 - val_loss: 0.5179 - val_acc: 0.7865\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.5524 - acc: 0.7362 - val_loss: 0.5054 - val_acc: 0.8030\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5452 - acc: 0.7449 - val_loss: 0.4963 - val_acc: 0.7964\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5327 - acc: 0.7453 - val_loss: 0.4853 - val_acc: 0.8079\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5221 - acc: 0.7594 - val_loss: 0.4743 - val_acc: 0.8128\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5150 - acc: 0.7604 - val_loss: 0.4669 - val_acc: 0.8079\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5109 - acc: 0.7610 - val_loss: 0.4590 - val_acc: 0.8128\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5023 - acc: 0.7677 - val_loss: 0.4521 - val_acc: 0.8177\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4951 - acc: 0.7776 - val_loss: 0.4455 - val_acc: 0.8161\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4919 - acc: 0.7745 - val_loss: 0.4396 - val_acc: 0.8177\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4832 - acc: 0.7785 - val_loss: 0.4369 - val_acc: 0.8194\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4785 - acc: 0.7811 - val_loss: 0.4316 - val_acc: 0.8243\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4762 - acc: 0.7885 - val_loss: 0.4311 - val_acc: 0.8276\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4720 - acc: 0.7831 - val_loss: 0.4215 - val_acc: 0.8309\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4647 - acc: 0.7915 - val_loss: 0.4168 - val_acc: 0.8243\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4585 - acc: 0.7944 - val_loss: 0.4148 - val_acc: 0.8325\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4596 - acc: 0.7940 - val_loss: 0.4141 - val_acc: 0.8292\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4578 - acc: 0.7929 - val_loss: 0.4071 - val_acc: 0.8391\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4485 - acc: 0.8030 - val_loss: 0.4047 - val_acc: 0.8342\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4477 - acc: 0.7971 - val_loss: 0.4011 - val_acc: 0.8391\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4410 - acc: 0.8051 - val_loss: 0.3991 - val_acc: 0.8391\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4401 - acc: 0.8084 - val_loss: 0.3973 - val_acc: 0.8424\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4365 - acc: 0.8064 - val_loss: 0.3947 - val_acc: 0.8391\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4372 - acc: 0.8046 - val_loss: 0.3942 - val_acc: 0.8424\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4303 - acc: 0.8055 - val_loss: 0.3918 - val_acc: 0.8424\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4310 - acc: 0.8108 - val_loss: 0.3910 - val_acc: 0.8424\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4253 - acc: 0.8141 - val_loss: 0.3897 - val_acc: 0.8407\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4263 - acc: 0.8128 - val_loss: 0.3891 - val_acc: 0.8456\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4215 - acc: 0.8103 - val_loss: 0.3871 - val_acc: 0.8473\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4223 - acc: 0.8130 - val_loss: 0.3878 - val_acc: 0.8489\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4178 - acc: 0.8141 - val_loss: 0.3842 - val_acc: 0.8456\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4177 - acc: 0.8134 - val_loss: 0.3837 - val_acc: 0.8489\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4125 - acc: 0.8159 - val_loss: 0.3833 - val_acc: 0.8506\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4132 - acc: 0.8174 - val_loss: 0.3937 - val_acc: 0.8358\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4126 - acc: 0.8121 - val_loss: 0.3810 - val_acc: 0.8489\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4095 - acc: 0.8177 - val_loss: 0.3826 - val_acc: 0.8539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4070 - acc: 0.8207 - val_loss: 0.3891 - val_acc: 0.8391\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4093 - acc: 0.8165 - val_loss: 0.3877 - val_acc: 0.8473\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4068 - acc: 0.8183 - val_loss: 0.3793 - val_acc: 0.8489\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4038 - acc: 0.8238 - val_loss: 0.3813 - val_acc: 0.8522\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4060 - acc: 0.8161 - val_loss: 0.3932 - val_acc: 0.8473\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4073 - acc: 0.8254 - val_loss: 0.3913 - val_acc: 0.8424\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3978 - acc: 0.8250 - val_loss: 0.3830 - val_acc: 0.8522\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3971 - acc: 0.8214 - val_loss: 0.3831 - val_acc: 0.8456\n",
      "Training with parameters {'batch_size': 2500, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_136\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_137 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_408 (Dense)            (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_272 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_409 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_273 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_410 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 1s 64ms/step - loss: 0.8177 - acc: 0.4379 - val_loss: 0.6900 - val_acc: 0.5386\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6958 - acc: 0.5267 - val_loss: 0.6823 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6917 - acc: 0.5694 - val_loss: 0.6781 - val_acc: 0.5747\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6838 - acc: 0.5720 - val_loss: 0.6608 - val_acc: 0.5747\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6761 - acc: 0.5771 - val_loss: 0.6516 - val_acc: 0.6190\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.6644 - acc: 0.6088 - val_loss: 0.6496 - val_acc: 0.6798\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6647 - acc: 0.6004 - val_loss: 0.6406 - val_acc: 0.6946\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6551 - acc: 0.6247 - val_loss: 0.6301 - val_acc: 0.6568\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6496 - acc: 0.6196 - val_loss: 0.6228 - val_acc: 0.6404\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6453 - acc: 0.6249 - val_loss: 0.6097 - val_acc: 0.7176\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6351 - acc: 0.6512 - val_loss: 0.6023 - val_acc: 0.7176\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6270 - acc: 0.6661 - val_loss: 0.5914 - val_acc: 0.7225\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6206 - acc: 0.6696 - val_loss: 0.5816 - val_acc: 0.7455\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6150 - acc: 0.6749 - val_loss: 0.5749 - val_acc: 0.7373\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6125 - acc: 0.6774 - val_loss: 0.5635 - val_acc: 0.7438\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6028 - acc: 0.6900 - val_loss: 0.5573 - val_acc: 0.7422\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5899 - acc: 0.6991 - val_loss: 0.5490 - val_acc: 0.7767\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.5896 - acc: 0.7090 - val_loss: 0.5421 - val_acc: 0.7816\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.5816 - acc: 0.7137 - val_loss: 0.5351 - val_acc: 0.7750\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5794 - acc: 0.7183 - val_loss: 0.5283 - val_acc: 0.7882\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5729 - acc: 0.7296 - val_loss: 0.5212 - val_acc: 0.7898\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5645 - acc: 0.7256 - val_loss: 0.5153 - val_acc: 0.7833\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5622 - acc: 0.7251 - val_loss: 0.5081 - val_acc: 0.7915\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5550 - acc: 0.7333 - val_loss: 0.5024 - val_acc: 0.8013\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5483 - acc: 0.7420 - val_loss: 0.4962 - val_acc: 0.7964\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5424 - acc: 0.7468 - val_loss: 0.4904 - val_acc: 0.7931\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.5358 - acc: 0.7499 - val_loss: 0.4846 - val_acc: 0.8013\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.5367 - acc: 0.7495 - val_loss: 0.4796 - val_acc: 0.7980\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5316 - acc: 0.7541 - val_loss: 0.4747 - val_acc: 0.8144\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5254 - acc: 0.7601 - val_loss: 0.4692 - val_acc: 0.7980\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5220 - acc: 0.7581 - val_loss: 0.4650 - val_acc: 0.8062\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.5165 - acc: 0.7666 - val_loss: 0.4614 - val_acc: 0.8144\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.5143 - acc: 0.7750 - val_loss: 0.4563 - val_acc: 0.8095\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.5071 - acc: 0.7732 - val_loss: 0.4534 - val_acc: 0.8128\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.5053 - acc: 0.7710 - val_loss: 0.4485 - val_acc: 0.8177\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.5046 - acc: 0.7730 - val_loss: 0.4453 - val_acc: 0.8144\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4936 - acc: 0.7747 - val_loss: 0.4423 - val_acc: 0.8144\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4941 - acc: 0.7794 - val_loss: 0.4403 - val_acc: 0.8194\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4951 - acc: 0.7787 - val_loss: 0.4365 - val_acc: 0.8161\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.4936 - acc: 0.7754 - val_loss: 0.4332 - val_acc: 0.8259\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.4860 - acc: 0.7869 - val_loss: 0.4308 - val_acc: 0.8292\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4882 - acc: 0.7814 - val_loss: 0.4297 - val_acc: 0.8227\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4785 - acc: 0.7913 - val_loss: 0.4285 - val_acc: 0.8227\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.4844 - acc: 0.7853 - val_loss: 0.4245 - val_acc: 0.8259\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.4767 - acc: 0.7845 - val_loss: 0.4219 - val_acc: 0.8325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4740 - acc: 0.7876 - val_loss: 0.4212 - val_acc: 0.8227\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4730 - acc: 0.7895 - val_loss: 0.4176 - val_acc: 0.8309\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4693 - acc: 0.7931 - val_loss: 0.4160 - val_acc: 0.8342\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4694 - acc: 0.7937 - val_loss: 0.4141 - val_acc: 0.8325\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4685 - acc: 0.7913 - val_loss: 0.4142 - val_acc: 0.8342\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4668 - acc: 0.7940 - val_loss: 0.4104 - val_acc: 0.8374\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4605 - acc: 0.8039 - val_loss: 0.4095 - val_acc: 0.8325\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4596 - acc: 0.8022 - val_loss: 0.4081 - val_acc: 0.8374\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4571 - acc: 0.7991 - val_loss: 0.4073 - val_acc: 0.8292\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4584 - acc: 0.7966 - val_loss: 0.4056 - val_acc: 0.8391\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4587 - acc: 0.7955 - val_loss: 0.4044 - val_acc: 0.8374\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4536 - acc: 0.7995 - val_loss: 0.4055 - val_acc: 0.8309\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4548 - acc: 0.8000 - val_loss: 0.4035 - val_acc: 0.8358\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4512 - acc: 0.7991 - val_loss: 0.4003 - val_acc: 0.8391\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4516 - acc: 0.7991 - val_loss: 0.3989 - val_acc: 0.8407\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4490 - acc: 0.8024 - val_loss: 0.3987 - val_acc: 0.8424\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4457 - acc: 0.8055 - val_loss: 0.3981 - val_acc: 0.8424\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4471 - acc: 0.8031 - val_loss: 0.3972 - val_acc: 0.8407\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4417 - acc: 0.8117 - val_loss: 0.3963 - val_acc: 0.8407\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4419 - acc: 0.8057 - val_loss: 0.3958 - val_acc: 0.8407\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4390 - acc: 0.8092 - val_loss: 0.3955 - val_acc: 0.8407\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4429 - acc: 0.8042 - val_loss: 0.3957 - val_acc: 0.8456\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4406 - acc: 0.8086 - val_loss: 0.3957 - val_acc: 0.8440\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4363 - acc: 0.8123 - val_loss: 0.3947 - val_acc: 0.8407\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4384 - acc: 0.8073 - val_loss: 0.3932 - val_acc: 0.8456\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4327 - acc: 0.8057 - val_loss: 0.3922 - val_acc: 0.8440\n",
      "Epoch 72/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4331 - acc: 0.8115 - val_loss: 0.3912 - val_acc: 0.8424\n",
      "Epoch 73/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4358 - acc: 0.8128 - val_loss: 0.3905 - val_acc: 0.8473\n",
      "Epoch 74/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4360 - acc: 0.8081 - val_loss: 0.3894 - val_acc: 0.8440\n",
      "Epoch 75/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4323 - acc: 0.8106 - val_loss: 0.3892 - val_acc: 0.8473\n",
      "Epoch 76/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4313 - acc: 0.8073 - val_loss: 0.3902 - val_acc: 0.8473\n",
      "Epoch 77/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4291 - acc: 0.8134 - val_loss: 0.3889 - val_acc: 0.8456\n",
      "Epoch 78/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4251 - acc: 0.8124 - val_loss: 0.3896 - val_acc: 0.8456\n",
      "Epoch 79/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4254 - acc: 0.8128 - val_loss: 0.3876 - val_acc: 0.8473\n",
      "Epoch 80/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4257 - acc: 0.8168 - val_loss: 0.3868 - val_acc: 0.8473\n",
      "Epoch 81/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4278 - acc: 0.8123 - val_loss: 0.3867 - val_acc: 0.8473\n",
      "Epoch 82/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4240 - acc: 0.8143 - val_loss: 0.3847 - val_acc: 0.8456\n",
      "Epoch 83/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4232 - acc: 0.8157 - val_loss: 0.3841 - val_acc: 0.8473\n",
      "Epoch 84/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4241 - acc: 0.8132 - val_loss: 0.3840 - val_acc: 0.8473\n",
      "Epoch 85/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4255 - acc: 0.8095 - val_loss: 0.3835 - val_acc: 0.8489\n",
      "Epoch 86/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4197 - acc: 0.8179 - val_loss: 0.3835 - val_acc: 0.8506\n",
      "Epoch 87/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4191 - acc: 0.8161 - val_loss: 0.3835 - val_acc: 0.8473\n",
      "Epoch 88/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4204 - acc: 0.8143 - val_loss: 0.3832 - val_acc: 0.8473\n",
      "Epoch 89/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4205 - acc: 0.8176 - val_loss: 0.3842 - val_acc: 0.8473\n",
      "Epoch 90/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4176 - acc: 0.8196 - val_loss: 0.3827 - val_acc: 0.8456\n",
      "Epoch 91/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4153 - acc: 0.8172 - val_loss: 0.3823 - val_acc: 0.8456\n",
      "Epoch 92/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4144 - acc: 0.8172 - val_loss: 0.3814 - val_acc: 0.8473\n",
      "Epoch 93/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4187 - acc: 0.8183 - val_loss: 0.3802 - val_acc: 0.8473\n",
      "Epoch 94/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4121 - acc: 0.8223 - val_loss: 0.3823 - val_acc: 0.8473\n",
      "Epoch 95/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4157 - acc: 0.8181 - val_loss: 0.3816 - val_acc: 0.8506\n",
      "Epoch 96/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4176 - acc: 0.8163 - val_loss: 0.3799 - val_acc: 0.8489\n",
      "Epoch 97/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4134 - acc: 0.8210 - val_loss: 0.3792 - val_acc: 0.8424\n",
      "Epoch 98/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4144 - acc: 0.8165 - val_loss: 0.3792 - val_acc: 0.8456\n",
      "Epoch 99/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4110 - acc: 0.8199 - val_loss: 0.3804 - val_acc: 0.8456\n",
      "Epoch 100/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4075 - acc: 0.8250 - val_loss: 0.3856 - val_acc: 0.8424\n",
      "Epoch 101/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4113 - acc: 0.8223 - val_loss: 0.3872 - val_acc: 0.8440\n",
      "Epoch 102/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4177 - acc: 0.8207 - val_loss: 0.3824 - val_acc: 0.8473\n",
      "Epoch 103/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4165 - acc: 0.8130 - val_loss: 0.3798 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 2500, 'dropout': 0.1, 'lay_conf': [1024, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_137\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_138 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_411 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_274 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_412 (Dense)            (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_275 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_413 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.8705 - acc: 0.4623 - val_loss: 0.7847 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.7751 - acc: 0.5676 - val_loss: 0.6584 - val_acc: 0.5764\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6776 - acc: 0.5774 - val_loss: 0.6834 - val_acc: 0.5452\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.6817 - acc: 0.5754 - val_loss: 0.6155 - val_acc: 0.7028\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.6374 - acc: 0.6369 - val_loss: 0.6234 - val_acc: 0.5829\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.6469 - acc: 0.6070 - val_loss: 0.5834 - val_acc: 0.7323\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.6162 - acc: 0.6698 - val_loss: 0.5987 - val_acc: 0.6864\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.6193 - acc: 0.6524 - val_loss: 0.5604 - val_acc: 0.7307\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5947 - acc: 0.6973 - val_loss: 0.5602 - val_acc: 0.7274\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5959 - acc: 0.6977 - val_loss: 0.5421 - val_acc: 0.7619\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5779 - acc: 0.7090 - val_loss: 0.5417 - val_acc: 0.7521\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.5710 - acc: 0.7154 - val_loss: 0.5219 - val_acc: 0.7915\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.5622 - acc: 0.7323 - val_loss: 0.5123 - val_acc: 0.7931\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5538 - acc: 0.7398 - val_loss: 0.5058 - val_acc: 0.7800\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5457 - acc: 0.7327 - val_loss: 0.4897 - val_acc: 0.8095\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5398 - acc: 0.7427 - val_loss: 0.4797 - val_acc: 0.8144\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5300 - acc: 0.7457 - val_loss: 0.4754 - val_acc: 0.8062\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.5215 - acc: 0.7521 - val_loss: 0.4698 - val_acc: 0.8079\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5180 - acc: 0.7601 - val_loss: 0.4677 - val_acc: 0.8079\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5154 - acc: 0.7537 - val_loss: 0.4524 - val_acc: 0.8227\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.5095 - acc: 0.7625 - val_loss: 0.4464 - val_acc: 0.8227\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5026 - acc: 0.7687 - val_loss: 0.4461 - val_acc: 0.8227\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4902 - acc: 0.7785 - val_loss: 0.4421 - val_acc: 0.8210\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4924 - acc: 0.7707 - val_loss: 0.4350 - val_acc: 0.8276\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4817 - acc: 0.7831 - val_loss: 0.4307 - val_acc: 0.8276\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4828 - acc: 0.7772 - val_loss: 0.4282 - val_acc: 0.8276\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4767 - acc: 0.7787 - val_loss: 0.4224 - val_acc: 0.8276\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4720 - acc: 0.7854 - val_loss: 0.4187 - val_acc: 0.8325\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4683 - acc: 0.7918 - val_loss: 0.4156 - val_acc: 0.8325\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4643 - acc: 0.7926 - val_loss: 0.4136 - val_acc: 0.8325\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4579 - acc: 0.7922 - val_loss: 0.4106 - val_acc: 0.8325\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4599 - acc: 0.7935 - val_loss: 0.4080 - val_acc: 0.8374\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4549 - acc: 0.7942 - val_loss: 0.4052 - val_acc: 0.8325\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4515 - acc: 0.7991 - val_loss: 0.4077 - val_acc: 0.8309\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4543 - acc: 0.7929 - val_loss: 0.4093 - val_acc: 0.8374\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4548 - acc: 0.8004 - val_loss: 0.4068 - val_acc: 0.8309\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4481 - acc: 0.7986 - val_loss: 0.4055 - val_acc: 0.8358\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4486 - acc: 0.8002 - val_loss: 0.3998 - val_acc: 0.8391\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4415 - acc: 0.8011 - val_loss: 0.3973 - val_acc: 0.8391\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4405 - acc: 0.8022 - val_loss: 0.3917 - val_acc: 0.8489\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4371 - acc: 0.8057 - val_loss: 0.3913 - val_acc: 0.8424\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4322 - acc: 0.8099 - val_loss: 0.3897 - val_acc: 0.8424\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4315 - acc: 0.8051 - val_loss: 0.3891 - val_acc: 0.8424\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4285 - acc: 0.8106 - val_loss: 0.3864 - val_acc: 0.8489\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4256 - acc: 0.8090 - val_loss: 0.3856 - val_acc: 0.8473\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4240 - acc: 0.8110 - val_loss: 0.3858 - val_acc: 0.8440\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4268 - acc: 0.8106 - val_loss: 0.3839 - val_acc: 0.8473\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4216 - acc: 0.8112 - val_loss: 0.3933 - val_acc: 0.8473\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4315 - acc: 0.8066 - val_loss: 0.3928 - val_acc: 0.8374\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4174 - acc: 0.8144 - val_loss: 0.3872 - val_acc: 0.8506\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4153 - acc: 0.8197 - val_loss: 0.3906 - val_acc: 0.8358\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4168 - acc: 0.8172 - val_loss: 0.3801 - val_acc: 0.8489\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4146 - acc: 0.8165 - val_loss: 0.3789 - val_acc: 0.8424\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4100 - acc: 0.8186 - val_loss: 0.3782 - val_acc: 0.8440\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4089 - acc: 0.8165 - val_loss: 0.3778 - val_acc: 0.8539\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4119 - acc: 0.8197 - val_loss: 0.3787 - val_acc: 0.8456\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4062 - acc: 0.8205 - val_loss: 0.3774 - val_acc: 0.8555\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4039 - acc: 0.8230 - val_loss: 0.3758 - val_acc: 0.8473\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4041 - acc: 0.8248 - val_loss: 0.3753 - val_acc: 0.8456\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.3997 - acc: 0.8261 - val_loss: 0.3754 - val_acc: 0.8506\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3982 - acc: 0.8261 - val_loss: 0.3747 - val_acc: 0.8506\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3989 - acc: 0.8199 - val_loss: 0.3731 - val_acc: 0.8473\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.3956 - acc: 0.8258 - val_loss: 0.3727 - val_acc: 0.8456\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.3930 - acc: 0.8267 - val_loss: 0.3746 - val_acc: 0.8489\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.3930 - acc: 0.8323 - val_loss: 0.3748 - val_acc: 0.8506\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.3885 - acc: 0.8300 - val_loss: 0.3734 - val_acc: 0.8489\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.3902 - acc: 0.8280 - val_loss: 0.3715 - val_acc: 0.8522\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.3919 - acc: 0.8281 - val_loss: 0.3801 - val_acc: 0.8407\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.3901 - acc: 0.8301 - val_loss: 0.3802 - val_acc: 0.8456\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.3937 - acc: 0.8311 - val_loss: 0.3827 - val_acc: 0.8407\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.3841 - acc: 0.8309 - val_loss: 0.3759 - val_acc: 0.8522\n",
      "Epoch 72/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.3889 - acc: 0.8278 - val_loss: 0.3735 - val_acc: 0.8522\n",
      "Training with parameters {'batch_size': 2500, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_138\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_139 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_414 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_276 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_415 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_277 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_416 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,985\n",
      "Trainable params: 409,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 1.6634 - acc: 0.4320 - val_loss: 0.9337 - val_acc: 0.4253\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.8766 - acc: 0.4441 - val_loss: 0.7067 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.7388 - acc: 0.5373 - val_loss: 0.7777 - val_acc: 0.5747\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.7844 - acc: 0.5660 - val_loss: 0.7543 - val_acc: 0.5747\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.7516 - acc: 0.5661 - val_loss: 0.6946 - val_acc: 0.5747\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.7019 - acc: 0.5727 - val_loss: 0.6612 - val_acc: 0.5846\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6822 - acc: 0.5722 - val_loss: 0.6610 - val_acc: 0.6700\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6785 - acc: 0.5619 - val_loss: 0.6600 - val_acc: 0.6420\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6750 - acc: 0.5791 - val_loss: 0.6441 - val_acc: 0.6995\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6602 - acc: 0.6130 - val_loss: 0.6296 - val_acc: 0.6732\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6563 - acc: 0.6117 - val_loss: 0.6240 - val_acc: 0.6158\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6480 - acc: 0.6154 - val_loss: 0.6166 - val_acc: 0.6355\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6400 - acc: 0.6389 - val_loss: 0.6068 - val_acc: 0.7209\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6324 - acc: 0.6550 - val_loss: 0.5988 - val_acc: 0.7389\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6293 - acc: 0.6533 - val_loss: 0.5913 - val_acc: 0.7504\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6236 - acc: 0.6683 - val_loss: 0.5840 - val_acc: 0.7553\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6171 - acc: 0.6747 - val_loss: 0.5774 - val_acc: 0.7652\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6095 - acc: 0.6833 - val_loss: 0.5713 - val_acc: 0.7635\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6056 - acc: 0.6937 - val_loss: 0.5649 - val_acc: 0.7685\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6003 - acc: 0.6906 - val_loss: 0.5591 - val_acc: 0.7701\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5971 - acc: 0.6953 - val_loss: 0.5527 - val_acc: 0.7800\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5937 - acc: 0.7010 - val_loss: 0.5475 - val_acc: 0.7898\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5908 - acc: 0.7072 - val_loss: 0.5423 - val_acc: 0.7898\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5839 - acc: 0.7137 - val_loss: 0.5376 - val_acc: 0.7964\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5825 - acc: 0.7117 - val_loss: 0.5332 - val_acc: 0.7980\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5760 - acc: 0.7115 - val_loss: 0.5296 - val_acc: 0.7964\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5760 - acc: 0.7106 - val_loss: 0.5247 - val_acc: 0.8030\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5676 - acc: 0.7265 - val_loss: 0.5203 - val_acc: 0.8013\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5645 - acc: 0.7249 - val_loss: 0.5159 - val_acc: 0.8079\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5636 - acc: 0.7327 - val_loss: 0.5120 - val_acc: 0.8046\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5588 - acc: 0.7271 - val_loss: 0.5074 - val_acc: 0.8046\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5546 - acc: 0.7353 - val_loss: 0.5025 - val_acc: 0.8112\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5507 - acc: 0.7440 - val_loss: 0.4982 - val_acc: 0.8112\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5452 - acc: 0.7446 - val_loss: 0.4941 - val_acc: 0.8144\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5428 - acc: 0.7437 - val_loss: 0.4902 - val_acc: 0.8112\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5373 - acc: 0.7515 - val_loss: 0.4861 - val_acc: 0.8161\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5346 - acc: 0.7479 - val_loss: 0.4820 - val_acc: 0.8161\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5345 - acc: 0.7457 - val_loss: 0.4798 - val_acc: 0.8177\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5232 - acc: 0.7568 - val_loss: 0.4740 - val_acc: 0.8259\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5209 - acc: 0.7584 - val_loss: 0.4708 - val_acc: 0.8210\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5164 - acc: 0.7615 - val_loss: 0.4685 - val_acc: 0.8177\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5194 - acc: 0.7584 - val_loss: 0.4625 - val_acc: 0.8177\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5136 - acc: 0.7608 - val_loss: 0.4598 - val_acc: 0.8177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5131 - acc: 0.7581 - val_loss: 0.4576 - val_acc: 0.8112\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5089 - acc: 0.7677 - val_loss: 0.4542 - val_acc: 0.8194\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5043 - acc: 0.7606 - val_loss: 0.4521 - val_acc: 0.8227\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5051 - acc: 0.7692 - val_loss: 0.4527 - val_acc: 0.8210\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5020 - acc: 0.7729 - val_loss: 0.4478 - val_acc: 0.8210\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4992 - acc: 0.7701 - val_loss: 0.4456 - val_acc: 0.8243\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4952 - acc: 0.7816 - val_loss: 0.4461 - val_acc: 0.8194\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4963 - acc: 0.7772 - val_loss: 0.4413 - val_acc: 0.8276\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4901 - acc: 0.7780 - val_loss: 0.4400 - val_acc: 0.8243\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4878 - acc: 0.7791 - val_loss: 0.4380 - val_acc: 0.8243\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4857 - acc: 0.7843 - val_loss: 0.4356 - val_acc: 0.8292\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4880 - acc: 0.7770 - val_loss: 0.4351 - val_acc: 0.8227\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4834 - acc: 0.7822 - val_loss: 0.4313 - val_acc: 0.8309\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4822 - acc: 0.7831 - val_loss: 0.4313 - val_acc: 0.8243\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4803 - acc: 0.7893 - val_loss: 0.4286 - val_acc: 0.8342\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4791 - acc: 0.7814 - val_loss: 0.4273 - val_acc: 0.8358\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.4763 - acc: 0.7853 - val_loss: 0.4263 - val_acc: 0.8309\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4684 - acc: 0.7942 - val_loss: 0.4249 - val_acc: 0.8374\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4729 - acc: 0.7900 - val_loss: 0.4240 - val_acc: 0.8325\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4699 - acc: 0.7895 - val_loss: 0.4228 - val_acc: 0.8407\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4655 - acc: 0.7869 - val_loss: 0.4218 - val_acc: 0.8391\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4634 - acc: 0.7947 - val_loss: 0.4206 - val_acc: 0.8374\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4660 - acc: 0.7895 - val_loss: 0.4196 - val_acc: 0.8391\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4631 - acc: 0.7938 - val_loss: 0.4186 - val_acc: 0.8456\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4618 - acc: 0.7940 - val_loss: 0.4171 - val_acc: 0.8424\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4624 - acc: 0.7915 - val_loss: 0.4163 - val_acc: 0.8407\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4633 - acc: 0.7944 - val_loss: 0.4149 - val_acc: 0.8473\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4596 - acc: 0.7946 - val_loss: 0.4135 - val_acc: 0.8456\n",
      "Epoch 72/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4565 - acc: 0.7991 - val_loss: 0.4133 - val_acc: 0.8407\n",
      "Epoch 73/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4554 - acc: 0.8006 - val_loss: 0.4105 - val_acc: 0.8489\n",
      "Epoch 74/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4561 - acc: 0.7898 - val_loss: 0.4099 - val_acc: 0.8473\n",
      "Epoch 75/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4534 - acc: 0.7997 - val_loss: 0.4105 - val_acc: 0.8456\n",
      "Epoch 76/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4513 - acc: 0.8013 - val_loss: 0.4107 - val_acc: 0.8473\n",
      "Epoch 77/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4531 - acc: 0.7957 - val_loss: 0.4084 - val_acc: 0.8506\n",
      "Epoch 78/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4514 - acc: 0.7999 - val_loss: 0.4075 - val_acc: 0.8506\n",
      "Epoch 79/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4482 - acc: 0.8017 - val_loss: 0.4062 - val_acc: 0.8456\n",
      "Epoch 80/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4441 - acc: 0.8046 - val_loss: 0.4055 - val_acc: 0.8506\n",
      "Epoch 81/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4470 - acc: 0.8082 - val_loss: 0.4046 - val_acc: 0.8473\n",
      "Epoch 82/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4427 - acc: 0.8048 - val_loss: 0.4041 - val_acc: 0.8489\n",
      "Epoch 83/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4465 - acc: 0.8037 - val_loss: 0.4052 - val_acc: 0.8456\n",
      "Epoch 84/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4454 - acc: 0.8061 - val_loss: 0.4047 - val_acc: 0.8440\n",
      "Epoch 85/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4468 - acc: 0.7989 - val_loss: 0.4053 - val_acc: 0.8407\n",
      "Epoch 86/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4501 - acc: 0.8046 - val_loss: 0.4034 - val_acc: 0.8473\n",
      "Epoch 87/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4414 - acc: 0.8026 - val_loss: 0.4034 - val_acc: 0.8456\n",
      "Epoch 88/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4450 - acc: 0.8055 - val_loss: 0.4027 - val_acc: 0.8456\n",
      "Epoch 89/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4413 - acc: 0.8072 - val_loss: 0.4019 - val_acc: 0.8489\n",
      "Epoch 90/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4410 - acc: 0.8019 - val_loss: 0.4026 - val_acc: 0.8473\n",
      "Epoch 91/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4387 - acc: 0.8053 - val_loss: 0.4015 - val_acc: 0.8473\n",
      "Epoch 92/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4384 - acc: 0.8115 - val_loss: 0.3992 - val_acc: 0.8440\n",
      "Epoch 93/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4338 - acc: 0.8092 - val_loss: 0.4004 - val_acc: 0.8473\n",
      "Epoch 94/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4316 - acc: 0.8082 - val_loss: 0.3977 - val_acc: 0.8456\n",
      "Epoch 95/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4332 - acc: 0.8108 - val_loss: 0.3981 - val_acc: 0.8489\n",
      "Epoch 96/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4335 - acc: 0.8090 - val_loss: 0.3992 - val_acc: 0.8440\n",
      "Epoch 97/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4311 - acc: 0.8092 - val_loss: 0.3982 - val_acc: 0.8489\n",
      "Epoch 98/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4358 - acc: 0.8106 - val_loss: 0.3958 - val_acc: 0.8424\n",
      "Epoch 99/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4296 - acc: 0.8126 - val_loss: 0.4005 - val_acc: 0.8440\n",
      "Epoch 100/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4339 - acc: 0.8090 - val_loss: 0.3971 - val_acc: 0.8489\n",
      "Epoch 101/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4314 - acc: 0.8115 - val_loss: 0.3954 - val_acc: 0.8424\n",
      "Epoch 102/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4305 - acc: 0.8141 - val_loss: 0.3957 - val_acc: 0.8456\n",
      "Epoch 103/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4261 - acc: 0.8126 - val_loss: 0.3978 - val_acc: 0.8456\n",
      "Epoch 104/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4303 - acc: 0.8123 - val_loss: 0.3942 - val_acc: 0.8456\n",
      "Epoch 105/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4249 - acc: 0.8084 - val_loss: 0.3952 - val_acc: 0.8473\n",
      "Epoch 106/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4217 - acc: 0.8141 - val_loss: 0.3928 - val_acc: 0.8440\n",
      "Epoch 107/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4192 - acc: 0.8152 - val_loss: 0.3926 - val_acc: 0.8456\n",
      "Epoch 108/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4255 - acc: 0.8106 - val_loss: 0.3937 - val_acc: 0.8456\n",
      "Epoch 109/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4247 - acc: 0.8103 - val_loss: 0.3935 - val_acc: 0.8456\n",
      "Epoch 110/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4223 - acc: 0.8210 - val_loss: 0.3923 - val_acc: 0.8456\n",
      "Epoch 111/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4220 - acc: 0.8163 - val_loss: 0.3933 - val_acc: 0.8456\n",
      "Epoch 112/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4198 - acc: 0.8174 - val_loss: 0.3939 - val_acc: 0.8456\n",
      "Epoch 113/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4231 - acc: 0.8148 - val_loss: 0.3922 - val_acc: 0.8456\n",
      "Epoch 114/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4203 - acc: 0.8110 - val_loss: 0.3910 - val_acc: 0.8424\n",
      "Epoch 115/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4190 - acc: 0.8146 - val_loss: 0.3960 - val_acc: 0.8489\n",
      "Epoch 116/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4226 - acc: 0.8155 - val_loss: 0.3930 - val_acc: 0.8424\n",
      "Epoch 117/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4220 - acc: 0.8165 - val_loss: 0.3906 - val_acc: 0.8440\n",
      "Epoch 118/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4186 - acc: 0.8185 - val_loss: 0.3949 - val_acc: 0.8473\n",
      "Epoch 119/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4206 - acc: 0.8239 - val_loss: 0.3960 - val_acc: 0.8440\n",
      "Epoch 120/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4198 - acc: 0.8154 - val_loss: 0.3899 - val_acc: 0.8424\n",
      "Epoch 121/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4192 - acc: 0.8183 - val_loss: 0.3926 - val_acc: 0.8473\n",
      "Epoch 122/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4145 - acc: 0.8183 - val_loss: 0.3923 - val_acc: 0.8440\n",
      "Epoch 123/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4169 - acc: 0.8177 - val_loss: 0.3884 - val_acc: 0.8440\n",
      "Epoch 124/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4129 - acc: 0.8232 - val_loss: 0.3892 - val_acc: 0.8489\n",
      "Epoch 125/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4135 - acc: 0.8227 - val_loss: 0.3898 - val_acc: 0.8456\n",
      "Epoch 126/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4139 - acc: 0.8216 - val_loss: 0.3885 - val_acc: 0.8407\n",
      "Epoch 127/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4118 - acc: 0.8214 - val_loss: 0.3893 - val_acc: 0.8473\n",
      "Epoch 128/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4116 - acc: 0.8225 - val_loss: 0.3879 - val_acc: 0.8391\n",
      "Epoch 129/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4134 - acc: 0.8165 - val_loss: 0.3880 - val_acc: 0.8456\n",
      "Epoch 130/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4103 - acc: 0.8236 - val_loss: 0.3877 - val_acc: 0.8424\n",
      "Epoch 131/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4108 - acc: 0.8186 - val_loss: 0.3868 - val_acc: 0.8424\n",
      "Epoch 132/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4085 - acc: 0.8185 - val_loss: 0.3865 - val_acc: 0.8424\n",
      "Epoch 133/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4082 - acc: 0.8212 - val_loss: 0.3861 - val_acc: 0.8440\n",
      "Epoch 134/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4071 - acc: 0.8256 - val_loss: 0.3858 - val_acc: 0.8440\n",
      "Epoch 135/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4123 - acc: 0.8208 - val_loss: 0.3893 - val_acc: 0.8424\n",
      "Epoch 136/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4099 - acc: 0.8243 - val_loss: 0.3881 - val_acc: 0.8489\n",
      "Epoch 137/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4058 - acc: 0.8292 - val_loss: 0.3856 - val_acc: 0.8440\n",
      "Epoch 138/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4070 - acc: 0.8219 - val_loss: 0.3861 - val_acc: 0.8440\n",
      "Epoch 139/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4049 - acc: 0.8250 - val_loss: 0.3851 - val_acc: 0.8440\n",
      "Epoch 140/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4040 - acc: 0.8252 - val_loss: 0.3853 - val_acc: 0.8424\n",
      "Epoch 141/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4065 - acc: 0.8227 - val_loss: 0.3857 - val_acc: 0.8424\n",
      "Epoch 142/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4024 - acc: 0.8261 - val_loss: 0.3867 - val_acc: 0.8407\n",
      "Epoch 143/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4029 - acc: 0.8230 - val_loss: 0.3871 - val_acc: 0.8473\n",
      "Epoch 144/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4043 - acc: 0.8259 - val_loss: 0.3856 - val_acc: 0.8440\n",
      "Training with parameters {'batch_size': 2500, 'dropout': 0.1, 'lay_conf': [1024, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_139\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_140 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_417 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_278 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_418 (Dense)            (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_279 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_419 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,639,553\n",
      "Trainable params: 1,639,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.7848 - acc: 0.4955 - val_loss: 0.7149 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.7049 - acc: 0.5700 - val_loss: 0.6474 - val_acc: 0.6601\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6642 - acc: 0.6035 - val_loss: 0.6147 - val_acc: 0.7077\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.6333 - acc: 0.6568 - val_loss: 0.5945 - val_acc: 0.6732\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6151 - acc: 0.6738 - val_loss: 0.5655 - val_acc: 0.7110\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.6019 - acc: 0.6751 - val_loss: 0.5439 - val_acc: 0.7718\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5831 - acc: 0.7050 - val_loss: 0.5272 - val_acc: 0.7865\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5750 - acc: 0.7108 - val_loss: 0.5169 - val_acc: 0.7701\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5614 - acc: 0.7251 - val_loss: 0.5010 - val_acc: 0.8030\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.5479 - acc: 0.7407 - val_loss: 0.4892 - val_acc: 0.7997\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.5374 - acc: 0.7371 - val_loss: 0.4831 - val_acc: 0.8112\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5318 - acc: 0.7458 - val_loss: 0.4682 - val_acc: 0.8144\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5223 - acc: 0.7515 - val_loss: 0.4600 - val_acc: 0.8177\n",
      "Epoch 14/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5104 - acc: 0.7632 - val_loss: 0.4520 - val_acc: 0.8276\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5071 - acc: 0.7672 - val_loss: 0.4456 - val_acc: 0.8227\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5013 - acc: 0.7672 - val_loss: 0.4414 - val_acc: 0.8342\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4951 - acc: 0.7745 - val_loss: 0.4323 - val_acc: 0.8292\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4889 - acc: 0.7752 - val_loss: 0.4279 - val_acc: 0.8276\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4847 - acc: 0.7778 - val_loss: 0.4234 - val_acc: 0.8276\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4769 - acc: 0.7811 - val_loss: 0.4190 - val_acc: 0.8342\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4750 - acc: 0.7781 - val_loss: 0.4154 - val_acc: 0.8358\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4694 - acc: 0.7891 - val_loss: 0.4128 - val_acc: 0.8342\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4712 - acc: 0.7856 - val_loss: 0.4102 - val_acc: 0.8358\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4636 - acc: 0.7915 - val_loss: 0.4071 - val_acc: 0.8391\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4588 - acc: 0.7929 - val_loss: 0.4051 - val_acc: 0.8440\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4593 - acc: 0.7960 - val_loss: 0.4007 - val_acc: 0.8407\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4571 - acc: 0.7949 - val_loss: 0.3996 - val_acc: 0.8440\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4525 - acc: 0.7958 - val_loss: 0.3972 - val_acc: 0.8489\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4490 - acc: 0.7960 - val_loss: 0.3944 - val_acc: 0.8440\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4470 - acc: 0.7977 - val_loss: 0.3924 - val_acc: 0.8440\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4432 - acc: 0.8009 - val_loss: 0.3917 - val_acc: 0.8522\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4398 - acc: 0.8061 - val_loss: 0.3896 - val_acc: 0.8473\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4397 - acc: 0.8037 - val_loss: 0.3900 - val_acc: 0.8489\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4388 - acc: 0.8070 - val_loss: 0.3905 - val_acc: 0.8473\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4349 - acc: 0.8061 - val_loss: 0.3956 - val_acc: 0.8440\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4371 - acc: 0.8057 - val_loss: 0.3910 - val_acc: 0.8407\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4363 - acc: 0.8053 - val_loss: 0.3826 - val_acc: 0.8489\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4314 - acc: 0.8066 - val_loss: 0.3839 - val_acc: 0.8456\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4260 - acc: 0.8143 - val_loss: 0.3812 - val_acc: 0.8506\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4234 - acc: 0.8137 - val_loss: 0.3808 - val_acc: 0.8522\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4219 - acc: 0.8174 - val_loss: 0.3817 - val_acc: 0.8539\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4200 - acc: 0.8144 - val_loss: 0.3803 - val_acc: 0.8522\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4200 - acc: 0.8144 - val_loss: 0.3825 - val_acc: 0.8489\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4207 - acc: 0.8117 - val_loss: 0.3831 - val_acc: 0.8522\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4198 - acc: 0.8183 - val_loss: 0.3776 - val_acc: 0.8604\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4203 - acc: 0.8174 - val_loss: 0.3785 - val_acc: 0.8522\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4129 - acc: 0.8166 - val_loss: 0.3763 - val_acc: 0.8555\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4079 - acc: 0.8214 - val_loss: 0.3744 - val_acc: 0.8539\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4080 - acc: 0.8228 - val_loss: 0.3750 - val_acc: 0.8539\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4090 - acc: 0.8228 - val_loss: 0.3782 - val_acc: 0.8539\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4092 - acc: 0.8183 - val_loss: 0.3856 - val_acc: 0.8489\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4146 - acc: 0.8203 - val_loss: 0.3817 - val_acc: 0.8391\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4082 - acc: 0.8201 - val_loss: 0.3762 - val_acc: 0.8522\n",
      "Training with parameters {'batch_size': 2500, 'dropout': 0.1, 'lay_conf': [1024, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_140\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_141 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_420 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_280 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_421 (Dense)            (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_281 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_422 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,606,721\n",
      "Trainable params: 1,606,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.8200 - acc: 0.4857 - val_loss: 0.7267 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.7197 - acc: 0.5661 - val_loss: 0.6713 - val_acc: 0.5813\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.6969 - acc: 0.5508 - val_loss: 0.6174 - val_acc: 0.7159\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.6516 - acc: 0.6229 - val_loss: 0.6194 - val_acc: 0.5961\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.6364 - acc: 0.6382 - val_loss: 0.5798 - val_acc: 0.7340\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.6167 - acc: 0.6681 - val_loss: 0.5621 - val_acc: 0.7373\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.6007 - acc: 0.6869 - val_loss: 0.5536 - val_acc: 0.7323\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.5959 - acc: 0.6980 - val_loss: 0.5221 - val_acc: 0.7750\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.5753 - acc: 0.7035 - val_loss: 0.5103 - val_acc: 0.7783\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.5653 - acc: 0.7174 - val_loss: 0.5050 - val_acc: 0.7915\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.5516 - acc: 0.7386 - val_loss: 0.4864 - val_acc: 0.8112\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.5431 - acc: 0.7382 - val_loss: 0.4756 - val_acc: 0.8079\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.5262 - acc: 0.7499 - val_loss: 0.4659 - val_acc: 0.8227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.5258 - acc: 0.7521 - val_loss: 0.4569 - val_acc: 0.8276\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.5178 - acc: 0.7592 - val_loss: 0.4498 - val_acc: 0.8227\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.5083 - acc: 0.7621 - val_loss: 0.4425 - val_acc: 0.8227\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.5033 - acc: 0.7674 - val_loss: 0.4374 - val_acc: 0.8227\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.4967 - acc: 0.7699 - val_loss: 0.4310 - val_acc: 0.8210\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.4917 - acc: 0.7696 - val_loss: 0.4311 - val_acc: 0.8276\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4939 - acc: 0.7708 - val_loss: 0.4254 - val_acc: 0.8276\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4899 - acc: 0.7701 - val_loss: 0.4217 - val_acc: 0.8309\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4828 - acc: 0.7780 - val_loss: 0.4159 - val_acc: 0.8325\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4731 - acc: 0.7825 - val_loss: 0.4131 - val_acc: 0.8358\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4697 - acc: 0.7869 - val_loss: 0.4107 - val_acc: 0.8374\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4705 - acc: 0.7889 - val_loss: 0.4086 - val_acc: 0.8374\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4641 - acc: 0.7880 - val_loss: 0.4063 - val_acc: 0.8374\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4641 - acc: 0.7891 - val_loss: 0.4030 - val_acc: 0.8374\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4574 - acc: 0.7933 - val_loss: 0.3988 - val_acc: 0.8391\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4557 - acc: 0.7938 - val_loss: 0.3967 - val_acc: 0.8456\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4495 - acc: 0.8011 - val_loss: 0.3944 - val_acc: 0.8407\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4470 - acc: 0.8039 - val_loss: 0.3927 - val_acc: 0.8473\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4485 - acc: 0.7938 - val_loss: 0.3925 - val_acc: 0.8424\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4453 - acc: 0.7973 - val_loss: 0.3966 - val_acc: 0.8407\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4444 - acc: 0.8019 - val_loss: 0.3922 - val_acc: 0.8456\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4444 - acc: 0.8013 - val_loss: 0.3896 - val_acc: 0.8424\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4380 - acc: 0.8024 - val_loss: 0.3906 - val_acc: 0.8440\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4436 - acc: 0.8046 - val_loss: 0.3916 - val_acc: 0.8440\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4400 - acc: 0.8046 - val_loss: 0.3845 - val_acc: 0.8522\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4341 - acc: 0.8079 - val_loss: 0.3822 - val_acc: 0.8456\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4286 - acc: 0.8123 - val_loss: 0.3836 - val_acc: 0.8473\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4296 - acc: 0.8092 - val_loss: 0.3817 - val_acc: 0.8440\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4263 - acc: 0.8115 - val_loss: 0.3817 - val_acc: 0.8473\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4247 - acc: 0.8134 - val_loss: 0.3797 - val_acc: 0.8456\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4260 - acc: 0.8088 - val_loss: 0.3787 - val_acc: 0.8473\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4243 - acc: 0.8112 - val_loss: 0.3787 - val_acc: 0.8489\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.4231 - acc: 0.8128 - val_loss: 0.3790 - val_acc: 0.8456\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.4251 - acc: 0.8130 - val_loss: 0.3780 - val_acc: 0.8522\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.4187 - acc: 0.8176 - val_loss: 0.3755 - val_acc: 0.8522\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.4193 - acc: 0.8174 - val_loss: 0.3755 - val_acc: 0.8522\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.4141 - acc: 0.8123 - val_loss: 0.3745 - val_acc: 0.8522\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.4093 - acc: 0.8239 - val_loss: 0.3730 - val_acc: 0.8539\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.4158 - acc: 0.8190 - val_loss: 0.3723 - val_acc: 0.8539\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.4105 - acc: 0.8203 - val_loss: 0.3739 - val_acc: 0.8539\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.4110 - acc: 0.8183 - val_loss: 0.3788 - val_acc: 0.8539\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.4162 - acc: 0.8155 - val_loss: 0.3771 - val_acc: 0.8522\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.4103 - acc: 0.8194 - val_loss: 0.3768 - val_acc: 0.8522\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.4098 - acc: 0.8196 - val_loss: 0.3741 - val_acc: 0.8539\n",
      "Training with parameters {'batch_size': 2500, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_141\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_142 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_423 (Dense)            (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_282 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_424 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_283 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_425 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,050,113\n",
      "Trainable params: 1,050,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 1s 85ms/step - loss: 0.7298 - acc: 0.5114 - val_loss: 0.6965 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6949 - acc: 0.5711 - val_loss: 0.6543 - val_acc: 0.6437\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6672 - acc: 0.6061 - val_loss: 0.6188 - val_acc: 0.6601\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6334 - acc: 0.6471 - val_loss: 0.6028 - val_acc: 0.6502\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.6292 - acc: 0.6373 - val_loss: 0.5777 - val_acc: 0.7209\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.6038 - acc: 0.6800 - val_loss: 0.5729 - val_acc: 0.6946\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5945 - acc: 0.6897 - val_loss: 0.5496 - val_acc: 0.7603\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.5793 - acc: 0.7179 - val_loss: 0.5339 - val_acc: 0.7849\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5676 - acc: 0.7190 - val_loss: 0.5254 - val_acc: 0.7668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.5560 - acc: 0.7320 - val_loss: 0.5098 - val_acc: 0.7915\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5465 - acc: 0.7451 - val_loss: 0.4983 - val_acc: 0.7964\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5344 - acc: 0.7448 - val_loss: 0.4885 - val_acc: 0.8095\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.5266 - acc: 0.7553 - val_loss: 0.4788 - val_acc: 0.8030\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5176 - acc: 0.7601 - val_loss: 0.4675 - val_acc: 0.8079\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.5080 - acc: 0.7694 - val_loss: 0.4594 - val_acc: 0.8112\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.5029 - acc: 0.7698 - val_loss: 0.4547 - val_acc: 0.8177\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4926 - acc: 0.7716 - val_loss: 0.4462 - val_acc: 0.8259\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4917 - acc: 0.7745 - val_loss: 0.4393 - val_acc: 0.8227\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.4874 - acc: 0.7796 - val_loss: 0.4359 - val_acc: 0.8210\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.4770 - acc: 0.7867 - val_loss: 0.4386 - val_acc: 0.8227\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.4796 - acc: 0.7781 - val_loss: 0.4267 - val_acc: 0.8243\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4712 - acc: 0.7860 - val_loss: 0.4260 - val_acc: 0.8243\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.4680 - acc: 0.7874 - val_loss: 0.4218 - val_acc: 0.8243\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4639 - acc: 0.7887 - val_loss: 0.4225 - val_acc: 0.8210\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4592 - acc: 0.7935 - val_loss: 0.4113 - val_acc: 0.8325\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4557 - acc: 0.7964 - val_loss: 0.4084 - val_acc: 0.8342\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4549 - acc: 0.7978 - val_loss: 0.4061 - val_acc: 0.8374\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.4514 - acc: 0.7933 - val_loss: 0.4041 - val_acc: 0.8391\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4489 - acc: 0.7986 - val_loss: 0.4022 - val_acc: 0.8358\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4458 - acc: 0.7999 - val_loss: 0.4014 - val_acc: 0.8424\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4429 - acc: 0.8030 - val_loss: 0.3994 - val_acc: 0.8342\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4404 - acc: 0.8072 - val_loss: 0.3983 - val_acc: 0.8358\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4357 - acc: 0.8084 - val_loss: 0.3970 - val_acc: 0.8374\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4358 - acc: 0.8082 - val_loss: 0.3979 - val_acc: 0.8374\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4326 - acc: 0.8097 - val_loss: 0.3942 - val_acc: 0.8424\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4288 - acc: 0.8101 - val_loss: 0.3948 - val_acc: 0.8407\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4283 - acc: 0.8084 - val_loss: 0.3962 - val_acc: 0.8440\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4280 - acc: 0.8059 - val_loss: 0.3962 - val_acc: 0.8391\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4277 - acc: 0.8128 - val_loss: 0.3894 - val_acc: 0.8489\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4204 - acc: 0.8168 - val_loss: 0.3954 - val_acc: 0.8391\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4253 - acc: 0.8126 - val_loss: 0.3869 - val_acc: 0.8440\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4141 - acc: 0.8150 - val_loss: 0.3886 - val_acc: 0.8440\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4136 - acc: 0.8201 - val_loss: 0.3895 - val_acc: 0.8473\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4180 - acc: 0.8192 - val_loss: 0.3856 - val_acc: 0.8506\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4143 - acc: 0.8176 - val_loss: 0.3867 - val_acc: 0.8506\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4112 - acc: 0.8214 - val_loss: 0.3852 - val_acc: 0.8440\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4098 - acc: 0.8207 - val_loss: 0.3865 - val_acc: 0.8506\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4122 - acc: 0.8199 - val_loss: 0.3823 - val_acc: 0.8489\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4073 - acc: 0.8216 - val_loss: 0.3809 - val_acc: 0.8506\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4055 - acc: 0.8252 - val_loss: 0.3872 - val_acc: 0.8489\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4082 - acc: 0.8168 - val_loss: 0.3818 - val_acc: 0.8440\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4021 - acc: 0.8285 - val_loss: 0.3798 - val_acc: 0.8473\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.3980 - acc: 0.8270 - val_loss: 0.3797 - val_acc: 0.8473\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.3989 - acc: 0.8254 - val_loss: 0.3769 - val_acc: 0.8506\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3964 - acc: 0.8269 - val_loss: 0.3774 - val_acc: 0.8539\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.3930 - acc: 0.8236 - val_loss: 0.3769 - val_acc: 0.8473\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.3943 - acc: 0.8259 - val_loss: 0.3772 - val_acc: 0.8473\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.3917 - acc: 0.8309 - val_loss: 0.3773 - val_acc: 0.8456\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.3892 - acc: 0.8307 - val_loss: 0.3827 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 2500, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_142\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_143 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_426 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_284 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_427 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_285 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_428 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 401,729\n",
      "Trainable params: 401,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.9917 - acc: 0.4320 - val_loss: 0.7141 - val_acc: 0.4351\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.7207 - acc: 0.4870 - val_loss: 0.6983 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.7141 - acc: 0.5512 - val_loss: 0.6828 - val_acc: 0.5747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6933 - acc: 0.5616 - val_loss: 0.6625 - val_acc: 0.5764\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6782 - acc: 0.5729 - val_loss: 0.6566 - val_acc: 0.6453\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.6711 - acc: 0.5849 - val_loss: 0.6522 - val_acc: 0.6765\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.6628 - acc: 0.6149 - val_loss: 0.6389 - val_acc: 0.6962\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6578 - acc: 0.6243 - val_loss: 0.6258 - val_acc: 0.6749\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6492 - acc: 0.6311 - val_loss: 0.6164 - val_acc: 0.6798\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.6378 - acc: 0.6437 - val_loss: 0.6062 - val_acc: 0.7192\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.6313 - acc: 0.6504 - val_loss: 0.5989 - val_acc: 0.7209\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6296 - acc: 0.6564 - val_loss: 0.5894 - val_acc: 0.7258\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6250 - acc: 0.6619 - val_loss: 0.5809 - val_acc: 0.7323\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.6177 - acc: 0.6670 - val_loss: 0.5738 - val_acc: 0.7438\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6115 - acc: 0.6811 - val_loss: 0.5661 - val_acc: 0.7422\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6040 - acc: 0.6816 - val_loss: 0.5605 - val_acc: 0.7488\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5991 - acc: 0.6917 - val_loss: 0.5500 - val_acc: 0.7570\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5869 - acc: 0.7081 - val_loss: 0.5438 - val_acc: 0.7750\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5849 - acc: 0.7117 - val_loss: 0.5377 - val_acc: 0.7635\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5769 - acc: 0.7052 - val_loss: 0.5319 - val_acc: 0.7718\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5737 - acc: 0.7130 - val_loss: 0.5237 - val_acc: 0.7865\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5654 - acc: 0.7267 - val_loss: 0.5163 - val_acc: 0.7915\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5642 - acc: 0.7302 - val_loss: 0.5117 - val_acc: 0.7865\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5559 - acc: 0.7285 - val_loss: 0.5041 - val_acc: 0.7997\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5526 - acc: 0.7365 - val_loss: 0.4984 - val_acc: 0.8013\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5463 - acc: 0.7420 - val_loss: 0.4935 - val_acc: 0.8013\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5414 - acc: 0.7427 - val_loss: 0.4875 - val_acc: 0.8046\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5355 - acc: 0.7471 - val_loss: 0.4825 - val_acc: 0.8030\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.5349 - acc: 0.7499 - val_loss: 0.4785 - val_acc: 0.8112\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5290 - acc: 0.7562 - val_loss: 0.4739 - val_acc: 0.8030\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5264 - acc: 0.7519 - val_loss: 0.4699 - val_acc: 0.8046\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5206 - acc: 0.7599 - val_loss: 0.4655 - val_acc: 0.8161\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5159 - acc: 0.7694 - val_loss: 0.4610 - val_acc: 0.8112\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5160 - acc: 0.7630 - val_loss: 0.4569 - val_acc: 0.8161\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5060 - acc: 0.7696 - val_loss: 0.4540 - val_acc: 0.8161\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.5028 - acc: 0.7703 - val_loss: 0.4511 - val_acc: 0.8194\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.5005 - acc: 0.7676 - val_loss: 0.4486 - val_acc: 0.8161\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4919 - acc: 0.7812 - val_loss: 0.4449 - val_acc: 0.8161\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4934 - acc: 0.7796 - val_loss: 0.4421 - val_acc: 0.8259\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.4864 - acc: 0.7836 - val_loss: 0.4394 - val_acc: 0.8243\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4914 - acc: 0.7774 - val_loss: 0.4371 - val_acc: 0.8194\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4854 - acc: 0.7891 - val_loss: 0.4347 - val_acc: 0.8292\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4822 - acc: 0.7873 - val_loss: 0.4327 - val_acc: 0.8243\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4800 - acc: 0.7874 - val_loss: 0.4311 - val_acc: 0.8342\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4757 - acc: 0.7893 - val_loss: 0.4285 - val_acc: 0.8358\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4738 - acc: 0.7874 - val_loss: 0.4275 - val_acc: 0.8276\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4741 - acc: 0.7947 - val_loss: 0.4242 - val_acc: 0.8325\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4696 - acc: 0.7924 - val_loss: 0.4229 - val_acc: 0.8276\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4677 - acc: 0.7942 - val_loss: 0.4216 - val_acc: 0.8342\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4707 - acc: 0.7871 - val_loss: 0.4181 - val_acc: 0.8276\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4668 - acc: 0.7898 - val_loss: 0.4166 - val_acc: 0.8358\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4670 - acc: 0.7980 - val_loss: 0.4161 - val_acc: 0.8309\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4619 - acc: 0.7971 - val_loss: 0.4162 - val_acc: 0.8292\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4631 - acc: 0.7907 - val_loss: 0.4148 - val_acc: 0.8325\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4587 - acc: 0.7986 - val_loss: 0.4171 - val_acc: 0.8243\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4618 - acc: 0.7975 - val_loss: 0.4106 - val_acc: 0.8374\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4614 - acc: 0.7953 - val_loss: 0.4083 - val_acc: 0.8391\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4565 - acc: 0.8011 - val_loss: 0.4073 - val_acc: 0.8358\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4531 - acc: 0.8026 - val_loss: 0.4129 - val_acc: 0.8276\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4495 - acc: 0.8051 - val_loss: 0.4092 - val_acc: 0.8292\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4554 - acc: 0.8015 - val_loss: 0.4053 - val_acc: 0.8342\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4526 - acc: 0.7975 - val_loss: 0.4039 - val_acc: 0.8342\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4488 - acc: 0.8011 - val_loss: 0.4034 - val_acc: 0.8391\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4483 - acc: 0.8037 - val_loss: 0.4033 - val_acc: 0.8325\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4447 - acc: 0.8077 - val_loss: 0.4019 - val_acc: 0.8358\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4446 - acc: 0.8070 - val_loss: 0.4018 - val_acc: 0.8342\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4366 - acc: 0.8081 - val_loss: 0.4010 - val_acc: 0.8407\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4393 - acc: 0.8088 - val_loss: 0.3990 - val_acc: 0.8374\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4391 - acc: 0.8097 - val_loss: 0.3981 - val_acc: 0.8374\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4414 - acc: 0.8090 - val_loss: 0.3972 - val_acc: 0.8391\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4381 - acc: 0.8062 - val_loss: 0.3971 - val_acc: 0.8358\n",
      "Epoch 72/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4356 - acc: 0.8115 - val_loss: 0.3967 - val_acc: 0.8358\n",
      "Epoch 73/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4386 - acc: 0.8092 - val_loss: 0.3960 - val_acc: 0.8342\n",
      "Epoch 74/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4358 - acc: 0.8103 - val_loss: 0.3957 - val_acc: 0.8424\n",
      "Epoch 75/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4382 - acc: 0.8104 - val_loss: 0.3948 - val_acc: 0.8440\n",
      "Epoch 76/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4366 - acc: 0.8141 - val_loss: 0.3943 - val_acc: 0.8342\n",
      "Epoch 77/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4352 - acc: 0.8079 - val_loss: 0.3938 - val_acc: 0.8342\n",
      "Epoch 78/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4291 - acc: 0.8124 - val_loss: 0.3935 - val_acc: 0.8391\n",
      "Epoch 79/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4288 - acc: 0.8128 - val_loss: 0.3946 - val_acc: 0.8325\n",
      "Epoch 80/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4317 - acc: 0.8092 - val_loss: 0.3928 - val_acc: 0.8374\n",
      "Epoch 81/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4291 - acc: 0.8132 - val_loss: 0.3926 - val_acc: 0.8407\n",
      "Epoch 82/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.4268 - acc: 0.8165 - val_loss: 0.3929 - val_acc: 0.8391\n",
      "Epoch 83/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4263 - acc: 0.8128 - val_loss: 0.3920 - val_acc: 0.8391\n",
      "Epoch 84/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4271 - acc: 0.8168 - val_loss: 0.3913 - val_acc: 0.8374\n",
      "Epoch 85/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4269 - acc: 0.8150 - val_loss: 0.3906 - val_acc: 0.8391\n",
      "Epoch 86/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4216 - acc: 0.8157 - val_loss: 0.3904 - val_acc: 0.8374\n",
      "Epoch 87/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4219 - acc: 0.8205 - val_loss: 0.3916 - val_acc: 0.8325\n",
      "Epoch 88/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4250 - acc: 0.8157 - val_loss: 0.3905 - val_acc: 0.8391\n",
      "Epoch 89/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4229 - acc: 0.8177 - val_loss: 0.3919 - val_acc: 0.8342\n",
      "Epoch 90/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4228 - acc: 0.8143 - val_loss: 0.3925 - val_acc: 0.8440\n",
      "Epoch 91/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4256 - acc: 0.8112 - val_loss: 0.3918 - val_acc: 0.8358\n",
      "Training with parameters {'batch_size': 2500, 'dropout': 0.1, 'lay_conf': [1024, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_143\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_144 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_429 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_286 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_430 (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_287 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_431 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,836,545\n",
      "Trainable params: 1,836,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 0.8997 - acc: 0.4733 - val_loss: 0.8053 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.7911 - acc: 0.5689 - val_loss: 0.6487 - val_acc: 0.5764\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.6647 - acc: 0.5992 - val_loss: 0.6668 - val_acc: 0.6174\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6758 - acc: 0.5820 - val_loss: 0.6142 - val_acc: 0.6880\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6318 - acc: 0.6455 - val_loss: 0.6096 - val_acc: 0.6108\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.6317 - acc: 0.6322 - val_loss: 0.5887 - val_acc: 0.7028\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.6123 - acc: 0.6751 - val_loss: 0.5755 - val_acc: 0.7274\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.6051 - acc: 0.6756 - val_loss: 0.5624 - val_acc: 0.7422\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5917 - acc: 0.6949 - val_loss: 0.5475 - val_acc: 0.7718\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5860 - acc: 0.7088 - val_loss: 0.5342 - val_acc: 0.7865\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5702 - acc: 0.7210 - val_loss: 0.5294 - val_acc: 0.7635\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5615 - acc: 0.7185 - val_loss: 0.5115 - val_acc: 0.7980\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5527 - acc: 0.7322 - val_loss: 0.5019 - val_acc: 0.8046\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.5379 - acc: 0.7402 - val_loss: 0.4976 - val_acc: 0.7898\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5344 - acc: 0.7415 - val_loss: 0.4835 - val_acc: 0.8095\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.5272 - acc: 0.7548 - val_loss: 0.4733 - val_acc: 0.8128\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5157 - acc: 0.7599 - val_loss: 0.4649 - val_acc: 0.8128\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5090 - acc: 0.7562 - val_loss: 0.4593 - val_acc: 0.8112\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5048 - acc: 0.7694 - val_loss: 0.4504 - val_acc: 0.8177\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4969 - acc: 0.7692 - val_loss: 0.4428 - val_acc: 0.8227\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4922 - acc: 0.7749 - val_loss: 0.4371 - val_acc: 0.8227\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4849 - acc: 0.7785 - val_loss: 0.4314 - val_acc: 0.8243\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4769 - acc: 0.7811 - val_loss: 0.4267 - val_acc: 0.8259\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4735 - acc: 0.7916 - val_loss: 0.4231 - val_acc: 0.8292\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4696 - acc: 0.7864 - val_loss: 0.4189 - val_acc: 0.8292\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4631 - acc: 0.7935 - val_loss: 0.4152 - val_acc: 0.8309\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4601 - acc: 0.7982 - val_loss: 0.4131 - val_acc: 0.8292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4583 - acc: 0.7964 - val_loss: 0.4101 - val_acc: 0.8325\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4516 - acc: 0.7977 - val_loss: 0.4104 - val_acc: 0.8358\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4521 - acc: 0.7935 - val_loss: 0.4100 - val_acc: 0.8374\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4502 - acc: 0.7971 - val_loss: 0.4012 - val_acc: 0.8374\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4521 - acc: 0.7944 - val_loss: 0.4037 - val_acc: 0.8456\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4500 - acc: 0.8019 - val_loss: 0.4046 - val_acc: 0.8424\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4455 - acc: 0.7964 - val_loss: 0.4003 - val_acc: 0.8473\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4427 - acc: 0.8046 - val_loss: 0.4005 - val_acc: 0.8407\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4397 - acc: 0.8009 - val_loss: 0.3935 - val_acc: 0.8506\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4370 - acc: 0.8057 - val_loss: 0.3932 - val_acc: 0.8424\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4320 - acc: 0.8044 - val_loss: 0.3903 - val_acc: 0.8456\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4296 - acc: 0.8093 - val_loss: 0.3922 - val_acc: 0.8440\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4280 - acc: 0.8079 - val_loss: 0.3880 - val_acc: 0.8456\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4280 - acc: 0.8135 - val_loss: 0.3866 - val_acc: 0.8424\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4223 - acc: 0.8170 - val_loss: 0.3899 - val_acc: 0.8506\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4235 - acc: 0.8126 - val_loss: 0.3855 - val_acc: 0.8424\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4229 - acc: 0.8106 - val_loss: 0.3842 - val_acc: 0.8440\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4134 - acc: 0.8174 - val_loss: 0.3841 - val_acc: 0.8522\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4180 - acc: 0.8141 - val_loss: 0.3933 - val_acc: 0.8473\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4210 - acc: 0.8084 - val_loss: 0.3986 - val_acc: 0.8407\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4238 - acc: 0.8159 - val_loss: 0.4076 - val_acc: 0.8309\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4236 - acc: 0.8055 - val_loss: 0.4076 - val_acc: 0.8440\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4287 - acc: 0.8123 - val_loss: 0.3908 - val_acc: 0.8456\n",
      "Training with parameters {'batch_size': 3000, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_144\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_145 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_432 (Dense)            (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_288 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_433 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_289 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_434 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.8325 - acc: 0.4313 - val_loss: 0.7159 - val_acc: 0.4433\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.7214 - acc: 0.4758 - val_loss: 0.6808 - val_acc: 0.5649\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6894 - acc: 0.5450 - val_loss: 0.6809 - val_acc: 0.5731\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6904 - acc: 0.5645 - val_loss: 0.6794 - val_acc: 0.5747\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6902 - acc: 0.5718 - val_loss: 0.6695 - val_acc: 0.5747\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6799 - acc: 0.5729 - val_loss: 0.6579 - val_acc: 0.5747\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6702 - acc: 0.5895 - val_loss: 0.6510 - val_acc: 0.5977\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6653 - acc: 0.5908 - val_loss: 0.6482 - val_acc: 0.6486\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6616 - acc: 0.6158 - val_loss: 0.6451 - val_acc: 0.6831\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6598 - acc: 0.6130 - val_loss: 0.6386 - val_acc: 0.6864\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6570 - acc: 0.6211 - val_loss: 0.6304 - val_acc: 0.7044\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6464 - acc: 0.6355 - val_loss: 0.6232 - val_acc: 0.6798\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6480 - acc: 0.6231 - val_loss: 0.6168 - val_acc: 0.6765\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6409 - acc: 0.6336 - val_loss: 0.6092 - val_acc: 0.7077\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6348 - acc: 0.6477 - val_loss: 0.6013 - val_acc: 0.7126\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6273 - acc: 0.6606 - val_loss: 0.5944 - val_acc: 0.7258\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6227 - acc: 0.6720 - val_loss: 0.5878 - val_acc: 0.7209\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6196 - acc: 0.6634 - val_loss: 0.5812 - val_acc: 0.7340\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6170 - acc: 0.6740 - val_loss: 0.5752 - val_acc: 0.7406\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6106 - acc: 0.6847 - val_loss: 0.5693 - val_acc: 0.7504\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6050 - acc: 0.6831 - val_loss: 0.5634 - val_acc: 0.7537\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6010 - acc: 0.6962 - val_loss: 0.5578 - val_acc: 0.7553\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5985 - acc: 0.6946 - val_loss: 0.5521 - val_acc: 0.7570\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5895 - acc: 0.7044 - val_loss: 0.5464 - val_acc: 0.7635\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5850 - acc: 0.7112 - val_loss: 0.5409 - val_acc: 0.7635\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5809 - acc: 0.7094 - val_loss: 0.5353 - val_acc: 0.7734\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5794 - acc: 0.7145 - val_loss: 0.5299 - val_acc: 0.7816\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5708 - acc: 0.7249 - val_loss: 0.5247 - val_acc: 0.7915\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5698 - acc: 0.7240 - val_loss: 0.5200 - val_acc: 0.7800\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5644 - acc: 0.7218 - val_loss: 0.5147 - val_acc: 0.7947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5606 - acc: 0.7345 - val_loss: 0.5099 - val_acc: 0.7964\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5532 - acc: 0.7380 - val_loss: 0.5053 - val_acc: 0.7947\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5510 - acc: 0.7433 - val_loss: 0.5008 - val_acc: 0.7997\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5451 - acc: 0.7482 - val_loss: 0.4964 - val_acc: 0.8030\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.5424 - acc: 0.7460 - val_loss: 0.4922 - val_acc: 0.8030\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5391 - acc: 0.7515 - val_loss: 0.4882 - val_acc: 0.8046\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5342 - acc: 0.7544 - val_loss: 0.4843 - val_acc: 0.8046\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5343 - acc: 0.7499 - val_loss: 0.4804 - val_acc: 0.8062\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5301 - acc: 0.7570 - val_loss: 0.4765 - val_acc: 0.8062\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5237 - acc: 0.7617 - val_loss: 0.4726 - val_acc: 0.8112\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5217 - acc: 0.7683 - val_loss: 0.4690 - val_acc: 0.8144\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5188 - acc: 0.7590 - val_loss: 0.4654 - val_acc: 0.8112\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5154 - acc: 0.7645 - val_loss: 0.4625 - val_acc: 0.8079\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5132 - acc: 0.7652 - val_loss: 0.4589 - val_acc: 0.8161\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5114 - acc: 0.7652 - val_loss: 0.4556 - val_acc: 0.8194\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5065 - acc: 0.7738 - val_loss: 0.4535 - val_acc: 0.8079\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5041 - acc: 0.7714 - val_loss: 0.4495 - val_acc: 0.8194\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4995 - acc: 0.7730 - val_loss: 0.4469 - val_acc: 0.8227\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4975 - acc: 0.7801 - val_loss: 0.4447 - val_acc: 0.8095\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4977 - acc: 0.7705 - val_loss: 0.4417 - val_acc: 0.8292\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4905 - acc: 0.7827 - val_loss: 0.4398 - val_acc: 0.8276\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4936 - acc: 0.7798 - val_loss: 0.4371 - val_acc: 0.8210\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4918 - acc: 0.7794 - val_loss: 0.4347 - val_acc: 0.8243\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4865 - acc: 0.7811 - val_loss: 0.4324 - val_acc: 0.8325\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4808 - acc: 0.7887 - val_loss: 0.4302 - val_acc: 0.8309\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4845 - acc: 0.7814 - val_loss: 0.4282 - val_acc: 0.8309\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4785 - acc: 0.7878 - val_loss: 0.4264 - val_acc: 0.8358\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4800 - acc: 0.7895 - val_loss: 0.4248 - val_acc: 0.8342\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4749 - acc: 0.7909 - val_loss: 0.4231 - val_acc: 0.8259\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4746 - acc: 0.7891 - val_loss: 0.4214 - val_acc: 0.8325\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4734 - acc: 0.7927 - val_loss: 0.4198 - val_acc: 0.8309\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4690 - acc: 0.7944 - val_loss: 0.4183 - val_acc: 0.8292\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4679 - acc: 0.7924 - val_loss: 0.4169 - val_acc: 0.8342\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4652 - acc: 0.7900 - val_loss: 0.4149 - val_acc: 0.8309\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4636 - acc: 0.7958 - val_loss: 0.4136 - val_acc: 0.8342\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4617 - acc: 0.7927 - val_loss: 0.4124 - val_acc: 0.8325\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4642 - acc: 0.7986 - val_loss: 0.4104 - val_acc: 0.8391\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4591 - acc: 0.7958 - val_loss: 0.4090 - val_acc: 0.8391\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4607 - acc: 0.7951 - val_loss: 0.4080 - val_acc: 0.8374\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4548 - acc: 0.7995 - val_loss: 0.4065 - val_acc: 0.8391\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4554 - acc: 0.7940 - val_loss: 0.4054 - val_acc: 0.8391\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4515 - acc: 0.8022 - val_loss: 0.4044 - val_acc: 0.8424\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4534 - acc: 0.8017 - val_loss: 0.4032 - val_acc: 0.8358\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4519 - acc: 0.8009 - val_loss: 0.4021 - val_acc: 0.8374\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4496 - acc: 0.8020 - val_loss: 0.4019 - val_acc: 0.8391\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4496 - acc: 0.8053 - val_loss: 0.4018 - val_acc: 0.8440\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4535 - acc: 0.8002 - val_loss: 0.3997 - val_acc: 0.8407\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4460 - acc: 0.8053 - val_loss: 0.3988 - val_acc: 0.8391\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4502 - acc: 0.8035 - val_loss: 0.3984 - val_acc: 0.8424\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4429 - acc: 0.8070 - val_loss: 0.3976 - val_acc: 0.8440\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4457 - acc: 0.8057 - val_loss: 0.3968 - val_acc: 0.8391\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4407 - acc: 0.8075 - val_loss: 0.3968 - val_acc: 0.8424\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4403 - acc: 0.8053 - val_loss: 0.3957 - val_acc: 0.8424\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4400 - acc: 0.8090 - val_loss: 0.3951 - val_acc: 0.8456\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4360 - acc: 0.8099 - val_loss: 0.3944 - val_acc: 0.8424\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4360 - acc: 0.8097 - val_loss: 0.3934 - val_acc: 0.8424\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4349 - acc: 0.8115 - val_loss: 0.3928 - val_acc: 0.8473\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4347 - acc: 0.8051 - val_loss: 0.3922 - val_acc: 0.8440\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4351 - acc: 0.8075 - val_loss: 0.3915 - val_acc: 0.8424\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4348 - acc: 0.8088 - val_loss: 0.3913 - val_acc: 0.8456\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4295 - acc: 0.8128 - val_loss: 0.3906 - val_acc: 0.8456\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4295 - acc: 0.8124 - val_loss: 0.3901 - val_acc: 0.8473\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4301 - acc: 0.8092 - val_loss: 0.3895 - val_acc: 0.8489\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4321 - acc: 0.8119 - val_loss: 0.3891 - val_acc: 0.8456\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4256 - acc: 0.8146 - val_loss: 0.3887 - val_acc: 0.8489\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4311 - acc: 0.8079 - val_loss: 0.3882 - val_acc: 0.8489\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4280 - acc: 0.8146 - val_loss: 0.3877 - val_acc: 0.8456\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4231 - acc: 0.8159 - val_loss: 0.3867 - val_acc: 0.8473\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4269 - acc: 0.8159 - val_loss: 0.3863 - val_acc: 0.8473\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4236 - acc: 0.8150 - val_loss: 0.3864 - val_acc: 0.8424\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4224 - acc: 0.8134 - val_loss: 0.3858 - val_acc: 0.8473\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4230 - acc: 0.8135 - val_loss: 0.3856 - val_acc: 0.8456\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4231 - acc: 0.8090 - val_loss: 0.3852 - val_acc: 0.8473\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4193 - acc: 0.8203 - val_loss: 0.3852 - val_acc: 0.8456\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4194 - acc: 0.8176 - val_loss: 0.3853 - val_acc: 0.8440\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4195 - acc: 0.8172 - val_loss: 0.3843 - val_acc: 0.8473\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4190 - acc: 0.8170 - val_loss: 0.3839 - val_acc: 0.8473\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4166 - acc: 0.8197 - val_loss: 0.3836 - val_acc: 0.8456\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4160 - acc: 0.8186 - val_loss: 0.3830 - val_acc: 0.8473\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4150 - acc: 0.8161 - val_loss: 0.3829 - val_acc: 0.8473\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4160 - acc: 0.8168 - val_loss: 0.3829 - val_acc: 0.8456\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4160 - acc: 0.8165 - val_loss: 0.3822 - val_acc: 0.8456\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4140 - acc: 0.8190 - val_loss: 0.3826 - val_acc: 0.8473\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4145 - acc: 0.8230 - val_loss: 0.3828 - val_acc: 0.8456\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4098 - acc: 0.8227 - val_loss: 0.3816 - val_acc: 0.8489\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4114 - acc: 0.8194 - val_loss: 0.3813 - val_acc: 0.8489\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4133 - acc: 0.8199 - val_loss: 0.3832 - val_acc: 0.8424\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4096 - acc: 0.8225 - val_loss: 0.3807 - val_acc: 0.8506\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4095 - acc: 0.8217 - val_loss: 0.3803 - val_acc: 0.8506\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4070 - acc: 0.8205 - val_loss: 0.3824 - val_acc: 0.8424\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4107 - acc: 0.8225 - val_loss: 0.3801 - val_acc: 0.8489\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4084 - acc: 0.8207 - val_loss: 0.3819 - val_acc: 0.8473\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4075 - acc: 0.8252 - val_loss: 0.3813 - val_acc: 0.8440\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4088 - acc: 0.8192 - val_loss: 0.3796 - val_acc: 0.8489\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4055 - acc: 0.8238 - val_loss: 0.3802 - val_acc: 0.8473\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4061 - acc: 0.8205 - val_loss: 0.3798 - val_acc: 0.8440\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4040 - acc: 0.8261 - val_loss: 0.3783 - val_acc: 0.8489\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4079 - acc: 0.8248 - val_loss: 0.3785 - val_acc: 0.8506\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4054 - acc: 0.8243 - val_loss: 0.3800 - val_acc: 0.8456\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4033 - acc: 0.8207 - val_loss: 0.3781 - val_acc: 0.8522\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4057 - acc: 0.8236 - val_loss: 0.3774 - val_acc: 0.8522\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4031 - acc: 0.8245 - val_loss: 0.3789 - val_acc: 0.8440\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4028 - acc: 0.8245 - val_loss: 0.3771 - val_acc: 0.8539\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3994 - acc: 0.8258 - val_loss: 0.3771 - val_acc: 0.8489\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3996 - acc: 0.8270 - val_loss: 0.3795 - val_acc: 0.8473\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4003 - acc: 0.8230 - val_loss: 0.3769 - val_acc: 0.8506\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3990 - acc: 0.8261 - val_loss: 0.3767 - val_acc: 0.8506\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3969 - acc: 0.8311 - val_loss: 0.3773 - val_acc: 0.8456\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4020 - acc: 0.8238 - val_loss: 0.3760 - val_acc: 0.8522\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3939 - acc: 0.8298 - val_loss: 0.3759 - val_acc: 0.8539\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3976 - acc: 0.8236 - val_loss: 0.3769 - val_acc: 0.8489\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3952 - acc: 0.8283 - val_loss: 0.3756 - val_acc: 0.8522\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.3958 - acc: 0.8267 - val_loss: 0.3756 - val_acc: 0.8539\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3956 - acc: 0.8254 - val_loss: 0.3762 - val_acc: 0.8489\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3935 - acc: 0.8303 - val_loss: 0.3751 - val_acc: 0.8522\n",
      "Epoch 146/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3940 - acc: 0.8321 - val_loss: 0.3748 - val_acc: 0.8489\n",
      "Epoch 147/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.3925 - acc: 0.8289 - val_loss: 0.3752 - val_acc: 0.8489\n",
      "Epoch 148/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3904 - acc: 0.8294 - val_loss: 0.3747 - val_acc: 0.8506\n",
      "Epoch 149/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3916 - acc: 0.8300 - val_loss: 0.3750 - val_acc: 0.8489\n",
      "Epoch 150/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3901 - acc: 0.8331 - val_loss: 0.3753 - val_acc: 0.8506\n",
      "Epoch 151/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3905 - acc: 0.8332 - val_loss: 0.3747 - val_acc: 0.8489\n",
      "Epoch 152/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3892 - acc: 0.8292 - val_loss: 0.3745 - val_acc: 0.8473\n",
      "Epoch 153/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3896 - acc: 0.8309 - val_loss: 0.3753 - val_acc: 0.8522\n",
      "Epoch 154/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3907 - acc: 0.8305 - val_loss: 0.3745 - val_acc: 0.8489\n",
      "Epoch 155/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3882 - acc: 0.8289 - val_loss: 0.3749 - val_acc: 0.8473\n",
      "Epoch 156/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3898 - acc: 0.8354 - val_loss: 0.3750 - val_acc: 0.8506\n",
      "Epoch 157/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3871 - acc: 0.8314 - val_loss: 0.3747 - val_acc: 0.8522\n",
      "Training with parameters {'batch_size': 3000, 'dropout': 0.1, 'lay_conf': [1024, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_145\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_146 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_435 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_290 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_436 (Dense)            (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_291 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_437 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.8855 - acc: 0.4589 - val_loss: 0.7627 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.7883 - acc: 0.5650 - val_loss: 0.7522 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.7380 - acc: 0.5689 - val_loss: 0.6596 - val_acc: 0.5780\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.6732 - acc: 0.5840 - val_loss: 0.6868 - val_acc: 0.5452\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.6942 - acc: 0.5526 - val_loss: 0.6636 - val_acc: 0.6223\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.6645 - acc: 0.6037 - val_loss: 0.6157 - val_acc: 0.7044\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6417 - acc: 0.6216 - val_loss: 0.6220 - val_acc: 0.5862\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.6484 - acc: 0.6008 - val_loss: 0.6069 - val_acc: 0.6256\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.6281 - acc: 0.6415 - val_loss: 0.5845 - val_acc: 0.7258\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.6150 - acc: 0.6721 - val_loss: 0.5909 - val_acc: 0.6831\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.6141 - acc: 0.6594 - val_loss: 0.5772 - val_acc: 0.7094\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.6017 - acc: 0.6825 - val_loss: 0.5582 - val_acc: 0.7635\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.5902 - acc: 0.7083 - val_loss: 0.5569 - val_acc: 0.7471\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.5953 - acc: 0.6957 - val_loss: 0.5435 - val_acc: 0.7767\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.5811 - acc: 0.7168 - val_loss: 0.5378 - val_acc: 0.7521\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.5743 - acc: 0.7077 - val_loss: 0.5325 - val_acc: 0.7570\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.5653 - acc: 0.7137 - val_loss: 0.5196 - val_acc: 0.7865\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.5598 - acc: 0.7351 - val_loss: 0.5155 - val_acc: 0.7767\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.5555 - acc: 0.7382 - val_loss: 0.5040 - val_acc: 0.8013\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.5446 - acc: 0.7362 - val_loss: 0.5006 - val_acc: 0.7898\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5418 - acc: 0.7344 - val_loss: 0.4893 - val_acc: 0.8062\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5315 - acc: 0.7468 - val_loss: 0.4841 - val_acc: 0.8013\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5293 - acc: 0.7479 - val_loss: 0.4773 - val_acc: 0.8112\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5212 - acc: 0.7519 - val_loss: 0.4713 - val_acc: 0.8194\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5163 - acc: 0.7579 - val_loss: 0.4646 - val_acc: 0.8161\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5123 - acc: 0.7612 - val_loss: 0.4585 - val_acc: 0.8227\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5062 - acc: 0.7637 - val_loss: 0.4535 - val_acc: 0.8243\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4996 - acc: 0.7723 - val_loss: 0.4474 - val_acc: 0.8210\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4918 - acc: 0.7750 - val_loss: 0.4426 - val_acc: 0.8292\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4927 - acc: 0.7741 - val_loss: 0.4380 - val_acc: 0.8292\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4874 - acc: 0.7747 - val_loss: 0.4339 - val_acc: 0.8276\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4798 - acc: 0.7767 - val_loss: 0.4296 - val_acc: 0.8276\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4747 - acc: 0.7871 - val_loss: 0.4259 - val_acc: 0.8292\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4694 - acc: 0.7842 - val_loss: 0.4236 - val_acc: 0.8276\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4714 - acc: 0.7849 - val_loss: 0.4189 - val_acc: 0.8292\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4676 - acc: 0.7895 - val_loss: 0.4158 - val_acc: 0.8325\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4620 - acc: 0.7916 - val_loss: 0.4130 - val_acc: 0.8342\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4575 - acc: 0.7905 - val_loss: 0.4104 - val_acc: 0.8292\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4547 - acc: 0.7922 - val_loss: 0.4092 - val_acc: 0.8358\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4540 - acc: 0.7949 - val_loss: 0.4062 - val_acc: 0.8325\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4499 - acc: 0.7944 - val_loss: 0.4036 - val_acc: 0.8407\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4478 - acc: 0.8030 - val_loss: 0.4023 - val_acc: 0.8391\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4461 - acc: 0.7999 - val_loss: 0.4002 - val_acc: 0.8374\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.4412 - acc: 0.8033 - val_loss: 0.4007 - val_acc: 0.8424\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4409 - acc: 0.8031 - val_loss: 0.3977 - val_acc: 0.8358\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4353 - acc: 0.8075 - val_loss: 0.3975 - val_acc: 0.8440\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4348 - acc: 0.8077 - val_loss: 0.3946 - val_acc: 0.8391\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4335 - acc: 0.8134 - val_loss: 0.3933 - val_acc: 0.8391\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4293 - acc: 0.8113 - val_loss: 0.3920 - val_acc: 0.8407\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4295 - acc: 0.8072 - val_loss: 0.3901 - val_acc: 0.8424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4266 - acc: 0.8090 - val_loss: 0.3892 - val_acc: 0.8424\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4247 - acc: 0.8135 - val_loss: 0.3898 - val_acc: 0.8407\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4247 - acc: 0.8108 - val_loss: 0.3875 - val_acc: 0.8440\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4222 - acc: 0.8168 - val_loss: 0.3871 - val_acc: 0.8407\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4220 - acc: 0.8128 - val_loss: 0.3850 - val_acc: 0.8424\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4154 - acc: 0.8168 - val_loss: 0.3840 - val_acc: 0.8374\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4133 - acc: 0.8159 - val_loss: 0.3844 - val_acc: 0.8407\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4149 - acc: 0.8163 - val_loss: 0.3825 - val_acc: 0.8374\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4118 - acc: 0.8199 - val_loss: 0.3826 - val_acc: 0.8391\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4128 - acc: 0.8208 - val_loss: 0.3821 - val_acc: 0.8391\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4080 - acc: 0.8205 - val_loss: 0.3814 - val_acc: 0.8358\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4065 - acc: 0.8223 - val_loss: 0.3806 - val_acc: 0.8358\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.4022 - acc: 0.8256 - val_loss: 0.3800 - val_acc: 0.8374\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4050 - acc: 0.8256 - val_loss: 0.3807 - val_acc: 0.8424\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4017 - acc: 0.8225 - val_loss: 0.3794 - val_acc: 0.8440\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4006 - acc: 0.8236 - val_loss: 0.3797 - val_acc: 0.8440\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3951 - acc: 0.8298 - val_loss: 0.3798 - val_acc: 0.8473\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.3969 - acc: 0.8254 - val_loss: 0.3839 - val_acc: 0.8440\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.3983 - acc: 0.8265 - val_loss: 0.3788 - val_acc: 0.8489\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.3966 - acc: 0.8272 - val_loss: 0.3791 - val_acc: 0.8473\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.3957 - acc: 0.8267 - val_loss: 0.3778 - val_acc: 0.8456\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.3928 - acc: 0.8280 - val_loss: 0.3791 - val_acc: 0.8506\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.3948 - acc: 0.8290 - val_loss: 0.3821 - val_acc: 0.8440\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.3953 - acc: 0.8250 - val_loss: 0.3773 - val_acc: 0.8506\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.3927 - acc: 0.8278 - val_loss: 0.3750 - val_acc: 0.8424\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.3873 - acc: 0.8327 - val_loss: 0.3741 - val_acc: 0.8440\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.3820 - acc: 0.8318 - val_loss: 0.3749 - val_acc: 0.8456\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.3866 - acc: 0.8305 - val_loss: 0.3753 - val_acc: 0.8473\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.3816 - acc: 0.8332 - val_loss: 0.3746 - val_acc: 0.8440\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.3828 - acc: 0.8334 - val_loss: 0.3755 - val_acc: 0.8456\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.3794 - acc: 0.8342 - val_loss: 0.3745 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 3000, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_146\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_147 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_438 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_292 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_439 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_293 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_440 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,985\n",
      "Trainable params: 409,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 1.7254 - acc: 0.4320 - val_loss: 1.1890 - val_acc: 0.4253\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 1.0955 - acc: 0.4337 - val_loss: 0.7781 - val_acc: 0.4286\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.7774 - acc: 0.4711 - val_loss: 0.7065 - val_acc: 0.5747\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.7296 - acc: 0.5349 - val_loss: 0.7575 - val_acc: 0.5731\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.7777 - acc: 0.5596 - val_loss: 0.7781 - val_acc: 0.5747\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.7876 - acc: 0.5661 - val_loss: 0.7544 - val_acc: 0.5747\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.7518 - acc: 0.5672 - val_loss: 0.7160 - val_acc: 0.5747\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.7226 - acc: 0.5680 - val_loss: 0.6824 - val_acc: 0.5747\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6880 - acc: 0.5700 - val_loss: 0.6636 - val_acc: 0.5862\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6799 - acc: 0.5780 - val_loss: 0.6599 - val_acc: 0.6322\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6819 - acc: 0.5736 - val_loss: 0.6611 - val_acc: 0.6765\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6856 - acc: 0.5594 - val_loss: 0.6578 - val_acc: 0.6847\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6786 - acc: 0.5749 - val_loss: 0.6481 - val_acc: 0.7094\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6673 - acc: 0.5970 - val_loss: 0.6370 - val_acc: 0.7143\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6549 - acc: 0.6150 - val_loss: 0.6305 - val_acc: 0.6256\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6518 - acc: 0.6201 - val_loss: 0.6270 - val_acc: 0.6092\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6478 - acc: 0.6183 - val_loss: 0.6221 - val_acc: 0.6092\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6431 - acc: 0.6240 - val_loss: 0.6144 - val_acc: 0.6535\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6412 - acc: 0.6369 - val_loss: 0.6071 - val_acc: 0.7225\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6319 - acc: 0.6515 - val_loss: 0.6028 - val_acc: 0.7471\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6339 - acc: 0.6539 - val_loss: 0.5994 - val_acc: 0.7356\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6306 - acc: 0.6561 - val_loss: 0.5939 - val_acc: 0.7373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6208 - acc: 0.6683 - val_loss: 0.5872 - val_acc: 0.7537\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6182 - acc: 0.6637 - val_loss: 0.5816 - val_acc: 0.7619\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6136 - acc: 0.6776 - val_loss: 0.5773 - val_acc: 0.7521\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6151 - acc: 0.6794 - val_loss: 0.5724 - val_acc: 0.7668\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6083 - acc: 0.6844 - val_loss: 0.5666 - val_acc: 0.7701\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6044 - acc: 0.6871 - val_loss: 0.5624 - val_acc: 0.7652\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5958 - acc: 0.6993 - val_loss: 0.5585 - val_acc: 0.7668\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5957 - acc: 0.6915 - val_loss: 0.5534 - val_acc: 0.7767\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5930 - acc: 0.6980 - val_loss: 0.5482 - val_acc: 0.7767\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5894 - acc: 0.7030 - val_loss: 0.5439 - val_acc: 0.7750\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5851 - acc: 0.7075 - val_loss: 0.5400 - val_acc: 0.7800\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5875 - acc: 0.7074 - val_loss: 0.5361 - val_acc: 0.7849\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5778 - acc: 0.7106 - val_loss: 0.5324 - val_acc: 0.7865\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5793 - acc: 0.7170 - val_loss: 0.5288 - val_acc: 0.7931\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5738 - acc: 0.7240 - val_loss: 0.5252 - val_acc: 0.7915\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5713 - acc: 0.7212 - val_loss: 0.5219 - val_acc: 0.8013\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5670 - acc: 0.7294 - val_loss: 0.5185 - val_acc: 0.7980\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5627 - acc: 0.7261 - val_loss: 0.5152 - val_acc: 0.7980\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5623 - acc: 0.7227 - val_loss: 0.5120 - val_acc: 0.8046\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5633 - acc: 0.7241 - val_loss: 0.5088 - val_acc: 0.8062\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5540 - acc: 0.7380 - val_loss: 0.5056 - val_acc: 0.8030\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5549 - acc: 0.7338 - val_loss: 0.5026 - val_acc: 0.8062\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5469 - acc: 0.7407 - val_loss: 0.4994 - val_acc: 0.8079\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5459 - acc: 0.7402 - val_loss: 0.4963 - val_acc: 0.8046\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5419 - acc: 0.7420 - val_loss: 0.4930 - val_acc: 0.8062\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.5446 - acc: 0.7396 - val_loss: 0.4901 - val_acc: 0.8112\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5351 - acc: 0.7468 - val_loss: 0.4870 - val_acc: 0.8095\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5361 - acc: 0.7482 - val_loss: 0.4842 - val_acc: 0.8128\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5306 - acc: 0.7480 - val_loss: 0.4810 - val_acc: 0.8112\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5283 - acc: 0.7526 - val_loss: 0.4780 - val_acc: 0.8128\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.5211 - acc: 0.7573 - val_loss: 0.4751 - val_acc: 0.8128\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5257 - acc: 0.7541 - val_loss: 0.4723 - val_acc: 0.8144\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5242 - acc: 0.7522 - val_loss: 0.4697 - val_acc: 0.8128\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5206 - acc: 0.7552 - val_loss: 0.4674 - val_acc: 0.8144\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5132 - acc: 0.7677 - val_loss: 0.4651 - val_acc: 0.8177\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5124 - acc: 0.7645 - val_loss: 0.4630 - val_acc: 0.8177\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5109 - acc: 0.7646 - val_loss: 0.4609 - val_acc: 0.8161\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5131 - acc: 0.7694 - val_loss: 0.4588 - val_acc: 0.8177\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5101 - acc: 0.7670 - val_loss: 0.4569 - val_acc: 0.8161\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5029 - acc: 0.7714 - val_loss: 0.4549 - val_acc: 0.8177\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5054 - acc: 0.7692 - val_loss: 0.4528 - val_acc: 0.8194\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5034 - acc: 0.7668 - val_loss: 0.4510 - val_acc: 0.8161\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5009 - acc: 0.7727 - val_loss: 0.4488 - val_acc: 0.8210\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5019 - acc: 0.7668 - val_loss: 0.4469 - val_acc: 0.8210\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4988 - acc: 0.7716 - val_loss: 0.4453 - val_acc: 0.8194\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4952 - acc: 0.7760 - val_loss: 0.4433 - val_acc: 0.8194\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4906 - acc: 0.7800 - val_loss: 0.4413 - val_acc: 0.8243\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4881 - acc: 0.7803 - val_loss: 0.4395 - val_acc: 0.8243\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4887 - acc: 0.7783 - val_loss: 0.4378 - val_acc: 0.8276\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4863 - acc: 0.7805 - val_loss: 0.4361 - val_acc: 0.8292\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4819 - acc: 0.7896 - val_loss: 0.4347 - val_acc: 0.8276\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4823 - acc: 0.7836 - val_loss: 0.4327 - val_acc: 0.8292\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4787 - acc: 0.7849 - val_loss: 0.4310 - val_acc: 0.8292\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4795 - acc: 0.7811 - val_loss: 0.4298 - val_acc: 0.8325\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4783 - acc: 0.7807 - val_loss: 0.4281 - val_acc: 0.8342\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4764 - acc: 0.7842 - val_loss: 0.4271 - val_acc: 0.8391\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4757 - acc: 0.7803 - val_loss: 0.4254 - val_acc: 0.8342\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4752 - acc: 0.7874 - val_loss: 0.4243 - val_acc: 0.8325\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4707 - acc: 0.7922 - val_loss: 0.4228 - val_acc: 0.8424\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4734 - acc: 0.7940 - val_loss: 0.4218 - val_acc: 0.8325\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4656 - acc: 0.7955 - val_loss: 0.4207 - val_acc: 0.8391\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4709 - acc: 0.7893 - val_loss: 0.4197 - val_acc: 0.8391\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4662 - acc: 0.7922 - val_loss: 0.4190 - val_acc: 0.8358\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4673 - acc: 0.7904 - val_loss: 0.4181 - val_acc: 0.8391\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4638 - acc: 0.7931 - val_loss: 0.4170 - val_acc: 0.8391\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4614 - acc: 0.7931 - val_loss: 0.4161 - val_acc: 0.8374\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4616 - acc: 0.7949 - val_loss: 0.4152 - val_acc: 0.8407\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4590 - acc: 0.7975 - val_loss: 0.4143 - val_acc: 0.8440\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4609 - acc: 0.8000 - val_loss: 0.4132 - val_acc: 0.8456\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4539 - acc: 0.7953 - val_loss: 0.4122 - val_acc: 0.8440\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4553 - acc: 0.7947 - val_loss: 0.4113 - val_acc: 0.8391\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4547 - acc: 0.7951 - val_loss: 0.4106 - val_acc: 0.8473\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4556 - acc: 0.7995 - val_loss: 0.4096 - val_acc: 0.8440\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4505 - acc: 0.7977 - val_loss: 0.4088 - val_acc: 0.8473\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4561 - acc: 0.7953 - val_loss: 0.4081 - val_acc: 0.8456\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4504 - acc: 0.8009 - val_loss: 0.4075 - val_acc: 0.8440\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4502 - acc: 0.8008 - val_loss: 0.4067 - val_acc: 0.8473\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4484 - acc: 0.8035 - val_loss: 0.4064 - val_acc: 0.8456\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4465 - acc: 0.7978 - val_loss: 0.4055 - val_acc: 0.8473\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4460 - acc: 0.8026 - val_loss: 0.4049 - val_acc: 0.8440\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4471 - acc: 0.8013 - val_loss: 0.4042 - val_acc: 0.8440\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4392 - acc: 0.8090 - val_loss: 0.4036 - val_acc: 0.8489\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4441 - acc: 0.8030 - val_loss: 0.4032 - val_acc: 0.8424\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4444 - acc: 0.8066 - val_loss: 0.4028 - val_acc: 0.8440\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4419 - acc: 0.8024 - val_loss: 0.4022 - val_acc: 0.8456\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4407 - acc: 0.8101 - val_loss: 0.4018 - val_acc: 0.8391\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4387 - acc: 0.8053 - val_loss: 0.4014 - val_acc: 0.8391\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4371 - acc: 0.8037 - val_loss: 0.4009 - val_acc: 0.8407\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4370 - acc: 0.8088 - val_loss: 0.4003 - val_acc: 0.8407\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4386 - acc: 0.8073 - val_loss: 0.3999 - val_acc: 0.8407\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4343 - acc: 0.8093 - val_loss: 0.3993 - val_acc: 0.8440\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4375 - acc: 0.8077 - val_loss: 0.3986 - val_acc: 0.8440\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4347 - acc: 0.8072 - val_loss: 0.3990 - val_acc: 0.8456\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4332 - acc: 0.8113 - val_loss: 0.3979 - val_acc: 0.8424\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4351 - acc: 0.8070 - val_loss: 0.3977 - val_acc: 0.8424\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4338 - acc: 0.8095 - val_loss: 0.3973 - val_acc: 0.8424\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4385 - acc: 0.8053 - val_loss: 0.3967 - val_acc: 0.8424\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4322 - acc: 0.8137 - val_loss: 0.3968 - val_acc: 0.8424\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4299 - acc: 0.8095 - val_loss: 0.3955 - val_acc: 0.8424\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4315 - acc: 0.8110 - val_loss: 0.3978 - val_acc: 0.8456\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4289 - acc: 0.8154 - val_loss: 0.3947 - val_acc: 0.8391\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4295 - acc: 0.8104 - val_loss: 0.3946 - val_acc: 0.8424\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4237 - acc: 0.8148 - val_loss: 0.3954 - val_acc: 0.8440\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4250 - acc: 0.8179 - val_loss: 0.3934 - val_acc: 0.8424\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4261 - acc: 0.8141 - val_loss: 0.3941 - val_acc: 0.8424\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4309 - acc: 0.8112 - val_loss: 0.3927 - val_acc: 0.8440\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4206 - acc: 0.8146 - val_loss: 0.3925 - val_acc: 0.8440\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4225 - acc: 0.8155 - val_loss: 0.3922 - val_acc: 0.8391\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4217 - acc: 0.8126 - val_loss: 0.3920 - val_acc: 0.8407\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4194 - acc: 0.8159 - val_loss: 0.3921 - val_acc: 0.8440\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4236 - acc: 0.8199 - val_loss: 0.3917 - val_acc: 0.8391\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4190 - acc: 0.8181 - val_loss: 0.3914 - val_acc: 0.8391\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4214 - acc: 0.8139 - val_loss: 0.3915 - val_acc: 0.8456\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4160 - acc: 0.8185 - val_loss: 0.3912 - val_acc: 0.8424\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4201 - acc: 0.8166 - val_loss: 0.3908 - val_acc: 0.8407\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4220 - acc: 0.8108 - val_loss: 0.3905 - val_acc: 0.8407\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4210 - acc: 0.8179 - val_loss: 0.3904 - val_acc: 0.8424\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4183 - acc: 0.8152 - val_loss: 0.3901 - val_acc: 0.8391\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4209 - acc: 0.8139 - val_loss: 0.3897 - val_acc: 0.8391\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4187 - acc: 0.8199 - val_loss: 0.3897 - val_acc: 0.8440\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4166 - acc: 0.8194 - val_loss: 0.3891 - val_acc: 0.8407\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4199 - acc: 0.8154 - val_loss: 0.3891 - val_acc: 0.8407\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4158 - acc: 0.8166 - val_loss: 0.3888 - val_acc: 0.8391\n",
      "Epoch 146/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4135 - acc: 0.8205 - val_loss: 0.3884 - val_acc: 0.8424\n",
      "Epoch 147/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4111 - acc: 0.8227 - val_loss: 0.3880 - val_acc: 0.8407\n",
      "Epoch 148/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4116 - acc: 0.8219 - val_loss: 0.3880 - val_acc: 0.8473\n",
      "Epoch 149/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4141 - acc: 0.8208 - val_loss: 0.3874 - val_acc: 0.8440\n",
      "Epoch 150/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4116 - acc: 0.8197 - val_loss: 0.3872 - val_acc: 0.8407\n",
      "Epoch 151/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4137 - acc: 0.8188 - val_loss: 0.3870 - val_acc: 0.8456\n",
      "Epoch 152/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4095 - acc: 0.8227 - val_loss: 0.3868 - val_acc: 0.8440\n",
      "Epoch 153/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4108 - acc: 0.8172 - val_loss: 0.3867 - val_acc: 0.8424\n",
      "Epoch 154/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4113 - acc: 0.8201 - val_loss: 0.3865 - val_acc: 0.8456\n",
      "Epoch 155/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4098 - acc: 0.8210 - val_loss: 0.3866 - val_acc: 0.8440\n",
      "Epoch 156/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4047 - acc: 0.8236 - val_loss: 0.3862 - val_acc: 0.8456\n",
      "Epoch 157/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4070 - acc: 0.8228 - val_loss: 0.3863 - val_acc: 0.8424\n",
      "Epoch 158/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4062 - acc: 0.8230 - val_loss: 0.3857 - val_acc: 0.8456\n",
      "Epoch 159/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4070 - acc: 0.8261 - val_loss: 0.3865 - val_acc: 0.8424\n",
      "Epoch 160/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4063 - acc: 0.8230 - val_loss: 0.3859 - val_acc: 0.8391\n",
      "Epoch 161/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4075 - acc: 0.8247 - val_loss: 0.3853 - val_acc: 0.8456\n",
      "Epoch 162/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4078 - acc: 0.8217 - val_loss: 0.3854 - val_acc: 0.8456\n",
      "Epoch 163/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4078 - acc: 0.8216 - val_loss: 0.3856 - val_acc: 0.8391\n",
      "Epoch 164/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4050 - acc: 0.8238 - val_loss: 0.3853 - val_acc: 0.8456\n",
      "Epoch 165/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4045 - acc: 0.8223 - val_loss: 0.3849 - val_acc: 0.8456\n",
      "Epoch 166/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4059 - acc: 0.8210 - val_loss: 0.3850 - val_acc: 0.8391\n",
      "Epoch 167/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4047 - acc: 0.8223 - val_loss: 0.3845 - val_acc: 0.8473\n",
      "Epoch 168/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4026 - acc: 0.8250 - val_loss: 0.3848 - val_acc: 0.8440\n",
      "Epoch 169/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4033 - acc: 0.8203 - val_loss: 0.3842 - val_acc: 0.8424\n",
      "Epoch 170/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4021 - acc: 0.8252 - val_loss: 0.3847 - val_acc: 0.8374\n",
      "Epoch 171/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4041 - acc: 0.8234 - val_loss: 0.3843 - val_acc: 0.8440\n",
      "Epoch 172/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3992 - acc: 0.8270 - val_loss: 0.3844 - val_acc: 0.8424\n",
      "Epoch 173/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4008 - acc: 0.8269 - val_loss: 0.3851 - val_acc: 0.8407\n",
      "Epoch 174/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4021 - acc: 0.8254 - val_loss: 0.3838 - val_acc: 0.8407\n",
      "Epoch 175/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3999 - acc: 0.8270 - val_loss: 0.3852 - val_acc: 0.8424\n",
      "Epoch 176/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3978 - acc: 0.8281 - val_loss: 0.3840 - val_acc: 0.8391\n",
      "Epoch 177/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3969 - acc: 0.8298 - val_loss: 0.3835 - val_acc: 0.8424\n",
      "Epoch 178/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.3937 - acc: 0.8276 - val_loss: 0.3844 - val_acc: 0.8407\n",
      "Epoch 179/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4014 - acc: 0.8245 - val_loss: 0.3835 - val_acc: 0.8407\n",
      "Epoch 180/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.3953 - acc: 0.8269 - val_loss: 0.3836 - val_acc: 0.8407\n",
      "Epoch 181/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3961 - acc: 0.8300 - val_loss: 0.3841 - val_acc: 0.8407\n",
      "Epoch 182/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3942 - acc: 0.8278 - val_loss: 0.3838 - val_acc: 0.8391\n",
      "Training with parameters {'batch_size': 3000, 'dropout': 0.1, 'lay_conf': [1024, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_147\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_148 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_441 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_294 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_442 (Dense)            (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_295 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_443 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,639,553\n",
      "Trainable params: 1,639,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.7884 - acc: 0.4930 - val_loss: 0.7570 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.7471 - acc: 0.5670 - val_loss: 0.6692 - val_acc: 0.5764\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6764 - acc: 0.5873 - val_loss: 0.6549 - val_acc: 0.6437\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6759 - acc: 0.5835 - val_loss: 0.6375 - val_acc: 0.6650\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6508 - acc: 0.6269 - val_loss: 0.6082 - val_acc: 0.6880\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6325 - acc: 0.6435 - val_loss: 0.6057 - val_acc: 0.6240\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6252 - acc: 0.6448 - val_loss: 0.5748 - val_acc: 0.7553\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6063 - acc: 0.6827 - val_loss: 0.5669 - val_acc: 0.7143\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6044 - acc: 0.6780 - val_loss: 0.5503 - val_acc: 0.7323\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5872 - acc: 0.7035 - val_loss: 0.5387 - val_acc: 0.7849\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5819 - acc: 0.7152 - val_loss: 0.5250 - val_acc: 0.7964\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5710 - acc: 0.7241 - val_loss: 0.5167 - val_acc: 0.7685\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5636 - acc: 0.7126 - val_loss: 0.5012 - val_acc: 0.8046\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5526 - acc: 0.7320 - val_loss: 0.4948 - val_acc: 0.8095\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5435 - acc: 0.7449 - val_loss: 0.4829 - val_acc: 0.8079\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5333 - acc: 0.7426 - val_loss: 0.4756 - val_acc: 0.8095\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5285 - acc: 0.7486 - val_loss: 0.4694 - val_acc: 0.8259\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5224 - acc: 0.7499 - val_loss: 0.4619 - val_acc: 0.8243\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5146 - acc: 0.7573 - val_loss: 0.4561 - val_acc: 0.8259\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.5083 - acc: 0.7632 - val_loss: 0.4511 - val_acc: 0.8243\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5057 - acc: 0.7595 - val_loss: 0.4446 - val_acc: 0.8325\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5003 - acc: 0.7707 - val_loss: 0.4393 - val_acc: 0.8309\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4950 - acc: 0.7734 - val_loss: 0.4350 - val_acc: 0.8309\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4911 - acc: 0.7694 - val_loss: 0.4304 - val_acc: 0.8309\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4878 - acc: 0.7729 - val_loss: 0.4267 - val_acc: 0.8309\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4801 - acc: 0.7794 - val_loss: 0.4224 - val_acc: 0.8342\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4774 - acc: 0.7842 - val_loss: 0.4189 - val_acc: 0.8325\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4743 - acc: 0.7862 - val_loss: 0.4157 - val_acc: 0.8342\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4735 - acc: 0.7845 - val_loss: 0.4130 - val_acc: 0.8342\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4701 - acc: 0.7893 - val_loss: 0.4116 - val_acc: 0.8391\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4652 - acc: 0.7916 - val_loss: 0.4096 - val_acc: 0.8374\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4601 - acc: 0.7924 - val_loss: 0.4087 - val_acc: 0.8407\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4584 - acc: 0.7896 - val_loss: 0.4054 - val_acc: 0.8391\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4623 - acc: 0.7880 - val_loss: 0.4034 - val_acc: 0.8424\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4565 - acc: 0.7915 - val_loss: 0.4032 - val_acc: 0.8424\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4518 - acc: 0.8037 - val_loss: 0.4037 - val_acc: 0.8309\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4526 - acc: 0.7960 - val_loss: 0.4024 - val_acc: 0.8391\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4514 - acc: 0.8000 - val_loss: 0.3980 - val_acc: 0.8489\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4461 - acc: 0.8006 - val_loss: 0.3960 - val_acc: 0.8506\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4443 - acc: 0.8028 - val_loss: 0.3953 - val_acc: 0.8424\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4416 - acc: 0.8044 - val_loss: 0.3938 - val_acc: 0.8539\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4404 - acc: 0.8022 - val_loss: 0.3926 - val_acc: 0.8456\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4393 - acc: 0.8064 - val_loss: 0.3908 - val_acc: 0.8522\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4370 - acc: 0.8088 - val_loss: 0.3897 - val_acc: 0.8555\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4341 - acc: 0.8084 - val_loss: 0.3891 - val_acc: 0.8473\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4353 - acc: 0.8090 - val_loss: 0.3880 - val_acc: 0.8473\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4312 - acc: 0.8084 - val_loss: 0.3873 - val_acc: 0.8539\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4343 - acc: 0.8055 - val_loss: 0.3874 - val_acc: 0.8473\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4290 - acc: 0.8135 - val_loss: 0.3854 - val_acc: 0.8506\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4265 - acc: 0.8097 - val_loss: 0.3855 - val_acc: 0.8522\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4308 - acc: 0.8059 - val_loss: 0.3867 - val_acc: 0.8489\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4259 - acc: 0.8108 - val_loss: 0.3877 - val_acc: 0.8440\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4247 - acc: 0.8121 - val_loss: 0.3883 - val_acc: 0.8456\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4269 - acc: 0.8150 - val_loss: 0.3827 - val_acc: 0.8506\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4197 - acc: 0.8155 - val_loss: 0.3826 - val_acc: 0.8555\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4207 - acc: 0.8168 - val_loss: 0.3844 - val_acc: 0.8522\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4190 - acc: 0.8181 - val_loss: 0.3823 - val_acc: 0.8506\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4211 - acc: 0.8190 - val_loss: 0.3806 - val_acc: 0.8522\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4178 - acc: 0.8168 - val_loss: 0.3790 - val_acc: 0.8539\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4154 - acc: 0.8161 - val_loss: 0.3785 - val_acc: 0.8571\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4126 - acc: 0.8225 - val_loss: 0.3783 - val_acc: 0.8539\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4118 - acc: 0.8183 - val_loss: 0.3772 - val_acc: 0.8555\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4117 - acc: 0.8236 - val_loss: 0.3769 - val_acc: 0.8539\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4074 - acc: 0.8230 - val_loss: 0.3758 - val_acc: 0.8555\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4088 - acc: 0.8238 - val_loss: 0.3761 - val_acc: 0.8539\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4061 - acc: 0.8270 - val_loss: 0.3746 - val_acc: 0.8571\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4087 - acc: 0.8214 - val_loss: 0.3743 - val_acc: 0.8555\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4038 - acc: 0.8230 - val_loss: 0.3738 - val_acc: 0.8571\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4053 - acc: 0.8228 - val_loss: 0.3733 - val_acc: 0.8555\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4030 - acc: 0.8239 - val_loss: 0.3732 - val_acc: 0.8571\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3999 - acc: 0.8221 - val_loss: 0.3727 - val_acc: 0.8539\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4035 - acc: 0.8245 - val_loss: 0.3728 - val_acc: 0.8604\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4013 - acc: 0.8239 - val_loss: 0.3721 - val_acc: 0.8539\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3962 - acc: 0.8281 - val_loss: 0.3714 - val_acc: 0.8539\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.3939 - acc: 0.8314 - val_loss: 0.3715 - val_acc: 0.8588\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.3954 - acc: 0.8278 - val_loss: 0.3728 - val_acc: 0.8522\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3958 - acc: 0.8285 - val_loss: 0.3725 - val_acc: 0.8506\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3953 - acc: 0.8309 - val_loss: 0.3702 - val_acc: 0.8539\n",
      "Epoch 79/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 70ms/step - loss: 0.3940 - acc: 0.8274 - val_loss: 0.3695 - val_acc: 0.8588\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3907 - acc: 0.8274 - val_loss: 0.3688 - val_acc: 0.8539\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3891 - acc: 0.8338 - val_loss: 0.3690 - val_acc: 0.8506\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3916 - acc: 0.8314 - val_loss: 0.3688 - val_acc: 0.8555\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3858 - acc: 0.8321 - val_loss: 0.3689 - val_acc: 0.8571\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.3869 - acc: 0.8307 - val_loss: 0.3694 - val_acc: 0.8473\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.3842 - acc: 0.8320 - val_loss: 0.3702 - val_acc: 0.8539\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.3880 - acc: 0.8365 - val_loss: 0.3717 - val_acc: 0.8489\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3871 - acc: 0.8303 - val_loss: 0.3697 - val_acc: 0.8522\n",
      "Training with parameters {'batch_size': 3000, 'dropout': 0.1, 'lay_conf': [1024, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_148\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_149 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_444 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_296 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_445 (Dense)            (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_297 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_446 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,606,721\n",
      "Trainable params: 1,606,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.8432 - acc: 0.4725 - val_loss: 0.7537 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.7592 - acc: 0.5650 - val_loss: 0.6702 - val_acc: 0.5731\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6860 - acc: 0.5683 - val_loss: 0.6633 - val_acc: 0.6043\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6884 - acc: 0.5601 - val_loss: 0.6342 - val_acc: 0.6732\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6566 - acc: 0.6192 - val_loss: 0.6115 - val_acc: 0.6486\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6467 - acc: 0.6287 - val_loss: 0.6065 - val_acc: 0.6305\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6341 - acc: 0.6448 - val_loss: 0.5799 - val_acc: 0.7373\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6163 - acc: 0.6741 - val_loss: 0.5767 - val_acc: 0.7143\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6124 - acc: 0.6630 - val_loss: 0.5549 - val_acc: 0.7635\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5948 - acc: 0.6962 - val_loss: 0.5471 - val_acc: 0.7586\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5869 - acc: 0.7046 - val_loss: 0.5283 - val_acc: 0.7980\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5741 - acc: 0.7150 - val_loss: 0.5193 - val_acc: 0.7685\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5726 - acc: 0.7121 - val_loss: 0.5075 - val_acc: 0.8013\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5575 - acc: 0.7302 - val_loss: 0.5014 - val_acc: 0.8046\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5552 - acc: 0.7260 - val_loss: 0.4911 - val_acc: 0.8112\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5412 - acc: 0.7333 - val_loss: 0.4843 - val_acc: 0.8177\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5354 - acc: 0.7493 - val_loss: 0.4790 - val_acc: 0.8161\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5291 - acc: 0.7469 - val_loss: 0.4722 - val_acc: 0.8177\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5253 - acc: 0.7502 - val_loss: 0.4651 - val_acc: 0.8210\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5189 - acc: 0.7508 - val_loss: 0.4593 - val_acc: 0.8276\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5196 - acc: 0.7530 - val_loss: 0.4529 - val_acc: 0.8243\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5120 - acc: 0.7568 - val_loss: 0.4468 - val_acc: 0.8177\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5031 - acc: 0.7692 - val_loss: 0.4425 - val_acc: 0.8276\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4975 - acc: 0.7707 - val_loss: 0.4367 - val_acc: 0.8276\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4985 - acc: 0.7721 - val_loss: 0.4319 - val_acc: 0.8276\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4930 - acc: 0.7710 - val_loss: 0.4283 - val_acc: 0.8276\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4913 - acc: 0.7767 - val_loss: 0.4241 - val_acc: 0.8358\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4818 - acc: 0.7792 - val_loss: 0.4207 - val_acc: 0.8342\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4832 - acc: 0.7812 - val_loss: 0.4200 - val_acc: 0.8391\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4754 - acc: 0.7887 - val_loss: 0.4158 - val_acc: 0.8358\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4766 - acc: 0.7851 - val_loss: 0.4129 - val_acc: 0.8325\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4705 - acc: 0.7874 - val_loss: 0.4109 - val_acc: 0.8424\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4657 - acc: 0.7937 - val_loss: 0.4075 - val_acc: 0.8325\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4629 - acc: 0.7909 - val_loss: 0.4061 - val_acc: 0.8374\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4630 - acc: 0.7918 - val_loss: 0.4031 - val_acc: 0.8440\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4588 - acc: 0.7942 - val_loss: 0.4014 - val_acc: 0.8440\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4567 - acc: 0.7907 - val_loss: 0.3996 - val_acc: 0.8407\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4536 - acc: 0.7913 - val_loss: 0.3977 - val_acc: 0.8407\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4526 - acc: 0.7993 - val_loss: 0.3965 - val_acc: 0.8391\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4480 - acc: 0.8035 - val_loss: 0.3948 - val_acc: 0.8440\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4472 - acc: 0.7997 - val_loss: 0.3936 - val_acc: 0.8407\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4437 - acc: 0.8013 - val_loss: 0.3917 - val_acc: 0.8424\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4425 - acc: 0.8037 - val_loss: 0.3905 - val_acc: 0.8424\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4424 - acc: 0.8044 - val_loss: 0.3902 - val_acc: 0.8424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4418 - acc: 0.8059 - val_loss: 0.3881 - val_acc: 0.8440\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4365 - acc: 0.8086 - val_loss: 0.3870 - val_acc: 0.8456\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4370 - acc: 0.8046 - val_loss: 0.3869 - val_acc: 0.8407\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4319 - acc: 0.8077 - val_loss: 0.3849 - val_acc: 0.8473\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4342 - acc: 0.8088 - val_loss: 0.3839 - val_acc: 0.8473\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4349 - acc: 0.8077 - val_loss: 0.3824 - val_acc: 0.8489\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4277 - acc: 0.8092 - val_loss: 0.3822 - val_acc: 0.8489\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4287 - acc: 0.8126 - val_loss: 0.3809 - val_acc: 0.8489\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.4289 - acc: 0.8134 - val_loss: 0.3802 - val_acc: 0.8506\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4200 - acc: 0.8152 - val_loss: 0.3799 - val_acc: 0.8473\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4218 - acc: 0.8174 - val_loss: 0.3801 - val_acc: 0.8489\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4226 - acc: 0.8143 - val_loss: 0.3794 - val_acc: 0.8522\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4190 - acc: 0.8161 - val_loss: 0.3776 - val_acc: 0.8522\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4194 - acc: 0.8163 - val_loss: 0.3768 - val_acc: 0.8489\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4168 - acc: 0.8155 - val_loss: 0.3755 - val_acc: 0.8522\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4161 - acc: 0.8196 - val_loss: 0.3757 - val_acc: 0.8489\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4180 - acc: 0.8197 - val_loss: 0.3749 - val_acc: 0.8489\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4160 - acc: 0.8225 - val_loss: 0.3748 - val_acc: 0.8489\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4131 - acc: 0.8219 - val_loss: 0.3772 - val_acc: 0.8539\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4131 - acc: 0.8225 - val_loss: 0.3776 - val_acc: 0.8473\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4113 - acc: 0.8192 - val_loss: 0.3737 - val_acc: 0.8522\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4085 - acc: 0.8210 - val_loss: 0.3715 - val_acc: 0.8539\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4129 - acc: 0.8161 - val_loss: 0.3701 - val_acc: 0.8539\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4097 - acc: 0.8245 - val_loss: 0.3706 - val_acc: 0.8555\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4065 - acc: 0.8197 - val_loss: 0.3744 - val_acc: 0.8522\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4042 - acc: 0.8227 - val_loss: 0.3742 - val_acc: 0.8522\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4022 - acc: 0.8221 - val_loss: 0.3743 - val_acc: 0.8489\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4044 - acc: 0.8236 - val_loss: 0.3711 - val_acc: 0.8539\n",
      "Training with parameters {'batch_size': 3000, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_149\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_150 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_447 (Dense)            (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_298 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_448 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_299 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_449 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,050,113\n",
      "Trainable params: 1,050,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.7358 - acc: 0.4912 - val_loss: 0.7098 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.7137 - acc: 0.5687 - val_loss: 0.6593 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6718 - acc: 0.5749 - val_loss: 0.6514 - val_acc: 0.6486\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6617 - acc: 0.6092 - val_loss: 0.6308 - val_acc: 0.6585\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6420 - acc: 0.6424 - val_loss: 0.6045 - val_acc: 0.7159\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6289 - acc: 0.6473 - val_loss: 0.5976 - val_acc: 0.6831\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6232 - acc: 0.6519 - val_loss: 0.5777 - val_acc: 0.7373\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6073 - acc: 0.6723 - val_loss: 0.5752 - val_acc: 0.6913\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6008 - acc: 0.6767 - val_loss: 0.5574 - val_acc: 0.7356\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5848 - acc: 0.7044 - val_loss: 0.5465 - val_acc: 0.7668\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5813 - acc: 0.7198 - val_loss: 0.5343 - val_acc: 0.7865\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5671 - acc: 0.7258 - val_loss: 0.5283 - val_acc: 0.7603\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5623 - acc: 0.7170 - val_loss: 0.5161 - val_acc: 0.7898\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5517 - acc: 0.7354 - val_loss: 0.5064 - val_acc: 0.7980\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5475 - acc: 0.7449 - val_loss: 0.4980 - val_acc: 0.7947\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5377 - acc: 0.7499 - val_loss: 0.4942 - val_acc: 0.8013\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5306 - acc: 0.7490 - val_loss: 0.4834 - val_acc: 0.8095\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5260 - acc: 0.7564 - val_loss: 0.4766 - val_acc: 0.8062\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5179 - acc: 0.7623 - val_loss: 0.4737 - val_acc: 0.8112\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5138 - acc: 0.7570 - val_loss: 0.4635 - val_acc: 0.8210\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5064 - acc: 0.7677 - val_loss: 0.4567 - val_acc: 0.8194\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5012 - acc: 0.7681 - val_loss: 0.4525 - val_acc: 0.8194\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4952 - acc: 0.7701 - val_loss: 0.4449 - val_acc: 0.8243\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4889 - acc: 0.7723 - val_loss: 0.4395 - val_acc: 0.8227\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4876 - acc: 0.7756 - val_loss: 0.4349 - val_acc: 0.8227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4826 - acc: 0.7741 - val_loss: 0.4308 - val_acc: 0.8259\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4781 - acc: 0.7854 - val_loss: 0.4273 - val_acc: 0.8243\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4734 - acc: 0.7838 - val_loss: 0.4240 - val_acc: 0.8243\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4679 - acc: 0.7845 - val_loss: 0.4197 - val_acc: 0.8325\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4639 - acc: 0.7909 - val_loss: 0.4164 - val_acc: 0.8292\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4649 - acc: 0.7847 - val_loss: 0.4134 - val_acc: 0.8374\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4589 - acc: 0.7922 - val_loss: 0.4105 - val_acc: 0.8358\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4543 - acc: 0.7980 - val_loss: 0.4081 - val_acc: 0.8358\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4516 - acc: 0.8002 - val_loss: 0.4060 - val_acc: 0.8374\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4505 - acc: 0.7986 - val_loss: 0.4039 - val_acc: 0.8391\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4442 - acc: 0.8031 - val_loss: 0.4027 - val_acc: 0.8358\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4469 - acc: 0.8028 - val_loss: 0.4000 - val_acc: 0.8374\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4409 - acc: 0.8042 - val_loss: 0.3983 - val_acc: 0.8424\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4367 - acc: 0.8055 - val_loss: 0.3969 - val_acc: 0.8456\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4352 - acc: 0.8093 - val_loss: 0.3962 - val_acc: 0.8424\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4299 - acc: 0.8104 - val_loss: 0.3948 - val_acc: 0.8456\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4279 - acc: 0.8139 - val_loss: 0.3931 - val_acc: 0.8440\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4272 - acc: 0.8112 - val_loss: 0.3916 - val_acc: 0.8539\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4226 - acc: 0.8163 - val_loss: 0.3905 - val_acc: 0.8539\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4206 - acc: 0.8168 - val_loss: 0.3900 - val_acc: 0.8539\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4222 - acc: 0.8143 - val_loss: 0.3889 - val_acc: 0.8489\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4204 - acc: 0.8165 - val_loss: 0.3875 - val_acc: 0.8506\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4190 - acc: 0.8165 - val_loss: 0.3877 - val_acc: 0.8539\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4166 - acc: 0.8174 - val_loss: 0.3890 - val_acc: 0.8424\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4159 - acc: 0.8186 - val_loss: 0.3870 - val_acc: 0.8522\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4128 - acc: 0.8179 - val_loss: 0.3851 - val_acc: 0.8489\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4089 - acc: 0.8239 - val_loss: 0.3846 - val_acc: 0.8506\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4103 - acc: 0.8214 - val_loss: 0.3830 - val_acc: 0.8522\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4056 - acc: 0.8227 - val_loss: 0.3823 - val_acc: 0.8522\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4017 - acc: 0.8223 - val_loss: 0.3816 - val_acc: 0.8522\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4029 - acc: 0.8239 - val_loss: 0.3813 - val_acc: 0.8489\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4029 - acc: 0.8265 - val_loss: 0.3823 - val_acc: 0.8539\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4030 - acc: 0.8203 - val_loss: 0.3827 - val_acc: 0.8440\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4004 - acc: 0.8258 - val_loss: 0.3846 - val_acc: 0.8588\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.3992 - acc: 0.8238 - val_loss: 0.3831 - val_acc: 0.8456\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3980 - acc: 0.8263 - val_loss: 0.3840 - val_acc: 0.8555\n",
      "Training with parameters {'batch_size': 3000, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_150\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_151 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_450 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_300 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_451 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_301 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_452 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 401,729\n",
      "Trainable params: 401,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 1.0216 - acc: 0.4291 - val_loss: 0.7655 - val_acc: 0.4269\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7565 - acc: 0.4558 - val_loss: 0.6988 - val_acc: 0.5550\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.7153 - acc: 0.5318 - val_loss: 0.7004 - val_acc: 0.5747\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.7125 - acc: 0.5559 - val_loss: 0.6917 - val_acc: 0.5747\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.7053 - acc: 0.5623 - val_loss: 0.6751 - val_acc: 0.5747\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6865 - acc: 0.5689 - val_loss: 0.6622 - val_acc: 0.5764\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6758 - acc: 0.5791 - val_loss: 0.6562 - val_acc: 0.6223\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6706 - acc: 0.5831 - val_loss: 0.6535 - val_acc: 0.6831\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6677 - acc: 0.6012 - val_loss: 0.6483 - val_acc: 0.6962\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6623 - acc: 0.6143 - val_loss: 0.6396 - val_acc: 0.6962\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6594 - acc: 0.6190 - val_loss: 0.6306 - val_acc: 0.7044\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6518 - acc: 0.6207 - val_loss: 0.6239 - val_acc: 0.6765\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6506 - acc: 0.6269 - val_loss: 0.6183 - val_acc: 0.6782\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6414 - acc: 0.6413 - val_loss: 0.6117 - val_acc: 0.6995\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6376 - acc: 0.6492 - val_loss: 0.6048 - val_acc: 0.7258\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6335 - acc: 0.6510 - val_loss: 0.5989 - val_acc: 0.7258\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6313 - acc: 0.6541 - val_loss: 0.5922 - val_acc: 0.7258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6280 - acc: 0.6583 - val_loss: 0.5851 - val_acc: 0.7291\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6206 - acc: 0.6645 - val_loss: 0.5785 - val_acc: 0.7258\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6165 - acc: 0.6741 - val_loss: 0.5727 - val_acc: 0.7291\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6136 - acc: 0.6703 - val_loss: 0.5672 - val_acc: 0.7356\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6028 - acc: 0.6847 - val_loss: 0.5628 - val_acc: 0.7406\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6032 - acc: 0.6882 - val_loss: 0.5571 - val_acc: 0.7422\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5971 - acc: 0.6942 - val_loss: 0.5505 - val_acc: 0.7553\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5936 - acc: 0.7021 - val_loss: 0.5450 - val_acc: 0.7504\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5875 - acc: 0.7064 - val_loss: 0.5402 - val_acc: 0.7521\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5850 - acc: 0.7070 - val_loss: 0.5358 - val_acc: 0.7635\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5800 - acc: 0.7176 - val_loss: 0.5313 - val_acc: 0.7701\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5773 - acc: 0.7134 - val_loss: 0.5268 - val_acc: 0.7718\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5712 - acc: 0.7216 - val_loss: 0.5221 - val_acc: 0.7849\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5643 - acc: 0.7272 - val_loss: 0.5179 - val_acc: 0.7849\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5617 - acc: 0.7265 - val_loss: 0.5141 - val_acc: 0.7800\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5568 - acc: 0.7291 - val_loss: 0.5095 - val_acc: 0.7915\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5526 - acc: 0.7300 - val_loss: 0.5045 - val_acc: 0.7980\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5543 - acc: 0.7296 - val_loss: 0.5001 - val_acc: 0.8013\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5431 - acc: 0.7420 - val_loss: 0.4964 - val_acc: 0.7997\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5410 - acc: 0.7460 - val_loss: 0.4916 - val_acc: 0.7997\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5414 - acc: 0.7426 - val_loss: 0.4877 - val_acc: 0.8079\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5408 - acc: 0.7477 - val_loss: 0.4838 - val_acc: 0.7997\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5339 - acc: 0.7519 - val_loss: 0.4811 - val_acc: 0.8030\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5319 - acc: 0.7541 - val_loss: 0.4769 - val_acc: 0.8046\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5256 - acc: 0.7562 - val_loss: 0.4735 - val_acc: 0.8079\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5217 - acc: 0.7615 - val_loss: 0.4700 - val_acc: 0.8095\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5199 - acc: 0.7612 - val_loss: 0.4673 - val_acc: 0.8030\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5144 - acc: 0.7590 - val_loss: 0.4633 - val_acc: 0.8112\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5152 - acc: 0.7601 - val_loss: 0.4601 - val_acc: 0.8144\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5084 - acc: 0.7710 - val_loss: 0.4572 - val_acc: 0.8062\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5058 - acc: 0.7701 - val_loss: 0.4552 - val_acc: 0.8062\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5070 - acc: 0.7654 - val_loss: 0.4516 - val_acc: 0.8177\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5009 - acc: 0.7743 - val_loss: 0.4490 - val_acc: 0.8227\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5039 - acc: 0.7701 - val_loss: 0.4473 - val_acc: 0.8112\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4999 - acc: 0.7725 - val_loss: 0.4446 - val_acc: 0.8177\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4917 - acc: 0.7787 - val_loss: 0.4432 - val_acc: 0.8227\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4964 - acc: 0.7666 - val_loss: 0.4405 - val_acc: 0.8259\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4916 - acc: 0.7754 - val_loss: 0.4390 - val_acc: 0.8292\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4863 - acc: 0.7783 - val_loss: 0.4370 - val_acc: 0.8194\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4868 - acc: 0.7807 - val_loss: 0.4351 - val_acc: 0.8358\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4828 - acc: 0.7896 - val_loss: 0.4335 - val_acc: 0.8342\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4793 - acc: 0.7869 - val_loss: 0.4316 - val_acc: 0.8342\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4766 - acc: 0.7900 - val_loss: 0.4298 - val_acc: 0.8259\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4790 - acc: 0.7840 - val_loss: 0.4286 - val_acc: 0.8374\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4787 - acc: 0.7800 - val_loss: 0.4265 - val_acc: 0.8342\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4766 - acc: 0.7845 - val_loss: 0.4251 - val_acc: 0.8342\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4732 - acc: 0.7909 - val_loss: 0.4241 - val_acc: 0.8342\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4722 - acc: 0.7918 - val_loss: 0.4228 - val_acc: 0.8358\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4694 - acc: 0.7896 - val_loss: 0.4216 - val_acc: 0.8342\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4704 - acc: 0.7916 - val_loss: 0.4212 - val_acc: 0.8325\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4690 - acc: 0.7865 - val_loss: 0.4196 - val_acc: 0.8358\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4688 - acc: 0.7929 - val_loss: 0.4201 - val_acc: 0.8276\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4614 - acc: 0.7933 - val_loss: 0.4182 - val_acc: 0.8342\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4665 - acc: 0.7980 - val_loss: 0.4162 - val_acc: 0.8374\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4642 - acc: 0.7953 - val_loss: 0.4157 - val_acc: 0.8325\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4555 - acc: 0.7966 - val_loss: 0.4139 - val_acc: 0.8374\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4614 - acc: 0.7964 - val_loss: 0.4129 - val_acc: 0.8358\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4586 - acc: 0.7993 - val_loss: 0.4123 - val_acc: 0.8342\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4578 - acc: 0.7995 - val_loss: 0.4102 - val_acc: 0.8391\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4588 - acc: 0.7951 - val_loss: 0.4099 - val_acc: 0.8358\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4525 - acc: 0.8015 - val_loss: 0.4085 - val_acc: 0.8374\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4499 - acc: 0.8019 - val_loss: 0.4082 - val_acc: 0.8374\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4496 - acc: 0.8033 - val_loss: 0.4064 - val_acc: 0.8374\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4503 - acc: 0.8017 - val_loss: 0.4056 - val_acc: 0.8342\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4493 - acc: 0.8028 - val_loss: 0.4052 - val_acc: 0.8407\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4484 - acc: 0.8050 - val_loss: 0.4036 - val_acc: 0.8374\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4438 - acc: 0.8041 - val_loss: 0.4035 - val_acc: 0.8342\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4463 - acc: 0.8079 - val_loss: 0.4023 - val_acc: 0.8424\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4456 - acc: 0.8062 - val_loss: 0.4011 - val_acc: 0.8391\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4417 - acc: 0.8088 - val_loss: 0.4005 - val_acc: 0.8342\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4459 - acc: 0.8044 - val_loss: 0.3997 - val_acc: 0.8374\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4425 - acc: 0.8072 - val_loss: 0.3992 - val_acc: 0.8374\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4385 - acc: 0.8053 - val_loss: 0.3985 - val_acc: 0.8374\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4359 - acc: 0.8134 - val_loss: 0.3973 - val_acc: 0.8374\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4369 - acc: 0.8104 - val_loss: 0.3962 - val_acc: 0.8358\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4412 - acc: 0.8081 - val_loss: 0.3956 - val_acc: 0.8391\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4360 - acc: 0.8152 - val_loss: 0.3954 - val_acc: 0.8374\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4354 - acc: 0.8113 - val_loss: 0.3951 - val_acc: 0.8374\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4397 - acc: 0.8088 - val_loss: 0.3951 - val_acc: 0.8391\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4329 - acc: 0.8099 - val_loss: 0.3948 - val_acc: 0.8358\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4358 - acc: 0.8157 - val_loss: 0.3942 - val_acc: 0.8342\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4328 - acc: 0.8124 - val_loss: 0.3936 - val_acc: 0.8489\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4273 - acc: 0.8166 - val_loss: 0.3923 - val_acc: 0.8358\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4286 - acc: 0.8130 - val_loss: 0.3913 - val_acc: 0.8358\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4278 - acc: 0.8176 - val_loss: 0.3908 - val_acc: 0.8424\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4257 - acc: 0.8179 - val_loss: 0.3904 - val_acc: 0.8342\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4246 - acc: 0.8150 - val_loss: 0.3904 - val_acc: 0.8342\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4268 - acc: 0.8141 - val_loss: 0.3900 - val_acc: 0.8358\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4255 - acc: 0.8165 - val_loss: 0.3898 - val_acc: 0.8440\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4249 - acc: 0.8163 - val_loss: 0.3896 - val_acc: 0.8358\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4236 - acc: 0.8170 - val_loss: 0.3889 - val_acc: 0.8358\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4237 - acc: 0.8146 - val_loss: 0.3884 - val_acc: 0.8358\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4228 - acc: 0.8183 - val_loss: 0.3877 - val_acc: 0.8440\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4211 - acc: 0.8188 - val_loss: 0.3869 - val_acc: 0.8358\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4201 - acc: 0.8165 - val_loss: 0.3866 - val_acc: 0.8325\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4162 - acc: 0.8192 - val_loss: 0.3859 - val_acc: 0.8391\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4182 - acc: 0.8212 - val_loss: 0.3855 - val_acc: 0.8407\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4151 - acc: 0.8197 - val_loss: 0.3858 - val_acc: 0.8358\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4212 - acc: 0.8124 - val_loss: 0.3852 - val_acc: 0.8374\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4153 - acc: 0.8207 - val_loss: 0.3860 - val_acc: 0.8473\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4156 - acc: 0.8188 - val_loss: 0.3855 - val_acc: 0.8358\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4161 - acc: 0.8207 - val_loss: 0.3847 - val_acc: 0.8358\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4153 - acc: 0.8185 - val_loss: 0.3840 - val_acc: 0.8374\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4125 - acc: 0.8201 - val_loss: 0.3836 - val_acc: 0.8342\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4105 - acc: 0.8223 - val_loss: 0.3831 - val_acc: 0.8374\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4130 - acc: 0.8212 - val_loss: 0.3827 - val_acc: 0.8374\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4117 - acc: 0.8201 - val_loss: 0.3824 - val_acc: 0.8391\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4125 - acc: 0.8210 - val_loss: 0.3820 - val_acc: 0.8391\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4106 - acc: 0.8223 - val_loss: 0.3820 - val_acc: 0.8391\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4054 - acc: 0.8230 - val_loss: 0.3815 - val_acc: 0.8391\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4083 - acc: 0.8267 - val_loss: 0.3812 - val_acc: 0.8391\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4076 - acc: 0.8254 - val_loss: 0.3813 - val_acc: 0.8391\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4084 - acc: 0.8248 - val_loss: 0.3816 - val_acc: 0.8342\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4093 - acc: 0.8250 - val_loss: 0.3814 - val_acc: 0.8407\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4059 - acc: 0.8283 - val_loss: 0.3818 - val_acc: 0.8358\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4004 - acc: 0.8228 - val_loss: 0.3812 - val_acc: 0.8407\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4062 - acc: 0.8239 - val_loss: 0.3806 - val_acc: 0.8391\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4047 - acc: 0.8298 - val_loss: 0.3805 - val_acc: 0.8342\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4028 - acc: 0.8269 - val_loss: 0.3802 - val_acc: 0.8456\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4017 - acc: 0.8311 - val_loss: 0.3799 - val_acc: 0.8325\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3996 - acc: 0.8243 - val_loss: 0.3794 - val_acc: 0.8473\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4002 - acc: 0.8287 - val_loss: 0.3788 - val_acc: 0.8391\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3998 - acc: 0.8285 - val_loss: 0.3791 - val_acc: 0.8342\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3986 - acc: 0.8258 - val_loss: 0.3792 - val_acc: 0.8489\n",
      "Epoch 142/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3955 - acc: 0.8351 - val_loss: 0.3793 - val_acc: 0.8374\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3987 - acc: 0.8281 - val_loss: 0.3776 - val_acc: 0.8440\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3974 - acc: 0.8276 - val_loss: 0.3769 - val_acc: 0.8407\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3930 - acc: 0.8283 - val_loss: 0.3766 - val_acc: 0.8407\n",
      "Epoch 146/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3938 - acc: 0.8292 - val_loss: 0.3765 - val_acc: 0.8407\n",
      "Epoch 147/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3967 - acc: 0.8265 - val_loss: 0.3768 - val_acc: 0.8424\n",
      "Epoch 148/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3922 - acc: 0.8276 - val_loss: 0.3774 - val_acc: 0.8456\n",
      "Epoch 149/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3923 - acc: 0.8287 - val_loss: 0.3784 - val_acc: 0.8407\n",
      "Epoch 150/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3951 - acc: 0.8307 - val_loss: 0.3770 - val_acc: 0.8424\n",
      "Epoch 151/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3925 - acc: 0.8250 - val_loss: 0.3775 - val_acc: 0.8456\n",
      "Training with parameters {'batch_size': 3000, 'dropout': 0.1, 'lay_conf': [1024, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_151\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_152 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_453 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_302 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_454 (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_303 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_455 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,836,545\n",
      "Trainable params: 1,836,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 0.9055 - acc: 0.4665 - val_loss: 0.7612 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.7944 - acc: 0.5680 - val_loss: 0.7862 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.7591 - acc: 0.5678 - val_loss: 0.6497 - val_acc: 0.5747\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.6619 - acc: 0.6006 - val_loss: 0.6608 - val_acc: 0.6355\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 171ms/step - loss: 0.6790 - acc: 0.5749 - val_loss: 0.6691 - val_acc: 0.6059\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.6755 - acc: 0.5906 - val_loss: 0.6169 - val_acc: 0.6782\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.6320 - acc: 0.6543 - val_loss: 0.6030 - val_acc: 0.6831\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.6325 - acc: 0.6315 - val_loss: 0.6131 - val_acc: 0.5911\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.6379 - acc: 0.6200 - val_loss: 0.5897 - val_acc: 0.6946\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6126 - acc: 0.6685 - val_loss: 0.5741 - val_acc: 0.7389\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.6024 - acc: 0.6900 - val_loss: 0.5798 - val_acc: 0.7110\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.6031 - acc: 0.6707 - val_loss: 0.5681 - val_acc: 0.7291\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.5932 - acc: 0.6902 - val_loss: 0.5500 - val_acc: 0.7635\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.5828 - acc: 0.7110 - val_loss: 0.5473 - val_acc: 0.7652\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.5825 - acc: 0.7178 - val_loss: 0.5365 - val_acc: 0.7816\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.5718 - acc: 0.7236 - val_loss: 0.5304 - val_acc: 0.7701\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.5678 - acc: 0.7134 - val_loss: 0.5277 - val_acc: 0.7635\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.5591 - acc: 0.7216 - val_loss: 0.5128 - val_acc: 0.7980\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.5515 - acc: 0.7360 - val_loss: 0.5065 - val_acc: 0.8079\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5485 - acc: 0.7508 - val_loss: 0.4980 - val_acc: 0.8062\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5377 - acc: 0.7479 - val_loss: 0.4952 - val_acc: 0.7882\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5351 - acc: 0.7387 - val_loss: 0.4853 - val_acc: 0.8062\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5221 - acc: 0.7573 - val_loss: 0.4779 - val_acc: 0.8128\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5222 - acc: 0.7594 - val_loss: 0.4724 - val_acc: 0.8112\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5175 - acc: 0.7559 - val_loss: 0.4695 - val_acc: 0.8112\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5099 - acc: 0.7639 - val_loss: 0.4613 - val_acc: 0.8227\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5069 - acc: 0.7687 - val_loss: 0.4565 - val_acc: 0.8177\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5012 - acc: 0.7721 - val_loss: 0.4529 - val_acc: 0.8177\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.4963 - acc: 0.7747 - val_loss: 0.4470 - val_acc: 0.8161\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4923 - acc: 0.7692 - val_loss: 0.4429 - val_acc: 0.8177\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4915 - acc: 0.7754 - val_loss: 0.4414 - val_acc: 0.8194\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4861 - acc: 0.7774 - val_loss: 0.4358 - val_acc: 0.8194\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4809 - acc: 0.7840 - val_loss: 0.4314 - val_acc: 0.8210\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4756 - acc: 0.7833 - val_loss: 0.4294 - val_acc: 0.8243\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4731 - acc: 0.7827 - val_loss: 0.4260 - val_acc: 0.8292\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4735 - acc: 0.7909 - val_loss: 0.4235 - val_acc: 0.8276\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4699 - acc: 0.7833 - val_loss: 0.4192 - val_acc: 0.8276\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4611 - acc: 0.7931 - val_loss: 0.4166 - val_acc: 0.8292\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4632 - acc: 0.7887 - val_loss: 0.4141 - val_acc: 0.8309\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4626 - acc: 0.7947 - val_loss: 0.4118 - val_acc: 0.8342\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4545 - acc: 0.8000 - val_loss: 0.4103 - val_acc: 0.8358\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4545 - acc: 0.7944 - val_loss: 0.4074 - val_acc: 0.8325\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4501 - acc: 0.7949 - val_loss: 0.4050 - val_acc: 0.8358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4480 - acc: 0.8020 - val_loss: 0.4035 - val_acc: 0.8407\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4477 - acc: 0.8039 - val_loss: 0.4010 - val_acc: 0.8342\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4437 - acc: 0.8026 - val_loss: 0.3993 - val_acc: 0.8374\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4414 - acc: 0.8059 - val_loss: 0.3977 - val_acc: 0.8407\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4411 - acc: 0.8022 - val_loss: 0.3962 - val_acc: 0.8391\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4375 - acc: 0.8062 - val_loss: 0.3949 - val_acc: 0.8391\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4355 - acc: 0.8015 - val_loss: 0.3930 - val_acc: 0.8407\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4318 - acc: 0.8079 - val_loss: 0.3933 - val_acc: 0.8440\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4294 - acc: 0.8062 - val_loss: 0.3907 - val_acc: 0.8391\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4303 - acc: 0.8095 - val_loss: 0.3891 - val_acc: 0.8424\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4295 - acc: 0.8081 - val_loss: 0.3883 - val_acc: 0.8424\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4247 - acc: 0.8130 - val_loss: 0.3883 - val_acc: 0.8440\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4254 - acc: 0.8128 - val_loss: 0.3894 - val_acc: 0.8456\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4205 - acc: 0.8121 - val_loss: 0.3874 - val_acc: 0.8456\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4230 - acc: 0.8166 - val_loss: 0.3889 - val_acc: 0.8506\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4171 - acc: 0.8163 - val_loss: 0.3844 - val_acc: 0.8489\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4173 - acc: 0.8144 - val_loss: 0.3841 - val_acc: 0.8456\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4179 - acc: 0.8093 - val_loss: 0.3831 - val_acc: 0.8506\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4137 - acc: 0.8176 - val_loss: 0.3835 - val_acc: 0.8506\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4144 - acc: 0.8172 - val_loss: 0.3815 - val_acc: 0.8473\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4147 - acc: 0.8172 - val_loss: 0.3814 - val_acc: 0.8489\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4104 - acc: 0.8190 - val_loss: 0.3824 - val_acc: 0.8489\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4085 - acc: 0.8190 - val_loss: 0.3824 - val_acc: 0.8440\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4072 - acc: 0.8181 - val_loss: 0.3840 - val_acc: 0.8489\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4090 - acc: 0.8197 - val_loss: 0.3813 - val_acc: 0.8456\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4040 - acc: 0.8261 - val_loss: 0.3808 - val_acc: 0.8489\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4016 - acc: 0.8234 - val_loss: 0.3786 - val_acc: 0.8473\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4005 - acc: 0.8234 - val_loss: 0.3796 - val_acc: 0.8489\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4010 - acc: 0.8248 - val_loss: 0.3778 - val_acc: 0.8456\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.3998 - acc: 0.8234 - val_loss: 0.3780 - val_acc: 0.8506\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.3947 - acc: 0.8272 - val_loss: 0.3769 - val_acc: 0.8473\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.3943 - acc: 0.8292 - val_loss: 0.3774 - val_acc: 0.8489\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.3925 - acc: 0.8318 - val_loss: 0.3767 - val_acc: 0.8489\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.3895 - acc: 0.8256 - val_loss: 0.3765 - val_acc: 0.8473\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.3929 - acc: 0.8289 - val_loss: 0.3764 - val_acc: 0.8489\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.3888 - acc: 0.8285 - val_loss: 0.3761 - val_acc: 0.8489\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.3889 - acc: 0.8329 - val_loss: 0.3750 - val_acc: 0.8489\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.3846 - acc: 0.8351 - val_loss: 0.3751 - val_acc: 0.8489\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.3874 - acc: 0.8258 - val_loss: 0.3746 - val_acc: 0.8489\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.3845 - acc: 0.8311 - val_loss: 0.3746 - val_acc: 0.8489\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.3817 - acc: 0.8327 - val_loss: 0.3745 - val_acc: 0.8506\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.3794 - acc: 0.8358 - val_loss: 0.3739 - val_acc: 0.8473\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.3781 - acc: 0.8329 - val_loss: 0.3745 - val_acc: 0.8489\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.3785 - acc: 0.8325 - val_loss: 0.3738 - val_acc: 0.8489\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.3792 - acc: 0.8367 - val_loss: 0.3741 - val_acc: 0.8489\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.3771 - acc: 0.8363 - val_loss: 0.3745 - val_acc: 0.8473\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.3768 - acc: 0.8360 - val_loss: 0.3748 - val_acc: 0.8473\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.3709 - acc: 0.8380 - val_loss: 0.3755 - val_acc: 0.8489\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.3727 - acc: 0.8411 - val_loss: 0.3743 - val_acc: 0.8506\n",
      "Training with parameters {'batch_size': 4000, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_152\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_153 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_456 (Dense)            (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_304 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_457 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_305 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_458 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.8441 - acc: 0.4348 - val_loss: 0.7158 - val_acc: 0.4433\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.7281 - acc: 0.4689 - val_loss: 0.6810 - val_acc: 0.5714\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6921 - acc: 0.5430 - val_loss: 0.6818 - val_acc: 0.5731\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6937 - acc: 0.5607 - val_loss: 0.6819 - val_acc: 0.5747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6890 - acc: 0.5650 - val_loss: 0.6726 - val_acc: 0.5747\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6850 - acc: 0.5774 - val_loss: 0.6605 - val_acc: 0.5747\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6713 - acc: 0.5871 - val_loss: 0.6516 - val_acc: 0.5878\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6642 - acc: 0.5990 - val_loss: 0.6479 - val_acc: 0.6453\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6629 - acc: 0.6125 - val_loss: 0.6452 - val_acc: 0.6979\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6612 - acc: 0.6145 - val_loss: 0.6387 - val_acc: 0.7044\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6569 - acc: 0.6211 - val_loss: 0.6294 - val_acc: 0.7077\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6487 - acc: 0.6367 - val_loss: 0.6210 - val_acc: 0.7094\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6434 - acc: 0.6293 - val_loss: 0.6142 - val_acc: 0.7028\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6391 - acc: 0.6382 - val_loss: 0.6070 - val_acc: 0.7176\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6355 - acc: 0.6460 - val_loss: 0.5994 - val_acc: 0.7143\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6283 - acc: 0.6570 - val_loss: 0.5936 - val_acc: 0.7176\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6194 - acc: 0.6762 - val_loss: 0.5872 - val_acc: 0.7258\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6184 - acc: 0.6685 - val_loss: 0.5801 - val_acc: 0.7307\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6156 - acc: 0.6772 - val_loss: 0.5741 - val_acc: 0.7373\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6092 - acc: 0.6754 - val_loss: 0.5684 - val_acc: 0.7504\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6072 - acc: 0.6876 - val_loss: 0.5630 - val_acc: 0.7422\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5985 - acc: 0.6973 - val_loss: 0.5582 - val_acc: 0.7504\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5952 - acc: 0.6895 - val_loss: 0.5527 - val_acc: 0.7521\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5919 - acc: 0.7017 - val_loss: 0.5465 - val_acc: 0.7652\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5866 - acc: 0.7055 - val_loss: 0.5414 - val_acc: 0.7750\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5850 - acc: 0.7117 - val_loss: 0.5361 - val_acc: 0.7800\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5738 - acc: 0.7145 - val_loss: 0.5316 - val_acc: 0.7701\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5723 - acc: 0.7190 - val_loss: 0.5264 - val_acc: 0.7783\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5714 - acc: 0.7254 - val_loss: 0.5209 - val_acc: 0.7931\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5663 - acc: 0.7285 - val_loss: 0.5161 - val_acc: 0.7980\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5629 - acc: 0.7285 - val_loss: 0.5115 - val_acc: 0.7898\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5585 - acc: 0.7323 - val_loss: 0.5069 - val_acc: 0.7980\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5532 - acc: 0.7411 - val_loss: 0.5024 - val_acc: 0.7964\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5497 - acc: 0.7418 - val_loss: 0.4980 - val_acc: 0.8013\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5454 - acc: 0.7444 - val_loss: 0.4935 - val_acc: 0.8013\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5419 - acc: 0.7477 - val_loss: 0.4892 - val_acc: 0.7964\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5406 - acc: 0.7473 - val_loss: 0.4851 - val_acc: 0.7980\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5327 - acc: 0.7555 - val_loss: 0.4809 - val_acc: 0.7997\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5331 - acc: 0.7480 - val_loss: 0.4767 - val_acc: 0.8079\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5281 - acc: 0.7597 - val_loss: 0.4731 - val_acc: 0.8128\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5255 - acc: 0.7573 - val_loss: 0.4692 - val_acc: 0.7997\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5224 - acc: 0.7632 - val_loss: 0.4657 - val_acc: 0.8013\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5177 - acc: 0.7679 - val_loss: 0.4623 - val_acc: 0.8112\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5153 - acc: 0.7657 - val_loss: 0.4587 - val_acc: 0.8079\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5123 - acc: 0.7676 - val_loss: 0.4556 - val_acc: 0.8095\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5120 - acc: 0.7718 - val_loss: 0.4529 - val_acc: 0.8177\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5079 - acc: 0.7703 - val_loss: 0.4493 - val_acc: 0.8112\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5053 - acc: 0.7707 - val_loss: 0.4472 - val_acc: 0.8112\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5029 - acc: 0.7690 - val_loss: 0.4435 - val_acc: 0.8194\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4973 - acc: 0.7741 - val_loss: 0.4415 - val_acc: 0.8177\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4964 - acc: 0.7780 - val_loss: 0.4383 - val_acc: 0.8161\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4919 - acc: 0.7787 - val_loss: 0.4360 - val_acc: 0.8144\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4914 - acc: 0.7756 - val_loss: 0.4335 - val_acc: 0.8210\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4916 - acc: 0.7772 - val_loss: 0.4316 - val_acc: 0.8194\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4874 - acc: 0.7785 - val_loss: 0.4297 - val_acc: 0.8177\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4831 - acc: 0.7871 - val_loss: 0.4277 - val_acc: 0.8243\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4841 - acc: 0.7854 - val_loss: 0.4265 - val_acc: 0.8227\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4780 - acc: 0.7905 - val_loss: 0.4244 - val_acc: 0.8194\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4790 - acc: 0.7895 - val_loss: 0.4228 - val_acc: 0.8194\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4762 - acc: 0.7862 - val_loss: 0.4221 - val_acc: 0.8292\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4754 - acc: 0.7926 - val_loss: 0.4201 - val_acc: 0.8210\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4703 - acc: 0.7933 - val_loss: 0.4188 - val_acc: 0.8276\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4736 - acc: 0.7916 - val_loss: 0.4175 - val_acc: 0.8276\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4705 - acc: 0.7913 - val_loss: 0.4159 - val_acc: 0.8259\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4691 - acc: 0.7878 - val_loss: 0.4146 - val_acc: 0.8276\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4644 - acc: 0.7958 - val_loss: 0.4130 - val_acc: 0.8292\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4628 - acc: 0.7966 - val_loss: 0.4115 - val_acc: 0.8309\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4632 - acc: 0.7955 - val_loss: 0.4101 - val_acc: 0.8325\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4619 - acc: 0.7968 - val_loss: 0.4087 - val_acc: 0.8325\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4606 - acc: 0.7933 - val_loss: 0.4076 - val_acc: 0.8292\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4584 - acc: 0.7980 - val_loss: 0.4064 - val_acc: 0.8325\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4547 - acc: 0.7953 - val_loss: 0.4065 - val_acc: 0.8358\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4540 - acc: 0.7997 - val_loss: 0.4049 - val_acc: 0.8358\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4561 - acc: 0.7953 - val_loss: 0.4047 - val_acc: 0.8374\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4539 - acc: 0.7966 - val_loss: 0.4040 - val_acc: 0.8342\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4538 - acc: 0.8057 - val_loss: 0.4021 - val_acc: 0.8358\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4465 - acc: 0.8041 - val_loss: 0.4023 - val_acc: 0.8374\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4490 - acc: 0.7986 - val_loss: 0.4006 - val_acc: 0.8391\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4507 - acc: 0.7984 - val_loss: 0.3995 - val_acc: 0.8374\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4464 - acc: 0.8039 - val_loss: 0.3990 - val_acc: 0.8407\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4454 - acc: 0.8057 - val_loss: 0.3989 - val_acc: 0.8342\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4444 - acc: 0.8024 - val_loss: 0.3972 - val_acc: 0.8407\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4422 - acc: 0.8068 - val_loss: 0.3967 - val_acc: 0.8407\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4418 - acc: 0.8037 - val_loss: 0.3960 - val_acc: 0.8374\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4420 - acc: 0.8095 - val_loss: 0.3958 - val_acc: 0.8407\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4410 - acc: 0.8061 - val_loss: 0.3951 - val_acc: 0.8407\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4375 - acc: 0.8051 - val_loss: 0.3943 - val_acc: 0.8424\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4368 - acc: 0.8072 - val_loss: 0.3936 - val_acc: 0.8440\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4377 - acc: 0.8077 - val_loss: 0.3928 - val_acc: 0.8424\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4349 - acc: 0.8092 - val_loss: 0.3922 - val_acc: 0.8424\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4370 - acc: 0.8084 - val_loss: 0.3913 - val_acc: 0.8456\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4338 - acc: 0.8103 - val_loss: 0.3913 - val_acc: 0.8456\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4330 - acc: 0.8104 - val_loss: 0.3907 - val_acc: 0.8456\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4352 - acc: 0.8113 - val_loss: 0.3906 - val_acc: 0.8440\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4316 - acc: 0.8082 - val_loss: 0.3899 - val_acc: 0.8473\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4315 - acc: 0.8124 - val_loss: 0.3909 - val_acc: 0.8374\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4330 - acc: 0.8081 - val_loss: 0.3889 - val_acc: 0.8456\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4294 - acc: 0.8154 - val_loss: 0.3889 - val_acc: 0.8473\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4271 - acc: 0.8108 - val_loss: 0.3888 - val_acc: 0.8391\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4292 - acc: 0.8150 - val_loss: 0.3873 - val_acc: 0.8473\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4256 - acc: 0.8141 - val_loss: 0.3890 - val_acc: 0.8456\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4260 - acc: 0.8128 - val_loss: 0.3863 - val_acc: 0.8473\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4261 - acc: 0.8152 - val_loss: 0.3861 - val_acc: 0.8424\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4235 - acc: 0.8146 - val_loss: 0.3855 - val_acc: 0.8456\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4208 - acc: 0.8188 - val_loss: 0.3852 - val_acc: 0.8456\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4224 - acc: 0.8163 - val_loss: 0.3850 - val_acc: 0.8456\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4211 - acc: 0.8132 - val_loss: 0.3850 - val_acc: 0.8440\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4200 - acc: 0.8148 - val_loss: 0.3846 - val_acc: 0.8473\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4217 - acc: 0.8165 - val_loss: 0.3843 - val_acc: 0.8473\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4197 - acc: 0.8172 - val_loss: 0.3848 - val_acc: 0.8407\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4180 - acc: 0.8154 - val_loss: 0.3846 - val_acc: 0.8456\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4178 - acc: 0.8207 - val_loss: 0.3835 - val_acc: 0.8440\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4171 - acc: 0.8210 - val_loss: 0.3841 - val_acc: 0.8391\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4167 - acc: 0.8181 - val_loss: 0.3835 - val_acc: 0.8440\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4179 - acc: 0.8165 - val_loss: 0.3827 - val_acc: 0.8456\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4181 - acc: 0.8194 - val_loss: 0.3827 - val_acc: 0.8407\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4137 - acc: 0.8227 - val_loss: 0.3835 - val_acc: 0.8440\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4147 - acc: 0.8201 - val_loss: 0.3818 - val_acc: 0.8473\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4155 - acc: 0.8199 - val_loss: 0.3818 - val_acc: 0.8473\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4120 - acc: 0.8232 - val_loss: 0.3842 - val_acc: 0.8424\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4147 - acc: 0.8168 - val_loss: 0.3815 - val_acc: 0.8456\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4142 - acc: 0.8201 - val_loss: 0.3835 - val_acc: 0.8424\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4106 - acc: 0.8219 - val_loss: 0.3840 - val_acc: 0.8440\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4120 - acc: 0.8197 - val_loss: 0.3835 - val_acc: 0.8456\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4145 - acc: 0.8181 - val_loss: 0.3826 - val_acc: 0.8456\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4083 - acc: 0.8217 - val_loss: 0.3812 - val_acc: 0.8489\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4103 - acc: 0.8194 - val_loss: 0.3827 - val_acc: 0.8456\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4107 - acc: 0.8201 - val_loss: 0.3814 - val_acc: 0.8440\n",
      "Epoch 129/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4077 - acc: 0.8232 - val_loss: 0.3805 - val_acc: 0.8489\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4078 - acc: 0.8225 - val_loss: 0.3814 - val_acc: 0.8440\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4076 - acc: 0.8241 - val_loss: 0.3822 - val_acc: 0.8440\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4103 - acc: 0.8265 - val_loss: 0.3797 - val_acc: 0.8473\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4034 - acc: 0.8252 - val_loss: 0.3804 - val_acc: 0.8440\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4047 - acc: 0.8186 - val_loss: 0.3786 - val_acc: 0.8539\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4062 - acc: 0.8274 - val_loss: 0.3787 - val_acc: 0.8473\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4052 - acc: 0.8205 - val_loss: 0.3786 - val_acc: 0.8473\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4030 - acc: 0.8232 - val_loss: 0.3777 - val_acc: 0.8522\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4009 - acc: 0.8272 - val_loss: 0.3783 - val_acc: 0.8489\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4027 - acc: 0.8227 - val_loss: 0.3774 - val_acc: 0.8506\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4036 - acc: 0.8250 - val_loss: 0.3775 - val_acc: 0.8506\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4007 - acc: 0.8248 - val_loss: 0.3776 - val_acc: 0.8506\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4000 - acc: 0.8241 - val_loss: 0.3776 - val_acc: 0.8489\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.3995 - acc: 0.8276 - val_loss: 0.3781 - val_acc: 0.8473\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.3980 - acc: 0.8287 - val_loss: 0.3769 - val_acc: 0.8506\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4016 - acc: 0.8241 - val_loss: 0.3766 - val_acc: 0.8506\n",
      "Epoch 146/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3991 - acc: 0.8269 - val_loss: 0.3764 - val_acc: 0.8506\n",
      "Epoch 147/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.3997 - acc: 0.8292 - val_loss: 0.3765 - val_acc: 0.8489\n",
      "Epoch 148/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.3986 - acc: 0.8294 - val_loss: 0.3762 - val_acc: 0.8522\n",
      "Epoch 149/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.3965 - acc: 0.8269 - val_loss: 0.3767 - val_acc: 0.8489\n",
      "Epoch 150/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3937 - acc: 0.8298 - val_loss: 0.3766 - val_acc: 0.8473\n",
      "Epoch 151/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3971 - acc: 0.8278 - val_loss: 0.3769 - val_acc: 0.8489\n",
      "Epoch 152/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3941 - acc: 0.8300 - val_loss: 0.3769 - val_acc: 0.8473\n",
      "Epoch 153/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3951 - acc: 0.8311 - val_loss: 0.3767 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 4000, 'dropout': 0.1, 'lay_conf': [1024, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_153\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_154 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_459 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_306 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_460 (Dense)            (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_307 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_461 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 1s 111ms/step - loss: 0.9401 - acc: 0.4512 - val_loss: 0.7616 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.7834 - acc: 0.5676 - val_loss: 0.7502 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.7467 - acc: 0.5687 - val_loss: 0.6597 - val_acc: 0.5862\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6764 - acc: 0.5762 - val_loss: 0.6898 - val_acc: 0.5255\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6998 - acc: 0.5490 - val_loss: 0.6658 - val_acc: 0.6108\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6738 - acc: 0.5913 - val_loss: 0.6172 - val_acc: 0.7028\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6407 - acc: 0.6360 - val_loss: 0.6198 - val_acc: 0.5944\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6454 - acc: 0.6008 - val_loss: 0.6079 - val_acc: 0.6207\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6304 - acc: 0.6322 - val_loss: 0.5842 - val_acc: 0.7406\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6172 - acc: 0.6667 - val_loss: 0.5860 - val_acc: 0.6913\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6150 - acc: 0.6617 - val_loss: 0.5760 - val_acc: 0.7061\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6072 - acc: 0.6676 - val_loss: 0.5566 - val_acc: 0.7389\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5942 - acc: 0.6946 - val_loss: 0.5528 - val_acc: 0.7553\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5922 - acc: 0.7001 - val_loss: 0.5423 - val_acc: 0.7734\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5796 - acc: 0.7159 - val_loss: 0.5352 - val_acc: 0.7537\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5718 - acc: 0.7125 - val_loss: 0.5313 - val_acc: 0.7586\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5682 - acc: 0.7112 - val_loss: 0.5189 - val_acc: 0.7898\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5593 - acc: 0.7338 - val_loss: 0.5120 - val_acc: 0.7964\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5556 - acc: 0.7349 - val_loss: 0.5038 - val_acc: 0.8013\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5482 - acc: 0.7391 - val_loss: 0.4997 - val_acc: 0.7947\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5451 - acc: 0.7345 - val_loss: 0.4902 - val_acc: 0.8079\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5378 - acc: 0.7495 - val_loss: 0.4850 - val_acc: 0.8079\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5272 - acc: 0.7535 - val_loss: 0.4804 - val_acc: 0.8128\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5246 - acc: 0.7491 - val_loss: 0.4746 - val_acc: 0.8161\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5181 - acc: 0.7573 - val_loss: 0.4688 - val_acc: 0.8095\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5196 - acc: 0.7601 - val_loss: 0.4607 - val_acc: 0.8161\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5085 - acc: 0.7659 - val_loss: 0.4596 - val_acc: 0.8177\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5041 - acc: 0.7683 - val_loss: 0.4525 - val_acc: 0.8210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5021 - acc: 0.7657 - val_loss: 0.4469 - val_acc: 0.8259\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4995 - acc: 0.7672 - val_loss: 0.4471 - val_acc: 0.8161\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4930 - acc: 0.7692 - val_loss: 0.4400 - val_acc: 0.8276\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4899 - acc: 0.7741 - val_loss: 0.4371 - val_acc: 0.8227\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4857 - acc: 0.7763 - val_loss: 0.4340 - val_acc: 0.8194\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4861 - acc: 0.7732 - val_loss: 0.4298 - val_acc: 0.8309\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4826 - acc: 0.7783 - val_loss: 0.4271 - val_acc: 0.8325\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4735 - acc: 0.7845 - val_loss: 0.4241 - val_acc: 0.8325\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4726 - acc: 0.7853 - val_loss: 0.4209 - val_acc: 0.8276\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4708 - acc: 0.7904 - val_loss: 0.4202 - val_acc: 0.8292\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4679 - acc: 0.7880 - val_loss: 0.4152 - val_acc: 0.8309\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4616 - acc: 0.7949 - val_loss: 0.4126 - val_acc: 0.8325\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4599 - acc: 0.7937 - val_loss: 0.4129 - val_acc: 0.8325\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4600 - acc: 0.7911 - val_loss: 0.4077 - val_acc: 0.8309\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4546 - acc: 0.7949 - val_loss: 0.4055 - val_acc: 0.8358\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4539 - acc: 0.7969 - val_loss: 0.4044 - val_acc: 0.8391\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4487 - acc: 0.7984 - val_loss: 0.4028 - val_acc: 0.8342\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4461 - acc: 0.8000 - val_loss: 0.4047 - val_acc: 0.8407\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4490 - acc: 0.7968 - val_loss: 0.4001 - val_acc: 0.8342\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4465 - acc: 0.7980 - val_loss: 0.3987 - val_acc: 0.8391\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4417 - acc: 0.8008 - val_loss: 0.3970 - val_acc: 0.8374\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.4396 - acc: 0.8033 - val_loss: 0.3958 - val_acc: 0.8391\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4391 - acc: 0.8064 - val_loss: 0.3965 - val_acc: 0.8358\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4383 - acc: 0.8026 - val_loss: 0.3935 - val_acc: 0.8358\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4333 - acc: 0.8073 - val_loss: 0.3943 - val_acc: 0.8391\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4280 - acc: 0.8112 - val_loss: 0.3922 - val_acc: 0.8424\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4307 - acc: 0.8097 - val_loss: 0.3914 - val_acc: 0.8391\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4294 - acc: 0.8070 - val_loss: 0.3894 - val_acc: 0.8391\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4256 - acc: 0.8104 - val_loss: 0.3885 - val_acc: 0.8424\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4275 - acc: 0.8093 - val_loss: 0.3928 - val_acc: 0.8374\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4278 - acc: 0.8093 - val_loss: 0.3895 - val_acc: 0.8440\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4269 - acc: 0.8165 - val_loss: 0.3888 - val_acc: 0.8374\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4197 - acc: 0.8139 - val_loss: 0.3889 - val_acc: 0.8374\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4217 - acc: 0.8132 - val_loss: 0.3862 - val_acc: 0.8374\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4187 - acc: 0.8124 - val_loss: 0.3906 - val_acc: 0.8358\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4221 - acc: 0.8108 - val_loss: 0.3846 - val_acc: 0.8391\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4171 - acc: 0.8165 - val_loss: 0.3850 - val_acc: 0.8391\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4124 - acc: 0.8203 - val_loss: 0.3834 - val_acc: 0.8407\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4169 - acc: 0.8155 - val_loss: 0.3833 - val_acc: 0.8407\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4126 - acc: 0.8216 - val_loss: 0.3844 - val_acc: 0.8440\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4100 - acc: 0.8216 - val_loss: 0.3824 - val_acc: 0.8473\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4097 - acc: 0.8150 - val_loss: 0.3853 - val_acc: 0.8424\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4144 - acc: 0.8174 - val_loss: 0.3811 - val_acc: 0.8473\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4065 - acc: 0.8225 - val_loss: 0.3807 - val_acc: 0.8407\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4033 - acc: 0.8241 - val_loss: 0.3803 - val_acc: 0.8407\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4048 - acc: 0.8227 - val_loss: 0.3804 - val_acc: 0.8473\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4051 - acc: 0.8225 - val_loss: 0.3815 - val_acc: 0.8473\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3992 - acc: 0.8245 - val_loss: 0.3781 - val_acc: 0.8440\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4023 - acc: 0.8241 - val_loss: 0.3866 - val_acc: 0.8440\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4042 - acc: 0.8176 - val_loss: 0.3849 - val_acc: 0.8506\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4068 - acc: 0.8228 - val_loss: 0.3796 - val_acc: 0.8440\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3982 - acc: 0.8267 - val_loss: 0.3775 - val_acc: 0.8456\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3940 - acc: 0.8270 - val_loss: 0.3774 - val_acc: 0.8473\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3986 - acc: 0.8241 - val_loss: 0.3787 - val_acc: 0.8473\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3958 - acc: 0.8290 - val_loss: 0.3759 - val_acc: 0.8440\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.3916 - acc: 0.8327 - val_loss: 0.3803 - val_acc: 0.8506\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.3885 - acc: 0.8298 - val_loss: 0.3760 - val_acc: 0.8456\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.3916 - acc: 0.8270 - val_loss: 0.3784 - val_acc: 0.8522\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.3913 - acc: 0.8300 - val_loss: 0.3753 - val_acc: 0.8456\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.3920 - acc: 0.8300 - val_loss: 0.3752 - val_acc: 0.8440\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3828 - acc: 0.8323 - val_loss: 0.3743 - val_acc: 0.8456\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3849 - acc: 0.8312 - val_loss: 0.3738 - val_acc: 0.8456\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3848 - acc: 0.8305 - val_loss: 0.3761 - val_acc: 0.8555\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3842 - acc: 0.8289 - val_loss: 0.3743 - val_acc: 0.8473\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3822 - acc: 0.8303 - val_loss: 0.3737 - val_acc: 0.8522\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3789 - acc: 0.8345 - val_loss: 0.3727 - val_acc: 0.8456\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3811 - acc: 0.8309 - val_loss: 0.3769 - val_acc: 0.8522\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3827 - acc: 0.8309 - val_loss: 0.3724 - val_acc: 0.8506\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3789 - acc: 0.8329 - val_loss: 0.3716 - val_acc: 0.8473\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3745 - acc: 0.8398 - val_loss: 0.3720 - val_acc: 0.8506\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3764 - acc: 0.8358 - val_loss: 0.3724 - val_acc: 0.8489\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3718 - acc: 0.8371 - val_loss: 0.3728 - val_acc: 0.8522\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3722 - acc: 0.8380 - val_loss: 0.3731 - val_acc: 0.8555\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3678 - acc: 0.8387 - val_loss: 0.3729 - val_acc: 0.8555\n",
      "Training with parameters {'batch_size': 4000, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_154\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_155 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_462 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_308 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_463 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_309 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_464 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,985\n",
      "Trainable params: 409,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 1.7958 - acc: 0.4320 - val_loss: 1.1887 - val_acc: 0.4253\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.1417 - acc: 0.4346 - val_loss: 0.7786 - val_acc: 0.4269\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.7880 - acc: 0.4559 - val_loss: 0.7067 - val_acc: 0.5731\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.7344 - acc: 0.5244 - val_loss: 0.7585 - val_acc: 0.5731\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.7752 - acc: 0.5599 - val_loss: 0.7793 - val_acc: 0.5747\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7894 - acc: 0.5639 - val_loss: 0.7556 - val_acc: 0.5747\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.7595 - acc: 0.5627 - val_loss: 0.7161 - val_acc: 0.5747\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7277 - acc: 0.5654 - val_loss: 0.6817 - val_acc: 0.5747\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6946 - acc: 0.5694 - val_loss: 0.6628 - val_acc: 0.5862\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6840 - acc: 0.5676 - val_loss: 0.6587 - val_acc: 0.6420\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6797 - acc: 0.5738 - val_loss: 0.6597 - val_acc: 0.6683\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6799 - acc: 0.5762 - val_loss: 0.6568 - val_acc: 0.6765\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6768 - acc: 0.5807 - val_loss: 0.6481 - val_acc: 0.6814\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6682 - acc: 0.5993 - val_loss: 0.6380 - val_acc: 0.7061\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6599 - acc: 0.6094 - val_loss: 0.6314 - val_acc: 0.6371\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6526 - acc: 0.6169 - val_loss: 0.6278 - val_acc: 0.6059\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6525 - acc: 0.6065 - val_loss: 0.6237 - val_acc: 0.6076\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6447 - acc: 0.6296 - val_loss: 0.6166 - val_acc: 0.6502\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6435 - acc: 0.6285 - val_loss: 0.6090 - val_acc: 0.6995\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6312 - acc: 0.6528 - val_loss: 0.6036 - val_acc: 0.7274\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6334 - acc: 0.6504 - val_loss: 0.5990 - val_acc: 0.7323\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6266 - acc: 0.6521 - val_loss: 0.5924 - val_acc: 0.7356\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6278 - acc: 0.6603 - val_loss: 0.5847 - val_acc: 0.7455\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6157 - acc: 0.6765 - val_loss: 0.5778 - val_acc: 0.7553\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6109 - acc: 0.6800 - val_loss: 0.5738 - val_acc: 0.7570\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6125 - acc: 0.6763 - val_loss: 0.5684 - val_acc: 0.7586\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6045 - acc: 0.6882 - val_loss: 0.5625 - val_acc: 0.7619\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5999 - acc: 0.6915 - val_loss: 0.5587 - val_acc: 0.7553\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5986 - acc: 0.6911 - val_loss: 0.5546 - val_acc: 0.7619\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5948 - acc: 0.6944 - val_loss: 0.5496 - val_acc: 0.7668\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5939 - acc: 0.6913 - val_loss: 0.5456 - val_acc: 0.7816\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5890 - acc: 0.6986 - val_loss: 0.5421 - val_acc: 0.7783\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5884 - acc: 0.7033 - val_loss: 0.5380 - val_acc: 0.7833\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5844 - acc: 0.7083 - val_loss: 0.5343 - val_acc: 0.7816\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5810 - acc: 0.7084 - val_loss: 0.5308 - val_acc: 0.7849\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5771 - acc: 0.7154 - val_loss: 0.5266 - val_acc: 0.7849\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5705 - acc: 0.7232 - val_loss: 0.5219 - val_acc: 0.7980\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5644 - acc: 0.7236 - val_loss: 0.5172 - val_acc: 0.8030\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5667 - acc: 0.7240 - val_loss: 0.5130 - val_acc: 0.8013\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5618 - acc: 0.7278 - val_loss: 0.5093 - val_acc: 0.7980\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5591 - acc: 0.7291 - val_loss: 0.5059 - val_acc: 0.7980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5555 - acc: 0.7327 - val_loss: 0.5025 - val_acc: 0.7997\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5558 - acc: 0.7323 - val_loss: 0.4994 - val_acc: 0.8030\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5492 - acc: 0.7387 - val_loss: 0.4961 - val_acc: 0.8046\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5470 - acc: 0.7354 - val_loss: 0.4935 - val_acc: 0.8046\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5426 - acc: 0.7455 - val_loss: 0.4902 - val_acc: 0.8079\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5386 - acc: 0.7475 - val_loss: 0.4874 - val_acc: 0.8112\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5391 - acc: 0.7391 - val_loss: 0.4845 - val_acc: 0.8095\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5327 - acc: 0.7480 - val_loss: 0.4820 - val_acc: 0.8144\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5292 - acc: 0.7531 - val_loss: 0.4790 - val_acc: 0.8095\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5289 - acc: 0.7521 - val_loss: 0.4763 - val_acc: 0.8095\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5297 - acc: 0.7537 - val_loss: 0.4740 - val_acc: 0.8161\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5281 - acc: 0.7546 - val_loss: 0.4715 - val_acc: 0.8161\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5258 - acc: 0.7568 - val_loss: 0.4687 - val_acc: 0.8095\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5208 - acc: 0.7621 - val_loss: 0.4664 - val_acc: 0.8112\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5192 - acc: 0.7594 - val_loss: 0.4639 - val_acc: 0.8112\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5174 - acc: 0.7615 - val_loss: 0.4621 - val_acc: 0.8144\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5146 - acc: 0.7637 - val_loss: 0.4598 - val_acc: 0.8128\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5139 - acc: 0.7645 - val_loss: 0.4574 - val_acc: 0.8128\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5086 - acc: 0.7712 - val_loss: 0.4553 - val_acc: 0.8177\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5089 - acc: 0.7654 - val_loss: 0.4535 - val_acc: 0.8210\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5078 - acc: 0.7650 - val_loss: 0.4514 - val_acc: 0.8161\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5060 - acc: 0.7716 - val_loss: 0.4497 - val_acc: 0.8177\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5021 - acc: 0.7710 - val_loss: 0.4477 - val_acc: 0.8177\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5000 - acc: 0.7750 - val_loss: 0.4462 - val_acc: 0.8227\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4978 - acc: 0.7738 - val_loss: 0.4444 - val_acc: 0.8227\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4943 - acc: 0.7752 - val_loss: 0.4430 - val_acc: 0.8210\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4928 - acc: 0.7798 - val_loss: 0.4415 - val_acc: 0.8227\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4930 - acc: 0.7723 - val_loss: 0.4399 - val_acc: 0.8259\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4872 - acc: 0.7816 - val_loss: 0.4384 - val_acc: 0.8259\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4913 - acc: 0.7794 - val_loss: 0.4368 - val_acc: 0.8276\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4863 - acc: 0.7798 - val_loss: 0.4354 - val_acc: 0.8325\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4840 - acc: 0.7838 - val_loss: 0.4341 - val_acc: 0.8325\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4825 - acc: 0.7862 - val_loss: 0.4329 - val_acc: 0.8342\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4835 - acc: 0.7818 - val_loss: 0.4316 - val_acc: 0.8325\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4798 - acc: 0.7805 - val_loss: 0.4302 - val_acc: 0.8342\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4799 - acc: 0.7822 - val_loss: 0.4289 - val_acc: 0.8374\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4753 - acc: 0.7902 - val_loss: 0.4276 - val_acc: 0.8342\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4742 - acc: 0.7876 - val_loss: 0.4264 - val_acc: 0.8342\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4815 - acc: 0.7803 - val_loss: 0.4251 - val_acc: 0.8374\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4736 - acc: 0.7900 - val_loss: 0.4240 - val_acc: 0.8424\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4745 - acc: 0.7893 - val_loss: 0.4229 - val_acc: 0.8374\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4716 - acc: 0.7885 - val_loss: 0.4221 - val_acc: 0.8374\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4689 - acc: 0.7964 - val_loss: 0.4210 - val_acc: 0.8374\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4648 - acc: 0.7942 - val_loss: 0.4199 - val_acc: 0.8391\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4656 - acc: 0.7873 - val_loss: 0.4186 - val_acc: 0.8391\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4629 - acc: 0.7962 - val_loss: 0.4174 - val_acc: 0.8407\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4657 - acc: 0.7893 - val_loss: 0.4164 - val_acc: 0.8391\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4590 - acc: 0.7971 - val_loss: 0.4157 - val_acc: 0.8391\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4605 - acc: 0.7991 - val_loss: 0.4145 - val_acc: 0.8407\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4603 - acc: 0.7989 - val_loss: 0.4137 - val_acc: 0.8424\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4640 - acc: 0.7900 - val_loss: 0.4126 - val_acc: 0.8424\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4554 - acc: 0.7931 - val_loss: 0.4129 - val_acc: 0.8424\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4587 - acc: 0.7955 - val_loss: 0.4117 - val_acc: 0.8473\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4562 - acc: 0.7958 - val_loss: 0.4107 - val_acc: 0.8407\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4515 - acc: 0.7986 - val_loss: 0.4097 - val_acc: 0.8440\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4526 - acc: 0.7975 - val_loss: 0.4097 - val_acc: 0.8440\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4500 - acc: 0.8019 - val_loss: 0.4086 - val_acc: 0.8407\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4515 - acc: 0.7958 - val_loss: 0.4080 - val_acc: 0.8424\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4497 - acc: 0.8019 - val_loss: 0.4077 - val_acc: 0.8456\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4484 - acc: 0.8055 - val_loss: 0.4067 - val_acc: 0.8473\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4470 - acc: 0.8030 - val_loss: 0.4062 - val_acc: 0.8489\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4494 - acc: 0.7999 - val_loss: 0.4051 - val_acc: 0.8424\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4438 - acc: 0.8050 - val_loss: 0.4050 - val_acc: 0.8473\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4451 - acc: 0.8061 - val_loss: 0.4042 - val_acc: 0.8440\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4465 - acc: 0.7997 - val_loss: 0.4035 - val_acc: 0.8456\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4420 - acc: 0.8084 - val_loss: 0.4031 - val_acc: 0.8440\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4437 - acc: 0.8072 - val_loss: 0.4022 - val_acc: 0.8456\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4425 - acc: 0.8121 - val_loss: 0.4026 - val_acc: 0.8424\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4415 - acc: 0.8064 - val_loss: 0.4017 - val_acc: 0.8440\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4411 - acc: 0.8075 - val_loss: 0.4012 - val_acc: 0.8456\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4374 - acc: 0.8090 - val_loss: 0.4010 - val_acc: 0.8424\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4403 - acc: 0.8088 - val_loss: 0.4006 - val_acc: 0.8473\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4376 - acc: 0.8075 - val_loss: 0.4008 - val_acc: 0.8489\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4364 - acc: 0.8101 - val_loss: 0.3995 - val_acc: 0.8407\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4335 - acc: 0.8090 - val_loss: 0.3988 - val_acc: 0.8440\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4333 - acc: 0.8110 - val_loss: 0.3990 - val_acc: 0.8456\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4350 - acc: 0.8123 - val_loss: 0.3975 - val_acc: 0.8424\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4299 - acc: 0.8112 - val_loss: 0.3973 - val_acc: 0.8424\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4335 - acc: 0.8084 - val_loss: 0.3962 - val_acc: 0.8456\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4299 - acc: 0.8119 - val_loss: 0.3958 - val_acc: 0.8456\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4350 - acc: 0.8082 - val_loss: 0.3953 - val_acc: 0.8424\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4318 - acc: 0.8055 - val_loss: 0.3950 - val_acc: 0.8440\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4282 - acc: 0.8126 - val_loss: 0.3947 - val_acc: 0.8440\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4305 - acc: 0.8126 - val_loss: 0.3944 - val_acc: 0.8424\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4287 - acc: 0.8126 - val_loss: 0.3943 - val_acc: 0.8407\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4238 - acc: 0.8163 - val_loss: 0.3936 - val_acc: 0.8440\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4271 - acc: 0.8112 - val_loss: 0.3937 - val_acc: 0.8440\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4286 - acc: 0.8177 - val_loss: 0.3935 - val_acc: 0.8440\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4249 - acc: 0.8093 - val_loss: 0.3926 - val_acc: 0.8456\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4241 - acc: 0.8115 - val_loss: 0.3923 - val_acc: 0.8440\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4225 - acc: 0.8159 - val_loss: 0.3920 - val_acc: 0.8440\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4240 - acc: 0.8157 - val_loss: 0.3917 - val_acc: 0.8440\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4235 - acc: 0.8168 - val_loss: 0.3925 - val_acc: 0.8456\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4247 - acc: 0.8124 - val_loss: 0.3915 - val_acc: 0.8407\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4263 - acc: 0.8132 - val_loss: 0.3912 - val_acc: 0.8440\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4239 - acc: 0.8146 - val_loss: 0.3910 - val_acc: 0.8407\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4213 - acc: 0.8155 - val_loss: 0.3905 - val_acc: 0.8424\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4190 - acc: 0.8115 - val_loss: 0.3906 - val_acc: 0.8440\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4157 - acc: 0.8199 - val_loss: 0.3901 - val_acc: 0.8424\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4197 - acc: 0.8199 - val_loss: 0.3903 - val_acc: 0.8440\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4213 - acc: 0.8194 - val_loss: 0.3901 - val_acc: 0.8440\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4191 - acc: 0.8179 - val_loss: 0.3895 - val_acc: 0.8424\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4170 - acc: 0.8188 - val_loss: 0.3894 - val_acc: 0.8456\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4164 - acc: 0.8201 - val_loss: 0.3889 - val_acc: 0.8424\n",
      "Epoch 146/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4132 - acc: 0.8154 - val_loss: 0.3896 - val_acc: 0.8456\n",
      "Epoch 147/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4173 - acc: 0.8194 - val_loss: 0.3889 - val_acc: 0.8456\n",
      "Epoch 148/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4189 - acc: 0.8194 - val_loss: 0.3886 - val_acc: 0.8424\n",
      "Epoch 149/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4167 - acc: 0.8157 - val_loss: 0.3911 - val_acc: 0.8391\n",
      "Epoch 150/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4158 - acc: 0.8159 - val_loss: 0.3888 - val_acc: 0.8440\n",
      "Epoch 151/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4126 - acc: 0.8227 - val_loss: 0.3884 - val_acc: 0.8391\n",
      "Epoch 152/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4113 - acc: 0.8205 - val_loss: 0.3900 - val_acc: 0.8424\n",
      "Epoch 153/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4134 - acc: 0.8194 - val_loss: 0.3880 - val_acc: 0.8407\n",
      "Epoch 154/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4120 - acc: 0.8214 - val_loss: 0.3877 - val_acc: 0.8440\n",
      "Epoch 155/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4130 - acc: 0.8243 - val_loss: 0.3897 - val_acc: 0.8407\n",
      "Epoch 156/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4127 - acc: 0.8174 - val_loss: 0.3869 - val_acc: 0.8440\n",
      "Epoch 157/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4089 - acc: 0.8225 - val_loss: 0.3879 - val_acc: 0.8473\n",
      "Epoch 158/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4113 - acc: 0.8232 - val_loss: 0.3865 - val_acc: 0.8440\n",
      "Epoch 159/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4111 - acc: 0.8205 - val_loss: 0.3871 - val_acc: 0.8440\n",
      "Epoch 160/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4100 - acc: 0.8212 - val_loss: 0.3863 - val_acc: 0.8424\n",
      "Epoch 161/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4064 - acc: 0.8258 - val_loss: 0.3865 - val_acc: 0.8440\n",
      "Epoch 162/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4073 - acc: 0.8212 - val_loss: 0.3869 - val_acc: 0.8424\n",
      "Epoch 163/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4047 - acc: 0.8223 - val_loss: 0.3868 - val_acc: 0.8424\n",
      "Epoch 164/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4074 - acc: 0.8230 - val_loss: 0.3872 - val_acc: 0.8440\n",
      "Epoch 165/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4050 - acc: 0.8269 - val_loss: 0.3864 - val_acc: 0.8440\n",
      "Training with parameters {'batch_size': 4000, 'dropout': 0.1, 'lay_conf': [1024, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_155\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_156 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_465 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_310 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_466 (Dense)            (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_311 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_467 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,639,553\n",
      "Trainable params: 1,639,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.8154 - acc: 0.4714 - val_loss: 0.7596 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.7569 - acc: 0.5660 - val_loss: 0.6717 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.6821 - acc: 0.5765 - val_loss: 0.6578 - val_acc: 0.6371\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.6809 - acc: 0.5711 - val_loss: 0.6448 - val_acc: 0.6585\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.6606 - acc: 0.6110 - val_loss: 0.6101 - val_acc: 0.7094\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.6357 - acc: 0.6398 - val_loss: 0.6125 - val_acc: 0.6043\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.6356 - acc: 0.6245 - val_loss: 0.5851 - val_acc: 0.7192\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.6125 - acc: 0.6740 - val_loss: 0.5724 - val_acc: 0.7143\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.6056 - acc: 0.6740 - val_loss: 0.5632 - val_acc: 0.7159\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.5975 - acc: 0.6840 - val_loss: 0.5446 - val_acc: 0.7816\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.5862 - acc: 0.7132 - val_loss: 0.5394 - val_acc: 0.7685\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.5788 - acc: 0.7172 - val_loss: 0.5203 - val_acc: 0.7685\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.5674 - acc: 0.7130 - val_loss: 0.5146 - val_acc: 0.7635\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.5624 - acc: 0.7152 - val_loss: 0.5007 - val_acc: 0.8046\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.5543 - acc: 0.7354 - val_loss: 0.4928 - val_acc: 0.8177\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.5433 - acc: 0.7417 - val_loss: 0.4868 - val_acc: 0.7997\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5385 - acc: 0.7367 - val_loss: 0.4759 - val_acc: 0.8227\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5313 - acc: 0.7537 - val_loss: 0.4745 - val_acc: 0.8194\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.5262 - acc: 0.7581 - val_loss: 0.4656 - val_acc: 0.8112\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.5181 - acc: 0.7561 - val_loss: 0.4594 - val_acc: 0.8177\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.5136 - acc: 0.7595 - val_loss: 0.4579 - val_acc: 0.8243\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5131 - acc: 0.7656 - val_loss: 0.4481 - val_acc: 0.8259\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5007 - acc: 0.7676 - val_loss: 0.4471 - val_acc: 0.8194\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5025 - acc: 0.7641 - val_loss: 0.4399 - val_acc: 0.8309\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4952 - acc: 0.7734 - val_loss: 0.4373 - val_acc: 0.8292\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4937 - acc: 0.7719 - val_loss: 0.4324 - val_acc: 0.8243\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4865 - acc: 0.7787 - val_loss: 0.4282 - val_acc: 0.8309\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4823 - acc: 0.7811 - val_loss: 0.4257 - val_acc: 0.8292\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4817 - acc: 0.7820 - val_loss: 0.4230 - val_acc: 0.8276\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4786 - acc: 0.7780 - val_loss: 0.4190 - val_acc: 0.8309\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4730 - acc: 0.7887 - val_loss: 0.4179 - val_acc: 0.8391\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4715 - acc: 0.7874 - val_loss: 0.4151 - val_acc: 0.8342\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4700 - acc: 0.7854 - val_loss: 0.4124 - val_acc: 0.8424\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4677 - acc: 0.7915 - val_loss: 0.4098 - val_acc: 0.8391\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4624 - acc: 0.7926 - val_loss: 0.4080 - val_acc: 0.8407\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4614 - acc: 0.7916 - val_loss: 0.4071 - val_acc: 0.8407\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4586 - acc: 0.7977 - val_loss: 0.4057 - val_acc: 0.8342\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4571 - acc: 0.7929 - val_loss: 0.4045 - val_acc: 0.8407\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4567 - acc: 0.7935 - val_loss: 0.4022 - val_acc: 0.8424\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4530 - acc: 0.7951 - val_loss: 0.4094 - val_acc: 0.8391\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4594 - acc: 0.7947 - val_loss: 0.4027 - val_acc: 0.8391\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4508 - acc: 0.7984 - val_loss: 0.3984 - val_acc: 0.8456\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4471 - acc: 0.8019 - val_loss: 0.4030 - val_acc: 0.8407\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4531 - acc: 0.7929 - val_loss: 0.3973 - val_acc: 0.8424\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4470 - acc: 0.8024 - val_loss: 0.3945 - val_acc: 0.8473\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4411 - acc: 0.8030 - val_loss: 0.3961 - val_acc: 0.8456\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4436 - acc: 0.7975 - val_loss: 0.3920 - val_acc: 0.8489\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4408 - acc: 0.8073 - val_loss: 0.3897 - val_acc: 0.8489\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4398 - acc: 0.8037 - val_loss: 0.3906 - val_acc: 0.8489\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4400 - acc: 0.8086 - val_loss: 0.3907 - val_acc: 0.8440\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4348 - acc: 0.8099 - val_loss: 0.3879 - val_acc: 0.8539\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4345 - acc: 0.8095 - val_loss: 0.3891 - val_acc: 0.8489\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4305 - acc: 0.8062 - val_loss: 0.3868 - val_acc: 0.8506\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4335 - acc: 0.8072 - val_loss: 0.3858 - val_acc: 0.8555\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4308 - acc: 0.8104 - val_loss: 0.3864 - val_acc: 0.8522\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4309 - acc: 0.8095 - val_loss: 0.3840 - val_acc: 0.8555\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4270 - acc: 0.8124 - val_loss: 0.3822 - val_acc: 0.8555\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4252 - acc: 0.8128 - val_loss: 0.3810 - val_acc: 0.8522\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4263 - acc: 0.8139 - val_loss: 0.3812 - val_acc: 0.8539\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4218 - acc: 0.8128 - val_loss: 0.3800 - val_acc: 0.8506\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4245 - acc: 0.8157 - val_loss: 0.3796 - val_acc: 0.8555\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4186 - acc: 0.8190 - val_loss: 0.3793 - val_acc: 0.8506\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4206 - acc: 0.8154 - val_loss: 0.3784 - val_acc: 0.8555\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4182 - acc: 0.8157 - val_loss: 0.3775 - val_acc: 0.8555\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4179 - acc: 0.8155 - val_loss: 0.3770 - val_acc: 0.8522\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4148 - acc: 0.8154 - val_loss: 0.3765 - val_acc: 0.8571\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4149 - acc: 0.8177 - val_loss: 0.3755 - val_acc: 0.8588\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4127 - acc: 0.8170 - val_loss: 0.3751 - val_acc: 0.8522\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4132 - acc: 0.8205 - val_loss: 0.3744 - val_acc: 0.8555\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4120 - acc: 0.8208 - val_loss: 0.3744 - val_acc: 0.8555\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4102 - acc: 0.8219 - val_loss: 0.3761 - val_acc: 0.8588\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4117 - acc: 0.8179 - val_loss: 0.3742 - val_acc: 0.8506\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4082 - acc: 0.8203 - val_loss: 0.3734 - val_acc: 0.8571\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4070 - acc: 0.8234 - val_loss: 0.3729 - val_acc: 0.8604\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4029 - acc: 0.8210 - val_loss: 0.3734 - val_acc: 0.8506\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4057 - acc: 0.8243 - val_loss: 0.3718 - val_acc: 0.8588\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4045 - acc: 0.8239 - val_loss: 0.3711 - val_acc: 0.8522\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3998 - acc: 0.8252 - val_loss: 0.3704 - val_acc: 0.8571\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4010 - acc: 0.8294 - val_loss: 0.3720 - val_acc: 0.8588\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4016 - acc: 0.8227 - val_loss: 0.3710 - val_acc: 0.8489\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4010 - acc: 0.8285 - val_loss: 0.3702 - val_acc: 0.8621\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3968 - acc: 0.8274 - val_loss: 0.3716 - val_acc: 0.8539\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3978 - acc: 0.8241 - val_loss: 0.3695 - val_acc: 0.8539\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3949 - acc: 0.8289 - val_loss: 0.3696 - val_acc: 0.8571\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3965 - acc: 0.8276 - val_loss: 0.3695 - val_acc: 0.8571\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3917 - acc: 0.8305 - val_loss: 0.3689 - val_acc: 0.8555\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3941 - acc: 0.8281 - val_loss: 0.3709 - val_acc: 0.8571\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3931 - acc: 0.8325 - val_loss: 0.3683 - val_acc: 0.8522\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3896 - acc: 0.8323 - val_loss: 0.3680 - val_acc: 0.8555\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3914 - acc: 0.8300 - val_loss: 0.3679 - val_acc: 0.8539\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.3911 - acc: 0.8314 - val_loss: 0.3682 - val_acc: 0.8522\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3873 - acc: 0.8290 - val_loss: 0.3678 - val_acc: 0.8522\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3871 - acc: 0.8327 - val_loss: 0.3674 - val_acc: 0.8539\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3852 - acc: 0.8327 - val_loss: 0.3718 - val_acc: 0.8473\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3864 - acc: 0.8327 - val_loss: 0.3728 - val_acc: 0.8571\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3875 - acc: 0.8320 - val_loss: 0.3668 - val_acc: 0.8555\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3848 - acc: 0.8300 - val_loss: 0.3660 - val_acc: 0.8506\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3815 - acc: 0.8338 - val_loss: 0.3657 - val_acc: 0.8489\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3798 - acc: 0.8365 - val_loss: 0.3661 - val_acc: 0.8506\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3774 - acc: 0.8380 - val_loss: 0.3660 - val_acc: 0.8506\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3795 - acc: 0.8349 - val_loss: 0.3660 - val_acc: 0.8489\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3790 - acc: 0.8362 - val_loss: 0.3655 - val_acc: 0.8489\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3771 - acc: 0.8362 - val_loss: 0.3680 - val_acc: 0.8489\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3751 - acc: 0.8345 - val_loss: 0.3663 - val_acc: 0.8473\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3753 - acc: 0.8369 - val_loss: 0.3659 - val_acc: 0.8506\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3701 - acc: 0.8405 - val_loss: 0.3649 - val_acc: 0.8506\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3701 - acc: 0.8389 - val_loss: 0.3652 - val_acc: 0.8539\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3716 - acc: 0.8360 - val_loss: 0.3652 - val_acc: 0.8506\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3677 - acc: 0.8435 - val_loss: 0.3660 - val_acc: 0.8473\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3704 - acc: 0.8400 - val_loss: 0.3660 - val_acc: 0.8473\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3645 - acc: 0.8446 - val_loss: 0.3665 - val_acc: 0.8473\n",
      "Training with parameters {'batch_size': 4000, 'dropout': 0.1, 'lay_conf': [1024, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_156\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_157 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_468 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_312 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_469 (Dense)            (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_313 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_470 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,606,721\n",
      "Trainable params: 1,606,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 1s 408ms/step - loss: 0.8779 - acc: 0.4563 - val_loss: 0.7594 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.7634 - acc: 0.5656 - val_loss: 0.6689 - val_acc: 0.5731\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.6907 - acc: 0.5672 - val_loss: 0.6680 - val_acc: 0.5895\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.6930 - acc: 0.5579 - val_loss: 0.6364 - val_acc: 0.6749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.6663 - acc: 0.5926 - val_loss: 0.6151 - val_acc: 0.6355\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.6477 - acc: 0.6201 - val_loss: 0.6081 - val_acc: 0.6305\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.6387 - acc: 0.6324 - val_loss: 0.5801 - val_acc: 0.7406\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.6199 - acc: 0.6687 - val_loss: 0.5768 - val_acc: 0.7110\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.6159 - acc: 0.6585 - val_loss: 0.5558 - val_acc: 0.7586\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.6005 - acc: 0.6895 - val_loss: 0.5515 - val_acc: 0.7521\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.5946 - acc: 0.7035 - val_loss: 0.5357 - val_acc: 0.7849\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.5784 - acc: 0.7117 - val_loss: 0.5243 - val_acc: 0.7701\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.5764 - acc: 0.7002 - val_loss: 0.5130 - val_acc: 0.7849\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.5698 - acc: 0.7216 - val_loss: 0.5069 - val_acc: 0.7964\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.5592 - acc: 0.7247 - val_loss: 0.4968 - val_acc: 0.7980\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.5530 - acc: 0.7272 - val_loss: 0.4883 - val_acc: 0.8079\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.5421 - acc: 0.7369 - val_loss: 0.4818 - val_acc: 0.8062\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5341 - acc: 0.7464 - val_loss: 0.4768 - val_acc: 0.8112\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5257 - acc: 0.7528 - val_loss: 0.4676 - val_acc: 0.8095\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5214 - acc: 0.7564 - val_loss: 0.4611 - val_acc: 0.8112\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5189 - acc: 0.7577 - val_loss: 0.4560 - val_acc: 0.8210\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5171 - acc: 0.7597 - val_loss: 0.4482 - val_acc: 0.8128\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5071 - acc: 0.7604 - val_loss: 0.4430 - val_acc: 0.8144\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5030 - acc: 0.7694 - val_loss: 0.4403 - val_acc: 0.8210\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5017 - acc: 0.7688 - val_loss: 0.4352 - val_acc: 0.8177\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4946 - acc: 0.7699 - val_loss: 0.4307 - val_acc: 0.8227\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4963 - acc: 0.7714 - val_loss: 0.4312 - val_acc: 0.8243\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4891 - acc: 0.7754 - val_loss: 0.4248 - val_acc: 0.8259\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4848 - acc: 0.7807 - val_loss: 0.4206 - val_acc: 0.8309\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4803 - acc: 0.7772 - val_loss: 0.4190 - val_acc: 0.8292\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4762 - acc: 0.7860 - val_loss: 0.4145 - val_acc: 0.8227\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4765 - acc: 0.7840 - val_loss: 0.4129 - val_acc: 0.8374\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4727 - acc: 0.7843 - val_loss: 0.4107 - val_acc: 0.8358\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4680 - acc: 0.7922 - val_loss: 0.4072 - val_acc: 0.8276\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.4655 - acc: 0.7902 - val_loss: 0.4053 - val_acc: 0.8391\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.4664 - acc: 0.7873 - val_loss: 0.4033 - val_acc: 0.8374\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.4648 - acc: 0.7873 - val_loss: 0.4021 - val_acc: 0.8391\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.4589 - acc: 0.7946 - val_loss: 0.4027 - val_acc: 0.8407\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.4523 - acc: 0.8017 - val_loss: 0.4001 - val_acc: 0.8374\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.4498 - acc: 0.7940 - val_loss: 0.3980 - val_acc: 0.8391\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.4515 - acc: 0.7980 - val_loss: 0.3949 - val_acc: 0.8407\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.4508 - acc: 0.7951 - val_loss: 0.3933 - val_acc: 0.8424\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.4475 - acc: 0.7978 - val_loss: 0.3935 - val_acc: 0.8407\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.4493 - acc: 0.8061 - val_loss: 0.3903 - val_acc: 0.8407\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.4458 - acc: 0.8015 - val_loss: 0.3887 - val_acc: 0.8440\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.4394 - acc: 0.8079 - val_loss: 0.3875 - val_acc: 0.8407\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.4404 - acc: 0.8072 - val_loss: 0.3867 - val_acc: 0.8440\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.4399 - acc: 0.8046 - val_loss: 0.3865 - val_acc: 0.8440\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.4385 - acc: 0.8070 - val_loss: 0.3852 - val_acc: 0.8489\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.4372 - acc: 0.8050 - val_loss: 0.3852 - val_acc: 0.8424\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4377 - acc: 0.8070 - val_loss: 0.3838 - val_acc: 0.8456\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4339 - acc: 0.8082 - val_loss: 0.3849 - val_acc: 0.8456\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4336 - acc: 0.8046 - val_loss: 0.3869 - val_acc: 0.8424\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4337 - acc: 0.8082 - val_loss: 0.3847 - val_acc: 0.8440\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4333 - acc: 0.8062 - val_loss: 0.3827 - val_acc: 0.8456\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4308 - acc: 0.8070 - val_loss: 0.3823 - val_acc: 0.8489\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4255 - acc: 0.8132 - val_loss: 0.3801 - val_acc: 0.8456\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4295 - acc: 0.8092 - val_loss: 0.3787 - val_acc: 0.8506\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4215 - acc: 0.8121 - val_loss: 0.3769 - val_acc: 0.8473\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4229 - acc: 0.8144 - val_loss: 0.3773 - val_acc: 0.8506\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4228 - acc: 0.8183 - val_loss: 0.3762 - val_acc: 0.8506\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4214 - acc: 0.8143 - val_loss: 0.3789 - val_acc: 0.8473\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4167 - acc: 0.8141 - val_loss: 0.3759 - val_acc: 0.8539\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4196 - acc: 0.8150 - val_loss: 0.3747 - val_acc: 0.8506\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4149 - acc: 0.8197 - val_loss: 0.3754 - val_acc: 0.8489\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4176 - acc: 0.8157 - val_loss: 0.3780 - val_acc: 0.8522\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4165 - acc: 0.8161 - val_loss: 0.3754 - val_acc: 0.8506\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4125 - acc: 0.8179 - val_loss: 0.3733 - val_acc: 0.8522\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4143 - acc: 0.8186 - val_loss: 0.3724 - val_acc: 0.8506\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4145 - acc: 0.8155 - val_loss: 0.3742 - val_acc: 0.8539\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4091 - acc: 0.8259 - val_loss: 0.3752 - val_acc: 0.8506\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4105 - acc: 0.8223 - val_loss: 0.3728 - val_acc: 0.8522\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4092 - acc: 0.8208 - val_loss: 0.3705 - val_acc: 0.8555\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4072 - acc: 0.8243 - val_loss: 0.3706 - val_acc: 0.8506\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4045 - acc: 0.8232 - val_loss: 0.3706 - val_acc: 0.8539\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4037 - acc: 0.8247 - val_loss: 0.3716 - val_acc: 0.8473\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4048 - acc: 0.8236 - val_loss: 0.3720 - val_acc: 0.8539\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4078 - acc: 0.8219 - val_loss: 0.3700 - val_acc: 0.8555\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4011 - acc: 0.8234 - val_loss: 0.3701 - val_acc: 0.8522\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4002 - acc: 0.8254 - val_loss: 0.3709 - val_acc: 0.8539\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4027 - acc: 0.8254 - val_loss: 0.3702 - val_acc: 0.8489\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4024 - acc: 0.8254 - val_loss: 0.3696 - val_acc: 0.8555\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3973 - acc: 0.8276 - val_loss: 0.3690 - val_acc: 0.8555\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3975 - acc: 0.8274 - val_loss: 0.3685 - val_acc: 0.8555\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3976 - acc: 0.8294 - val_loss: 0.3687 - val_acc: 0.8571\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3959 - acc: 0.8296 - val_loss: 0.3692 - val_acc: 0.8506\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3939 - acc: 0.8307 - val_loss: 0.3691 - val_acc: 0.8588\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3927 - acc: 0.8300 - val_loss: 0.3680 - val_acc: 0.8555\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3948 - acc: 0.8318 - val_loss: 0.3672 - val_acc: 0.8571\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3916 - acc: 0.8281 - val_loss: 0.3690 - val_acc: 0.8571\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3926 - acc: 0.8283 - val_loss: 0.3679 - val_acc: 0.8489\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3954 - acc: 0.8276 - val_loss: 0.3682 - val_acc: 0.8571\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3907 - acc: 0.8252 - val_loss: 0.3674 - val_acc: 0.8604\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3897 - acc: 0.8329 - val_loss: 0.3697 - val_acc: 0.8506\n",
      "Training with parameters {'batch_size': 4000, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_157\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_158 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_471 (Dense)            (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_314 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_472 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_315 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_473 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,050,113\n",
      "Trainable params: 1,050,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.7420 - acc: 0.4808 - val_loss: 0.7145 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7211 - acc: 0.5672 - val_loss: 0.6601 - val_acc: 0.5731\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6742 - acc: 0.5873 - val_loss: 0.6591 - val_acc: 0.6371\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6688 - acc: 0.5937 - val_loss: 0.6395 - val_acc: 0.6634\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6508 - acc: 0.6293 - val_loss: 0.6056 - val_acc: 0.7241\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.6283 - acc: 0.6446 - val_loss: 0.6035 - val_acc: 0.6338\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.6321 - acc: 0.6338 - val_loss: 0.5804 - val_acc: 0.7471\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.6100 - acc: 0.6805 - val_loss: 0.5790 - val_acc: 0.6913\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.6067 - acc: 0.6731 - val_loss: 0.5659 - val_acc: 0.7011\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.5932 - acc: 0.6915 - val_loss: 0.5484 - val_acc: 0.7865\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.5817 - acc: 0.7088 - val_loss: 0.5403 - val_acc: 0.7783\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5779 - acc: 0.7147 - val_loss: 0.5307 - val_acc: 0.7685\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5651 - acc: 0.7271 - val_loss: 0.5264 - val_acc: 0.7553\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.5613 - acc: 0.7201 - val_loss: 0.5114 - val_acc: 0.7915\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5497 - acc: 0.7484 - val_loss: 0.5043 - val_acc: 0.7997\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.5451 - acc: 0.7451 - val_loss: 0.4973 - val_acc: 0.8013\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.5359 - acc: 0.7486 - val_loss: 0.4893 - val_acc: 0.8013\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5294 - acc: 0.7469 - val_loss: 0.4810 - val_acc: 0.8112\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.5240 - acc: 0.7599 - val_loss: 0.4753 - val_acc: 0.8095\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.5149 - acc: 0.7628 - val_loss: 0.4672 - val_acc: 0.8128\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.5117 - acc: 0.7615 - val_loss: 0.4619 - val_acc: 0.8194\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5055 - acc: 0.7648 - val_loss: 0.4557 - val_acc: 0.8144\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4995 - acc: 0.7696 - val_loss: 0.4493 - val_acc: 0.8227\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.4963 - acc: 0.7692 - val_loss: 0.4446 - val_acc: 0.8259\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4918 - acc: 0.7741 - val_loss: 0.4427 - val_acc: 0.8243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4902 - acc: 0.7696 - val_loss: 0.4353 - val_acc: 0.8276\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4827 - acc: 0.7847 - val_loss: 0.4313 - val_acc: 0.8276\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4768 - acc: 0.7796 - val_loss: 0.4305 - val_acc: 0.8243\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4739 - acc: 0.7833 - val_loss: 0.4251 - val_acc: 0.8276\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4690 - acc: 0.7902 - val_loss: 0.4225 - val_acc: 0.8309\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4692 - acc: 0.7865 - val_loss: 0.4196 - val_acc: 0.8292\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4630 - acc: 0.7920 - val_loss: 0.4167 - val_acc: 0.8325\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4610 - acc: 0.7916 - val_loss: 0.4134 - val_acc: 0.8292\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4575 - acc: 0.7957 - val_loss: 0.4108 - val_acc: 0.8342\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4540 - acc: 0.7969 - val_loss: 0.4092 - val_acc: 0.8407\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4539 - acc: 0.7980 - val_loss: 0.4063 - val_acc: 0.8342\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4470 - acc: 0.7986 - val_loss: 0.4044 - val_acc: 0.8391\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4449 - acc: 0.8002 - val_loss: 0.4036 - val_acc: 0.8440\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4450 - acc: 0.8046 - val_loss: 0.4018 - val_acc: 0.8358\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4417 - acc: 0.8008 - val_loss: 0.4004 - val_acc: 0.8424\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4401 - acc: 0.8011 - val_loss: 0.3992 - val_acc: 0.8424\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4382 - acc: 0.8075 - val_loss: 0.3985 - val_acc: 0.8391\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4362 - acc: 0.8073 - val_loss: 0.3984 - val_acc: 0.8456\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4362 - acc: 0.8061 - val_loss: 0.3960 - val_acc: 0.8407\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4291 - acc: 0.8146 - val_loss: 0.3967 - val_acc: 0.8440\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4329 - acc: 0.8086 - val_loss: 0.3933 - val_acc: 0.8473\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4251 - acc: 0.8152 - val_loss: 0.3946 - val_acc: 0.8391\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4308 - acc: 0.8101 - val_loss: 0.3929 - val_acc: 0.8473\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4286 - acc: 0.8097 - val_loss: 0.3922 - val_acc: 0.8407\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4247 - acc: 0.8148 - val_loss: 0.3916 - val_acc: 0.8506\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4225 - acc: 0.8115 - val_loss: 0.3891 - val_acc: 0.8506\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4231 - acc: 0.8159 - val_loss: 0.3883 - val_acc: 0.8506\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4229 - acc: 0.8126 - val_loss: 0.3880 - val_acc: 0.8506\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4196 - acc: 0.8150 - val_loss: 0.3879 - val_acc: 0.8473\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4197 - acc: 0.8146 - val_loss: 0.3896 - val_acc: 0.8555\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4185 - acc: 0.8144 - val_loss: 0.3881 - val_acc: 0.8424\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4148 - acc: 0.8227 - val_loss: 0.3876 - val_acc: 0.8571\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4143 - acc: 0.8166 - val_loss: 0.3848 - val_acc: 0.8539\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4097 - acc: 0.8245 - val_loss: 0.3842 - val_acc: 0.8539\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4095 - acc: 0.8219 - val_loss: 0.3838 - val_acc: 0.8539\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4078 - acc: 0.8214 - val_loss: 0.3829 - val_acc: 0.8522\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4062 - acc: 0.8225 - val_loss: 0.3823 - val_acc: 0.8539\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4069 - acc: 0.8214 - val_loss: 0.3820 - val_acc: 0.8539\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4031 - acc: 0.8230 - val_loss: 0.3821 - val_acc: 0.8489\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4051 - acc: 0.8236 - val_loss: 0.3817 - val_acc: 0.8489\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4022 - acc: 0.8217 - val_loss: 0.3813 - val_acc: 0.8489\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3997 - acc: 0.8258 - val_loss: 0.3808 - val_acc: 0.8506\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3987 - acc: 0.8274 - val_loss: 0.3815 - val_acc: 0.8539\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.3985 - acc: 0.8250 - val_loss: 0.3802 - val_acc: 0.8539\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3980 - acc: 0.8270 - val_loss: 0.3794 - val_acc: 0.8522\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3945 - acc: 0.8276 - val_loss: 0.3791 - val_acc: 0.8522\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3941 - acc: 0.8276 - val_loss: 0.3791 - val_acc: 0.8539\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3880 - acc: 0.8318 - val_loss: 0.3785 - val_acc: 0.8522\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3890 - acc: 0.8318 - val_loss: 0.3782 - val_acc: 0.8489\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.3876 - acc: 0.8296 - val_loss: 0.3780 - val_acc: 0.8489\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.3856 - acc: 0.8294 - val_loss: 0.3777 - val_acc: 0.8489\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.3852 - acc: 0.8281 - val_loss: 0.3776 - val_acc: 0.8539\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3859 - acc: 0.8309 - val_loss: 0.3796 - val_acc: 0.8456\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.3864 - acc: 0.8289 - val_loss: 0.3876 - val_acc: 0.8539\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.3876 - acc: 0.8298 - val_loss: 0.3787 - val_acc: 0.8473\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3812 - acc: 0.8360 - val_loss: 0.3776 - val_acc: 0.8604\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3796 - acc: 0.8389 - val_loss: 0.3773 - val_acc: 0.8588\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.3813 - acc: 0.8329 - val_loss: 0.3769 - val_acc: 0.8506\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.3781 - acc: 0.8316 - val_loss: 0.3772 - val_acc: 0.8539\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3747 - acc: 0.8373 - val_loss: 0.3778 - val_acc: 0.8473\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3752 - acc: 0.8358 - val_loss: 0.3784 - val_acc: 0.8555\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.3758 - acc: 0.8363 - val_loss: 0.3784 - val_acc: 0.8456\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.3733 - acc: 0.8360 - val_loss: 0.3814 - val_acc: 0.8522\n",
      "Training with parameters {'batch_size': 4000, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_158\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_159 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_474 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_316 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_475 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_317 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_476 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 401,729\n",
      "Trainable params: 401,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 1s 107ms/step - loss: 1.0543 - acc: 0.4322 - val_loss: 0.7654 - val_acc: 0.4269\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7765 - acc: 0.4317 - val_loss: 0.6992 - val_acc: 0.5567\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.7133 - acc: 0.5236 - val_loss: 0.7009 - val_acc: 0.5747\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.7126 - acc: 0.5565 - val_loss: 0.6923 - val_acc: 0.5747\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.7051 - acc: 0.5634 - val_loss: 0.6756 - val_acc: 0.5747\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6926 - acc: 0.5629 - val_loss: 0.6625 - val_acc: 0.5764\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6760 - acc: 0.5784 - val_loss: 0.6566 - val_acc: 0.6190\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6710 - acc: 0.5891 - val_loss: 0.6542 - val_acc: 0.6814\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6708 - acc: 0.5913 - val_loss: 0.6496 - val_acc: 0.6929\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6671 - acc: 0.5990 - val_loss: 0.6417 - val_acc: 0.6995\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6600 - acc: 0.6187 - val_loss: 0.6335 - val_acc: 0.6962\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6528 - acc: 0.6251 - val_loss: 0.6275 - val_acc: 0.6634\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6493 - acc: 0.6163 - val_loss: 0.6220 - val_acc: 0.6700\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6503 - acc: 0.6221 - val_loss: 0.6155 - val_acc: 0.6929\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6443 - acc: 0.6320 - val_loss: 0.6092 - val_acc: 0.7241\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6404 - acc: 0.6373 - val_loss: 0.6047 - val_acc: 0.7225\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6335 - acc: 0.6497 - val_loss: 0.5996 - val_acc: 0.7323\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6269 - acc: 0.6667 - val_loss: 0.5934 - val_acc: 0.7258\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6240 - acc: 0.6628 - val_loss: 0.5876 - val_acc: 0.7323\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6210 - acc: 0.6756 - val_loss: 0.5825 - val_acc: 0.7340\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6157 - acc: 0.6705 - val_loss: 0.5771 - val_acc: 0.7340\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6178 - acc: 0.6729 - val_loss: 0.5719 - val_acc: 0.7356\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6110 - acc: 0.6814 - val_loss: 0.5670 - val_acc: 0.7471\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6075 - acc: 0.6845 - val_loss: 0.5624 - val_acc: 0.7455\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6014 - acc: 0.6935 - val_loss: 0.5566 - val_acc: 0.7521\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5974 - acc: 0.6909 - val_loss: 0.5506 - val_acc: 0.7521\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5936 - acc: 0.6926 - val_loss: 0.5452 - val_acc: 0.7521\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5898 - acc: 0.7008 - val_loss: 0.5401 - val_acc: 0.7504\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5830 - acc: 0.7103 - val_loss: 0.5356 - val_acc: 0.7603\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5816 - acc: 0.7108 - val_loss: 0.5310 - val_acc: 0.7750\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5763 - acc: 0.7201 - val_loss: 0.5268 - val_acc: 0.7783\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5696 - acc: 0.7205 - val_loss: 0.5226 - val_acc: 0.7865\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5673 - acc: 0.7236 - val_loss: 0.5186 - val_acc: 0.7849\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5633 - acc: 0.7251 - val_loss: 0.5145 - val_acc: 0.7898\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5595 - acc: 0.7292 - val_loss: 0.5104 - val_acc: 0.7997\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5550 - acc: 0.7342 - val_loss: 0.5064 - val_acc: 0.8013\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5507 - acc: 0.7318 - val_loss: 0.5034 - val_acc: 0.7964\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5461 - acc: 0.7422 - val_loss: 0.4991 - val_acc: 0.7964\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5424 - acc: 0.7411 - val_loss: 0.4937 - val_acc: 0.8062\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5437 - acc: 0.7477 - val_loss: 0.4893 - val_acc: 0.8095\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5359 - acc: 0.7493 - val_loss: 0.4853 - val_acc: 0.8030\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5335 - acc: 0.7488 - val_loss: 0.4828 - val_acc: 0.7997\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5305 - acc: 0.7570 - val_loss: 0.4780 - val_acc: 0.8046\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5228 - acc: 0.7586 - val_loss: 0.4744 - val_acc: 0.8079\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5236 - acc: 0.7522 - val_loss: 0.4713 - val_acc: 0.8062\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5251 - acc: 0.7510 - val_loss: 0.4681 - val_acc: 0.8062\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5165 - acc: 0.7595 - val_loss: 0.4653 - val_acc: 0.8095\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5113 - acc: 0.7681 - val_loss: 0.4626 - val_acc: 0.8079\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5126 - acc: 0.7632 - val_loss: 0.4595 - val_acc: 0.8079\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5097 - acc: 0.7634 - val_loss: 0.4569 - val_acc: 0.8161\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5066 - acc: 0.7663 - val_loss: 0.4540 - val_acc: 0.8194\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5043 - acc: 0.7692 - val_loss: 0.4505 - val_acc: 0.8177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5014 - acc: 0.7679 - val_loss: 0.4470 - val_acc: 0.8194\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4972 - acc: 0.7707 - val_loss: 0.4448 - val_acc: 0.8194\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4936 - acc: 0.7725 - val_loss: 0.4420 - val_acc: 0.8243\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4941 - acc: 0.7712 - val_loss: 0.4403 - val_acc: 0.8177\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4931 - acc: 0.7668 - val_loss: 0.4380 - val_acc: 0.8276\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4901 - acc: 0.7712 - val_loss: 0.4368 - val_acc: 0.8243\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4853 - acc: 0.7834 - val_loss: 0.4350 - val_acc: 0.8309\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4853 - acc: 0.7822 - val_loss: 0.4337 - val_acc: 0.8309\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4836 - acc: 0.7823 - val_loss: 0.4326 - val_acc: 0.8194\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4766 - acc: 0.7814 - val_loss: 0.4304 - val_acc: 0.8292\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4749 - acc: 0.7840 - val_loss: 0.4285 - val_acc: 0.8391\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4744 - acc: 0.7862 - val_loss: 0.4269 - val_acc: 0.8358\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4726 - acc: 0.7935 - val_loss: 0.4256 - val_acc: 0.8325\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4743 - acc: 0.7831 - val_loss: 0.4238 - val_acc: 0.8424\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4703 - acc: 0.7929 - val_loss: 0.4226 - val_acc: 0.8358\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4704 - acc: 0.7937 - val_loss: 0.4222 - val_acc: 0.8292\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4701 - acc: 0.7887 - val_loss: 0.4199 - val_acc: 0.8391\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4645 - acc: 0.7878 - val_loss: 0.4191 - val_acc: 0.8374\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4666 - acc: 0.7913 - val_loss: 0.4172 - val_acc: 0.8374\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4643 - acc: 0.7926 - val_loss: 0.4161 - val_acc: 0.8374\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4640 - acc: 0.7911 - val_loss: 0.4149 - val_acc: 0.8358\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4632 - acc: 0.7999 - val_loss: 0.4140 - val_acc: 0.8358\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4568 - acc: 0.7973 - val_loss: 0.4133 - val_acc: 0.8407\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4558 - acc: 0.8022 - val_loss: 0.4120 - val_acc: 0.8391\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4557 - acc: 0.7982 - val_loss: 0.4107 - val_acc: 0.8358\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4549 - acc: 0.8020 - val_loss: 0.4102 - val_acc: 0.8391\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4578 - acc: 0.7980 - val_loss: 0.4091 - val_acc: 0.8358\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4520 - acc: 0.7988 - val_loss: 0.4084 - val_acc: 0.8358\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4549 - acc: 0.7980 - val_loss: 0.4076 - val_acc: 0.8358\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4483 - acc: 0.8033 - val_loss: 0.4069 - val_acc: 0.8342\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4533 - acc: 0.7986 - val_loss: 0.4062 - val_acc: 0.8342\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4475 - acc: 0.8006 - val_loss: 0.4060 - val_acc: 0.8407\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4477 - acc: 0.8044 - val_loss: 0.4049 - val_acc: 0.8374\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4447 - acc: 0.8051 - val_loss: 0.4053 - val_acc: 0.8358\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4499 - acc: 0.8015 - val_loss: 0.4036 - val_acc: 0.8374\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4438 - acc: 0.8033 - val_loss: 0.4029 - val_acc: 0.8358\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4446 - acc: 0.8020 - val_loss: 0.4017 - val_acc: 0.8374\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4411 - acc: 0.8035 - val_loss: 0.4011 - val_acc: 0.8391\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4415 - acc: 0.8044 - val_loss: 0.4005 - val_acc: 0.8358\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4386 - acc: 0.8110 - val_loss: 0.3993 - val_acc: 0.8374\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4376 - acc: 0.8051 - val_loss: 0.3986 - val_acc: 0.8374\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4392 - acc: 0.8075 - val_loss: 0.4015 - val_acc: 0.8391\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4370 - acc: 0.8086 - val_loss: 0.3979 - val_acc: 0.8391\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4390 - acc: 0.8112 - val_loss: 0.3976 - val_acc: 0.8374\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4365 - acc: 0.8108 - val_loss: 0.3964 - val_acc: 0.8358\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4287 - acc: 0.8106 - val_loss: 0.3950 - val_acc: 0.8407\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4341 - acc: 0.8108 - val_loss: 0.3942 - val_acc: 0.8407\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4331 - acc: 0.8146 - val_loss: 0.3943 - val_acc: 0.8358\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4335 - acc: 0.8110 - val_loss: 0.3939 - val_acc: 0.8407\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4338 - acc: 0.8168 - val_loss: 0.3935 - val_acc: 0.8391\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4316 - acc: 0.8106 - val_loss: 0.3944 - val_acc: 0.8358\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4257 - acc: 0.8134 - val_loss: 0.3935 - val_acc: 0.8407\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4268 - acc: 0.8108 - val_loss: 0.3932 - val_acc: 0.8424\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4268 - acc: 0.8157 - val_loss: 0.3929 - val_acc: 0.8358\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4293 - acc: 0.8113 - val_loss: 0.3926 - val_acc: 0.8358\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4233 - acc: 0.8168 - val_loss: 0.3919 - val_acc: 0.8424\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4286 - acc: 0.8134 - val_loss: 0.3914 - val_acc: 0.8358\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4238 - acc: 0.8123 - val_loss: 0.3914 - val_acc: 0.8407\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4229 - acc: 0.8177 - val_loss: 0.3907 - val_acc: 0.8440\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4242 - acc: 0.8150 - val_loss: 0.3907 - val_acc: 0.8391\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4233 - acc: 0.8148 - val_loss: 0.3902 - val_acc: 0.8391\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4211 - acc: 0.8172 - val_loss: 0.3904 - val_acc: 0.8424\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4233 - acc: 0.8205 - val_loss: 0.3903 - val_acc: 0.8374\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4207 - acc: 0.8174 - val_loss: 0.3892 - val_acc: 0.8440\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4192 - acc: 0.8179 - val_loss: 0.3893 - val_acc: 0.8489\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4210 - acc: 0.8166 - val_loss: 0.3901 - val_acc: 0.8342\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4193 - acc: 0.8216 - val_loss: 0.3876 - val_acc: 0.8424\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4198 - acc: 0.8194 - val_loss: 0.3883 - val_acc: 0.8456\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4160 - acc: 0.8232 - val_loss: 0.3889 - val_acc: 0.8358\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4165 - acc: 0.8205 - val_loss: 0.3874 - val_acc: 0.8407\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4125 - acc: 0.8197 - val_loss: 0.3875 - val_acc: 0.8424\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4166 - acc: 0.8194 - val_loss: 0.3871 - val_acc: 0.8407\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4155 - acc: 0.8177 - val_loss: 0.3867 - val_acc: 0.8407\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4111 - acc: 0.8230 - val_loss: 0.3862 - val_acc: 0.8424\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4091 - acc: 0.8232 - val_loss: 0.3855 - val_acc: 0.8424\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4127 - acc: 0.8221 - val_loss: 0.3855 - val_acc: 0.8407\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4094 - acc: 0.8199 - val_loss: 0.3855 - val_acc: 0.8473\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4147 - acc: 0.8186 - val_loss: 0.3845 - val_acc: 0.8407\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4123 - acc: 0.8259 - val_loss: 0.3845 - val_acc: 0.8424\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4125 - acc: 0.8219 - val_loss: 0.3854 - val_acc: 0.8473\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4089 - acc: 0.8201 - val_loss: 0.3849 - val_acc: 0.8407\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4081 - acc: 0.8269 - val_loss: 0.3847 - val_acc: 0.8391\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091 - acc: 0.8212 - val_loss: 0.3842 - val_acc: 0.8473\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4070 - acc: 0.8263 - val_loss: 0.3843 - val_acc: 0.8407\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4044 - acc: 0.8238 - val_loss: 0.3835 - val_acc: 0.8440\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4082 - acc: 0.8219 - val_loss: 0.3832 - val_acc: 0.8424\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4062 - acc: 0.8265 - val_loss: 0.3830 - val_acc: 0.8424\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4065 - acc: 0.8272 - val_loss: 0.3862 - val_acc: 0.8391\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4042 - acc: 0.8256 - val_loss: 0.3834 - val_acc: 0.8489\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4045 - acc: 0.8232 - val_loss: 0.3839 - val_acc: 0.8522\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4051 - acc: 0.8274 - val_loss: 0.3838 - val_acc: 0.8358\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4035 - acc: 0.8252 - val_loss: 0.3828 - val_acc: 0.8489\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4002 - acc: 0.8296 - val_loss: 0.3822 - val_acc: 0.8440\n",
      "Epoch 146/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4046 - acc: 0.8301 - val_loss: 0.3824 - val_acc: 0.8424\n",
      "Epoch 147/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3998 - acc: 0.8320 - val_loss: 0.3814 - val_acc: 0.8407\n",
      "Epoch 148/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4031 - acc: 0.8270 - val_loss: 0.3813 - val_acc: 0.8424\n",
      "Epoch 149/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3999 - acc: 0.8258 - val_loss: 0.3814 - val_acc: 0.8424\n",
      "Epoch 150/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3981 - acc: 0.8290 - val_loss: 0.3812 - val_acc: 0.8407\n",
      "Epoch 151/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.3937 - acc: 0.8336 - val_loss: 0.3807 - val_acc: 0.8424\n",
      "Epoch 152/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.3972 - acc: 0.8309 - val_loss: 0.3804 - val_acc: 0.8424\n",
      "Epoch 153/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.3948 - acc: 0.8305 - val_loss: 0.3802 - val_acc: 0.8440\n",
      "Epoch 154/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3945 - acc: 0.8305 - val_loss: 0.3803 - val_acc: 0.8506\n",
      "Epoch 155/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3945 - acc: 0.8320 - val_loss: 0.3790 - val_acc: 0.8424\n",
      "Epoch 156/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.3936 - acc: 0.8331 - val_loss: 0.3789 - val_acc: 0.8391\n",
      "Epoch 157/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3940 - acc: 0.8301 - val_loss: 0.3789 - val_acc: 0.8456\n",
      "Epoch 158/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3948 - acc: 0.8300 - val_loss: 0.3788 - val_acc: 0.8424\n",
      "Epoch 159/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3921 - acc: 0.8347 - val_loss: 0.3801 - val_acc: 0.8424\n",
      "Epoch 160/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3901 - acc: 0.8336 - val_loss: 0.3797 - val_acc: 0.8506\n",
      "Epoch 161/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3925 - acc: 0.8331 - val_loss: 0.3785 - val_acc: 0.8456\n",
      "Epoch 162/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.3903 - acc: 0.8331 - val_loss: 0.3791 - val_acc: 0.8440\n",
      "Epoch 163/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3886 - acc: 0.8342 - val_loss: 0.3795 - val_acc: 0.8473\n",
      "Epoch 164/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.3900 - acc: 0.8318 - val_loss: 0.3782 - val_acc: 0.8424\n",
      "Epoch 165/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3934 - acc: 0.8300 - val_loss: 0.3773 - val_acc: 0.8440\n",
      "Epoch 166/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.3875 - acc: 0.8354 - val_loss: 0.3781 - val_acc: 0.8489\n",
      "Epoch 167/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3866 - acc: 0.8331 - val_loss: 0.3789 - val_acc: 0.8456\n",
      "Epoch 168/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.3897 - acc: 0.8329 - val_loss: 0.3763 - val_acc: 0.8456\n",
      "Epoch 169/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3863 - acc: 0.8332 - val_loss: 0.3766 - val_acc: 0.8473\n",
      "Epoch 170/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.3867 - acc: 0.8334 - val_loss: 0.3772 - val_acc: 0.8456\n",
      "Epoch 171/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3860 - acc: 0.8325 - val_loss: 0.3768 - val_acc: 0.8456\n",
      "Epoch 172/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3864 - acc: 0.8356 - val_loss: 0.3775 - val_acc: 0.8440\n",
      "Epoch 173/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3849 - acc: 0.8371 - val_loss: 0.3776 - val_acc: 0.8424\n",
      "Training with parameters {'batch_size': 4000, 'dropout': 0.1, 'lay_conf': [1024, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_159\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_160 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_477 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_318 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_478 (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_319 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_479 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,836,545\n",
      "Trainable params: 1,836,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.9992 - acc: 0.4486 - val_loss: 0.7452 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.7650 - acc: 0.5676 - val_loss: 0.7728 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7646 - acc: 0.5691 - val_loss: 0.6476 - val_acc: 0.5764\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6595 - acc: 0.6041 - val_loss: 0.6580 - val_acc: 0.6388\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6753 - acc: 0.5833 - val_loss: 0.6657 - val_acc: 0.6207\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6783 - acc: 0.5831 - val_loss: 0.6159 - val_acc: 0.6732\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6393 - acc: 0.6450 - val_loss: 0.6009 - val_acc: 0.7028\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6320 - acc: 0.6377 - val_loss: 0.6060 - val_acc: 0.6158\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6337 - acc: 0.6284 - val_loss: 0.5849 - val_acc: 0.7110\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6121 - acc: 0.6727 - val_loss: 0.5733 - val_acc: 0.7323\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6077 - acc: 0.6820 - val_loss: 0.5746 - val_acc: 0.7143\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.6004 - acc: 0.6772 - val_loss: 0.5600 - val_acc: 0.7455\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5921 - acc: 0.6909 - val_loss: 0.5475 - val_acc: 0.7783\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5838 - acc: 0.7119 - val_loss: 0.5424 - val_acc: 0.7816\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5803 - acc: 0.7179 - val_loss: 0.5310 - val_acc: 0.7816\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5715 - acc: 0.7218 - val_loss: 0.5268 - val_acc: 0.7718\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5604 - acc: 0.7199 - val_loss: 0.5174 - val_acc: 0.7849\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5562 - acc: 0.7298 - val_loss: 0.5084 - val_acc: 0.8013\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5453 - acc: 0.7497 - val_loss: 0.5016 - val_acc: 0.8112\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5430 - acc: 0.7453 - val_loss: 0.4961 - val_acc: 0.7964\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5374 - acc: 0.7458 - val_loss: 0.4902 - val_acc: 0.7980\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5299 - acc: 0.7455 - val_loss: 0.4817 - val_acc: 0.8128\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5240 - acc: 0.7610 - val_loss: 0.4760 - val_acc: 0.8112\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5183 - acc: 0.7652 - val_loss: 0.4726 - val_acc: 0.8095\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5146 - acc: 0.7625 - val_loss: 0.4645 - val_acc: 0.8194\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5106 - acc: 0.7615 - val_loss: 0.4592 - val_acc: 0.8161\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5067 - acc: 0.7683 - val_loss: 0.4564 - val_acc: 0.8161\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5010 - acc: 0.7668 - val_loss: 0.4496 - val_acc: 0.8194\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4964 - acc: 0.7712 - val_loss: 0.4453 - val_acc: 0.8210\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4909 - acc: 0.7716 - val_loss: 0.4411 - val_acc: 0.8210\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4883 - acc: 0.7796 - val_loss: 0.4368 - val_acc: 0.8177\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4817 - acc: 0.7809 - val_loss: 0.4333 - val_acc: 0.8177\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4791 - acc: 0.7778 - val_loss: 0.4287 - val_acc: 0.8210\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4742 - acc: 0.7825 - val_loss: 0.4248 - val_acc: 0.8194\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4715 - acc: 0.7871 - val_loss: 0.4238 - val_acc: 0.8259\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4700 - acc: 0.7843 - val_loss: 0.4197 - val_acc: 0.8292\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4637 - acc: 0.7853 - val_loss: 0.4163 - val_acc: 0.8259\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4602 - acc: 0.7915 - val_loss: 0.4136 - val_acc: 0.8276\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4597 - acc: 0.7907 - val_loss: 0.4114 - val_acc: 0.8292\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4563 - acc: 0.7980 - val_loss: 0.4108 - val_acc: 0.8259\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4516 - acc: 0.7962 - val_loss: 0.4068 - val_acc: 0.8342\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4529 - acc: 0.7986 - val_loss: 0.4049 - val_acc: 0.8358\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4447 - acc: 0.7982 - val_loss: 0.4027 - val_acc: 0.8342\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4440 - acc: 0.8070 - val_loss: 0.4033 - val_acc: 0.8325\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4435 - acc: 0.8053 - val_loss: 0.3991 - val_acc: 0.8325\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4392 - acc: 0.8075 - val_loss: 0.3981 - val_acc: 0.8358\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4377 - acc: 0.8081 - val_loss: 0.3955 - val_acc: 0.8374\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4364 - acc: 0.8090 - val_loss: 0.3942 - val_acc: 0.8358\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4319 - acc: 0.8110 - val_loss: 0.3961 - val_acc: 0.8407\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4319 - acc: 0.8070 - val_loss: 0.3924 - val_acc: 0.8391\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4301 - acc: 0.8106 - val_loss: 0.3921 - val_acc: 0.8407\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4326 - acc: 0.8072 - val_loss: 0.3904 - val_acc: 0.8407\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4303 - acc: 0.8104 - val_loss: 0.3897 - val_acc: 0.8407\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4291 - acc: 0.8059 - val_loss: 0.3882 - val_acc: 0.8424\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4247 - acc: 0.8108 - val_loss: 0.3866 - val_acc: 0.8456\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4211 - acc: 0.8132 - val_loss: 0.3863 - val_acc: 0.8407\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4191 - acc: 0.8135 - val_loss: 0.3843 - val_acc: 0.8473\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4185 - acc: 0.8199 - val_loss: 0.3843 - val_acc: 0.8440\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4166 - acc: 0.8137 - val_loss: 0.3852 - val_acc: 0.8456\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4156 - acc: 0.8163 - val_loss: 0.3883 - val_acc: 0.8473\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4180 - acc: 0.8143 - val_loss: 0.3845 - val_acc: 0.8456\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4176 - acc: 0.8179 - val_loss: 0.3821 - val_acc: 0.8440\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4112 - acc: 0.8186 - val_loss: 0.3812 - val_acc: 0.8489\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4084 - acc: 0.8234 - val_loss: 0.3814 - val_acc: 0.8506\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4076 - acc: 0.8261 - val_loss: 0.3831 - val_acc: 0.8506\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4091 - acc: 0.8165 - val_loss: 0.3805 - val_acc: 0.8506\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4027 - acc: 0.8221 - val_loss: 0.3793 - val_acc: 0.8489\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4044 - acc: 0.8208 - val_loss: 0.3779 - val_acc: 0.8522\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4046 - acc: 0.8192 - val_loss: 0.3805 - val_acc: 0.8473\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4036 - acc: 0.8239 - val_loss: 0.3778 - val_acc: 0.8489\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4045 - acc: 0.8245 - val_loss: 0.3785 - val_acc: 0.8456\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.3956 - acc: 0.8234 - val_loss: 0.3769 - val_acc: 0.8473\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.3979 - acc: 0.8241 - val_loss: 0.3774 - val_acc: 0.8456\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.3962 - acc: 0.8223 - val_loss: 0.3758 - val_acc: 0.8489\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.3961 - acc: 0.8307 - val_loss: 0.3772 - val_acc: 0.8489\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3953 - acc: 0.8239 - val_loss: 0.3753 - val_acc: 0.8424\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3895 - acc: 0.8314 - val_loss: 0.3760 - val_acc: 0.8522\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.3899 - acc: 0.8314 - val_loss: 0.3775 - val_acc: 0.8456\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.3895 - acc: 0.8320 - val_loss: 0.3750 - val_acc: 0.8456\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3845 - acc: 0.8323 - val_loss: 0.3749 - val_acc: 0.8489\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.3851 - acc: 0.8336 - val_loss: 0.3744 - val_acc: 0.8489\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3870 - acc: 0.8311 - val_loss: 0.3747 - val_acc: 0.8456\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3822 - acc: 0.8303 - val_loss: 0.3750 - val_acc: 0.8522\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.3841 - acc: 0.8340 - val_loss: 0.3756 - val_acc: 0.8473\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.3797 - acc: 0.8363 - val_loss: 0.3779 - val_acc: 0.8489\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3836 - acc: 0.8354 - val_loss: 0.3781 - val_acc: 0.8440\n",
      "Training with parameters {'batch_size': 5000, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_160\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_161 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_480 (Dense)            (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_320 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_481 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_321 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_482 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.8599 - acc: 0.4326 - val_loss: 0.7169 - val_acc: 0.4433\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.7313 - acc: 0.4634 - val_loss: 0.6808 - val_acc: 0.5632\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6940 - acc: 0.5359 - val_loss: 0.6803 - val_acc: 0.5731\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6914 - acc: 0.5650 - val_loss: 0.6786 - val_acc: 0.5747\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6905 - acc: 0.5700 - val_loss: 0.6684 - val_acc: 0.5747\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6827 - acc: 0.5691 - val_loss: 0.6574 - val_acc: 0.5747\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6694 - acc: 0.5849 - val_loss: 0.6500 - val_acc: 0.6043\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6663 - acc: 0.5933 - val_loss: 0.6465 - val_acc: 0.6700\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6632 - acc: 0.6128 - val_loss: 0.6433 - val_acc: 0.6929\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6578 - acc: 0.6267 - val_loss: 0.6367 - val_acc: 0.6929\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6569 - acc: 0.6198 - val_loss: 0.6294 - val_acc: 0.7077\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6478 - acc: 0.6335 - val_loss: 0.6234 - val_acc: 0.6716\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6459 - acc: 0.6291 - val_loss: 0.6166 - val_acc: 0.6847\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6408 - acc: 0.6388 - val_loss: 0.6090 - val_acc: 0.7192\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6379 - acc: 0.6420 - val_loss: 0.6025 - val_acc: 0.7110\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6298 - acc: 0.6612 - val_loss: 0.5974 - val_acc: 0.7126\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6288 - acc: 0.6572 - val_loss: 0.5913 - val_acc: 0.7159\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6229 - acc: 0.6689 - val_loss: 0.5842 - val_acc: 0.7209\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6205 - acc: 0.6637 - val_loss: 0.5788 - val_acc: 0.7323\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6152 - acc: 0.6836 - val_loss: 0.5730 - val_acc: 0.7274\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6108 - acc: 0.6853 - val_loss: 0.5682 - val_acc: 0.7340\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6046 - acc: 0.6918 - val_loss: 0.5637 - val_acc: 0.7340\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6026 - acc: 0.6878 - val_loss: 0.5571 - val_acc: 0.7373\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5964 - acc: 0.6959 - val_loss: 0.5509 - val_acc: 0.7603\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5933 - acc: 0.7022 - val_loss: 0.5459 - val_acc: 0.7619\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5875 - acc: 0.7043 - val_loss: 0.5406 - val_acc: 0.7619\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5855 - acc: 0.7097 - val_loss: 0.5361 - val_acc: 0.7652\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5755 - acc: 0.7243 - val_loss: 0.5312 - val_acc: 0.7685\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5755 - acc: 0.7148 - val_loss: 0.5259 - val_acc: 0.7800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5729 - acc: 0.7178 - val_loss: 0.5211 - val_acc: 0.7833\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.5674 - acc: 0.7240 - val_loss: 0.5170 - val_acc: 0.7898\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5653 - acc: 0.7282 - val_loss: 0.5130 - val_acc: 0.7898\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5571 - acc: 0.7384 - val_loss: 0.5091 - val_acc: 0.7947\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.5586 - acc: 0.7331 - val_loss: 0.5058 - val_acc: 0.7947\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5563 - acc: 0.7422 - val_loss: 0.5016 - val_acc: 0.7931\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5472 - acc: 0.7479 - val_loss: 0.4997 - val_acc: 0.7915\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5500 - acc: 0.7360 - val_loss: 0.4940 - val_acc: 0.7947\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5436 - acc: 0.7438 - val_loss: 0.4906 - val_acc: 0.8046\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5399 - acc: 0.7544 - val_loss: 0.4865 - val_acc: 0.8030\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5380 - acc: 0.7524 - val_loss: 0.4849 - val_acc: 0.8013\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5323 - acc: 0.7533 - val_loss: 0.4827 - val_acc: 0.8030\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5333 - acc: 0.7521 - val_loss: 0.4763 - val_acc: 0.8030\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5257 - acc: 0.7646 - val_loss: 0.4731 - val_acc: 0.8046\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5251 - acc: 0.7557 - val_loss: 0.4710 - val_acc: 0.8030\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5242 - acc: 0.7572 - val_loss: 0.4677 - val_acc: 0.8046\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5192 - acc: 0.7628 - val_loss: 0.4650 - val_acc: 0.8079\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5116 - acc: 0.7716 - val_loss: 0.4622 - val_acc: 0.8062\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5147 - acc: 0.7672 - val_loss: 0.4600 - val_acc: 0.8095\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5124 - acc: 0.7698 - val_loss: 0.4573 - val_acc: 0.8079\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5086 - acc: 0.7729 - val_loss: 0.4554 - val_acc: 0.8128\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5097 - acc: 0.7699 - val_loss: 0.4529 - val_acc: 0.8128\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5035 - acc: 0.7725 - val_loss: 0.4504 - val_acc: 0.8161\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5023 - acc: 0.7752 - val_loss: 0.4484 - val_acc: 0.8112\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5004 - acc: 0.7719 - val_loss: 0.4465 - val_acc: 0.8128\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4964 - acc: 0.7734 - val_loss: 0.4438 - val_acc: 0.8243\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4949 - acc: 0.7772 - val_loss: 0.4423 - val_acc: 0.8227\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4924 - acc: 0.7752 - val_loss: 0.4389 - val_acc: 0.8210\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4904 - acc: 0.7816 - val_loss: 0.4381 - val_acc: 0.8144\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4918 - acc: 0.7730 - val_loss: 0.4356 - val_acc: 0.8144\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4865 - acc: 0.7827 - val_loss: 0.4373 - val_acc: 0.8227\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4895 - acc: 0.7796 - val_loss: 0.4348 - val_acc: 0.8259\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4915 - acc: 0.7791 - val_loss: 0.4308 - val_acc: 0.8177\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4857 - acc: 0.7811 - val_loss: 0.4296 - val_acc: 0.8177\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4840 - acc: 0.7778 - val_loss: 0.4279 - val_acc: 0.8227\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4810 - acc: 0.7829 - val_loss: 0.4267 - val_acc: 0.8243\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4784 - acc: 0.7871 - val_loss: 0.4255 - val_acc: 0.8259\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4769 - acc: 0.7873 - val_loss: 0.4250 - val_acc: 0.8276\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4785 - acc: 0.7823 - val_loss: 0.4235 - val_acc: 0.8292\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4754 - acc: 0.7895 - val_loss: 0.4227 - val_acc: 0.8276\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4745 - acc: 0.7927 - val_loss: 0.4218 - val_acc: 0.8292\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4758 - acc: 0.7926 - val_loss: 0.4211 - val_acc: 0.8325\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4728 - acc: 0.7918 - val_loss: 0.4201 - val_acc: 0.8309\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4692 - acc: 0.7893 - val_loss: 0.4190 - val_acc: 0.8309\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4704 - acc: 0.7969 - val_loss: 0.4199 - val_acc: 0.8309\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4729 - acc: 0.7898 - val_loss: 0.4172 - val_acc: 0.8325\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4659 - acc: 0.7916 - val_loss: 0.4174 - val_acc: 0.8309\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4665 - acc: 0.7942 - val_loss: 0.4144 - val_acc: 0.8325\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4640 - acc: 0.7953 - val_loss: 0.4144 - val_acc: 0.8325\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4623 - acc: 0.7966 - val_loss: 0.4118 - val_acc: 0.8325\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4603 - acc: 0.7986 - val_loss: 0.4137 - val_acc: 0.8325\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4633 - acc: 0.7915 - val_loss: 0.4095 - val_acc: 0.8325\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4626 - acc: 0.7964 - val_loss: 0.4119 - val_acc: 0.8325\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4624 - acc: 0.7947 - val_loss: 0.4077 - val_acc: 0.8276\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4580 - acc: 0.7978 - val_loss: 0.4114 - val_acc: 0.8309\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4604 - acc: 0.7960 - val_loss: 0.4068 - val_acc: 0.8391\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4570 - acc: 0.8008 - val_loss: 0.4094 - val_acc: 0.8358\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4603 - acc: 0.8015 - val_loss: 0.4068 - val_acc: 0.8391\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4539 - acc: 0.8024 - val_loss: 0.4052 - val_acc: 0.8407\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4564 - acc: 0.7973 - val_loss: 0.4050 - val_acc: 0.8407\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4539 - acc: 0.7977 - val_loss: 0.4046 - val_acc: 0.8374\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4488 - acc: 0.8057 - val_loss: 0.4056 - val_acc: 0.8374\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4545 - acc: 0.7997 - val_loss: 0.4041 - val_acc: 0.8424\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4519 - acc: 0.8026 - val_loss: 0.4035 - val_acc: 0.8391\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4497 - acc: 0.7989 - val_loss: 0.4026 - val_acc: 0.8424\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4497 - acc: 0.7988 - val_loss: 0.4020 - val_acc: 0.8391\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4501 - acc: 0.8048 - val_loss: 0.4018 - val_acc: 0.8358\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4475 - acc: 0.8048 - val_loss: 0.3995 - val_acc: 0.8391\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4446 - acc: 0.8024 - val_loss: 0.3987 - val_acc: 0.8424\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4427 - acc: 0.8093 - val_loss: 0.3984 - val_acc: 0.8407\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4460 - acc: 0.8035 - val_loss: 0.3972 - val_acc: 0.8424\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4437 - acc: 0.8082 - val_loss: 0.3967 - val_acc: 0.8407\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4431 - acc: 0.8048 - val_loss: 0.3957 - val_acc: 0.8391\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4417 - acc: 0.8062 - val_loss: 0.3951 - val_acc: 0.8391\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4391 - acc: 0.8095 - val_loss: 0.3954 - val_acc: 0.8440\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4400 - acc: 0.8064 - val_loss: 0.3945 - val_acc: 0.8424\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4396 - acc: 0.8066 - val_loss: 0.3960 - val_acc: 0.8391\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4370 - acc: 0.8092 - val_loss: 0.3941 - val_acc: 0.8440\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4376 - acc: 0.8051 - val_loss: 0.3959 - val_acc: 0.8391\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4404 - acc: 0.8064 - val_loss: 0.3947 - val_acc: 0.8407\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4364 - acc: 0.8108 - val_loss: 0.3939 - val_acc: 0.8407\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4362 - acc: 0.8042 - val_loss: 0.3935 - val_acc: 0.8456\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4340 - acc: 0.8106 - val_loss: 0.3931 - val_acc: 0.8407\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4323 - acc: 0.8053 - val_loss: 0.3984 - val_acc: 0.8358\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4393 - acc: 0.8072 - val_loss: 0.3907 - val_acc: 0.8440\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4339 - acc: 0.8099 - val_loss: 0.3940 - val_acc: 0.8440\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4364 - acc: 0.8117 - val_loss: 0.3903 - val_acc: 0.8473\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4296 - acc: 0.8143 - val_loss: 0.3943 - val_acc: 0.8374\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4344 - acc: 0.8110 - val_loss: 0.3895 - val_acc: 0.8473\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4314 - acc: 0.8117 - val_loss: 0.3922 - val_acc: 0.8440\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4303 - acc: 0.8086 - val_loss: 0.3894 - val_acc: 0.8473\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4315 - acc: 0.8124 - val_loss: 0.3892 - val_acc: 0.8473\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4288 - acc: 0.8148 - val_loss: 0.3885 - val_acc: 0.8489\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4276 - acc: 0.8139 - val_loss: 0.3902 - val_acc: 0.8440\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4286 - acc: 0.8141 - val_loss: 0.3903 - val_acc: 0.8456\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4285 - acc: 0.8084 - val_loss: 0.3880 - val_acc: 0.8456\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4292 - acc: 0.8104 - val_loss: 0.3915 - val_acc: 0.8391\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4317 - acc: 0.8141 - val_loss: 0.3872 - val_acc: 0.8489\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4265 - acc: 0.8176 - val_loss: 0.3915 - val_acc: 0.8407\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4269 - acc: 0.8099 - val_loss: 0.3876 - val_acc: 0.8473\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4247 - acc: 0.8139 - val_loss: 0.3906 - val_acc: 0.8391\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4254 - acc: 0.8152 - val_loss: 0.3876 - val_acc: 0.8473\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4225 - acc: 0.8143 - val_loss: 0.3911 - val_acc: 0.8440\n",
      "Training with parameters {'batch_size': 5000, 'dropout': 0.1, 'lay_conf': [1024, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_161\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_162 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_483 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_322 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_484 (Dense)            (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_323 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_485 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.9990 - acc: 0.4395 - val_loss: 0.7736 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.7811 - acc: 0.5661 - val_loss: 0.7699 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7825 - acc: 0.5689 - val_loss: 0.6635 - val_acc: 0.5764\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6787 - acc: 0.5698 - val_loss: 0.6861 - val_acc: 0.5419\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6985 - acc: 0.5452 - val_loss: 0.6818 - val_acc: 0.5550\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6864 - acc: 0.5712 - val_loss: 0.6264 - val_acc: 0.6913\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6493 - acc: 0.6238 - val_loss: 0.6241 - val_acc: 0.5895\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6485 - acc: 0.6057 - val_loss: 0.6274 - val_acc: 0.5780\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6517 - acc: 0.6048 - val_loss: 0.5946 - val_acc: 0.7011\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6228 - acc: 0.6550 - val_loss: 0.5875 - val_acc: 0.6979\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.6178 - acc: 0.6663 - val_loss: 0.5869 - val_acc: 0.6913\n",
      "Epoch 12/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6123 - acc: 0.6672 - val_loss: 0.5682 - val_acc: 0.7291\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5996 - acc: 0.6858 - val_loss: 0.5590 - val_acc: 0.7619\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5953 - acc: 0.6986 - val_loss: 0.5545 - val_acc: 0.7553\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5925 - acc: 0.7019 - val_loss: 0.5427 - val_acc: 0.7619\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5797 - acc: 0.7128 - val_loss: 0.5395 - val_acc: 0.7488\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5737 - acc: 0.7077 - val_loss: 0.5326 - val_acc: 0.7570\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5704 - acc: 0.7097 - val_loss: 0.5213 - val_acc: 0.7931\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5610 - acc: 0.7316 - val_loss: 0.5152 - val_acc: 0.7915\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5570 - acc: 0.7371 - val_loss: 0.5082 - val_acc: 0.7997\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5509 - acc: 0.7387 - val_loss: 0.5061 - val_acc: 0.7931\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5474 - acc: 0.7325 - val_loss: 0.4967 - val_acc: 0.8046\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5392 - acc: 0.7479 - val_loss: 0.4922 - val_acc: 0.8013\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5325 - acc: 0.7524 - val_loss: 0.4880 - val_acc: 0.8128\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5283 - acc: 0.7528 - val_loss: 0.4852 - val_acc: 0.8144\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5267 - acc: 0.7479 - val_loss: 0.4763 - val_acc: 0.8062\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5197 - acc: 0.7610 - val_loss: 0.4700 - val_acc: 0.8112\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5180 - acc: 0.7557 - val_loss: 0.4676 - val_acc: 0.8177\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5101 - acc: 0.7634 - val_loss: 0.4603 - val_acc: 0.8227\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5036 - acc: 0.7659 - val_loss: 0.4550 - val_acc: 0.8177\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5002 - acc: 0.7685 - val_loss: 0.4547 - val_acc: 0.8309\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5006 - acc: 0.7634 - val_loss: 0.4473 - val_acc: 0.8259\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4939 - acc: 0.7687 - val_loss: 0.4451 - val_acc: 0.8227\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4933 - acc: 0.7710 - val_loss: 0.4385 - val_acc: 0.8259\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4835 - acc: 0.7829 - val_loss: 0.4349 - val_acc: 0.8276\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4816 - acc: 0.7865 - val_loss: 0.4319 - val_acc: 0.8259\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4760 - acc: 0.7827 - val_loss: 0.4297 - val_acc: 0.8276\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4738 - acc: 0.7885 - val_loss: 0.4267 - val_acc: 0.8292\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4731 - acc: 0.7878 - val_loss: 0.4286 - val_acc: 0.8227\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4713 - acc: 0.7831 - val_loss: 0.4211 - val_acc: 0.8292\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4695 - acc: 0.7873 - val_loss: 0.4212 - val_acc: 0.8325\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4689 - acc: 0.7865 - val_loss: 0.4262 - val_acc: 0.8259\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4717 - acc: 0.7805 - val_loss: 0.4173 - val_acc: 0.8309\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4645 - acc: 0.7902 - val_loss: 0.4152 - val_acc: 0.8325\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4641 - acc: 0.7898 - val_loss: 0.4153 - val_acc: 0.8259\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4629 - acc: 0.7905 - val_loss: 0.4086 - val_acc: 0.8325\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4596 - acc: 0.7907 - val_loss: 0.4065 - val_acc: 0.8325\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4548 - acc: 0.7962 - val_loss: 0.4051 - val_acc: 0.8325\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4517 - acc: 0.7989 - val_loss: 0.4043 - val_acc: 0.8358\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4471 - acc: 0.7995 - val_loss: 0.4026 - val_acc: 0.8342\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4473 - acc: 0.7999 - val_loss: 0.4020 - val_acc: 0.8374\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4432 - acc: 0.8017 - val_loss: 0.4098 - val_acc: 0.8276\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4517 - acc: 0.7920 - val_loss: 0.3980 - val_acc: 0.8374\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4457 - acc: 0.8039 - val_loss: 0.4110 - val_acc: 0.8391\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4511 - acc: 0.7991 - val_loss: 0.4029 - val_acc: 0.8325\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4459 - acc: 0.7989 - val_loss: 0.3979 - val_acc: 0.8391\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4396 - acc: 0.8035 - val_loss: 0.4024 - val_acc: 0.8440\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4474 - acc: 0.7988 - val_loss: 0.3937 - val_acc: 0.8374\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4366 - acc: 0.8013 - val_loss: 0.3947 - val_acc: 0.8407\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4366 - acc: 0.8046 - val_loss: 0.3964 - val_acc: 0.8374\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4401 - acc: 0.8053 - val_loss: 0.3968 - val_acc: 0.8407\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4355 - acc: 0.8066 - val_loss: 0.3967 - val_acc: 0.8391\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4325 - acc: 0.8062 - val_loss: 0.4128 - val_acc: 0.8309\n",
      "Training with parameters {'batch_size': 5000, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_162\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_163 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_486 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_324 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_487 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_325 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_488 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 409,985\n",
      "Trainable params: 409,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 1.8702 - acc: 0.4320 - val_loss: 1.1885 - val_acc: 0.4253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.1957 - acc: 0.4339 - val_loss: 0.7786 - val_acc: 0.4269\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.8050 - acc: 0.4512 - val_loss: 0.7062 - val_acc: 0.5714\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.7255 - acc: 0.5284 - val_loss: 0.7558 - val_acc: 0.5731\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.7695 - acc: 0.5570 - val_loss: 0.7743 - val_acc: 0.5747\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.7790 - acc: 0.5667 - val_loss: 0.7519 - val_acc: 0.5747\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.7564 - acc: 0.5643 - val_loss: 0.7142 - val_acc: 0.5747\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7276 - acc: 0.5650 - val_loss: 0.6802 - val_acc: 0.5747\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6887 - acc: 0.5725 - val_loss: 0.6616 - val_acc: 0.5862\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6791 - acc: 0.5716 - val_loss: 0.6580 - val_acc: 0.6519\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6793 - acc: 0.5702 - val_loss: 0.6591 - val_acc: 0.6847\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6811 - acc: 0.5605 - val_loss: 0.6555 - val_acc: 0.6864\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6772 - acc: 0.5760 - val_loss: 0.6469 - val_acc: 0.7044\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6663 - acc: 0.5962 - val_loss: 0.6370 - val_acc: 0.7192\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6567 - acc: 0.6121 - val_loss: 0.6301 - val_acc: 0.6568\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6529 - acc: 0.6212 - val_loss: 0.6273 - val_acc: 0.6076\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6499 - acc: 0.6116 - val_loss: 0.6245 - val_acc: 0.6076\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6496 - acc: 0.6110 - val_loss: 0.6174 - val_acc: 0.6388\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6431 - acc: 0.6296 - val_loss: 0.6097 - val_acc: 0.7061\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6383 - acc: 0.6464 - val_loss: 0.6052 - val_acc: 0.7356\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6331 - acc: 0.6572 - val_loss: 0.6027 - val_acc: 0.7291\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6327 - acc: 0.6442 - val_loss: 0.5973 - val_acc: 0.7274\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6250 - acc: 0.6572 - val_loss: 0.5900 - val_acc: 0.7422\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6226 - acc: 0.6665 - val_loss: 0.5834 - val_acc: 0.7603\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6199 - acc: 0.6716 - val_loss: 0.5785 - val_acc: 0.7521\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6153 - acc: 0.6760 - val_loss: 0.5738 - val_acc: 0.7586\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6117 - acc: 0.6776 - val_loss: 0.5687 - val_acc: 0.7619\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6103 - acc: 0.6845 - val_loss: 0.5641 - val_acc: 0.7718\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6058 - acc: 0.6911 - val_loss: 0.5604 - val_acc: 0.7635\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5965 - acc: 0.7008 - val_loss: 0.5565 - val_acc: 0.7668\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5946 - acc: 0.6984 - val_loss: 0.5523 - val_acc: 0.7783\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5940 - acc: 0.6991 - val_loss: 0.5486 - val_acc: 0.7783\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5904 - acc: 0.7019 - val_loss: 0.5452 - val_acc: 0.7816\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5834 - acc: 0.7115 - val_loss: 0.5419 - val_acc: 0.7882\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5866 - acc: 0.7092 - val_loss: 0.5392 - val_acc: 0.7882\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5831 - acc: 0.7074 - val_loss: 0.5349 - val_acc: 0.7964\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5803 - acc: 0.7117 - val_loss: 0.5313 - val_acc: 0.7980\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5728 - acc: 0.7188 - val_loss: 0.5279 - val_acc: 0.8030\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5695 - acc: 0.7249 - val_loss: 0.5253 - val_acc: 0.7997\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5722 - acc: 0.7176 - val_loss: 0.5214 - val_acc: 0.8062\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5651 - acc: 0.7269 - val_loss: 0.5182 - val_acc: 0.7980\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5644 - acc: 0.7289 - val_loss: 0.5149 - val_acc: 0.7980\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5615 - acc: 0.7318 - val_loss: 0.5117 - val_acc: 0.7997\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5547 - acc: 0.7345 - val_loss: 0.5086 - val_acc: 0.8046\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5564 - acc: 0.7314 - val_loss: 0.5055 - val_acc: 0.8095\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5500 - acc: 0.7358 - val_loss: 0.5020 - val_acc: 0.8062\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5475 - acc: 0.7325 - val_loss: 0.4988 - val_acc: 0.8030\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5462 - acc: 0.7418 - val_loss: 0.4956 - val_acc: 0.8062\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5440 - acc: 0.7398 - val_loss: 0.4928 - val_acc: 0.8062\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5413 - acc: 0.7431 - val_loss: 0.4902 - val_acc: 0.8079\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5348 - acc: 0.7510 - val_loss: 0.4877 - val_acc: 0.8112\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5368 - acc: 0.7508 - val_loss: 0.4857 - val_acc: 0.8128\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5327 - acc: 0.7477 - val_loss: 0.4861 - val_acc: 0.8161\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.5311 - acc: 0.7499 - val_loss: 0.4820 - val_acc: 0.8144\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5281 - acc: 0.7510 - val_loss: 0.4770 - val_acc: 0.8161\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5287 - acc: 0.7528 - val_loss: 0.4752 - val_acc: 0.8161\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5299 - acc: 0.7471 - val_loss: 0.4720 - val_acc: 0.8144\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5222 - acc: 0.7581 - val_loss: 0.4707 - val_acc: 0.8161\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5215 - acc: 0.7612 - val_loss: 0.4675 - val_acc: 0.8177\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5161 - acc: 0.7625 - val_loss: 0.4655 - val_acc: 0.8210\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5169 - acc: 0.7588 - val_loss: 0.4634 - val_acc: 0.8161\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5119 - acc: 0.7601 - val_loss: 0.4630 - val_acc: 0.8144\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5127 - acc: 0.7632 - val_loss: 0.4614 - val_acc: 0.8144\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5156 - acc: 0.7630 - val_loss: 0.4596 - val_acc: 0.8144\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5108 - acc: 0.7679 - val_loss: 0.4570 - val_acc: 0.8128\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5043 - acc: 0.7739 - val_loss: 0.4549 - val_acc: 0.8243\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5051 - acc: 0.7643 - val_loss: 0.4531 - val_acc: 0.8259\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5079 - acc: 0.7698 - val_loss: 0.4519 - val_acc: 0.8144\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4980 - acc: 0.7763 - val_loss: 0.4518 - val_acc: 0.8128\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5010 - acc: 0.7734 - val_loss: 0.4489 - val_acc: 0.8177\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4978 - acc: 0.7756 - val_loss: 0.4470 - val_acc: 0.8177\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4972 - acc: 0.7694 - val_loss: 0.4466 - val_acc: 0.8210\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4990 - acc: 0.7730 - val_loss: 0.4435 - val_acc: 0.8276\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4910 - acc: 0.7743 - val_loss: 0.4425 - val_acc: 0.8227\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4900 - acc: 0.7772 - val_loss: 0.4411 - val_acc: 0.8342\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4924 - acc: 0.7729 - val_loss: 0.4397 - val_acc: 0.8342\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4888 - acc: 0.7796 - val_loss: 0.4387 - val_acc: 0.8259\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4894 - acc: 0.7812 - val_loss: 0.4390 - val_acc: 0.8227\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4844 - acc: 0.7820 - val_loss: 0.4362 - val_acc: 0.8259\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4817 - acc: 0.7800 - val_loss: 0.4355 - val_acc: 0.8292\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4830 - acc: 0.7778 - val_loss: 0.4340 - val_acc: 0.8374\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4795 - acc: 0.7871 - val_loss: 0.4348 - val_acc: 0.8276\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4847 - acc: 0.7862 - val_loss: 0.4349 - val_acc: 0.8194\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4811 - acc: 0.7862 - val_loss: 0.4307 - val_acc: 0.8358\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4786 - acc: 0.7858 - val_loss: 0.4292 - val_acc: 0.8456\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4786 - acc: 0.7882 - val_loss: 0.4275 - val_acc: 0.8374\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4772 - acc: 0.7860 - val_loss: 0.4285 - val_acc: 0.8309\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4725 - acc: 0.7864 - val_loss: 0.4274 - val_acc: 0.8325\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4782 - acc: 0.7880 - val_loss: 0.4242 - val_acc: 0.8407\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4748 - acc: 0.7838 - val_loss: 0.4239 - val_acc: 0.8424\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4744 - acc: 0.7889 - val_loss: 0.4224 - val_acc: 0.8391\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4720 - acc: 0.7885 - val_loss: 0.4220 - val_acc: 0.8391\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4693 - acc: 0.7909 - val_loss: 0.4207 - val_acc: 0.8391\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4650 - acc: 0.7964 - val_loss: 0.4200 - val_acc: 0.8424\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4642 - acc: 0.7951 - val_loss: 0.4194 - val_acc: 0.8391\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4664 - acc: 0.7951 - val_loss: 0.4184 - val_acc: 0.8391\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4638 - acc: 0.7933 - val_loss: 0.4172 - val_acc: 0.8424\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4642 - acc: 0.7940 - val_loss: 0.4160 - val_acc: 0.8424\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4588 - acc: 0.7973 - val_loss: 0.4160 - val_acc: 0.8440\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4637 - acc: 0.7953 - val_loss: 0.4144 - val_acc: 0.8440\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4596 - acc: 0.7929 - val_loss: 0.4146 - val_acc: 0.8424\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4602 - acc: 0.7942 - val_loss: 0.4129 - val_acc: 0.8407\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4626 - acc: 0.7935 - val_loss: 0.4122 - val_acc: 0.8440\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4521 - acc: 0.7991 - val_loss: 0.4141 - val_acc: 0.8407\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4622 - acc: 0.7924 - val_loss: 0.4114 - val_acc: 0.8440\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4557 - acc: 0.7989 - val_loss: 0.4150 - val_acc: 0.8374\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4601 - acc: 0.7942 - val_loss: 0.4116 - val_acc: 0.8456\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4527 - acc: 0.7971 - val_loss: 0.4153 - val_acc: 0.8407\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4634 - acc: 0.7851 - val_loss: 0.4103 - val_acc: 0.8456\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4517 - acc: 0.7997 - val_loss: 0.4154 - val_acc: 0.8309\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4575 - acc: 0.7904 - val_loss: 0.4103 - val_acc: 0.8424\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4534 - acc: 0.7975 - val_loss: 0.4097 - val_acc: 0.8440\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4518 - acc: 0.7929 - val_loss: 0.4110 - val_acc: 0.8424\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4533 - acc: 0.7975 - val_loss: 0.4060 - val_acc: 0.8440\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4508 - acc: 0.8017 - val_loss: 0.4080 - val_acc: 0.8424\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4493 - acc: 0.8068 - val_loss: 0.4051 - val_acc: 0.8424\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4520 - acc: 0.7991 - val_loss: 0.4051 - val_acc: 0.8473\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4476 - acc: 0.8046 - val_loss: 0.4048 - val_acc: 0.8473\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4454 - acc: 0.8051 - val_loss: 0.4044 - val_acc: 0.8473\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4467 - acc: 0.8044 - val_loss: 0.4037 - val_acc: 0.8473\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4441 - acc: 0.8024 - val_loss: 0.4034 - val_acc: 0.8456\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4414 - acc: 0.8070 - val_loss: 0.4034 - val_acc: 0.8456\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4454 - acc: 0.8082 - val_loss: 0.4032 - val_acc: 0.8440\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4423 - acc: 0.8068 - val_loss: 0.4028 - val_acc: 0.8440\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4444 - acc: 0.8092 - val_loss: 0.4031 - val_acc: 0.8440\n",
      "Epoch 126/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4428 - acc: 0.8057 - val_loss: 0.4015 - val_acc: 0.8456\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4417 - acc: 0.8046 - val_loss: 0.4016 - val_acc: 0.8489\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4394 - acc: 0.8053 - val_loss: 0.4006 - val_acc: 0.8440\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4380 - acc: 0.8079 - val_loss: 0.4022 - val_acc: 0.8424\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4429 - acc: 0.8068 - val_loss: 0.3998 - val_acc: 0.8456\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4413 - acc: 0.8035 - val_loss: 0.4017 - val_acc: 0.8456\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4387 - acc: 0.8044 - val_loss: 0.4012 - val_acc: 0.8456\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4371 - acc: 0.8088 - val_loss: 0.3995 - val_acc: 0.8440\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4370 - acc: 0.8062 - val_loss: 0.4031 - val_acc: 0.8424\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4413 - acc: 0.8084 - val_loss: 0.4010 - val_acc: 0.8456\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4377 - acc: 0.8103 - val_loss: 0.4007 - val_acc: 0.8440\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4314 - acc: 0.8086 - val_loss: 0.4008 - val_acc: 0.8424\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4367 - acc: 0.8075 - val_loss: 0.3994 - val_acc: 0.8456\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4353 - acc: 0.8082 - val_loss: 0.4004 - val_acc: 0.8456\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4369 - acc: 0.8070 - val_loss: 0.3987 - val_acc: 0.8489\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4316 - acc: 0.8117 - val_loss: 0.4009 - val_acc: 0.8440\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4371 - acc: 0.8101 - val_loss: 0.3992 - val_acc: 0.8473\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4350 - acc: 0.8092 - val_loss: 0.3993 - val_acc: 0.8424\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4342 - acc: 0.8072 - val_loss: 0.3991 - val_acc: 0.8440\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4326 - acc: 0.8101 - val_loss: 0.3991 - val_acc: 0.8440\n",
      "Training with parameters {'batch_size': 5000, 'dropout': 0.1, 'lay_conf': [1024, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_163\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_164 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_489 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_326 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_490 (Dense)            (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_327 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_491 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,639,553\n",
      "Trainable params: 1,639,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 1s 230ms/step - loss: 0.8421 - acc: 0.4457 - val_loss: 0.7578 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.7618 - acc: 0.5667 - val_loss: 0.6705 - val_acc: 0.5764\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6885 - acc: 0.5714 - val_loss: 0.6567 - val_acc: 0.6486\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6719 - acc: 0.5831 - val_loss: 0.6408 - val_acc: 0.6716\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6632 - acc: 0.6030 - val_loss: 0.6141 - val_acc: 0.6864\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6393 - acc: 0.6307 - val_loss: 0.6059 - val_acc: 0.6355\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6335 - acc: 0.6367 - val_loss: 0.5813 - val_acc: 0.7471\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6091 - acc: 0.6816 - val_loss: 0.5809 - val_acc: 0.7028\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6155 - acc: 0.6614 - val_loss: 0.5624 - val_acc: 0.7323\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5962 - acc: 0.6937 - val_loss: 0.5525 - val_acc: 0.7783\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5903 - acc: 0.7084 - val_loss: 0.5442 - val_acc: 0.7833\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5853 - acc: 0.7161 - val_loss: 0.5330 - val_acc: 0.7521\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5777 - acc: 0.7055 - val_loss: 0.5256 - val_acc: 0.7488\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5674 - acc: 0.7115 - val_loss: 0.5143 - val_acc: 0.7964\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5574 - acc: 0.7393 - val_loss: 0.5062 - val_acc: 0.7964\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5555 - acc: 0.7364 - val_loss: 0.4993 - val_acc: 0.7833\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5474 - acc: 0.7274 - val_loss: 0.4894 - val_acc: 0.7997\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5374 - acc: 0.7422 - val_loss: 0.4890 - val_acc: 0.8112\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5363 - acc: 0.7466 - val_loss: 0.4762 - val_acc: 0.8144\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5266 - acc: 0.7497 - val_loss: 0.4748 - val_acc: 0.8030\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5285 - acc: 0.7433 - val_loss: 0.4660 - val_acc: 0.8210\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5184 - acc: 0.7566 - val_loss: 0.4597 - val_acc: 0.8210\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5136 - acc: 0.7610 - val_loss: 0.4619 - val_acc: 0.8112\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5152 - acc: 0.7577 - val_loss: 0.4535 - val_acc: 0.8243\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5061 - acc: 0.7690 - val_loss: 0.4521 - val_acc: 0.8259\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5068 - acc: 0.7639 - val_loss: 0.4495 - val_acc: 0.8194\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5011 - acc: 0.7645 - val_loss: 0.4452 - val_acc: 0.8210\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4967 - acc: 0.7714 - val_loss: 0.4408 - val_acc: 0.8309\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4939 - acc: 0.7745 - val_loss: 0.4363 - val_acc: 0.8243\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4881 - acc: 0.7805 - val_loss: 0.4341 - val_acc: 0.8227\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4858 - acc: 0.7836 - val_loss: 0.4302 - val_acc: 0.8243\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4823 - acc: 0.7791 - val_loss: 0.4271 - val_acc: 0.8227\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4778 - acc: 0.7869 - val_loss: 0.4244 - val_acc: 0.8276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4765 - acc: 0.7840 - val_loss: 0.4217 - val_acc: 0.8276\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4714 - acc: 0.7860 - val_loss: 0.4183 - val_acc: 0.8325\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4677 - acc: 0.7889 - val_loss: 0.4178 - val_acc: 0.8227\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4707 - acc: 0.7856 - val_loss: 0.4196 - val_acc: 0.8342\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4723 - acc: 0.7854 - val_loss: 0.4119 - val_acc: 0.8358\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4592 - acc: 0.7947 - val_loss: 0.4165 - val_acc: 0.8292\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4663 - acc: 0.7871 - val_loss: 0.4116 - val_acc: 0.8391\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4643 - acc: 0.7893 - val_loss: 0.4088 - val_acc: 0.8325\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4615 - acc: 0.7918 - val_loss: 0.4137 - val_acc: 0.8309\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4638 - acc: 0.7864 - val_loss: 0.4045 - val_acc: 0.8374\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4536 - acc: 0.7953 - val_loss: 0.4077 - val_acc: 0.8358\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4527 - acc: 0.8015 - val_loss: 0.4164 - val_acc: 0.8276\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4618 - acc: 0.7854 - val_loss: 0.4004 - val_acc: 0.8407\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4478 - acc: 0.7999 - val_loss: 0.4086 - val_acc: 0.8391\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4555 - acc: 0.7975 - val_loss: 0.4089 - val_acc: 0.8325\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4542 - acc: 0.7949 - val_loss: 0.3988 - val_acc: 0.8407\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4485 - acc: 0.8000 - val_loss: 0.4073 - val_acc: 0.8358\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4522 - acc: 0.7982 - val_loss: 0.3978 - val_acc: 0.8407\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4434 - acc: 0.8017 - val_loss: 0.4001 - val_acc: 0.8374\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4454 - acc: 0.8013 - val_loss: 0.4032 - val_acc: 0.8374\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4448 - acc: 0.8050 - val_loss: 0.3957 - val_acc: 0.8424\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4384 - acc: 0.8092 - val_loss: 0.3975 - val_acc: 0.8407\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4434 - acc: 0.8008 - val_loss: 0.3909 - val_acc: 0.8407\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4381 - acc: 0.8039 - val_loss: 0.3995 - val_acc: 0.8407\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4411 - acc: 0.8093 - val_loss: 0.3903 - val_acc: 0.8440\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4319 - acc: 0.8062 - val_loss: 0.3921 - val_acc: 0.8473\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4393 - acc: 0.8046 - val_loss: 0.3940 - val_acc: 0.8440\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4375 - acc: 0.8086 - val_loss: 0.3890 - val_acc: 0.8440\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4273 - acc: 0.8130 - val_loss: 0.3908 - val_acc: 0.8506\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4335 - acc: 0.8093 - val_loss: 0.3916 - val_acc: 0.8473\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4337 - acc: 0.8030 - val_loss: 0.3867 - val_acc: 0.8424\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4299 - acc: 0.8101 - val_loss: 0.3870 - val_acc: 0.8522\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4271 - acc: 0.8123 - val_loss: 0.3864 - val_acc: 0.8522\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4298 - acc: 0.8124 - val_loss: 0.3843 - val_acc: 0.8440\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4234 - acc: 0.8124 - val_loss: 0.3845 - val_acc: 0.8489\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4255 - acc: 0.8130 - val_loss: 0.3828 - val_acc: 0.8473\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4236 - acc: 0.8104 - val_loss: 0.3828 - val_acc: 0.8539\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4213 - acc: 0.8172 - val_loss: 0.3809 - val_acc: 0.8473\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4221 - acc: 0.8148 - val_loss: 0.3806 - val_acc: 0.8473\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4226 - acc: 0.8150 - val_loss: 0.3833 - val_acc: 0.8506\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4240 - acc: 0.8108 - val_loss: 0.3802 - val_acc: 0.8522\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4205 - acc: 0.8163 - val_loss: 0.3780 - val_acc: 0.8440\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4182 - acc: 0.8174 - val_loss: 0.3834 - val_acc: 0.8489\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4237 - acc: 0.8157 - val_loss: 0.3840 - val_acc: 0.8473\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4205 - acc: 0.8139 - val_loss: 0.3790 - val_acc: 0.8506\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4181 - acc: 0.8150 - val_loss: 0.3862 - val_acc: 0.8539\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4180 - acc: 0.8172 - val_loss: 0.3819 - val_acc: 0.8489\n",
      "Training with parameters {'batch_size': 5000, 'dropout': 0.1, 'lay_conf': [1024, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_164\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_165 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_492 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_328 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_493 (Dense)            (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dropout_329 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_494 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,606,721\n",
      "Trainable params: 1,606,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.9102 - acc: 0.4415 - val_loss: 0.7444 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.7617 - acc: 0.5636 - val_loss: 0.6681 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6945 - acc: 0.5725 - val_loss: 0.6465 - val_acc: 0.6535\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6774 - acc: 0.5844 - val_loss: 0.6209 - val_acc: 0.6847\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6483 - acc: 0.6331 - val_loss: 0.6092 - val_acc: 0.6453\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6408 - acc: 0.6358 - val_loss: 0.6009 - val_acc: 0.6765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6324 - acc: 0.6499 - val_loss: 0.5872 - val_acc: 0.7061\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6224 - acc: 0.6552 - val_loss: 0.5781 - val_acc: 0.7077\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6169 - acc: 0.6665 - val_loss: 0.5564 - val_acc: 0.7603\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5947 - acc: 0.6960 - val_loss: 0.5451 - val_acc: 0.7668\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5902 - acc: 0.7004 - val_loss: 0.5271 - val_acc: 0.7882\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5791 - acc: 0.7055 - val_loss: 0.5221 - val_acc: 0.7619\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5748 - acc: 0.6984 - val_loss: 0.5040 - val_acc: 0.7997\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5589 - acc: 0.7305 - val_loss: 0.5065 - val_acc: 0.7997\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5606 - acc: 0.7289 - val_loss: 0.4876 - val_acc: 0.8112\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5445 - acc: 0.7364 - val_loss: 0.4891 - val_acc: 0.7898\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5512 - acc: 0.7272 - val_loss: 0.4766 - val_acc: 0.8095\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5343 - acc: 0.7519 - val_loss: 0.4784 - val_acc: 0.8062\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5298 - acc: 0.7475 - val_loss: 0.4697 - val_acc: 0.8112\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5285 - acc: 0.7480 - val_loss: 0.4655 - val_acc: 0.8161\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5230 - acc: 0.7513 - val_loss: 0.4661 - val_acc: 0.8161\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5209 - acc: 0.7573 - val_loss: 0.4634 - val_acc: 0.8144\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5167 - acc: 0.7608 - val_loss: 0.4540 - val_acc: 0.8128\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5135 - acc: 0.7533 - val_loss: 0.4489 - val_acc: 0.8161\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5074 - acc: 0.7630 - val_loss: 0.4453 - val_acc: 0.8227\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5011 - acc: 0.7712 - val_loss: 0.4364 - val_acc: 0.8210\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4994 - acc: 0.7666 - val_loss: 0.4348 - val_acc: 0.8227\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5016 - acc: 0.7619 - val_loss: 0.4317 - val_acc: 0.8243\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4902 - acc: 0.7772 - val_loss: 0.4314 - val_acc: 0.8243\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4889 - acc: 0.7800 - val_loss: 0.4285 - val_acc: 0.8243\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4913 - acc: 0.7712 - val_loss: 0.4259 - val_acc: 0.8259\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4880 - acc: 0.7741 - val_loss: 0.4248 - val_acc: 0.8325\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4817 - acc: 0.7833 - val_loss: 0.4228 - val_acc: 0.8391\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4804 - acc: 0.7798 - val_loss: 0.4197 - val_acc: 0.8276\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4778 - acc: 0.7814 - val_loss: 0.4172 - val_acc: 0.8292\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4736 - acc: 0.7896 - val_loss: 0.4141 - val_acc: 0.8309\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4692 - acc: 0.7905 - val_loss: 0.4118 - val_acc: 0.8342\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4672 - acc: 0.7854 - val_loss: 0.4102 - val_acc: 0.8374\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4679 - acc: 0.7920 - val_loss: 0.4074 - val_acc: 0.8391\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4656 - acc: 0.7958 - val_loss: 0.4051 - val_acc: 0.8374\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4588 - acc: 0.7993 - val_loss: 0.4033 - val_acc: 0.8374\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4591 - acc: 0.7944 - val_loss: 0.4067 - val_acc: 0.8358\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4651 - acc: 0.7916 - val_loss: 0.4063 - val_acc: 0.8358\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4568 - acc: 0.7999 - val_loss: 0.4022 - val_acc: 0.8358\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4570 - acc: 0.7940 - val_loss: 0.3998 - val_acc: 0.8407\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4537 - acc: 0.7982 - val_loss: 0.4020 - val_acc: 0.8374\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4560 - acc: 0.7977 - val_loss: 0.3994 - val_acc: 0.8292\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4601 - acc: 0.7891 - val_loss: 0.3957 - val_acc: 0.8407\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4480 - acc: 0.7957 - val_loss: 0.3960 - val_acc: 0.8424\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4547 - acc: 0.8000 - val_loss: 0.3939 - val_acc: 0.8391\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4485 - acc: 0.7915 - val_loss: 0.3929 - val_acc: 0.8424\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4443 - acc: 0.8006 - val_loss: 0.3955 - val_acc: 0.8440\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4480 - acc: 0.7997 - val_loss: 0.3902 - val_acc: 0.8424\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4401 - acc: 0.8079 - val_loss: 0.3898 - val_acc: 0.8424\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4429 - acc: 0.8028 - val_loss: 0.3877 - val_acc: 0.8440\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4393 - acc: 0.8066 - val_loss: 0.3901 - val_acc: 0.8407\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4447 - acc: 0.7975 - val_loss: 0.3888 - val_acc: 0.8456\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4381 - acc: 0.8055 - val_loss: 0.3863 - val_acc: 0.8440\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4319 - acc: 0.8073 - val_loss: 0.3859 - val_acc: 0.8456\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4350 - acc: 0.8086 - val_loss: 0.3906 - val_acc: 0.8440\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4428 - acc: 0.8015 - val_loss: 0.3835 - val_acc: 0.8456\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4340 - acc: 0.8079 - val_loss: 0.3916 - val_acc: 0.8407\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4388 - acc: 0.8008 - val_loss: 0.3973 - val_acc: 0.8391\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4450 - acc: 0.7986 - val_loss: 0.3822 - val_acc: 0.8440\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4334 - acc: 0.8077 - val_loss: 0.4002 - val_acc: 0.8309\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4477 - acc: 0.7949 - val_loss: 0.3887 - val_acc: 0.8473\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4376 - acc: 0.8061 - val_loss: 0.3842 - val_acc: 0.8506\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4293 - acc: 0.8095 - val_loss: 0.3936 - val_acc: 0.8407\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4401 - acc: 0.7999 - val_loss: 0.3796 - val_acc: 0.8506\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4279 - acc: 0.8124 - val_loss: 0.3934 - val_acc: 0.8456\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4352 - acc: 0.8055 - val_loss: 0.3800 - val_acc: 0.8489\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4249 - acc: 0.8141 - val_loss: 0.3868 - val_acc: 0.8456\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4299 - acc: 0.8092 - val_loss: 0.3867 - val_acc: 0.8506\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4288 - acc: 0.8121 - val_loss: 0.3796 - val_acc: 0.8506\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4201 - acc: 0.8148 - val_loss: 0.3933 - val_acc: 0.8407\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4307 - acc: 0.8048 - val_loss: 0.3802 - val_acc: 0.8506\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4239 - acc: 0.8154 - val_loss: 0.3845 - val_acc: 0.8506\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4209 - acc: 0.8166 - val_loss: 0.3764 - val_acc: 0.8489\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4158 - acc: 0.8170 - val_loss: 0.3746 - val_acc: 0.8506\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4189 - acc: 0.8168 - val_loss: 0.3732 - val_acc: 0.8522\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4169 - acc: 0.8161 - val_loss: 0.3728 - val_acc: 0.8539\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4158 - acc: 0.8119 - val_loss: 0.3734 - val_acc: 0.8522\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4097 - acc: 0.8227 - val_loss: 0.3740 - val_acc: 0.8522\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4138 - acc: 0.8177 - val_loss: 0.3741 - val_acc: 0.8539\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4102 - acc: 0.8190 - val_loss: 0.3735 - val_acc: 0.8539\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4121 - acc: 0.8190 - val_loss: 0.3733 - val_acc: 0.8506\n",
      "Training with parameters {'batch_size': 5000, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_165\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_166 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_495 (Dense)            (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_330 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_496 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_331 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_497 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,050,113\n",
      "Trainable params: 1,050,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 1s 301ms/step - loss: 0.7618 - acc: 0.4472 - val_loss: 0.6992 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.7043 - acc: 0.5660 - val_loss: 0.6571 - val_acc: 0.5813\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6741 - acc: 0.5773 - val_loss: 0.6481 - val_acc: 0.6585\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.6604 - acc: 0.6192 - val_loss: 0.6231 - val_acc: 0.6535\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6385 - acc: 0.6526 - val_loss: 0.6057 - val_acc: 0.6765\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.6301 - acc: 0.6371 - val_loss: 0.5935 - val_acc: 0.7044\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6240 - acc: 0.6554 - val_loss: 0.5782 - val_acc: 0.7225\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.6082 - acc: 0.6811 - val_loss: 0.5724 - val_acc: 0.6962\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5985 - acc: 0.6827 - val_loss: 0.5525 - val_acc: 0.7685\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5915 - acc: 0.7044 - val_loss: 0.5452 - val_acc: 0.7718\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5815 - acc: 0.7178 - val_loss: 0.5378 - val_acc: 0.7685\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5710 - acc: 0.7125 - val_loss: 0.5333 - val_acc: 0.7521\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.5673 - acc: 0.7132 - val_loss: 0.5189 - val_acc: 0.7882\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5564 - acc: 0.7378 - val_loss: 0.5127 - val_acc: 0.7849\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5551 - acc: 0.7356 - val_loss: 0.5075 - val_acc: 0.8013\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.5443 - acc: 0.7400 - val_loss: 0.5014 - val_acc: 0.8046\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5376 - acc: 0.7433 - val_loss: 0.4905 - val_acc: 0.7964\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5310 - acc: 0.7510 - val_loss: 0.4840 - val_acc: 0.8062\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5246 - acc: 0.7552 - val_loss: 0.4776 - val_acc: 0.8046\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5168 - acc: 0.7575 - val_loss: 0.4721 - val_acc: 0.8095\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5123 - acc: 0.7606 - val_loss: 0.4637 - val_acc: 0.8112\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5071 - acc: 0.7714 - val_loss: 0.4594 - val_acc: 0.8144\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5067 - acc: 0.7661 - val_loss: 0.4562 - val_acc: 0.8128\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5003 - acc: 0.7661 - val_loss: 0.4547 - val_acc: 0.8210\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4960 - acc: 0.7699 - val_loss: 0.4456 - val_acc: 0.8177\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4884 - acc: 0.7776 - val_loss: 0.4414 - val_acc: 0.8227\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4868 - acc: 0.7791 - val_loss: 0.4435 - val_acc: 0.8276\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4883 - acc: 0.7758 - val_loss: 0.4442 - val_acc: 0.8128\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4891 - acc: 0.7734 - val_loss: 0.4335 - val_acc: 0.8276\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4760 - acc: 0.7845 - val_loss: 0.4323 - val_acc: 0.8276\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4740 - acc: 0.7842 - val_loss: 0.4316 - val_acc: 0.8292\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4756 - acc: 0.7843 - val_loss: 0.4268 - val_acc: 0.8276\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4702 - acc: 0.7882 - val_loss: 0.4228 - val_acc: 0.8309\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4643 - acc: 0.7905 - val_loss: 0.4199 - val_acc: 0.8309\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4653 - acc: 0.7905 - val_loss: 0.4190 - val_acc: 0.8292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4609 - acc: 0.7927 - val_loss: 0.4161 - val_acc: 0.8292\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4591 - acc: 0.7942 - val_loss: 0.4137 - val_acc: 0.8358\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4551 - acc: 0.7933 - val_loss: 0.4215 - val_acc: 0.8210\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4642 - acc: 0.7913 - val_loss: 0.4138 - val_acc: 0.8325\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4565 - acc: 0.7973 - val_loss: 0.4126 - val_acc: 0.8358\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4498 - acc: 0.7980 - val_loss: 0.4079 - val_acc: 0.8325\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4456 - acc: 0.7988 - val_loss: 0.4068 - val_acc: 0.8325\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4416 - acc: 0.8061 - val_loss: 0.4099 - val_acc: 0.8358\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4467 - acc: 0.8000 - val_loss: 0.4083 - val_acc: 0.8342\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4441 - acc: 0.8053 - val_loss: 0.4058 - val_acc: 0.8391\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4431 - acc: 0.8006 - val_loss: 0.4046 - val_acc: 0.8358\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4418 - acc: 0.8061 - val_loss: 0.4039 - val_acc: 0.8374\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4397 - acc: 0.8024 - val_loss: 0.4019 - val_acc: 0.8424\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4377 - acc: 0.8053 - val_loss: 0.4001 - val_acc: 0.8456\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4346 - acc: 0.8033 - val_loss: 0.3996 - val_acc: 0.8407\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4344 - acc: 0.8068 - val_loss: 0.4026 - val_acc: 0.8358\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4357 - acc: 0.8090 - val_loss: 0.3963 - val_acc: 0.8473\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4298 - acc: 0.8146 - val_loss: 0.3950 - val_acc: 0.8539\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4289 - acc: 0.8106 - val_loss: 0.3967 - val_acc: 0.8424\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4307 - acc: 0.8104 - val_loss: 0.3953 - val_acc: 0.8424\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4289 - acc: 0.8086 - val_loss: 0.3931 - val_acc: 0.8522\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4246 - acc: 0.8132 - val_loss: 0.3930 - val_acc: 0.8473\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4258 - acc: 0.8119 - val_loss: 0.3934 - val_acc: 0.8473\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4238 - acc: 0.8110 - val_loss: 0.4063 - val_acc: 0.8177\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4379 - acc: 0.8093 - val_loss: 0.3967 - val_acc: 0.8424\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4302 - acc: 0.8104 - val_loss: 0.3954 - val_acc: 0.8440\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4239 - acc: 0.8051 - val_loss: 0.4126 - val_acc: 0.8227\n",
      "Training with parameters {'batch_size': 5000, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_166\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_167 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_498 (Dense)            (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "dropout_332 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_499 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_333 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_500 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 401,729\n",
      "Trainable params: 401,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 1.0982 - acc: 0.4311 - val_loss: 0.7675 - val_acc: 0.4253\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.7892 - acc: 0.4298 - val_loss: 0.7002 - val_acc: 0.5583\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.7122 - acc: 0.5158 - val_loss: 0.7006 - val_acc: 0.5747\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.7147 - acc: 0.5550 - val_loss: 0.6912 - val_acc: 0.5747\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7018 - acc: 0.5665 - val_loss: 0.6748 - val_acc: 0.5764\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6934 - acc: 0.5598 - val_loss: 0.6626 - val_acc: 0.5764\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6760 - acc: 0.5804 - val_loss: 0.6565 - val_acc: 0.6141\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6721 - acc: 0.5849 - val_loss: 0.6526 - val_acc: 0.6568\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6702 - acc: 0.5950 - val_loss: 0.6473 - val_acc: 0.6847\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6655 - acc: 0.6032 - val_loss: 0.6397 - val_acc: 0.6979\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6576 - acc: 0.6187 - val_loss: 0.6313 - val_acc: 0.7110\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6570 - acc: 0.6236 - val_loss: 0.6240 - val_acc: 0.6913\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6493 - acc: 0.6287 - val_loss: 0.6185 - val_acc: 0.6716\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6477 - acc: 0.6282 - val_loss: 0.6126 - val_acc: 0.6831\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6409 - acc: 0.6408 - val_loss: 0.6061 - val_acc: 0.7323\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6355 - acc: 0.6530 - val_loss: 0.6015 - val_acc: 0.7209\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6334 - acc: 0.6492 - val_loss: 0.5964 - val_acc: 0.7225\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6283 - acc: 0.6628 - val_loss: 0.5903 - val_acc: 0.7389\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6248 - acc: 0.6585 - val_loss: 0.5852 - val_acc: 0.7356\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6195 - acc: 0.6710 - val_loss: 0.5804 - val_acc: 0.7406\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6166 - acc: 0.6692 - val_loss: 0.5758 - val_acc: 0.7389\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6138 - acc: 0.6749 - val_loss: 0.5713 - val_acc: 0.7406\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6100 - acc: 0.6776 - val_loss: 0.5665 - val_acc: 0.7471\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6056 - acc: 0.6880 - val_loss: 0.5616 - val_acc: 0.7488\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6010 - acc: 0.6906 - val_loss: 0.5564 - val_acc: 0.7553\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5979 - acc: 0.6990 - val_loss: 0.5510 - val_acc: 0.7537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5927 - acc: 0.6988 - val_loss: 0.5461 - val_acc: 0.7471\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5908 - acc: 0.6993 - val_loss: 0.5423 - val_acc: 0.7537\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5843 - acc: 0.7039 - val_loss: 0.5369 - val_acc: 0.7537\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5831 - acc: 0.7101 - val_loss: 0.5321 - val_acc: 0.7570\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5765 - acc: 0.7176 - val_loss: 0.5284 - val_acc: 0.7685\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5734 - acc: 0.7119 - val_loss: 0.5250 - val_acc: 0.7701\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5683 - acc: 0.7209 - val_loss: 0.5204 - val_acc: 0.7767\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5608 - acc: 0.7274 - val_loss: 0.5163 - val_acc: 0.7833\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5646 - acc: 0.7269 - val_loss: 0.5135 - val_acc: 0.7931\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5606 - acc: 0.7345 - val_loss: 0.5093 - val_acc: 0.7980\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5553 - acc: 0.7291 - val_loss: 0.5053 - val_acc: 0.7915\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5499 - acc: 0.7387 - val_loss: 0.5028 - val_acc: 0.7931\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5501 - acc: 0.7393 - val_loss: 0.4998 - val_acc: 0.7997\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.5465 - acc: 0.7400 - val_loss: 0.4949 - val_acc: 0.8013\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5432 - acc: 0.7457 - val_loss: 0.4906 - val_acc: 0.7997\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5396 - acc: 0.7442 - val_loss: 0.4874 - val_acc: 0.8046\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5410 - acc: 0.7431 - val_loss: 0.4836 - val_acc: 0.8030\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5382 - acc: 0.7420 - val_loss: 0.4799 - val_acc: 0.8013\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5263 - acc: 0.7544 - val_loss: 0.4778 - val_acc: 0.8030\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5296 - acc: 0.7502 - val_loss: 0.4755 - val_acc: 0.8030\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5278 - acc: 0.7526 - val_loss: 0.4729 - val_acc: 0.8030\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5170 - acc: 0.7661 - val_loss: 0.4711 - val_acc: 0.8013\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5206 - acc: 0.7572 - val_loss: 0.4702 - val_acc: 0.8079\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5158 - acc: 0.7650 - val_loss: 0.4669 - val_acc: 0.8079\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5179 - acc: 0.7599 - val_loss: 0.4622 - val_acc: 0.8046\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5126 - acc: 0.7630 - val_loss: 0.4589 - val_acc: 0.8046\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5109 - acc: 0.7656 - val_loss: 0.4581 - val_acc: 0.8144\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5063 - acc: 0.7665 - val_loss: 0.4535 - val_acc: 0.8079\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5020 - acc: 0.7721 - val_loss: 0.4500 - val_acc: 0.8112\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4989 - acc: 0.7650 - val_loss: 0.4487 - val_acc: 0.8128\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5004 - acc: 0.7730 - val_loss: 0.4464 - val_acc: 0.8128\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5033 - acc: 0.7730 - val_loss: 0.4469 - val_acc: 0.8177\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4990 - acc: 0.7698 - val_loss: 0.4450 - val_acc: 0.8177\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4942 - acc: 0.7738 - val_loss: 0.4429 - val_acc: 0.8161\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4905 - acc: 0.7754 - val_loss: 0.4427 - val_acc: 0.8292\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4889 - acc: 0.7781 - val_loss: 0.4419 - val_acc: 0.8325\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4903 - acc: 0.7760 - val_loss: 0.4394 - val_acc: 0.8210\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4864 - acc: 0.7803 - val_loss: 0.4381 - val_acc: 0.8210\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4868 - acc: 0.7811 - val_loss: 0.4362 - val_acc: 0.8374\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4854 - acc: 0.7787 - val_loss: 0.4330 - val_acc: 0.8309\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4822 - acc: 0.7847 - val_loss: 0.4317 - val_acc: 0.8177\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4778 - acc: 0.7873 - val_loss: 0.4288 - val_acc: 0.8374\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4793 - acc: 0.7776 - val_loss: 0.4341 - val_acc: 0.8177\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4852 - acc: 0.7749 - val_loss: 0.4259 - val_acc: 0.8358\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4794 - acc: 0.7823 - val_loss: 0.4246 - val_acc: 0.8243\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4754 - acc: 0.7847 - val_loss: 0.4240 - val_acc: 0.8358\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4734 - acc: 0.7858 - val_loss: 0.4237 - val_acc: 0.8325\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4671 - acc: 0.7896 - val_loss: 0.4207 - val_acc: 0.8325\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4696 - acc: 0.7889 - val_loss: 0.4195 - val_acc: 0.8309\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4706 - acc: 0.7880 - val_loss: 0.4187 - val_acc: 0.8292\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4675 - acc: 0.7896 - val_loss: 0.4173 - val_acc: 0.8325\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4640 - acc: 0.7869 - val_loss: 0.4161 - val_acc: 0.8374\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4638 - acc: 0.7927 - val_loss: 0.4157 - val_acc: 0.8342\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4624 - acc: 0.7922 - val_loss: 0.4153 - val_acc: 0.8342\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4593 - acc: 0.7920 - val_loss: 0.4153 - val_acc: 0.8292\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4573 - acc: 0.7995 - val_loss: 0.4155 - val_acc: 0.8391\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4595 - acc: 0.7907 - val_loss: 0.4141 - val_acc: 0.8358\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4608 - acc: 0.7964 - val_loss: 0.4132 - val_acc: 0.8342\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4564 - acc: 0.8004 - val_loss: 0.4118 - val_acc: 0.8358\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4556 - acc: 0.7980 - val_loss: 0.4101 - val_acc: 0.8374\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4545 - acc: 0.7993 - val_loss: 0.4085 - val_acc: 0.8342\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4555 - acc: 0.7980 - val_loss: 0.4062 - val_acc: 0.8391\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4517 - acc: 0.7942 - val_loss: 0.4046 - val_acc: 0.8374\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4534 - acc: 0.7993 - val_loss: 0.4036 - val_acc: 0.8342\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4493 - acc: 0.8002 - val_loss: 0.4025 - val_acc: 0.8407\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4498 - acc: 0.8006 - val_loss: 0.4021 - val_acc: 0.8424\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4481 - acc: 0.8026 - val_loss: 0.4006 - val_acc: 0.8358\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4443 - acc: 0.8022 - val_loss: 0.4004 - val_acc: 0.8358\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4464 - acc: 0.8035 - val_loss: 0.4024 - val_acc: 0.8391\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4454 - acc: 0.8046 - val_loss: 0.4003 - val_acc: 0.8342\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4448 - acc: 0.8075 - val_loss: 0.4013 - val_acc: 0.8325\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4419 - acc: 0.8093 - val_loss: 0.3995 - val_acc: 0.8374\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4456 - acc: 0.7984 - val_loss: 0.3987 - val_acc: 0.8407\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4418 - acc: 0.8028 - val_loss: 0.3977 - val_acc: 0.8407\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4408 - acc: 0.8041 - val_loss: 0.3959 - val_acc: 0.8358\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4403 - acc: 0.8062 - val_loss: 0.3973 - val_acc: 0.8374\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4384 - acc: 0.8066 - val_loss: 0.3948 - val_acc: 0.8358\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4365 - acc: 0.8104 - val_loss: 0.3972 - val_acc: 0.8407\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4406 - acc: 0.8077 - val_loss: 0.3975 - val_acc: 0.8358\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4361 - acc: 0.8093 - val_loss: 0.3978 - val_acc: 0.8358\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4329 - acc: 0.8092 - val_loss: 0.3998 - val_acc: 0.8407\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4400 - acc: 0.8088 - val_loss: 0.3962 - val_acc: 0.8391\n",
      "Training with parameters {'batch_size': 5000, 'dropout': 0.1, 'lay_conf': [1024, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_167\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_168 (InputLayer)       [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_501 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_334 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_502 (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_335 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_503 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,836,545\n",
      "Trainable params: 1,836,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 1.0578 - acc: 0.4348 - val_loss: 0.7438 - val_acc: 0.5747\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7524 - acc: 0.5678 - val_loss: 0.7816 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.7859 - acc: 0.5681 - val_loss: 0.6513 - val_acc: 0.5747\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6689 - acc: 0.5915 - val_loss: 0.6533 - val_acc: 0.6420\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6703 - acc: 0.5900 - val_loss: 0.6634 - val_acc: 0.6273\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6793 - acc: 0.5809 - val_loss: 0.6110 - val_acc: 0.6782\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6336 - acc: 0.6473 - val_loss: 0.6018 - val_acc: 0.6700\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6322 - acc: 0.6362 - val_loss: 0.6092 - val_acc: 0.6010\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6411 - acc: 0.6190 - val_loss: 0.5846 - val_acc: 0.7110\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6146 - acc: 0.6652 - val_loss: 0.5736 - val_acc: 0.7225\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6063 - acc: 0.6774 - val_loss: 0.5805 - val_acc: 0.7011\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6045 - acc: 0.6703 - val_loss: 0.5650 - val_acc: 0.7340\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5953 - acc: 0.6844 - val_loss: 0.5483 - val_acc: 0.7718\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5839 - acc: 0.7110 - val_loss: 0.5456 - val_acc: 0.7701\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5841 - acc: 0.7165 - val_loss: 0.5348 - val_acc: 0.7816\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5726 - acc: 0.7176 - val_loss: 0.5346 - val_acc: 0.7668\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5706 - acc: 0.7165 - val_loss: 0.5274 - val_acc: 0.7668\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5623 - acc: 0.7258 - val_loss: 0.5156 - val_acc: 0.7964\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5535 - acc: 0.7402 - val_loss: 0.5085 - val_acc: 0.8128\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5511 - acc: 0.7389 - val_loss: 0.5015 - val_acc: 0.8046\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5441 - acc: 0.7442 - val_loss: 0.4957 - val_acc: 0.8030\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5370 - acc: 0.7511 - val_loss: 0.4891 - val_acc: 0.8030\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5313 - acc: 0.7508 - val_loss: 0.4814 - val_acc: 0.8095\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5229 - acc: 0.7583 - val_loss: 0.4750 - val_acc: 0.8095\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5204 - acc: 0.7594 - val_loss: 0.4692 - val_acc: 0.8128\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5123 - acc: 0.7608 - val_loss: 0.4629 - val_acc: 0.8161\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5073 - acc: 0.7674 - val_loss: 0.4580 - val_acc: 0.8177\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5059 - acc: 0.7657 - val_loss: 0.4538 - val_acc: 0.8161\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5001 - acc: 0.7727 - val_loss: 0.4500 - val_acc: 0.8144\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4941 - acc: 0.7756 - val_loss: 0.4473 - val_acc: 0.8144\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4910 - acc: 0.7761 - val_loss: 0.4434 - val_acc: 0.8194\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4893 - acc: 0.7754 - val_loss: 0.4392 - val_acc: 0.8161\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.4878 - acc: 0.7822 - val_loss: 0.4351 - val_acc: 0.8210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4794 - acc: 0.7849 - val_loss: 0.4320 - val_acc: 0.8259\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4794 - acc: 0.7820 - val_loss: 0.4266 - val_acc: 0.8177\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4733 - acc: 0.7865 - val_loss: 0.4250 - val_acc: 0.8276\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4777 - acc: 0.7853 - val_loss: 0.4214 - val_acc: 0.8259\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4682 - acc: 0.7834 - val_loss: 0.4181 - val_acc: 0.8259\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4699 - acc: 0.7884 - val_loss: 0.4185 - val_acc: 0.8276\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4625 - acc: 0.7918 - val_loss: 0.4224 - val_acc: 0.8325\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4638 - acc: 0.7887 - val_loss: 0.4142 - val_acc: 0.8292\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4587 - acc: 0.7977 - val_loss: 0.4129 - val_acc: 0.8243\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4551 - acc: 0.7962 - val_loss: 0.4110 - val_acc: 0.8292\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4576 - acc: 0.7933 - val_loss: 0.4082 - val_acc: 0.8325\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4547 - acc: 0.7931 - val_loss: 0.4071 - val_acc: 0.8342\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4492 - acc: 0.7988 - val_loss: 0.4051 - val_acc: 0.8342\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4491 - acc: 0.8039 - val_loss: 0.4036 - val_acc: 0.8342\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4484 - acc: 0.7997 - val_loss: 0.4030 - val_acc: 0.8342\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4444 - acc: 0.8039 - val_loss: 0.4017 - val_acc: 0.8358\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4478 - acc: 0.8046 - val_loss: 0.4009 - val_acc: 0.8358\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4432 - acc: 0.8026 - val_loss: 0.4006 - val_acc: 0.8374\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4412 - acc: 0.8042 - val_loss: 0.3984 - val_acc: 0.8342\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4383 - acc: 0.8062 - val_loss: 0.3985 - val_acc: 0.8407\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4377 - acc: 0.8097 - val_loss: 0.3963 - val_acc: 0.8325\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4356 - acc: 0.8106 - val_loss: 0.3940 - val_acc: 0.8407\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4387 - acc: 0.8039 - val_loss: 0.3928 - val_acc: 0.8456\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4361 - acc: 0.8070 - val_loss: 0.3921 - val_acc: 0.8407\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4340 - acc: 0.8051 - val_loss: 0.3906 - val_acc: 0.8440\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4322 - acc: 0.8108 - val_loss: 0.3884 - val_acc: 0.8440\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4313 - acc: 0.8079 - val_loss: 0.3893 - val_acc: 0.8456\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4280 - acc: 0.8123 - val_loss: 0.3885 - val_acc: 0.8391\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4258 - acc: 0.8135 - val_loss: 0.3960 - val_acc: 0.8424\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4295 - acc: 0.8084 - val_loss: 0.3977 - val_acc: 0.8391\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4355 - acc: 0.8106 - val_loss: 0.3887 - val_acc: 0.8424\n"
     ]
    }
   ],
   "source": [
    "mlp_perf_metrics2 = make_MLP_exp(X_train_vect, y_train.values, params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.832512</td>\n",
       "      <td>0.845649</td>\n",
       "      <td>0.388130</td>\n",
       "      <td>0.376457</td>\n",
       "      <td>14</td>\n",
       "      <td>0.831254</td>\n",
       "      <td>0.827411</td>\n",
       "      <td>0.759317</td>\n",
       "      <td>0.883959</td>\n",
       "      <td>0.791903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.844736</td>\n",
       "      <td>0.852217</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>0.379807</td>\n",
       "      <td>17</td>\n",
       "      <td>0.837163</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.751553</td>\n",
       "      <td>0.899886</td>\n",
       "      <td>0.796053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.832877</td>\n",
       "      <td>0.835796</td>\n",
       "      <td>0.396141</td>\n",
       "      <td>0.401011</td>\n",
       "      <td>14</td>\n",
       "      <td>0.827971</td>\n",
       "      <td>0.819398</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.877133</td>\n",
       "      <td>0.789050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 64]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.832330</td>\n",
       "      <td>0.850575</td>\n",
       "      <td>0.378857</td>\n",
       "      <td>0.364484</td>\n",
       "      <td>16</td>\n",
       "      <td>0.830598</td>\n",
       "      <td>0.843416</td>\n",
       "      <td>0.736025</td>\n",
       "      <td>0.899886</td>\n",
       "      <td>0.786070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.844007</td>\n",
       "      <td>0.844007</td>\n",
       "      <td>0.372300</td>\n",
       "      <td>0.368458</td>\n",
       "      <td>17</td>\n",
       "      <td>0.826658</td>\n",
       "      <td>0.813531</td>\n",
       "      <td>0.765528</td>\n",
       "      <td>0.871445</td>\n",
       "      <td>0.788800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 64]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.817187</td>\n",
       "      <td>0.848933</td>\n",
       "      <td>0.418020</td>\n",
       "      <td>0.381936</td>\n",
       "      <td>80</td>\n",
       "      <td>0.823375</td>\n",
       "      <td>0.809917</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.869170</td>\n",
       "      <td>0.784628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.819011</td>\n",
       "      <td>0.850575</td>\n",
       "      <td>0.412117</td>\n",
       "      <td>0.373288</td>\n",
       "      <td>86</td>\n",
       "      <td>0.829941</td>\n",
       "      <td>0.826825</td>\n",
       "      <td>0.756211</td>\n",
       "      <td>0.883959</td>\n",
       "      <td>0.789943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.805145</td>\n",
       "      <td>0.822660</td>\n",
       "      <td>0.423896</td>\n",
       "      <td>0.412553</td>\n",
       "      <td>62</td>\n",
       "      <td>0.814839</td>\n",
       "      <td>0.792880</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.854380</td>\n",
       "      <td>0.776545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.808794</td>\n",
       "      <td>0.839080</td>\n",
       "      <td>0.440008</td>\n",
       "      <td>0.396250</td>\n",
       "      <td>108</td>\n",
       "      <td>0.818122</td>\n",
       "      <td>0.803306</td>\n",
       "      <td>0.754658</td>\n",
       "      <td>0.864619</td>\n",
       "      <td>0.778223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.810619</td>\n",
       "      <td>0.842365</td>\n",
       "      <td>0.435497</td>\n",
       "      <td>0.388711</td>\n",
       "      <td>64</td>\n",
       "      <td>0.826001</td>\n",
       "      <td>0.821732</td>\n",
       "      <td>0.751553</td>\n",
       "      <td>0.880546</td>\n",
       "      <td>0.785077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     n_layers   layer_conf      lr  dropout  batch_size  accuracy_train  \\\n",
       "0           2   [128, 128]  0.0001      0.1         NaN        0.832512   \n",
       "1           2  [1024, 128]  0.0001      0.1         NaN        0.844736   \n",
       "2           2    [256, 64]  0.0001      0.1         NaN        0.832877   \n",
       "3           2   [1024, 64]  0.0001      0.1         NaN        0.832330   \n",
       "4           2   [1024, 32]  0.0001      0.1         NaN        0.844007   \n",
       "..        ...          ...     ...      ...         ...             ...   \n",
       "163         2   [1024, 64]  0.0001      0.1      5000.0        0.817187   \n",
       "164         2   [1024, 32]  0.0001      0.1      5000.0        0.819011   \n",
       "165         2   [512, 512]  0.0001      0.1      5000.0        0.805145   \n",
       "166         2    [256, 32]  0.0001      0.1      5000.0        0.808794   \n",
       "167         2  [1024, 256]  0.0001      0.1      5000.0        0.810619   \n",
       "\n",
       "     accuracy_val  loss_train  loss_val  epochs  accuracy  precision  \\\n",
       "0        0.845649    0.388130  0.376457      14  0.831254   0.827411   \n",
       "1        0.852217    0.355500  0.379807      17  0.837163   0.846154   \n",
       "2        0.835796    0.396141  0.401011      14  0.827971   0.819398   \n",
       "3        0.850575    0.378857  0.364484      16  0.830598   0.843416   \n",
       "4        0.844007    0.372300  0.368458      17  0.826658   0.813531   \n",
       "..            ...         ...       ...     ...       ...        ...   \n",
       "163      0.848933    0.418020  0.381936      80  0.823375   0.809917   \n",
       "164      0.850575    0.412117  0.373288      86  0.829941   0.826825   \n",
       "165      0.822660    0.423896  0.412553      62  0.814839   0.792880   \n",
       "166      0.839080    0.440008  0.396250     108  0.818122   0.803306   \n",
       "167      0.842365    0.435497  0.388711      64  0.826001   0.821732   \n",
       "\n",
       "       recall  specificity  f1_score  \n",
       "0    0.759317     0.883959  0.791903  \n",
       "1    0.751553     0.899886  0.796053  \n",
       "2    0.760870     0.877133  0.789050  \n",
       "3    0.736025     0.899886  0.786070  \n",
       "4    0.765528     0.871445  0.788800  \n",
       "..        ...          ...       ...  \n",
       "163  0.760870     0.869170  0.784628  \n",
       "164  0.756211     0.883959  0.789943  \n",
       "165  0.760870     0.854380  0.776545  \n",
       "166  0.754658     0.864619  0.778223  \n",
       "167  0.751553     0.880546  0.785077  \n",
       "\n",
       "[168 rows x 15 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.844736</td>\n",
       "      <td>0.852217</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>0.379807</td>\n",
       "      <td>17</td>\n",
       "      <td>0.837163</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.751553</td>\n",
       "      <td>0.899886</td>\n",
       "      <td>0.796053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 64]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.842182</td>\n",
       "      <td>0.842365</td>\n",
       "      <td>0.369521</td>\n",
       "      <td>0.367837</td>\n",
       "      <td>18</td>\n",
       "      <td>0.836507</td>\n",
       "      <td>0.841105</td>\n",
       "      <td>0.756211</td>\n",
       "      <td>0.895336</td>\n",
       "      <td>0.796402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.833789</td>\n",
       "      <td>0.844007</td>\n",
       "      <td>0.384280</td>\n",
       "      <td>0.385244</td>\n",
       "      <td>18</td>\n",
       "      <td>0.835850</td>\n",
       "      <td>0.833898</td>\n",
       "      <td>0.763975</td>\n",
       "      <td>0.888510</td>\n",
       "      <td>0.797407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>700.0</td>\n",
       "      <td>0.832330</td>\n",
       "      <td>0.850575</td>\n",
       "      <td>0.380837</td>\n",
       "      <td>0.378106</td>\n",
       "      <td>67</td>\n",
       "      <td>0.835850</td>\n",
       "      <td>0.837329</td>\n",
       "      <td>0.759317</td>\n",
       "      <td>0.891923</td>\n",
       "      <td>0.796417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.838533</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.366473</td>\n",
       "      <td>0.366711</td>\n",
       "      <td>62</td>\n",
       "      <td>0.835850</td>\n",
       "      <td>0.835034</td>\n",
       "      <td>0.762422</td>\n",
       "      <td>0.889647</td>\n",
       "      <td>0.797078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     n_layers   layer_conf      lr  dropout  batch_size  accuracy_train  \\\n",
       "1           2  [1024, 128]  0.0001      0.1         NaN        0.844736   \n",
       "19          2   [1024, 64]  0.0001      0.1        10.0        0.842182   \n",
       "57          2  [1024, 128]  0.0001      0.1       150.0        0.833789   \n",
       "88          2   [128, 128]  0.0001      0.1       700.0        0.832330   \n",
       "108         2   [1024, 32]  0.0001      0.1      1100.0        0.838533   \n",
       "\n",
       "     accuracy_val  loss_train  loss_val  epochs  accuracy  precision  \\\n",
       "1        0.852217    0.355500  0.379807      17  0.837163   0.846154   \n",
       "19       0.842365    0.369521  0.367837      18  0.836507   0.841105   \n",
       "57       0.844007    0.384280  0.385244      18  0.835850   0.833898   \n",
       "88       0.850575    0.380837  0.378106      67  0.835850   0.837329   \n",
       "108      0.857143    0.366473  0.366711      62  0.835850   0.835034   \n",
       "\n",
       "       recall  specificity  f1_score  \n",
       "1    0.751553     0.899886  0.796053  \n",
       "19   0.756211     0.895336  0.796402  \n",
       "57   0.763975     0.888510  0.797407  \n",
       "88   0.759317     0.891923  0.796417  \n",
       "108  0.762422     0.889647  0.797078  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics2.nlargest(5, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.853859</td>\n",
       "      <td>0.844007</td>\n",
       "      <td>0.347148</td>\n",
       "      <td>0.381044</td>\n",
       "      <td>30</td>\n",
       "      <td>0.831911</td>\n",
       "      <td>0.812903</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.868032</td>\n",
       "      <td>0.797468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.833789</td>\n",
       "      <td>0.844007</td>\n",
       "      <td>0.384280</td>\n",
       "      <td>0.385244</td>\n",
       "      <td>18</td>\n",
       "      <td>0.835850</td>\n",
       "      <td>0.833898</td>\n",
       "      <td>0.763975</td>\n",
       "      <td>0.888510</td>\n",
       "      <td>0.797407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.831418</td>\n",
       "      <td>0.852217</td>\n",
       "      <td>0.387146</td>\n",
       "      <td>0.374688</td>\n",
       "      <td>157</td>\n",
       "      <td>0.835850</td>\n",
       "      <td>0.833898</td>\n",
       "      <td>0.763975</td>\n",
       "      <td>0.888510</td>\n",
       "      <td>0.797407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.837074</td>\n",
       "      <td>0.853859</td>\n",
       "      <td>0.373680</td>\n",
       "      <td>0.379271</td>\n",
       "      <td>81</td>\n",
       "      <td>0.835194</td>\n",
       "      <td>0.831366</td>\n",
       "      <td>0.765528</td>\n",
       "      <td>0.886234</td>\n",
       "      <td>0.797090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2</td>\n",
       "      <td>[1024, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.838533</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.366473</td>\n",
       "      <td>0.366711</td>\n",
       "      <td>62</td>\n",
       "      <td>0.835850</td>\n",
       "      <td>0.835034</td>\n",
       "      <td>0.762422</td>\n",
       "      <td>0.889647</td>\n",
       "      <td>0.797078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     n_layers   layer_conf      lr  dropout  batch_size  accuracy_train  \\\n",
       "32          2   [128, 128]  0.0001      0.1        64.0        0.853859   \n",
       "57          2  [1024, 128]  0.0001      0.1       150.0        0.833789   \n",
       "144         2   [128, 128]  0.0001      0.1      3000.0        0.831418   \n",
       "96          2   [128, 128]  0.0001      0.1      1000.0        0.837074   \n",
       "108         2   [1024, 32]  0.0001      0.1      1100.0        0.838533   \n",
       "\n",
       "     accuracy_val  loss_train  loss_val  epochs  accuracy  precision  \\\n",
       "32       0.844007    0.347148  0.381044      30  0.831911   0.812903   \n",
       "57       0.844007    0.384280  0.385244      18  0.835850   0.833898   \n",
       "144      0.852217    0.387146  0.374688     157  0.835850   0.833898   \n",
       "96       0.853859    0.373680  0.379271      81  0.835194   0.831366   \n",
       "108      0.857143    0.366473  0.366711      62  0.835850   0.835034   \n",
       "\n",
       "       recall  specificity  f1_score  \n",
       "32   0.782609     0.868032  0.797468  \n",
       "57   0.763975     0.888510  0.797407  \n",
       "144  0.763975     0.888510  0.797407  \n",
       "96   0.765528     0.886234  0.797090  \n",
       "108  0.762422     0.889647  0.797078  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics2.nlargest(5, 'f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_perf_metrics2.to_csv('mlp_perf_metrics2_BERT_hyb_appr.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze performance metrics evolution against `batch_size` and per architecture configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mlp_summ_performance(perf_df, x_ax_param: str, group_param: str, x_ax_log=False):\n",
    "\n",
    "    fig, ax = plt.subplots(3,2, figsize=(16,20))\n",
    "    groups = perf_df[group_param].drop_duplicates()\n",
    "\n",
    "    # Plot accuracy\n",
    "    for gr in groups:\n",
    "        ax[0, 0].plot(perf_df.loc[perf_df[group_param] == gr, x_ax_param].values,\n",
    "                      perf_df.loc[perf_df[group_param] == gr, 'accuracy'].values,\n",
    "                      label=gr)\n",
    "    if x_ax_log:\n",
    "        ax[0, 0].set_xscale('log')\n",
    "    ax[0, 0].set_xlabel(x_ax_param)\n",
    "    ax[0, 0].set_ylabel('Accuracy')\n",
    "    ax[0, 0].legend(title=group_param)\n",
    "    ax[0, 0].set_title('Accuracy evolution against ' + x_ax_param)\n",
    "    \n",
    "    # Plot precision\n",
    "    for gr in groups:\n",
    "        ax[0, 1].plot(perf_df.loc[perf_df[group_param] == gr, x_ax_param].values,\n",
    "                      perf_df.loc[perf_df[group_param] == gr, 'precision'].values,\n",
    "                      label=gr)\n",
    "    if x_ax_log:\n",
    "        ax[0, 1].set_xscale('log')\n",
    "    ax[0, 1].set_xlabel(x_ax_param)\n",
    "    ax[0, 1].set_ylabel('Precision')\n",
    "    ax[0, 1].legend(title=group_param)\n",
    "    ax[0, 1].set_title('Precision evolution against ' + x_ax_param)\n",
    "    \n",
    "    # Plot recall\n",
    "    for gr in groups:\n",
    "        ax[1, 0].plot(perf_df.loc[perf_df[group_param] == gr, x_ax_param].values,\n",
    "                      perf_df.loc[perf_df[group_param] == gr, 'recall'].values,\n",
    "                      label=gr)\n",
    "    if x_ax_log:\n",
    "        ax[1, 0].set_xscale('log')\n",
    "    ax[1, 0].set_xlabel(x_ax_param)\n",
    "    ax[1, 0].set_ylabel('Recall')\n",
    "    ax[1, 0].legend(title=group_param)\n",
    "    ax[1, 0].set_title('Recall evolution against ' + x_ax_param)\n",
    "    \n",
    "    # Plot specificity\n",
    "    for gr in groups:\n",
    "        ax[1, 1].plot(perf_df.loc[perf_df[group_param] == gr, x_ax_param].values,\n",
    "                      perf_df.loc[perf_df[group_param] == gr, 'specificity'].values,\n",
    "                      label=gr)\n",
    "    if x_ax_log:\n",
    "        ax[1, 1].set_xscale('log')\n",
    "    ax[1, 1].set_xlabel(x_ax_param)\n",
    "    ax[1, 1].set_ylabel('Specificity')\n",
    "    ax[1, 1].legend(title=group_param)\n",
    "    ax[1, 1].set_title('Specificity evolution against ' + x_ax_param)\n",
    "\n",
    "    # Plot f1 score\n",
    "    for gr in groups:\n",
    "        ax[2, 0].plot(perf_df.loc[perf_df[group_param] == gr, x_ax_param].values,\n",
    "                      perf_df.loc[perf_df[group_param] == gr, 'f1_score'].values,\n",
    "                      label=gr)\n",
    "    if x_ax_log:\n",
    "        ax[2, 0].set_xscale('log')\n",
    "    ax[2, 0].set_xlabel(x_ax_param)\n",
    "    ax[2, 0].set_ylabel('F1 Score')\n",
    "    ax[2, 0].legend(title=group_param)\n",
    "    ax[2, 0].set_title('F1 Score evolution against ' + x_ax_param)\n",
    "    \n",
    "    plt.show()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1440 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<Figure size 1152x1440 with 6 Axes>,\n",
       " array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fcfe456fc50>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7fcfe4507f28>],\n",
       "        [<matplotlib.axes._subplots.AxesSubplot object at 0x7fcfe44c5518>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7fcfe44f1a90>],\n",
       "        [<matplotlib.axes._subplots.AxesSubplot object at 0x7fcfe44ac080>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7fcfe4459630>]],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_mlp_summ_performance(mlp_perf_metrics2, x_ax_param='batch_size', group_param='layer_conf', x_ax_log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mlp_summ_avg_performance(perf_df, x_ax_param: str, group_param: str, x_ax_log=False):\n",
    "\n",
    "    fig, ax = plt.subplots(3,2, figsize=(16,20))\n",
    "    avg_group = perf_df.groupby(by=x_ax_param).agg(np.mean)\n",
    "    max_group = perf_df.groupby(by=x_ax_param).agg(np.max)\n",
    "    min_group = perf_df.groupby(by=x_ax_param).agg(np.min)\n",
    "\n",
    "    # Plot accuracy\n",
    "    ax[0, 0].plot(avg_group.index.values, avg_group['accuracy'].values, color='tab:blue')\n",
    "    ax[0, 0].fill_between(avg_group.index.values, min_group['accuracy'].values, max_group['accuracy'].values,\n",
    "                          color='lightblue')\n",
    "    if x_ax_log:\n",
    "        ax[0, 0].set_xscale('log')\n",
    "    ax[0, 0].set_xlabel(x_ax_param)\n",
    "    ax[0, 0].set_ylabel('Accuracy')\n",
    "    ax[0, 0].set_title('Accuracy evolution against ' + x_ax_param)\n",
    "    \n",
    "    # Plot precision\n",
    "    ax[0, 1].plot(avg_group.index.values, avg_group['precision'].values, color='tab:red')\n",
    "    ax[0, 1].fill_between(avg_group.index.values, min_group['precision'].values, max_group['precision'].values,\n",
    "                          color='coral')\n",
    "    if x_ax_log:\n",
    "        ax[0, 1].set_xscale('log')\n",
    "    ax[0, 1].set_xlabel(x_ax_param)\n",
    "    ax[0, 1].set_ylabel('Precision')\n",
    "    ax[0, 1].set_title('Precision evolution against ' + x_ax_param)\n",
    "    \n",
    "    # Plot recall\n",
    "    ax[1, 0].plot(avg_group.index.values, avg_group['recall'].values, color='tab:green')\n",
    "    ax[1, 0].fill_between(avg_group.index.values, min_group['recall'].values, max_group['recall'].values,\n",
    "                          color='lightgreen')\n",
    "    if x_ax_log:\n",
    "        ax[1, 0].set_xscale('log')\n",
    "    ax[1, 0].set_xlabel(x_ax_param)\n",
    "    ax[1, 0].set_ylabel('Recall')\n",
    "    ax[1, 0].set_title('Recall evolution against ' + x_ax_param)\n",
    "    \n",
    "    # Plot specificity\n",
    "    ax[1, 1].plot(avg_group.index.values, avg_group['specificity'].values, color='tab:orange')\n",
    "    ax[1, 1].fill_between(avg_group.index.values, min_group['specificity'].values, max_group['specificity'].values,\n",
    "                          color='orange')\n",
    "    if x_ax_log:\n",
    "        ax[1, 1].set_xscale('log')\n",
    "    ax[1, 1].set_xlabel(x_ax_param)\n",
    "    ax[1, 1].set_ylabel('Specificity')\n",
    "    ax[1, 1].set_title('Specificity evolution against ' + x_ax_param)\n",
    "\n",
    "    # Plot f1 score\n",
    "    ax[2, 0].plot(avg_group.index.values, avg_group['f1_score'].values, color='tab:pink')\n",
    "    ax[2, 0].fill_between(avg_group.index.values, min_group['f1_score'].values, max_group['f1_score'].values,\n",
    "                          color='lightpink')\n",
    "    if x_ax_log:\n",
    "        ax[2, 0].set_xscale('log')\n",
    "    ax[2, 0].set_xlabel(x_ax_param)\n",
    "    ax[2, 0].set_ylabel('F1 Score')\n",
    "    ax[2, 0].set_title('F1 Score evolution against ' + x_ax_param)\n",
    "    \n",
    "    plt.show()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1440 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<Figure size 1152x1440 with 6 Axes>,\n",
       " array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fcfe122a710>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7fcfe464b9e8>],\n",
       "        [<matplotlib.axes._subplots.AxesSubplot object at 0x7fcfe463d0f0>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7fcfeca63b38>],\n",
       "        [<matplotlib.axes._subplots.AxesSubplot object at 0x7fcfec8d8da0>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7fcfe104a390>]],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_mlp_summ_avg_performance(mlp_perf_metrics2, x_ax_param='batch_size', group_param='layer_conf', x_ax_log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00e+00, 1.00e+01, 3.20e+01, 6.40e+01, 7.00e+01, 1.00e+02,\n",
       "       1.50e+02, 2.00e+02, 3.50e+02, 5.00e+02, 7.00e+02, 1.00e+03,\n",
       "       1.10e+03, 1.25e+03, 1.50e+03, 2.00e+03, 2.50e+03, 3.00e+03,\n",
       "       4.00e+03, 5.00e+03])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics2.groupby(by='batch_size').agg(np.mean).index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics evolution respecting `batch_size` fit a disruptive schedule and no clear tendence is extracted at first.\n",
    "\n",
    "Models with architectures [1024, 64], [1024, 128], [1024, 532], [128, 128] offer a balanced high-prediction over both classes for different `batch_size` values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict\n",
    "\n",
    "Let's predict test set by using model with architecture [1024, 128] and `batch_size=150` which proves higher accuracy and f1-score on the rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "46/46 [==============================] - 1s 7ms/step - loss: 0.6340 - acc: 0.6577 - val_loss: 0.5572 - val_acc: 0.7336\n",
      "Epoch 2/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.5214 - acc: 0.7530 - val_loss: 0.4560 - val_acc: 0.7782\n",
      "Epoch 3/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4715 - acc: 0.7827 - val_loss: 0.4200 - val_acc: 0.8136\n",
      "Epoch 4/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4542 - acc: 0.7968 - val_loss: 0.4269 - val_acc: 0.7940\n",
      "Epoch 5/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4410 - acc: 0.8035 - val_loss: 0.4015 - val_acc: 0.8110\n",
      "Epoch 6/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4315 - acc: 0.8130 - val_loss: 0.4000 - val_acc: 0.8136\n",
      "Epoch 7/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4221 - acc: 0.8133 - val_loss: 0.3929 - val_acc: 0.8281\n",
      "Epoch 8/300\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4145 - acc: 0.8186 - val_loss: 0.4136 - val_acc: 0.8228\n",
      "Epoch 9/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4111 - acc: 0.8215 - val_loss: 0.3923 - val_acc: 0.8215\n",
      "Epoch 10/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4010 - acc: 0.8272 - val_loss: 0.3872 - val_acc: 0.8346\n",
      "Epoch 11/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4042 - acc: 0.8244 - val_loss: 0.3951 - val_acc: 0.8163\n",
      "Epoch 12/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3914 - acc: 0.8326 - val_loss: 0.3840 - val_acc: 0.8294\n",
      "Epoch 13/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3973 - acc: 0.8251 - val_loss: 0.4060 - val_acc: 0.8255\n",
      "Epoch 14/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3852 - acc: 0.8340 - val_loss: 0.4187 - val_acc: 0.8241\n",
      "Epoch 15/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3862 - acc: 0.8323 - val_loss: 0.3868 - val_acc: 0.8320\n",
      "Epoch 16/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3781 - acc: 0.8359 - val_loss: 0.3892 - val_acc: 0.8333\n",
      "Epoch 17/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3670 - acc: 0.8421 - val_loss: 0.3827 - val_acc: 0.8281\n",
      "Epoch 18/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3731 - acc: 0.8384 - val_loss: 0.4158 - val_acc: 0.8307\n",
      "Epoch 19/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3683 - acc: 0.8391 - val_loss: 0.3825 - val_acc: 0.8307\n",
      "Epoch 20/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3628 - acc: 0.8421 - val_loss: 0.4038 - val_acc: 0.8307\n",
      "Epoch 21/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3524 - acc: 0.8478 - val_loss: 0.3847 - val_acc: 0.8320\n",
      "Epoch 22/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3460 - acc: 0.8533 - val_loss: 0.3911 - val_acc: 0.8241\n",
      "Epoch 23/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3417 - acc: 0.8533 - val_loss: 0.3982 - val_acc: 0.8215\n",
      "Epoch 24/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3437 - acc: 0.8499 - val_loss: 0.4062 - val_acc: 0.8215\n"
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "mlp_model = build_MLP_network(input_dim=X_train_vect.shape[1],\n",
    "                                      layers_dim=(1024, 128),\n",
    "                                      dropout=0.1, lr=1e-4,\n",
    "                                      seed=123456)\n",
    "mlp_model.summary()\n",
    "\n",
    "# Fit model using train test keeping \n",
    "hist = mlp_model.fit(X_train_vect, y_train.values,\n",
    "                     batch_size=150,\n",
    "                     epochs=300,\n",
    "                      validation_split=0.1,\n",
    "                      callbacks=[EarlyStopping(monitor='val_loss', patience=5,\n",
    "                                               restore_best_weights=True,\n",
    "                                              min_delta=1e-7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot performance evolution metrics\n",
    "fig, ax = plt.subplots(1,2, figsize=(20, 12))\n",
    "\n",
    "# Accuracy\n",
    "ax[0].plot(hist.history['acc'], label='train')\n",
    "ax[0].plot(hist.history['val_acc'], label='val')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_xlabel('Accuracy')\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Accuracy')\n",
    "\n",
    "# Loss\n",
    "ax[1].plot(hist.history['loss'], label='train')\n",
    "ax[1].plot(hist.history['val_loss'], label='val')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_xlabel('Loss')\n",
    "ax[1].legend()\n",
    "ax[1].set_title('Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict and save test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp_model.predict(X_test_vect)\n",
    "y_pred[y_pred >= 0.5] = 1\n",
    "y_pred[y_pred < 0.5] = 0\n",
    "y_pred = y_pred.squeeze().astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge index and labels in dataframe\n",
    "pred_out = pd.DataFrame({'id':test['id'], 'target': y_pred})\n",
    "pred_out.to_csv('BERT_trans_hyb_des_mlp_pred_out.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to decrease `lr`parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,705,217\n",
      "Trainable params: 1,705,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "46/46 [==============================] - 1s 6ms/step - loss: 0.7383 - acc: 0.5211 - val_loss: 0.6667 - val_acc: 0.5669\n",
      "Epoch 2/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.6574 - acc: 0.6094 - val_loss: 0.6195 - val_acc: 0.6942\n",
      "Epoch 3/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.6176 - acc: 0.6598 - val_loss: 0.5799 - val_acc: 0.7585\n",
      "Epoch 4/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.5907 - acc: 0.7009 - val_loss: 0.5597 - val_acc: 0.7664\n",
      "Epoch 5/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.5711 - acc: 0.7181 - val_loss: 0.5293 - val_acc: 0.7782\n",
      "Epoch 6/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.5483 - acc: 0.7397 - val_loss: 0.5097 - val_acc: 0.7730\n",
      "Epoch 7/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.5337 - acc: 0.7510 - val_loss: 0.5014 - val_acc: 0.7887\n",
      "Epoch 8/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.5199 - acc: 0.7606 - val_loss: 0.4847 - val_acc: 0.8031\n",
      "Epoch 9/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.5091 - acc: 0.7669 - val_loss: 0.4741 - val_acc: 0.8071\n",
      "Epoch 10/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4970 - acc: 0.7755 - val_loss: 0.4610 - val_acc: 0.8018\n",
      "Epoch 11/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4908 - acc: 0.7768 - val_loss: 0.4549 - val_acc: 0.8123\n",
      "Epoch 12/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4865 - acc: 0.7781 - val_loss: 0.4474 - val_acc: 0.7992\n",
      "Epoch 13/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4767 - acc: 0.7886 - val_loss: 0.4454 - val_acc: 0.8163\n",
      "Epoch 14/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4734 - acc: 0.7885 - val_loss: 0.4385 - val_acc: 0.8136\n",
      "Epoch 15/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4660 - acc: 0.7926 - val_loss: 0.4367 - val_acc: 0.8215\n",
      "Epoch 16/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4632 - acc: 0.7927 - val_loss: 0.4287 - val_acc: 0.8058\n",
      "Epoch 17/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4587 - acc: 0.7980 - val_loss: 0.4258 - val_acc: 0.8097\n",
      "Epoch 18/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4520 - acc: 0.8003 - val_loss: 0.4280 - val_acc: 0.8241\n",
      "Epoch 19/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4494 - acc: 0.8008 - val_loss: 0.4211 - val_acc: 0.8202\n",
      "Epoch 20/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4452 - acc: 0.8082 - val_loss: 0.4204 - val_acc: 0.8228\n",
      "Epoch 21/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4449 - acc: 0.8029 - val_loss: 0.4164 - val_acc: 0.8176\n",
      "Epoch 22/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4404 - acc: 0.8056 - val_loss: 0.4151 - val_acc: 0.8241\n",
      "Epoch 23/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4381 - acc: 0.8056 - val_loss: 0.4116 - val_acc: 0.8202\n",
      "Epoch 24/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4355 - acc: 0.8102 - val_loss: 0.4111 - val_acc: 0.8241\n",
      "Epoch 25/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4314 - acc: 0.8085 - val_loss: 0.4114 - val_acc: 0.8281\n",
      "Epoch 26/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4296 - acc: 0.8139 - val_loss: 0.4092 - val_acc: 0.8268\n",
      "Epoch 27/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4299 - acc: 0.8075 - val_loss: 0.4065 - val_acc: 0.8163\n",
      "Epoch 28/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4269 - acc: 0.8146 - val_loss: 0.4055 - val_acc: 0.8163\n",
      "Epoch 29/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4248 - acc: 0.8146 - val_loss: 0.4069 - val_acc: 0.8110\n",
      "Epoch 30/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4203 - acc: 0.8158 - val_loss: 0.4032 - val_acc: 0.8202\n",
      "Epoch 31/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4269 - acc: 0.8091 - val_loss: 0.4024 - val_acc: 0.8228\n",
      "Epoch 32/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4196 - acc: 0.8164 - val_loss: 0.4035 - val_acc: 0.8281\n",
      "Epoch 33/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4185 - acc: 0.8187 - val_loss: 0.4025 - val_acc: 0.8202\n",
      "Epoch 34/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4139 - acc: 0.8222 - val_loss: 0.4003 - val_acc: 0.8255\n",
      "Epoch 35/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4137 - acc: 0.8221 - val_loss: 0.4012 - val_acc: 0.8294\n",
      "Epoch 36/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4140 - acc: 0.8219 - val_loss: 0.3981 - val_acc: 0.8215\n",
      "Epoch 37/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4102 - acc: 0.8205 - val_loss: 0.3984 - val_acc: 0.8202\n",
      "Epoch 38/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4089 - acc: 0.8254 - val_loss: 0.3988 - val_acc: 0.8176\n",
      "Epoch 39/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4106 - acc: 0.8203 - val_loss: 0.3985 - val_acc: 0.8333\n",
      "Epoch 40/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4070 - acc: 0.8240 - val_loss: 0.3949 - val_acc: 0.8202\n",
      "Epoch 41/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4032 - acc: 0.8272 - val_loss: 0.3948 - val_acc: 0.8360\n",
      "Epoch 42/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4052 - acc: 0.8219 - val_loss: 0.3952 - val_acc: 0.8373\n",
      "Epoch 43/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4039 - acc: 0.8221 - val_loss: 0.3938 - val_acc: 0.8307\n",
      "Epoch 44/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4014 - acc: 0.8247 - val_loss: 0.3930 - val_acc: 0.8294\n",
      "Epoch 45/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3978 - acc: 0.8270 - val_loss: 0.3932 - val_acc: 0.8333\n",
      "Epoch 46/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3997 - acc: 0.8281 - val_loss: 0.3970 - val_acc: 0.8346\n",
      "Epoch 47/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3974 - acc: 0.8300 - val_loss: 0.3920 - val_acc: 0.8320\n",
      "Epoch 48/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3954 - acc: 0.8316 - val_loss: 0.3914 - val_acc: 0.8268\n",
      "Epoch 49/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3945 - acc: 0.8329 - val_loss: 0.3968 - val_acc: 0.8373\n",
      "Epoch 50/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3944 - acc: 0.8300 - val_loss: 0.3937 - val_acc: 0.8202\n",
      "Epoch 51/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3936 - acc: 0.8308 - val_loss: 0.3928 - val_acc: 0.8346\n",
      "Epoch 52/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3889 - acc: 0.8298 - val_loss: 0.3913 - val_acc: 0.8333\n",
      "Epoch 53/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3900 - acc: 0.8305 - val_loss: 0.3914 - val_acc: 0.8241\n",
      "Epoch 54/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3865 - acc: 0.8356 - val_loss: 0.3904 - val_acc: 0.8241\n",
      "Epoch 55/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3890 - acc: 0.8311 - val_loss: 0.3903 - val_acc: 0.8307\n",
      "Epoch 56/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3902 - acc: 0.8330 - val_loss: 0.3906 - val_acc: 0.8241\n",
      "Epoch 57/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3881 - acc: 0.8324 - val_loss: 0.3906 - val_acc: 0.8294\n",
      "Epoch 58/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3849 - acc: 0.8345 - val_loss: 0.3901 - val_acc: 0.8255\n",
      "Epoch 59/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3840 - acc: 0.8343 - val_loss: 0.3884 - val_acc: 0.8320\n",
      "Epoch 60/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3819 - acc: 0.8346 - val_loss: 0.3882 - val_acc: 0.8307\n",
      "Epoch 61/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3784 - acc: 0.8383 - val_loss: 0.3879 - val_acc: 0.8307\n",
      "Epoch 62/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3798 - acc: 0.8352 - val_loss: 0.3883 - val_acc: 0.8307\n",
      "Epoch 63/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3808 - acc: 0.8364 - val_loss: 0.3879 - val_acc: 0.8294\n",
      "Epoch 64/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3773 - acc: 0.8428 - val_loss: 0.3928 - val_acc: 0.8320\n",
      "Epoch 65/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3792 - acc: 0.8380 - val_loss: 0.3924 - val_acc: 0.8320\n",
      "Epoch 66/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3736 - acc: 0.8394 - val_loss: 0.3877 - val_acc: 0.8294\n",
      "Epoch 67/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3729 - acc: 0.8421 - val_loss: 0.3874 - val_acc: 0.8268\n",
      "Epoch 68/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3722 - acc: 0.8374 - val_loss: 0.3874 - val_acc: 0.8307\n",
      "Epoch 69/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3739 - acc: 0.8384 - val_loss: 0.3879 - val_acc: 0.8255\n",
      "Epoch 70/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3735 - acc: 0.8373 - val_loss: 0.3877 - val_acc: 0.8346\n",
      "Epoch 71/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3704 - acc: 0.8415 - val_loss: 0.3869 - val_acc: 0.8268\n",
      "Epoch 72/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3674 - acc: 0.8444 - val_loss: 0.3857 - val_acc: 0.8320\n",
      "Epoch 73/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3688 - acc: 0.8424 - val_loss: 0.3851 - val_acc: 0.8281\n",
      "Epoch 74/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3667 - acc: 0.8403 - val_loss: 0.3848 - val_acc: 0.8281\n",
      "Epoch 75/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3667 - acc: 0.8434 - val_loss: 0.3852 - val_acc: 0.8255\n",
      "Epoch 76/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3640 - acc: 0.8464 - val_loss: 0.3939 - val_acc: 0.8320\n",
      "Epoch 77/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3667 - acc: 0.8427 - val_loss: 0.3862 - val_acc: 0.8268\n",
      "Epoch 78/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3581 - acc: 0.8505 - val_loss: 0.3849 - val_acc: 0.8281\n",
      "Epoch 79/300\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3583 - acc: 0.8485 - val_loss: 0.3883 - val_acc: 0.8307\n"
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "mlp_model2 = build_MLP_network(input_dim=X_train_vect.shape[1],\n",
    "                                      layers_dim=(1024, 128),\n",
    "                                      dropout=0.1, lr=1e-5,\n",
    "                                      seed=123456)\n",
    "mlp_model2.summary()\n",
    "\n",
    "# Fit model using train test keeping \n",
    "hist2 = mlp_model2.fit(X_train_vect, y_train.values,\n",
    "                     batch_size=150,\n",
    "                     epochs=300,\n",
    "                      validation_split=0.1,\n",
    "                      callbacks=[EarlyStopping(monitor='val_loss', patience=5,\n",
    "                                               restore_best_weights=True,\n",
    "                                              min_delta=1e-7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot performance evolution metrics\n",
    "fig, ax = plt.subplots(1,2, figsize=(20, 12))\n",
    "\n",
    "# Accuracy\n",
    "ax[0].plot(hist2.history['acc'], label='train')\n",
    "ax[0].plot(hist2.history['val_acc'], label='val')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_xlabel('Accuracy')\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Accuracy')\n",
    "\n",
    "# Loss\n",
    "ax[1].plot(hist2.history['loss'], label='train')\n",
    "ax[1].plot(hist2.history['val_loss'], label='val')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_xlabel('Loss')\n",
    "ax[1].legend()\n",
    "ax[1].set_title('Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict again over the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = mlp_model2.predict(X_test_vect)\n",
    "y_pred2[y_pred2 >= 0.5] = 1\n",
    "y_pred2[y_pred2 < 0.5] = 0\n",
    "y_pred2 = y_pred2.squeeze().astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge index and labels in dataframe\n",
    "pred_out2 = pd.DataFrame({'id':test['id'], 'target': y_pred2})\n",
    "pred_out2.to_csv('BERT_trans_hyb_des_mlp_pred_out2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
