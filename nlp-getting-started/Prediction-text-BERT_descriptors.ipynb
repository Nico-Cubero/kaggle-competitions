{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making an accurate detector for fake disaster tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use BERT pretrained model from Google in order to obtain rich tweet texts descriptors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this to disable GPU usage\n",
    "from tensorflow import config\n",
    "config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.math as tf_math\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Remove duplicated text from train set\n",
    "train[train['text'].duplicated(keep='first')]\n",
    "\n",
    "# Prepare train and test set\n",
    "X_train, y_train = train['text'], train['target']\n",
    "X_test = test['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text by using BERT pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenize input data by using the BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tweet_max_length = 280\n",
    "\n",
    "# Tokenize train y test set according to BERT model input format\n",
    "X_train_tok = tokenizer(text=X_train.tolist(),\n",
    "                        max_length=tweet_max_length,\n",
    "                        padding='max_length',\n",
    "                        add_special_tokens=True,\n",
    "                        return_attention_mask=True,\n",
    "                        return_token_type_ids=True,\n",
    "                        pad_to_max_length=True,\n",
    "                        return_tensors='tf')\n",
    "\n",
    "X_test_tok = tokenizer(text=X_test.tolist(),\n",
    "                        max_length=tweet_max_length,\n",
    "                        padding='max_length',\n",
    "                        add_special_tokens=True,\n",
    "                        return_attention_mask=True,\n",
    "                        return_token_type_ids=True,\n",
    "                        pad_to_max_length=True,\n",
    "                        return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tok.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7613, 280), dtype=int32, numpy=\n",
       "array([[  101,  2256, 15616, ...,     0,     0,     0],\n",
       "       [  101,  3224,  2543, ...,     0,     0,     0],\n",
       "       [  101,  2035,  3901, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101, 23290,  1012, ...,     0,     0,     0],\n",
       "       [  101,  2610, 11538, ...,     0,     0,     0],\n",
       "       [  101,  1996,  6745, ...,     0,     0,     0]], dtype=int32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tok['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7613, 280), dtype=int32, numpy=\n",
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tok['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7613, 280), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tok['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. User BERT pretrained model for high-descriptive features extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "def build_pre_core_BERT_model(max_length: int):\n",
    "    \n",
    "    # Prepare input layers with tokenized data\n",
    "    input_ids = Input(shape=(max_length,), dtype='int32', name='input_ids')\n",
    "    token_typ_ids = Input(shape=(max_length,), dtype='int32', name='token_typ_ids')\n",
    "    attention_masks = Input(shape=(max_length,), dtype='int32', name='attention_masks')\n",
    "    \n",
    "    # Load pretrained model\n",
    "    bert_pre_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "    bert_pre_model.trainable = False\n",
    "\n",
    "    sequence_output, pooled_output = bert_pre_model(input_ids, attention_masks, token_typ_ids)\n",
    "    \n",
    "    # Average sequence output to obtain a feture of dim 768\n",
    "    out_layer = Lambda(lambda x: tf_math.reduce_mean(x, axis=1))(bert_pre_model.output[0])\n",
    "    \n",
    "    # Prepare model\n",
    "    model = Model(inputs=(input_ids, attention_masks, token_typ_ids), outputs=out_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f83064911e8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7f8302ffa6c0> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f83064911e8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7f8302ffa6c0> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:From /home/nico/.local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    }
   ],
   "source": [
    "# Compute features for train and test sets and save into disk\n",
    "bert_feat_extractor = build_pre_core_BERT_model(tweet_max_length) #TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x7f7f7c5b32b0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_feat_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-dd3399ea3e32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 1926s 8s/step\n",
      "102/102 [==============================] - 828s 8s/step\n"
     ]
    }
   ],
   "source": [
    "X_train_vect = bert_feat_extractor.predict([X_train_tok['input_ids'],\n",
    "                                            X_train_tok['attention_mask'],\n",
    "                                            X_train_tok['token_type_ids']],\n",
    "                                          verbose=True)\n",
    "X_test_vect = bert_feat_extractor.predict([X_test_tok['input_ids'],\n",
    "                                            X_test_tok['attention_mask'],\n",
    "                                            X_test_tok['token_type_ids']],\n",
    "                                          verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save matrix to file\n",
    "np.save('X_train_BERT_avg_hidden_states.npy', X_train_vect)\n",
    "np.save('X_test_BERT_avg_hidden_states.npy', X_test_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction models\n",
    "\n",
    "Let's try non-neuran-network-based models being applying over the extracted features. Again stratified k-fold validation (`K=5`) on the train set will be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load matrix containing BERT tokenization from file\n",
    "X_train_vect = np.load('X_train_BERT_avg_hidden_states.npy')\n",
    "X_test_vect = np.load('X_test_BERT_avg_hidden_states.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grid following parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "svm_params = {\n",
    "        'C': np.arange(0.5, 10, 0.5),\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters {'C': 0.5, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 0.5, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 1.0, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 1.0, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 1.5, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 1.5, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 2.0, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 2.0, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 2.5, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 2.5, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 3.0, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 3.0, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 3.5, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 3.5, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 4.0, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 4.0, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 4.5, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 4.5, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 5.0, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 5.0, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 5.5, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 5.5, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 6.0, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 6.0, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 6.5, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 6.5, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 7.0, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 7.0, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 7.5, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 7.5, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 8.0, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 8.0, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 8.5, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 8.5, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 9.0, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 9.0, 'kernel': 'rbf'}\n",
      "Training with parameters {'C': 9.5, 'kernel': 'linear'}\n",
      "Training with parameters {'C': 9.5, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "smv_lin_rbf_perf_metrics = []\n",
    "\n",
    "for param in ParameterGrid(svm_params):\n",
    "    \n",
    "    meta = {\n",
    "        'kernel': param['kernel'],\n",
    "        'C': param['C'],\n",
    "        'accuracy_fold': [],\n",
    "        'precision_fold': [],\n",
    "        'recall_fold': [],\n",
    "        'specificity_fold': [],\n",
    "        'f1_score_fold': []\n",
    "    }\n",
    "    \n",
    "    print('Training with parameters {}'.format(param))\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X_train_vect, y_train):\n",
    "    \n",
    "        # Split train and test partition\n",
    "        X_train_part = X_train_vect[train_index]\n",
    "        y_train_part = y_train[train_index].values\n",
    "        \n",
    "        X_test_part = X_train_vect[test_index]\n",
    "        y_test_part = y_train[test_index].values\n",
    "        \n",
    "        # Fit SVM model with linear or rbf reknel\n",
    "        svm = SVC(kernel=param['kernel'], C=param['C'])\n",
    "        svm.fit(X_train_part, y_train_part)\n",
    "        \n",
    "        # Predict over test\n",
    "        y_pred = svm.predict(X_test_part)\n",
    "        \n",
    "        # Meassure performance metrics\n",
    "        meta['accuracy_fold'].append(metrics.accuracy_score(y_test_part, y_pred))\n",
    "        meta['precision_fold'].append(metrics.precision_score(y_test_part, y_pred))\n",
    "        meta['recall_fold'].append(metrics.recall_score(y_test_part, y_pred))\n",
    "        #  Exchange class 1 and 0 so measure recall for class 0 (specificity)\n",
    "        meta['specificity_fold'].append(metrics.recall_score(np.abs(y_test_part-1), np.abs(y_pred-1)))\n",
    "        meta['f1_score_fold'].append(metrics.f1_score(y_test_part, y_pred))\n",
    "    \n",
    "    # Compute global average metrics\n",
    "    meta['accuracy_avg'] = np.mean(meta['accuracy_fold'])\n",
    "    meta['accuracy_std'] = np.std(meta['accuracy_fold'])\n",
    "    \n",
    "    meta['precision_avg'] = np.mean(meta['precision_fold'])\n",
    "    meta['precision_std'] = np.std(meta['precision_fold'])\n",
    "    \n",
    "    meta['recall_avg'] = np.mean(meta['recall_fold'])\n",
    "    meta['recall_std'] = np.std(meta['recall_fold'])\n",
    "    \n",
    "    meta['specificity_avg'] = np.mean(meta['specificity_fold'])\n",
    "    meta['specificity_std'] = np.std(meta['specificity_fold'])\n",
    "    \n",
    "    meta['f1_score_avg'] = np.mean(meta['f1_score_fold'])\n",
    "    meta['f1_score_std'] = np.std(meta['f1_score_fold'])\n",
    "    \n",
    "    smv_lin_rbf_perf_metrics.append(meta)\n",
    "\n",
    "smv_lin_rbf_perf_metrics = pd.DataFrame(smv_lin_rbf_perf_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kernel</th>\n",
       "      <th>C</th>\n",
       "      <th>accuracy_fold</th>\n",
       "      <th>precision_fold</th>\n",
       "      <th>recall_fold</th>\n",
       "      <th>specificity_fold</th>\n",
       "      <th>f1_score_fold</th>\n",
       "      <th>accuracy_avg</th>\n",
       "      <th>accuracy_std</th>\n",
       "      <th>precision_avg</th>\n",
       "      <th>precision_std</th>\n",
       "      <th>recall_avg</th>\n",
       "      <th>recall_std</th>\n",
       "      <th>specificity_avg</th>\n",
       "      <th>specificity_std</th>\n",
       "      <th>f1_score_avg</th>\n",
       "      <th>f1_score_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[0.7971109652002626, 0.7852921864740644, 0.790...</td>\n",
       "      <td>[0.8227611940298507, 0.7843478260869565, 0.762...</td>\n",
       "      <td>[0.6732824427480916, 0.6896024464831805, 0.744...</td>\n",
       "      <td>[0.8905529953917051, 0.857307249712313, 0.8250...</td>\n",
       "      <td>[0.7405541561712847, 0.7339300244100895, 0.753...</td>\n",
       "      <td>0.791935</td>\n",
       "      <td>0.019310</td>\n",
       "      <td>0.783503</td>\n",
       "      <td>0.024916</td>\n",
       "      <td>0.713861</td>\n",
       "      <td>0.047422</td>\n",
       "      <td>0.850764</td>\n",
       "      <td>0.022711</td>\n",
       "      <td>0.746076</td>\n",
       "      <td>0.027959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[0.8108995403808273, 0.8010505581089954, 0.804...</td>\n",
       "      <td>[0.8767967145790554, 0.8128342245989305, 0.811...</td>\n",
       "      <td>[0.6519083969465649, 0.6972477064220184, 0.711...</td>\n",
       "      <td>[0.9308755760368663, 0.8791714614499425, 0.875...</td>\n",
       "      <td>[0.7478108581436076, 0.7506172839506174, 0.757...</td>\n",
       "      <td>0.808224</td>\n",
       "      <td>0.018011</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.029108</td>\n",
       "      <td>0.702859</td>\n",
       "      <td>0.042693</td>\n",
       "      <td>0.887614</td>\n",
       "      <td>0.022534</td>\n",
       "      <td>0.758481</td>\n",
       "      <td>0.025862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linear</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.8017071569271176, 0.7852921864740644, 0.785...</td>\n",
       "      <td>[0.818018018018018, 0.7775891341256367, 0.7566...</td>\n",
       "      <td>[0.6931297709923664, 0.7003058103975535, 0.737...</td>\n",
       "      <td>[0.8836405529953917, 0.8492520138089759, 0.821...</td>\n",
       "      <td>[0.7504132231404959, 0.7369267900241352, 0.746...</td>\n",
       "      <td>0.789570</td>\n",
       "      <td>0.019297</td>\n",
       "      <td>0.776260</td>\n",
       "      <td>0.025601</td>\n",
       "      <td>0.717831</td>\n",
       "      <td>0.041624</td>\n",
       "      <td>0.843624</td>\n",
       "      <td>0.022193</td>\n",
       "      <td>0.745196</td>\n",
       "      <td>0.026447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.8181221273801708, 0.7997373604727511, 0.808...</td>\n",
       "      <td>[0.8825910931174089, 0.8132854578096947, 0.809...</td>\n",
       "      <td>[0.665648854961832, 0.6926605504587156, 0.7232...</td>\n",
       "      <td>[0.9331797235023042, 0.8803222094361335, 0.872...</td>\n",
       "      <td>[0.7589208006962574, 0.7481420313790257, 0.764...</td>\n",
       "      <td>0.810062</td>\n",
       "      <td>0.018079</td>\n",
       "      <td>0.826171</td>\n",
       "      <td>0.031223</td>\n",
       "      <td>0.708359</td>\n",
       "      <td>0.041200</td>\n",
       "      <td>0.886693</td>\n",
       "      <td>0.024038</td>\n",
       "      <td>0.761739</td>\n",
       "      <td>0.025167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>linear</td>\n",
       "      <td>1.5</td>\n",
       "      <td>[0.7984241628365069, 0.7741300065659882, 0.780...</td>\n",
       "      <td>[0.8107142857142857, 0.7609427609427609, 0.748...</td>\n",
       "      <td>[0.6931297709923664, 0.691131498470948, 0.7354...</td>\n",
       "      <td>[0.8778801843317973, 0.8365937859608745, 0.813...</td>\n",
       "      <td>[0.7473251028806583, 0.7243589743589745, 0.741...</td>\n",
       "      <td>0.784973</td>\n",
       "      <td>0.018290</td>\n",
       "      <td>0.768598</td>\n",
       "      <td>0.024013</td>\n",
       "      <td>0.715690</td>\n",
       "      <td>0.041543</td>\n",
       "      <td>0.837177</td>\n",
       "      <td>0.021709</td>\n",
       "      <td>0.740485</td>\n",
       "      <td>0.025507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1.5</td>\n",
       "      <td>[0.8174655285620486, 0.7984241628365069, 0.805...</td>\n",
       "      <td>[0.8823529411764706, 0.8114901256732495, 0.801...</td>\n",
       "      <td>[0.6641221374045801, 0.691131498470948, 0.7278...</td>\n",
       "      <td>[0.9331797235023042, 0.8791714614499425, 0.864...</td>\n",
       "      <td>[0.7578397212543554, 0.7464905037159372, 0.762...</td>\n",
       "      <td>0.809537</td>\n",
       "      <td>0.018126</td>\n",
       "      <td>0.824026</td>\n",
       "      <td>0.031879</td>\n",
       "      <td>0.709889</td>\n",
       "      <td>0.043402</td>\n",
       "      <td>0.884621</td>\n",
       "      <td>0.025129</td>\n",
       "      <td>0.761567</td>\n",
       "      <td>0.025532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>linear</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[0.7964543663821405, 0.7708470124753776, 0.776...</td>\n",
       "      <td>[0.8053097345132744, 0.7537437603993344, 0.743...</td>\n",
       "      <td>[0.6946564885496184, 0.6926605504587156, 0.733...</td>\n",
       "      <td>[0.8732718894009217, 0.8296892980437284, 0.808...</td>\n",
       "      <td>[0.7459016393442623, 0.7219123505976095, 0.738...</td>\n",
       "      <td>0.784186</td>\n",
       "      <td>0.018617</td>\n",
       "      <td>0.765954</td>\n",
       "      <td>0.023648</td>\n",
       "      <td>0.717525</td>\n",
       "      <td>0.040325</td>\n",
       "      <td>0.834415</td>\n",
       "      <td>0.021136</td>\n",
       "      <td>0.740314</td>\n",
       "      <td>0.025456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rbf</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[0.8174655285620486, 0.7971109652002626, 0.806...</td>\n",
       "      <td>[0.8777555110220441, 0.8119349005424955, 0.798...</td>\n",
       "      <td>[0.6687022900763359, 0.6865443425076453, 0.733...</td>\n",
       "      <td>[0.9297235023041475, 0.8803222094361335, 0.860...</td>\n",
       "      <td>[0.7590987868284229, 0.743993371996686, 0.7649...</td>\n",
       "      <td>0.810326</td>\n",
       "      <td>0.017547</td>\n",
       "      <td>0.822935</td>\n",
       "      <td>0.029651</td>\n",
       "      <td>0.713557</td>\n",
       "      <td>0.044828</td>\n",
       "      <td>0.883239</td>\n",
       "      <td>0.024205</td>\n",
       "      <td>0.763184</td>\n",
       "      <td>0.025380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>linear</td>\n",
       "      <td>2.5</td>\n",
       "      <td>[0.7951411687458962, 0.767564018384767, 0.7754...</td>\n",
       "      <td>[0.8024691358024691, 0.75, 0.7407407407407407,...</td>\n",
       "      <td>[0.6946564885496184, 0.6880733944954128, 0.733...</td>\n",
       "      <td>[0.8709677419354839, 0.8273878020713463, 0.806...</td>\n",
       "      <td>[0.7446808510638299, 0.7177033492822965, 0.737...</td>\n",
       "      <td>0.781427</td>\n",
       "      <td>0.018339</td>\n",
       "      <td>0.762116</td>\n",
       "      <td>0.023827</td>\n",
       "      <td>0.715078</td>\n",
       "      <td>0.039085</td>\n",
       "      <td>0.831421</td>\n",
       "      <td>0.021328</td>\n",
       "      <td>0.737242</td>\n",
       "      <td>0.024934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rbf</td>\n",
       "      <td>2.5</td>\n",
       "      <td>[0.8161523309258043, 0.7931713722915299, 0.804...</td>\n",
       "      <td>[0.8787878787878788, 0.8065099457504521, 0.795...</td>\n",
       "      <td>[0.6641221374045801, 0.6819571865443425, 0.732...</td>\n",
       "      <td>[0.9308755760368663, 0.8768699654775605, 0.858...</td>\n",
       "      <td>[0.7565217391304349, 0.7390223695111846, 0.762...</td>\n",
       "      <td>0.808618</td>\n",
       "      <td>0.018446</td>\n",
       "      <td>0.820476</td>\n",
       "      <td>0.031349</td>\n",
       "      <td>0.712029</td>\n",
       "      <td>0.047258</td>\n",
       "      <td>0.881397</td>\n",
       "      <td>0.025572</td>\n",
       "      <td>0.761111</td>\n",
       "      <td>0.026677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>linear</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[0.7944845699277742, 0.768220617202889, 0.7780...</td>\n",
       "      <td>[0.8021201413427562, 0.7504159733777038, 0.742...</td>\n",
       "      <td>[0.6931297709923664, 0.6896024464831805, 0.740...</td>\n",
       "      <td>[0.8709677419354839, 0.8273878020713463, 0.806...</td>\n",
       "      <td>[0.7436527436527437, 0.7187250996015937, 0.741...</td>\n",
       "      <td>0.782347</td>\n",
       "      <td>0.017883</td>\n",
       "      <td>0.762702</td>\n",
       "      <td>0.023765</td>\n",
       "      <td>0.717219</td>\n",
       "      <td>0.038001</td>\n",
       "      <td>0.831421</td>\n",
       "      <td>0.021526</td>\n",
       "      <td>0.738664</td>\n",
       "      <td>0.024132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rbf</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[0.8181221273801708, 0.7918581746552856, 0.803...</td>\n",
       "      <td>[0.8810483870967742, 0.8025134649910234, 0.794...</td>\n",
       "      <td>[0.667175572519084, 0.6834862385321101, 0.7324...</td>\n",
       "      <td>[0.9320276497695853, 0.8734177215189873, 0.857...</td>\n",
       "      <td>[0.7593397046046917, 0.7382328654004954, 0.762...</td>\n",
       "      <td>0.808224</td>\n",
       "      <td>0.018943</td>\n",
       "      <td>0.818631</td>\n",
       "      <td>0.033112</td>\n",
       "      <td>0.713557</td>\n",
       "      <td>0.047538</td>\n",
       "      <td>0.879555</td>\n",
       "      <td>0.026833</td>\n",
       "      <td>0.761145</td>\n",
       "      <td>0.026946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>linear</td>\n",
       "      <td>3.5</td>\n",
       "      <td>[0.7925147734734077, 0.767564018384767, 0.7780...</td>\n",
       "      <td>[0.7968476357267951, 0.7483443708609272, 0.742...</td>\n",
       "      <td>[0.6946564885496184, 0.691131498470948, 0.7400...</td>\n",
       "      <td>[0.8663594470046083, 0.8250863060989643, 0.806...</td>\n",
       "      <td>[0.7422512234910277, 0.7186009538950715, 0.741...</td>\n",
       "      <td>0.780770</td>\n",
       "      <td>0.018679</td>\n",
       "      <td>0.759588</td>\n",
       "      <td>0.023218</td>\n",
       "      <td>0.717219</td>\n",
       "      <td>0.039013</td>\n",
       "      <td>0.828656</td>\n",
       "      <td>0.020430</td>\n",
       "      <td>0.737240</td>\n",
       "      <td>0.025322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rbf</td>\n",
       "      <td>3.5</td>\n",
       "      <td>[0.8200919238345371, 0.7931713722915299, 0.799...</td>\n",
       "      <td>[0.8832997987927566, 0.8043087971274686, 0.787...</td>\n",
       "      <td>[0.6702290076335878, 0.6850152905198776, 0.730...</td>\n",
       "      <td>[0.9331797235023042, 0.8745684695051784, 0.851...</td>\n",
       "      <td>[0.7621527777777778, 0.7398843930635839, 0.758...</td>\n",
       "      <td>0.808093</td>\n",
       "      <td>0.018867</td>\n",
       "      <td>0.818286</td>\n",
       "      <td>0.034879</td>\n",
       "      <td>0.713862</td>\n",
       "      <td>0.045369</td>\n",
       "      <td>0.879095</td>\n",
       "      <td>0.028210</td>\n",
       "      <td>0.761200</td>\n",
       "      <td>0.026047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>linear</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[0.7905449770190414, 0.7669074195666448, 0.776...</td>\n",
       "      <td>[0.7916666666666666, 0.7479270315091211, 0.737...</td>\n",
       "      <td>[0.6961832061068702, 0.6896024464831805, 0.743...</td>\n",
       "      <td>[0.8617511520737328, 0.8250863060989643, 0.800...</td>\n",
       "      <td>[0.7408610885458977, 0.7175815433571997, 0.740...</td>\n",
       "      <td>0.779720</td>\n",
       "      <td>0.020290</td>\n",
       "      <td>0.756916</td>\n",
       "      <td>0.024601</td>\n",
       "      <td>0.718442</td>\n",
       "      <td>0.039625</td>\n",
       "      <td>0.825893</td>\n",
       "      <td>0.021230</td>\n",
       "      <td>0.736655</td>\n",
       "      <td>0.026894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rbf</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[0.8194353250164149, 0.7905449770190414, 0.798...</td>\n",
       "      <td>[0.8784860557768924, 0.7985739750445633, 0.784...</td>\n",
       "      <td>[0.6732824427480916, 0.6850152905198776, 0.730...</td>\n",
       "      <td>[0.9297235023041475, 0.8699654775604143, 0.849...</td>\n",
       "      <td>[0.7623163353500432, 0.7374485596707819, 0.756...</td>\n",
       "      <td>0.806779</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.814436</td>\n",
       "      <td>0.034511</td>\n",
       "      <td>0.715085</td>\n",
       "      <td>0.045169</td>\n",
       "      <td>0.875871</td>\n",
       "      <td>0.027977</td>\n",
       "      <td>0.760285</td>\n",
       "      <td>0.026510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>linear</td>\n",
       "      <td>4.5</td>\n",
       "      <td>[0.7879185817465528, 0.767564018384767, 0.7747...</td>\n",
       "      <td>[0.789198606271777, 0.7483443708609272, 0.7359...</td>\n",
       "      <td>[0.6916030534351145, 0.691131498470948, 0.7415...</td>\n",
       "      <td>[0.8605990783410138, 0.8250863060989643, 0.799...</td>\n",
       "      <td>[0.7371847030105778, 0.7186009538950715, 0.738...</td>\n",
       "      <td>0.778406</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.755337</td>\n",
       "      <td>0.023525</td>\n",
       "      <td>0.716914</td>\n",
       "      <td>0.039570</td>\n",
       "      <td>0.824741</td>\n",
       "      <td>0.020917</td>\n",
       "      <td>0.735071</td>\n",
       "      <td>0.025970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rbf</td>\n",
       "      <td>4.5</td>\n",
       "      <td>[0.8194353250164149, 0.788575180564675, 0.7977...</td>\n",
       "      <td>[0.876984126984127, 0.7964285714285714, 0.7836...</td>\n",
       "      <td>[0.6748091603053435, 0.6819571865443425, 0.730...</td>\n",
       "      <td>[0.9285714285714286, 0.8688147295742232, 0.848...</td>\n",
       "      <td>[0.7627264883520276, 0.7347611202635913, 0.756...</td>\n",
       "      <td>0.806517</td>\n",
       "      <td>0.020207</td>\n",
       "      <td>0.813361</td>\n",
       "      <td>0.034173</td>\n",
       "      <td>0.715696</td>\n",
       "      <td>0.047713</td>\n",
       "      <td>0.874950</td>\n",
       "      <td>0.027797</td>\n",
       "      <td>0.760110</td>\n",
       "      <td>0.027979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>linear</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[0.7846355876559422, 0.7688772160210111, 0.774...</td>\n",
       "      <td>[0.7863397548161121, 0.7508305647840532, 0.734...</td>\n",
       "      <td>[0.6854961832061068, 0.691131498470948, 0.7415...</td>\n",
       "      <td>[0.8594470046082949, 0.8273878020713463, 0.798...</td>\n",
       "      <td>[0.732463295269168, 0.7197452229299363, 0.7382...</td>\n",
       "      <td>0.777355</td>\n",
       "      <td>0.019659</td>\n",
       "      <td>0.754404</td>\n",
       "      <td>0.023952</td>\n",
       "      <td>0.715081</td>\n",
       "      <td>0.040205</td>\n",
       "      <td>0.824280</td>\n",
       "      <td>0.021474</td>\n",
       "      <td>0.733638</td>\n",
       "      <td>0.026391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rbf</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[0.8168089297439265, 0.7866053841103086, 0.795...</td>\n",
       "      <td>[0.873015873015873, 0.7932263814616756, 0.7797...</td>\n",
       "      <td>[0.6717557251908397, 0.6804281345565749, 0.730...</td>\n",
       "      <td>[0.9262672811059908, 0.8665132336018412, 0.844...</td>\n",
       "      <td>[0.7592752372735115, 0.7325102880658435, 0.754...</td>\n",
       "      <td>0.804021</td>\n",
       "      <td>0.020513</td>\n",
       "      <td>0.809151</td>\n",
       "      <td>0.034051</td>\n",
       "      <td>0.714168</td>\n",
       "      <td>0.050304</td>\n",
       "      <td>0.871726</td>\n",
       "      <td>0.028232</td>\n",
       "      <td>0.757284</td>\n",
       "      <td>0.028868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>linear</td>\n",
       "      <td>5.5</td>\n",
       "      <td>[0.7839789888378201, 0.767564018384767, 0.7734...</td>\n",
       "      <td>[0.784965034965035, 0.7467105263157895, 0.7358...</td>\n",
       "      <td>[0.6854961832061068, 0.6941896024464832, 0.737...</td>\n",
       "      <td>[0.8582949308755761, 0.8227848101265823, 0.800...</td>\n",
       "      <td>[0.7318663406682967, 0.7194928684627576, 0.736...</td>\n",
       "      <td>0.775779</td>\n",
       "      <td>0.019648</td>\n",
       "      <td>0.752129</td>\n",
       "      <td>0.024433</td>\n",
       "      <td>0.713858</td>\n",
       "      <td>0.037759</td>\n",
       "      <td>0.822437</td>\n",
       "      <td>0.021314</td>\n",
       "      <td>0.731997</td>\n",
       "      <td>0.025796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rbf</td>\n",
       "      <td>5.5</td>\n",
       "      <td>[0.8168089297439265, 0.7866053841103086, 0.795...</td>\n",
       "      <td>[0.873015873015873, 0.7921847246891652, 0.7788...</td>\n",
       "      <td>[0.6717557251908397, 0.6819571865443425, 0.732...</td>\n",
       "      <td>[0.9262672811059908, 0.8653624856156502, 0.843...</td>\n",
       "      <td>[0.7592752372735115, 0.7329498767460969, 0.754...</td>\n",
       "      <td>0.803890</td>\n",
       "      <td>0.020298</td>\n",
       "      <td>0.808518</td>\n",
       "      <td>0.034111</td>\n",
       "      <td>0.714779</td>\n",
       "      <td>0.050964</td>\n",
       "      <td>0.871035</td>\n",
       "      <td>0.028581</td>\n",
       "      <td>0.757295</td>\n",
       "      <td>0.028750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>linear</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[0.783322390019698, 0.7662508207485227, 0.7721...</td>\n",
       "      <td>[0.7835951134380453, 0.7450657894736842, 0.733...</td>\n",
       "      <td>[0.6854961832061068, 0.6926605504587156, 0.737...</td>\n",
       "      <td>[0.8571428571428571, 0.8216340621403913, 0.798...</td>\n",
       "      <td>[0.7312703583061889, 0.7179080824088748, 0.735...</td>\n",
       "      <td>0.776436</td>\n",
       "      <td>0.019141</td>\n",
       "      <td>0.753466</td>\n",
       "      <td>0.023474</td>\n",
       "      <td>0.713552</td>\n",
       "      <td>0.037113</td>\n",
       "      <td>0.823820</td>\n",
       "      <td>0.020586</td>\n",
       "      <td>0.732491</td>\n",
       "      <td>0.025295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rbf</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[0.8161523309258043, 0.7859487852921865, 0.795...</td>\n",
       "      <td>[0.8727634194831014, 0.7907801418439716, 0.777...</td>\n",
       "      <td>[0.6702290076335878, 0.6819571865443425, 0.733...</td>\n",
       "      <td>[0.9262672811059908, 0.8642117376294591, 0.842...</td>\n",
       "      <td>[0.758203799654577, 0.7323481116584564, 0.7553...</td>\n",
       "      <td>0.803364</td>\n",
       "      <td>0.019899</td>\n",
       "      <td>0.807507</td>\n",
       "      <td>0.034263</td>\n",
       "      <td>0.714780</td>\n",
       "      <td>0.051336</td>\n",
       "      <td>0.870114</td>\n",
       "      <td>0.029044</td>\n",
       "      <td>0.756789</td>\n",
       "      <td>0.028382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>linear</td>\n",
       "      <td>6.5</td>\n",
       "      <td>[0.7820091923834537, 0.7655942219304005, 0.770...</td>\n",
       "      <td>[0.7779690189328744, 0.7438423645320197, 0.733...</td>\n",
       "      <td>[0.6900763358778625, 0.6926605504587156, 0.732...</td>\n",
       "      <td>[0.8513824884792627, 0.8204833141542003, 0.799...</td>\n",
       "      <td>[0.7313915857605178, 0.7173396674584323, 0.732...</td>\n",
       "      <td>0.775517</td>\n",
       "      <td>0.019913</td>\n",
       "      <td>0.751408</td>\n",
       "      <td>0.022754</td>\n",
       "      <td>0.713856</td>\n",
       "      <td>0.037298</td>\n",
       "      <td>0.821977</td>\n",
       "      <td>0.018754</td>\n",
       "      <td>0.731762</td>\n",
       "      <td>0.026305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rbf</td>\n",
       "      <td>6.5</td>\n",
       "      <td>[0.8161523309258043, 0.7859487852921865, 0.793...</td>\n",
       "      <td>[0.874251497005988, 0.791814946619217, 0.77419...</td>\n",
       "      <td>[0.6687022900763359, 0.6804281345565749, 0.733...</td>\n",
       "      <td>[0.9274193548387096, 0.8653624856156502, 0.838...</td>\n",
       "      <td>[0.7577854671280275, 0.7319078947368419, 0.753...</td>\n",
       "      <td>0.802970</td>\n",
       "      <td>0.020409</td>\n",
       "      <td>0.807040</td>\n",
       "      <td>0.035583</td>\n",
       "      <td>0.714474</td>\n",
       "      <td>0.052350</td>\n",
       "      <td>0.869653</td>\n",
       "      <td>0.030206</td>\n",
       "      <td>0.756325</td>\n",
       "      <td>0.028927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>linear</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[0.7800393959290873, 0.7655942219304005, 0.770...</td>\n",
       "      <td>[0.7758620689655172, 0.7438423645320197, 0.733...</td>\n",
       "      <td>[0.6870229007633588, 0.6926605504587156, 0.732...</td>\n",
       "      <td>[0.8502304147465438, 0.8204833141542003, 0.799...</td>\n",
       "      <td>[0.7287449392712549, 0.7173396674584323, 0.732...</td>\n",
       "      <td>0.774991</td>\n",
       "      <td>0.018780</td>\n",
       "      <td>0.750603</td>\n",
       "      <td>0.021027</td>\n",
       "      <td>0.713551</td>\n",
       "      <td>0.037406</td>\n",
       "      <td>0.821286</td>\n",
       "      <td>0.017826</td>\n",
       "      <td>0.731193</td>\n",
       "      <td>0.025310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>rbf</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[0.8154957321076822, 0.7852921864740644, 0.792...</td>\n",
       "      <td>[0.8725099601593626, 0.7914438502673797, 0.772...</td>\n",
       "      <td>[0.6687022900763359, 0.6788990825688074, 0.732...</td>\n",
       "      <td>[0.9262672811059908, 0.8653624856156502, 0.837...</td>\n",
       "      <td>[0.7571305099394987, 0.7308641975308643, 0.751...</td>\n",
       "      <td>0.802445</td>\n",
       "      <td>0.021251</td>\n",
       "      <td>0.806049</td>\n",
       "      <td>0.035655</td>\n",
       "      <td>0.714169</td>\n",
       "      <td>0.052995</td>\n",
       "      <td>0.868962</td>\n",
       "      <td>0.030083</td>\n",
       "      <td>0.755739</td>\n",
       "      <td>0.029919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>linear</td>\n",
       "      <td>7.5</td>\n",
       "      <td>[0.7793827971109653, 0.762967826657912, 0.7721...</td>\n",
       "      <td>[0.7754749568221071, 0.7397708674304418, 0.735...</td>\n",
       "      <td>[0.6854961832061068, 0.691131498470948, 0.7324...</td>\n",
       "      <td>[0.8502304147465438, 0.8170310701956272, 0.802...</td>\n",
       "      <td>[0.7277147487844409, 0.7146245059288537, 0.734...</td>\n",
       "      <td>0.773940</td>\n",
       "      <td>0.018651</td>\n",
       "      <td>0.749171</td>\n",
       "      <td>0.021268</td>\n",
       "      <td>0.712634</td>\n",
       "      <td>0.036459</td>\n",
       "      <td>0.820134</td>\n",
       "      <td>0.017904</td>\n",
       "      <td>0.730050</td>\n",
       "      <td>0.024951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>rbf</td>\n",
       "      <td>7.5</td>\n",
       "      <td>[0.8135259356533159, 0.7839789888378201, 0.791...</td>\n",
       "      <td>[0.863013698630137, 0.7886323268206039, 0.7718...</td>\n",
       "      <td>[0.6732824427480916, 0.6788990825688074, 0.729...</td>\n",
       "      <td>[0.9193548387096774, 0.8630609896432682, 0.837...</td>\n",
       "      <td>[0.7564322469982847, 0.7296631059983566, 0.749...</td>\n",
       "      <td>0.801657</td>\n",
       "      <td>0.021581</td>\n",
       "      <td>0.803296</td>\n",
       "      <td>0.032864</td>\n",
       "      <td>0.715085</td>\n",
       "      <td>0.051660</td>\n",
       "      <td>0.866889</td>\n",
       "      <td>0.027631</td>\n",
       "      <td>0.755268</td>\n",
       "      <td>0.030243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>linear</td>\n",
       "      <td>8.0</td>\n",
       "      <td>[0.7813525935653316, 0.762967826657912, 0.7728...</td>\n",
       "      <td>[0.7785467128027682, 0.7397708674304418, 0.736...</td>\n",
       "      <td>[0.6870229007633588, 0.691131498470948, 0.7339...</td>\n",
       "      <td>[0.8525345622119815, 0.8170310701956272, 0.802...</td>\n",
       "      <td>[0.7299270072992701, 0.7146245059288537, 0.735...</td>\n",
       "      <td>0.773940</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.749381</td>\n",
       "      <td>0.021385</td>\n",
       "      <td>0.712328</td>\n",
       "      <td>0.036482</td>\n",
       "      <td>0.820364</td>\n",
       "      <td>0.018308</td>\n",
       "      <td>0.729962</td>\n",
       "      <td>0.024607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>rbf</td>\n",
       "      <td>8.0</td>\n",
       "      <td>[0.8128693368351937, 0.7852921864740644, 0.790...</td>\n",
       "      <td>[0.861328125, 0.7904085257548845, 0.7714748784...</td>\n",
       "      <td>[0.6732824427480916, 0.6804281345565749, 0.727...</td>\n",
       "      <td>[0.9182027649769585, 0.8642117376294591, 0.837...</td>\n",
       "      <td>[0.7557840616966581, 0.7313064913722267, 0.749...</td>\n",
       "      <td>0.800869</td>\n",
       "      <td>0.020733</td>\n",
       "      <td>0.801894</td>\n",
       "      <td>0.032369</td>\n",
       "      <td>0.714779</td>\n",
       "      <td>0.050802</td>\n",
       "      <td>0.865737</td>\n",
       "      <td>0.027597</td>\n",
       "      <td>0.754479</td>\n",
       "      <td>0.029185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>linear</td>\n",
       "      <td>8.5</td>\n",
       "      <td>[0.7787261982928431, 0.7623112278397899, 0.773...</td>\n",
       "      <td>[0.7731958762886598, 0.7401315789473685, 0.735...</td>\n",
       "      <td>[0.6870229007633588, 0.6880733944954128, 0.737...</td>\n",
       "      <td>[0.847926267281106, 0.8181818181818182, 0.8009...</td>\n",
       "      <td>[0.7275666936135814, 0.7131537242472267, 0.736...</td>\n",
       "      <td>0.772889</td>\n",
       "      <td>0.018517</td>\n",
       "      <td>0.747667</td>\n",
       "      <td>0.020301</td>\n",
       "      <td>0.711717</td>\n",
       "      <td>0.037858</td>\n",
       "      <td>0.818982</td>\n",
       "      <td>0.017059</td>\n",
       "      <td>0.728831</td>\n",
       "      <td>0.025351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>rbf</td>\n",
       "      <td>8.5</td>\n",
       "      <td>[0.8115561391989494, 0.7852921864740644, 0.789...</td>\n",
       "      <td>[0.859375, 0.7893805309734513, 0.7702265372168...</td>\n",
       "      <td>[0.6717557251908397, 0.6819571865443425, 0.727...</td>\n",
       "      <td>[0.9170506912442397, 0.8630609896432682, 0.836...</td>\n",
       "      <td>[0.7540702656383891, 0.7317473338802296, 0.748...</td>\n",
       "      <td>0.800738</td>\n",
       "      <td>0.020396</td>\n",
       "      <td>0.801208</td>\n",
       "      <td>0.031672</td>\n",
       "      <td>0.715391</td>\n",
       "      <td>0.051236</td>\n",
       "      <td>0.865046</td>\n",
       "      <td>0.027390</td>\n",
       "      <td>0.754491</td>\n",
       "      <td>0.028949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>linear</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[0.778069599474721, 0.7603414313854235, 0.7741...</td>\n",
       "      <td>[0.7718696397941681, 0.7364975450081833, 0.737...</td>\n",
       "      <td>[0.6870229007633588, 0.6880733944954128, 0.737...</td>\n",
       "      <td>[0.8467741935483871, 0.8147295742232451, 0.802...</td>\n",
       "      <td>[0.7269789983844911, 0.7114624505928854, 0.737...</td>\n",
       "      <td>0.773021</td>\n",
       "      <td>0.018805</td>\n",
       "      <td>0.747367</td>\n",
       "      <td>0.019899</td>\n",
       "      <td>0.712634</td>\n",
       "      <td>0.038558</td>\n",
       "      <td>0.818521</td>\n",
       "      <td>0.016311</td>\n",
       "      <td>0.729178</td>\n",
       "      <td>0.025843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>rbf</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[0.8095863427445831, 0.7859487852921865, 0.789...</td>\n",
       "      <td>[0.8585461689587426, 0.7907801418439716, 0.770...</td>\n",
       "      <td>[0.667175572519084, 0.6819571865443425, 0.7247...</td>\n",
       "      <td>[0.9170506912442397, 0.8642117376294591, 0.837...</td>\n",
       "      <td>[0.7508591065292097, 0.7323481116584564, 0.747...</td>\n",
       "      <td>0.800344</td>\n",
       "      <td>0.019121</td>\n",
       "      <td>0.801307</td>\n",
       "      <td>0.030642</td>\n",
       "      <td>0.714169</td>\n",
       "      <td>0.051733</td>\n",
       "      <td>0.865276</td>\n",
       "      <td>0.027273</td>\n",
       "      <td>0.753767</td>\n",
       "      <td>0.027877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>linear</td>\n",
       "      <td>9.5</td>\n",
       "      <td>[0.7767564018384767, 0.7623112278397899, 0.772...</td>\n",
       "      <td>[0.7701543739279588, 0.738562091503268, 0.7343...</td>\n",
       "      <td>[0.6854961832061068, 0.691131498470948, 0.7354...</td>\n",
       "      <td>[0.8456221198156681, 0.8158803222094362, 0.799...</td>\n",
       "      <td>[0.7253634894991923, 0.7140600315955767, 0.734...</td>\n",
       "      <td>0.773415</td>\n",
       "      <td>0.019622</td>\n",
       "      <td>0.747914</td>\n",
       "      <td>0.020828</td>\n",
       "      <td>0.712940</td>\n",
       "      <td>0.038727</td>\n",
       "      <td>0.818983</td>\n",
       "      <td>0.016846</td>\n",
       "      <td>0.729615</td>\n",
       "      <td>0.026591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>rbf</td>\n",
       "      <td>9.5</td>\n",
       "      <td>[0.8082731451083388, 0.783322390019698, 0.7892...</td>\n",
       "      <td>[0.8565815324165029, 0.7903225806451613, 0.770...</td>\n",
       "      <td>[0.665648854961832, 0.6743119266055045, 0.7247...</td>\n",
       "      <td>[0.9158986175115207, 0.8653624856156502, 0.837...</td>\n",
       "      <td>[0.7491408934707904, 0.7277227722772278, 0.747...</td>\n",
       "      <td>0.799293</td>\n",
       "      <td>0.018955</td>\n",
       "      <td>0.800352</td>\n",
       "      <td>0.029778</td>\n",
       "      <td>0.712335</td>\n",
       "      <td>0.053749</td>\n",
       "      <td>0.864815</td>\n",
       "      <td>0.027154</td>\n",
       "      <td>0.752224</td>\n",
       "      <td>0.028379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    kernel    C                                      accuracy_fold  \\\n",
       "0   linear  0.5  [0.7971109652002626, 0.7852921864740644, 0.790...   \n",
       "1      rbf  0.5  [0.8108995403808273, 0.8010505581089954, 0.804...   \n",
       "2   linear  1.0  [0.8017071569271176, 0.7852921864740644, 0.785...   \n",
       "3      rbf  1.0  [0.8181221273801708, 0.7997373604727511, 0.808...   \n",
       "4   linear  1.5  [0.7984241628365069, 0.7741300065659882, 0.780...   \n",
       "5      rbf  1.5  [0.8174655285620486, 0.7984241628365069, 0.805...   \n",
       "6   linear  2.0  [0.7964543663821405, 0.7708470124753776, 0.776...   \n",
       "7      rbf  2.0  [0.8174655285620486, 0.7971109652002626, 0.806...   \n",
       "8   linear  2.5  [0.7951411687458962, 0.767564018384767, 0.7754...   \n",
       "9      rbf  2.5  [0.8161523309258043, 0.7931713722915299, 0.804...   \n",
       "10  linear  3.0  [0.7944845699277742, 0.768220617202889, 0.7780...   \n",
       "11     rbf  3.0  [0.8181221273801708, 0.7918581746552856, 0.803...   \n",
       "12  linear  3.5  [0.7925147734734077, 0.767564018384767, 0.7780...   \n",
       "13     rbf  3.5  [0.8200919238345371, 0.7931713722915299, 0.799...   \n",
       "14  linear  4.0  [0.7905449770190414, 0.7669074195666448, 0.776...   \n",
       "15     rbf  4.0  [0.8194353250164149, 0.7905449770190414, 0.798...   \n",
       "16  linear  4.5  [0.7879185817465528, 0.767564018384767, 0.7747...   \n",
       "17     rbf  4.5  [0.8194353250164149, 0.788575180564675, 0.7977...   \n",
       "18  linear  5.0  [0.7846355876559422, 0.7688772160210111, 0.774...   \n",
       "19     rbf  5.0  [0.8168089297439265, 0.7866053841103086, 0.795...   \n",
       "20  linear  5.5  [0.7839789888378201, 0.767564018384767, 0.7734...   \n",
       "21     rbf  5.5  [0.8168089297439265, 0.7866053841103086, 0.795...   \n",
       "22  linear  6.0  [0.783322390019698, 0.7662508207485227, 0.7721...   \n",
       "23     rbf  6.0  [0.8161523309258043, 0.7859487852921865, 0.795...   \n",
       "24  linear  6.5  [0.7820091923834537, 0.7655942219304005, 0.770...   \n",
       "25     rbf  6.5  [0.8161523309258043, 0.7859487852921865, 0.793...   \n",
       "26  linear  7.0  [0.7800393959290873, 0.7655942219304005, 0.770...   \n",
       "27     rbf  7.0  [0.8154957321076822, 0.7852921864740644, 0.792...   \n",
       "28  linear  7.5  [0.7793827971109653, 0.762967826657912, 0.7721...   \n",
       "29     rbf  7.5  [0.8135259356533159, 0.7839789888378201, 0.791...   \n",
       "30  linear  8.0  [0.7813525935653316, 0.762967826657912, 0.7728...   \n",
       "31     rbf  8.0  [0.8128693368351937, 0.7852921864740644, 0.790...   \n",
       "32  linear  8.5  [0.7787261982928431, 0.7623112278397899, 0.773...   \n",
       "33     rbf  8.5  [0.8115561391989494, 0.7852921864740644, 0.789...   \n",
       "34  linear  9.0  [0.778069599474721, 0.7603414313854235, 0.7741...   \n",
       "35     rbf  9.0  [0.8095863427445831, 0.7859487852921865, 0.789...   \n",
       "36  linear  9.5  [0.7767564018384767, 0.7623112278397899, 0.772...   \n",
       "37     rbf  9.5  [0.8082731451083388, 0.783322390019698, 0.7892...   \n",
       "\n",
       "                                       precision_fold  \\\n",
       "0   [0.8227611940298507, 0.7843478260869565, 0.762...   \n",
       "1   [0.8767967145790554, 0.8128342245989305, 0.811...   \n",
       "2   [0.818018018018018, 0.7775891341256367, 0.7566...   \n",
       "3   [0.8825910931174089, 0.8132854578096947, 0.809...   \n",
       "4   [0.8107142857142857, 0.7609427609427609, 0.748...   \n",
       "5   [0.8823529411764706, 0.8114901256732495, 0.801...   \n",
       "6   [0.8053097345132744, 0.7537437603993344, 0.743...   \n",
       "7   [0.8777555110220441, 0.8119349005424955, 0.798...   \n",
       "8   [0.8024691358024691, 0.75, 0.7407407407407407,...   \n",
       "9   [0.8787878787878788, 0.8065099457504521, 0.795...   \n",
       "10  [0.8021201413427562, 0.7504159733777038, 0.742...   \n",
       "11  [0.8810483870967742, 0.8025134649910234, 0.794...   \n",
       "12  [0.7968476357267951, 0.7483443708609272, 0.742...   \n",
       "13  [0.8832997987927566, 0.8043087971274686, 0.787...   \n",
       "14  [0.7916666666666666, 0.7479270315091211, 0.737...   \n",
       "15  [0.8784860557768924, 0.7985739750445633, 0.784...   \n",
       "16  [0.789198606271777, 0.7483443708609272, 0.7359...   \n",
       "17  [0.876984126984127, 0.7964285714285714, 0.7836...   \n",
       "18  [0.7863397548161121, 0.7508305647840532, 0.734...   \n",
       "19  [0.873015873015873, 0.7932263814616756, 0.7797...   \n",
       "20  [0.784965034965035, 0.7467105263157895, 0.7358...   \n",
       "21  [0.873015873015873, 0.7921847246891652, 0.7788...   \n",
       "22  [0.7835951134380453, 0.7450657894736842, 0.733...   \n",
       "23  [0.8727634194831014, 0.7907801418439716, 0.777...   \n",
       "24  [0.7779690189328744, 0.7438423645320197, 0.733...   \n",
       "25  [0.874251497005988, 0.791814946619217, 0.77419...   \n",
       "26  [0.7758620689655172, 0.7438423645320197, 0.733...   \n",
       "27  [0.8725099601593626, 0.7914438502673797, 0.772...   \n",
       "28  [0.7754749568221071, 0.7397708674304418, 0.735...   \n",
       "29  [0.863013698630137, 0.7886323268206039, 0.7718...   \n",
       "30  [0.7785467128027682, 0.7397708674304418, 0.736...   \n",
       "31  [0.861328125, 0.7904085257548845, 0.7714748784...   \n",
       "32  [0.7731958762886598, 0.7401315789473685, 0.735...   \n",
       "33  [0.859375, 0.7893805309734513, 0.7702265372168...   \n",
       "34  [0.7718696397941681, 0.7364975450081833, 0.737...   \n",
       "35  [0.8585461689587426, 0.7907801418439716, 0.770...   \n",
       "36  [0.7701543739279588, 0.738562091503268, 0.7343...   \n",
       "37  [0.8565815324165029, 0.7903225806451613, 0.770...   \n",
       "\n",
       "                                          recall_fold  \\\n",
       "0   [0.6732824427480916, 0.6896024464831805, 0.744...   \n",
       "1   [0.6519083969465649, 0.6972477064220184, 0.711...   \n",
       "2   [0.6931297709923664, 0.7003058103975535, 0.737...   \n",
       "3   [0.665648854961832, 0.6926605504587156, 0.7232...   \n",
       "4   [0.6931297709923664, 0.691131498470948, 0.7354...   \n",
       "5   [0.6641221374045801, 0.691131498470948, 0.7278...   \n",
       "6   [0.6946564885496184, 0.6926605504587156, 0.733...   \n",
       "7   [0.6687022900763359, 0.6865443425076453, 0.733...   \n",
       "8   [0.6946564885496184, 0.6880733944954128, 0.733...   \n",
       "9   [0.6641221374045801, 0.6819571865443425, 0.732...   \n",
       "10  [0.6931297709923664, 0.6896024464831805, 0.740...   \n",
       "11  [0.667175572519084, 0.6834862385321101, 0.7324...   \n",
       "12  [0.6946564885496184, 0.691131498470948, 0.7400...   \n",
       "13  [0.6702290076335878, 0.6850152905198776, 0.730...   \n",
       "14  [0.6961832061068702, 0.6896024464831805, 0.743...   \n",
       "15  [0.6732824427480916, 0.6850152905198776, 0.730...   \n",
       "16  [0.6916030534351145, 0.691131498470948, 0.7415...   \n",
       "17  [0.6748091603053435, 0.6819571865443425, 0.730...   \n",
       "18  [0.6854961832061068, 0.691131498470948, 0.7415...   \n",
       "19  [0.6717557251908397, 0.6804281345565749, 0.730...   \n",
       "20  [0.6854961832061068, 0.6941896024464832, 0.737...   \n",
       "21  [0.6717557251908397, 0.6819571865443425, 0.732...   \n",
       "22  [0.6854961832061068, 0.6926605504587156, 0.737...   \n",
       "23  [0.6702290076335878, 0.6819571865443425, 0.733...   \n",
       "24  [0.6900763358778625, 0.6926605504587156, 0.732...   \n",
       "25  [0.6687022900763359, 0.6804281345565749, 0.733...   \n",
       "26  [0.6870229007633588, 0.6926605504587156, 0.732...   \n",
       "27  [0.6687022900763359, 0.6788990825688074, 0.732...   \n",
       "28  [0.6854961832061068, 0.691131498470948, 0.7324...   \n",
       "29  [0.6732824427480916, 0.6788990825688074, 0.729...   \n",
       "30  [0.6870229007633588, 0.691131498470948, 0.7339...   \n",
       "31  [0.6732824427480916, 0.6804281345565749, 0.727...   \n",
       "32  [0.6870229007633588, 0.6880733944954128, 0.737...   \n",
       "33  [0.6717557251908397, 0.6819571865443425, 0.727...   \n",
       "34  [0.6870229007633588, 0.6880733944954128, 0.737...   \n",
       "35  [0.667175572519084, 0.6819571865443425, 0.7247...   \n",
       "36  [0.6854961832061068, 0.691131498470948, 0.7354...   \n",
       "37  [0.665648854961832, 0.6743119266055045, 0.7247...   \n",
       "\n",
       "                                     specificity_fold  \\\n",
       "0   [0.8905529953917051, 0.857307249712313, 0.8250...   \n",
       "1   [0.9308755760368663, 0.8791714614499425, 0.875...   \n",
       "2   [0.8836405529953917, 0.8492520138089759, 0.821...   \n",
       "3   [0.9331797235023042, 0.8803222094361335, 0.872...   \n",
       "4   [0.8778801843317973, 0.8365937859608745, 0.813...   \n",
       "5   [0.9331797235023042, 0.8791714614499425, 0.864...   \n",
       "6   [0.8732718894009217, 0.8296892980437284, 0.808...   \n",
       "7   [0.9297235023041475, 0.8803222094361335, 0.860...   \n",
       "8   [0.8709677419354839, 0.8273878020713463, 0.806...   \n",
       "9   [0.9308755760368663, 0.8768699654775605, 0.858...   \n",
       "10  [0.8709677419354839, 0.8273878020713463, 0.806...   \n",
       "11  [0.9320276497695853, 0.8734177215189873, 0.857...   \n",
       "12  [0.8663594470046083, 0.8250863060989643, 0.806...   \n",
       "13  [0.9331797235023042, 0.8745684695051784, 0.851...   \n",
       "14  [0.8617511520737328, 0.8250863060989643, 0.800...   \n",
       "15  [0.9297235023041475, 0.8699654775604143, 0.849...   \n",
       "16  [0.8605990783410138, 0.8250863060989643, 0.799...   \n",
       "17  [0.9285714285714286, 0.8688147295742232, 0.848...   \n",
       "18  [0.8594470046082949, 0.8273878020713463, 0.798...   \n",
       "19  [0.9262672811059908, 0.8665132336018412, 0.844...   \n",
       "20  [0.8582949308755761, 0.8227848101265823, 0.800...   \n",
       "21  [0.9262672811059908, 0.8653624856156502, 0.843...   \n",
       "22  [0.8571428571428571, 0.8216340621403913, 0.798...   \n",
       "23  [0.9262672811059908, 0.8642117376294591, 0.842...   \n",
       "24  [0.8513824884792627, 0.8204833141542003, 0.799...   \n",
       "25  [0.9274193548387096, 0.8653624856156502, 0.838...   \n",
       "26  [0.8502304147465438, 0.8204833141542003, 0.799...   \n",
       "27  [0.9262672811059908, 0.8653624856156502, 0.837...   \n",
       "28  [0.8502304147465438, 0.8170310701956272, 0.802...   \n",
       "29  [0.9193548387096774, 0.8630609896432682, 0.837...   \n",
       "30  [0.8525345622119815, 0.8170310701956272, 0.802...   \n",
       "31  [0.9182027649769585, 0.8642117376294591, 0.837...   \n",
       "32  [0.847926267281106, 0.8181818181818182, 0.8009...   \n",
       "33  [0.9170506912442397, 0.8630609896432682, 0.836...   \n",
       "34  [0.8467741935483871, 0.8147295742232451, 0.802...   \n",
       "35  [0.9170506912442397, 0.8642117376294591, 0.837...   \n",
       "36  [0.8456221198156681, 0.8158803222094362, 0.799...   \n",
       "37  [0.9158986175115207, 0.8653624856156502, 0.837...   \n",
       "\n",
       "                                        f1_score_fold  accuracy_avg  \\\n",
       "0   [0.7405541561712847, 0.7339300244100895, 0.753...      0.791935   \n",
       "1   [0.7478108581436076, 0.7506172839506174, 0.757...      0.808224   \n",
       "2   [0.7504132231404959, 0.7369267900241352, 0.746...      0.789570   \n",
       "3   [0.7589208006962574, 0.7481420313790257, 0.764...      0.810062   \n",
       "4   [0.7473251028806583, 0.7243589743589745, 0.741...      0.784973   \n",
       "5   [0.7578397212543554, 0.7464905037159372, 0.762...      0.809537   \n",
       "6   [0.7459016393442623, 0.7219123505976095, 0.738...      0.784186   \n",
       "7   [0.7590987868284229, 0.743993371996686, 0.7649...      0.810326   \n",
       "8   [0.7446808510638299, 0.7177033492822965, 0.737...      0.781427   \n",
       "9   [0.7565217391304349, 0.7390223695111846, 0.762...      0.808618   \n",
       "10  [0.7436527436527437, 0.7187250996015937, 0.741...      0.782347   \n",
       "11  [0.7593397046046917, 0.7382328654004954, 0.762...      0.808224   \n",
       "12  [0.7422512234910277, 0.7186009538950715, 0.741...      0.780770   \n",
       "13  [0.7621527777777778, 0.7398843930635839, 0.758...      0.808093   \n",
       "14  [0.7408610885458977, 0.7175815433571997, 0.740...      0.779720   \n",
       "15  [0.7623163353500432, 0.7374485596707819, 0.756...      0.806779   \n",
       "16  [0.7371847030105778, 0.7186009538950715, 0.738...      0.778406   \n",
       "17  [0.7627264883520276, 0.7347611202635913, 0.756...      0.806517   \n",
       "18  [0.732463295269168, 0.7197452229299363, 0.7382...      0.777355   \n",
       "19  [0.7592752372735115, 0.7325102880658435, 0.754...      0.804021   \n",
       "20  [0.7318663406682967, 0.7194928684627576, 0.736...      0.775779   \n",
       "21  [0.7592752372735115, 0.7329498767460969, 0.754...      0.803890   \n",
       "22  [0.7312703583061889, 0.7179080824088748, 0.735...      0.776436   \n",
       "23  [0.758203799654577, 0.7323481116584564, 0.7553...      0.803364   \n",
       "24  [0.7313915857605178, 0.7173396674584323, 0.732...      0.775517   \n",
       "25  [0.7577854671280275, 0.7319078947368419, 0.753...      0.802970   \n",
       "26  [0.7287449392712549, 0.7173396674584323, 0.732...      0.774991   \n",
       "27  [0.7571305099394987, 0.7308641975308643, 0.751...      0.802445   \n",
       "28  [0.7277147487844409, 0.7146245059288537, 0.734...      0.773940   \n",
       "29  [0.7564322469982847, 0.7296631059983566, 0.749...      0.801657   \n",
       "30  [0.7299270072992701, 0.7146245059288537, 0.735...      0.773940   \n",
       "31  [0.7557840616966581, 0.7313064913722267, 0.749...      0.800869   \n",
       "32  [0.7275666936135814, 0.7131537242472267, 0.736...      0.772889   \n",
       "33  [0.7540702656383891, 0.7317473338802296, 0.748...      0.800738   \n",
       "34  [0.7269789983844911, 0.7114624505928854, 0.737...      0.773021   \n",
       "35  [0.7508591065292097, 0.7323481116584564, 0.747...      0.800344   \n",
       "36  [0.7253634894991923, 0.7140600315955767, 0.734...      0.773415   \n",
       "37  [0.7491408934707904, 0.7277227722772278, 0.747...      0.799293   \n",
       "\n",
       "    accuracy_std  precision_avg  precision_std  recall_avg  recall_std  \\\n",
       "0       0.019310       0.783503       0.024916    0.713861    0.047422   \n",
       "1       0.018011       0.826087       0.029108    0.702859    0.042693   \n",
       "2       0.019297       0.776260       0.025601    0.717831    0.041624   \n",
       "3       0.018079       0.826171       0.031223    0.708359    0.041200   \n",
       "4       0.018290       0.768598       0.024013    0.715690    0.041543   \n",
       "5       0.018126       0.824026       0.031879    0.709889    0.043402   \n",
       "6       0.018617       0.765954       0.023648    0.717525    0.040325   \n",
       "7       0.017547       0.822935       0.029651    0.713557    0.044828   \n",
       "8       0.018339       0.762116       0.023827    0.715078    0.039085   \n",
       "9       0.018446       0.820476       0.031349    0.712029    0.047258   \n",
       "10      0.017883       0.762702       0.023765    0.717219    0.038001   \n",
       "11      0.018943       0.818631       0.033112    0.713557    0.047538   \n",
       "12      0.018679       0.759588       0.023218    0.717219    0.039013   \n",
       "13      0.018867       0.818286       0.034879    0.713862    0.045369   \n",
       "14      0.020290       0.756916       0.024601    0.718442    0.039625   \n",
       "15      0.019380       0.814436       0.034511    0.715085    0.045169   \n",
       "16      0.019324       0.755337       0.023525    0.716914    0.039570   \n",
       "17      0.020207       0.813361       0.034173    0.715696    0.047713   \n",
       "18      0.019659       0.754404       0.023952    0.715081    0.040205   \n",
       "19      0.020513       0.809151       0.034051    0.714168    0.050304   \n",
       "20      0.019648       0.752129       0.024433    0.713858    0.037759   \n",
       "21      0.020298       0.808518       0.034111    0.714779    0.050964   \n",
       "22      0.019141       0.753466       0.023474    0.713552    0.037113   \n",
       "23      0.019899       0.807507       0.034263    0.714780    0.051336   \n",
       "24      0.019913       0.751408       0.022754    0.713856    0.037298   \n",
       "25      0.020409       0.807040       0.035583    0.714474    0.052350   \n",
       "26      0.018780       0.750603       0.021027    0.713551    0.037406   \n",
       "27      0.021251       0.806049       0.035655    0.714169    0.052995   \n",
       "28      0.018651       0.749171       0.021268    0.712634    0.036459   \n",
       "29      0.021581       0.803296       0.032864    0.715085    0.051660   \n",
       "30      0.018300       0.749381       0.021385    0.712328    0.036482   \n",
       "31      0.020733       0.801894       0.032369    0.714779    0.050802   \n",
       "32      0.018517       0.747667       0.020301    0.711717    0.037858   \n",
       "33      0.020396       0.801208       0.031672    0.715391    0.051236   \n",
       "34      0.018805       0.747367       0.019899    0.712634    0.038558   \n",
       "35      0.019121       0.801307       0.030642    0.714169    0.051733   \n",
       "36      0.019622       0.747914       0.020828    0.712940    0.038727   \n",
       "37      0.018955       0.800352       0.029778    0.712335    0.053749   \n",
       "\n",
       "    specificity_avg  specificity_std  f1_score_avg  f1_score_std  \n",
       "0          0.850764         0.022711      0.746076      0.027959  \n",
       "1          0.887614         0.022534      0.758481      0.025862  \n",
       "2          0.843624         0.022193      0.745196      0.026447  \n",
       "3          0.886693         0.024038      0.761739      0.025167  \n",
       "4          0.837177         0.021709      0.740485      0.025507  \n",
       "5          0.884621         0.025129      0.761567      0.025532  \n",
       "6          0.834415         0.021136      0.740314      0.025456  \n",
       "7          0.883239         0.024205      0.763184      0.025380  \n",
       "8          0.831421         0.021328      0.737242      0.024934  \n",
       "9          0.881397         0.025572      0.761111      0.026677  \n",
       "10         0.831421         0.021526      0.738664      0.024132  \n",
       "11         0.879555         0.026833      0.761145      0.026946  \n",
       "12         0.828656         0.020430      0.737240      0.025322  \n",
       "13         0.879095         0.028210      0.761200      0.026047  \n",
       "14         0.825893         0.021230      0.736655      0.026894  \n",
       "15         0.875871         0.027977      0.760285      0.026510  \n",
       "16         0.824741         0.020917      0.735071      0.025970  \n",
       "17         0.874950         0.027797      0.760110      0.027979  \n",
       "18         0.824280         0.021474      0.733638      0.026391  \n",
       "19         0.871726         0.028232      0.757284      0.028868  \n",
       "20         0.822437         0.021314      0.731997      0.025796  \n",
       "21         0.871035         0.028581      0.757295      0.028750  \n",
       "22         0.823820         0.020586      0.732491      0.025295  \n",
       "23         0.870114         0.029044      0.756789      0.028382  \n",
       "24         0.821977         0.018754      0.731762      0.026305  \n",
       "25         0.869653         0.030206      0.756325      0.028927  \n",
       "26         0.821286         0.017826      0.731193      0.025310  \n",
       "27         0.868962         0.030083      0.755739      0.029919  \n",
       "28         0.820134         0.017904      0.730050      0.024951  \n",
       "29         0.866889         0.027631      0.755268      0.030243  \n",
       "30         0.820364         0.018308      0.729962      0.024607  \n",
       "31         0.865737         0.027597      0.754479      0.029185  \n",
       "32         0.818982         0.017059      0.728831      0.025351  \n",
       "33         0.865046         0.027390      0.754491      0.028949  \n",
       "34         0.818521         0.016311      0.729178      0.025843  \n",
       "35         0.865276         0.027273      0.753767      0.027877  \n",
       "36         0.818983         0.016846      0.729615      0.026591  \n",
       "37         0.864815         0.027154      0.752224      0.028379  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smv_lin_rbf_perf_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(perf_df, param, param_name=None):\n",
    "    \n",
    "    if not param_name:\n",
    "        param_name = param\n",
    "\n",
    "    fig, ax = plt.subplots(3,2, figsize=(16,20))\n",
    "\n",
    "    # Plot accuracy\n",
    "    ax[0, 0].fill_between(perf_df[param].values,\n",
    "                     perf_df['accuracy_fold'].apply(np.min),\n",
    "                     perf_df['accuracy_fold'].apply(np.max), color='lightblue')\n",
    "    ax[0, 0].plot(perf_df[param].values, perf_df['accuracy_avg'].values, color='tab:blue')\n",
    "    ax[0, 0].set_xlabel(param_name)\n",
    "    ax[0, 0].set_ylabel('Accuracy')\n",
    "    ax[0, 0].set_title('Accuracy evolution against {}'.format(param_name))\n",
    "    \n",
    "    # Plot precision\n",
    "    ax[0, 1].fill_between(perf_df[param].values,\n",
    "                     perf_df['precision_fold'].apply(np.min),\n",
    "                     perf_df['precision_fold'].apply(np.max), color='coral')\n",
    "    ax[0, 1].plot(perf_df[param].values, perf_df['precision_avg'].values, color='tab:red')\n",
    "    ax[0, 1].set_xlabel(param_name)\n",
    "    ax[0, 1].set_ylabel('Precision')\n",
    "    ax[0, 1].set_title('Precision evolution against {}'.format(param_name))\n",
    "    \n",
    "    # Plot recall\n",
    "    ax[1, 0].fill_between(perf_df[param].values,\n",
    "                     perf_df['recall_fold'].apply(np.min),\n",
    "                     perf_df['recall_fold'].apply(np.max), color='lightgreen')\n",
    "    ax[1, 0].plot(perf_df[param].values, perf_df['recall_avg'].values, color='tab:green')\n",
    "    ax[1, 0].set_xlabel(param_name)\n",
    "    ax[1, 0].set_ylabel('Recall')\n",
    "    ax[1, 0].set_title('Recall evolution against {}'.format(param_name))\n",
    "    \n",
    "    # Plot specificity\n",
    "    ax[1, 1].fill_between(perf_df[param].values,\n",
    "                     perf_df['specificity_fold'].apply(np.min),\n",
    "                     perf_df['specificity_fold'].apply(np.max), color='orange')\n",
    "    ax[1, 1].plot(perf_df[param].values, perf_df['specificity_avg'].values, color='tab:orange')\n",
    "    ax[1, 1].set_xlabel(param_name)\n",
    "    ax[1, 1].set_ylabel('Specificity')\n",
    "    ax[1, 1].set_title('Specificity evolution against {}'.format(param_name))\n",
    "\n",
    "    # Plot f1 score\n",
    "    ax[2, 0].fill_between(perf_df[param].values,\n",
    "                     perf_df['f1_score_fold'].apply(np.min),\n",
    "                     perf_df['f1_score_fold'].apply(np.max), color='lightpink')\n",
    "    ax[2, 0].plot(perf_df[param].values, perf_df['f1_score_avg'].values, color='tab:pink')\n",
    "    ax[2, 0].set_xlabel(param_name)\n",
    "    ax[2, 0].set_ylabel('F1 Score')\n",
    "    ax[2, 0].set_title('F1 Score evolution against {}'.format(param_name))\n",
    "    \n",
    "    plt.show()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance evolution for linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1440 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<Figure size 1152x1440 with 6 Axes>,\n",
       " array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f81100985c0>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7f810ad85668>],\n",
       "        [<matplotlib.axes._subplots.AxesSubplot object at 0x7f810ad35780>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7f810ace3898>],\n",
       "        [<matplotlib.axes._subplots.AxesSubplot object at 0x7f810ad149b0>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7f810acc6ac8>]],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_performance(smv_lin_rbf_perf_metrics[smv_lin_rbf_perf_metrics['kernel'] == 'linear'], param='C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance evolution for `rbf` kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA74AAAR8CAYAAAC6+UgwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdeZzdZX33/9dnzjmzb0kmC1mGhCRAAi5AAC1a+xC1wF3BWqug1lKpdFFrXWq97x9Vaq1Wf1VrK9oiVRAURFvvokZBXBERErZAEgIhZJvJMktmnznr5/7j+p7kZDJJJsmcOTNn3s/H43jOdznne82JzDXv77WZuyMiIiIiIiJSripKXQARERERERGRYlLwFRERERERkbKm4CsiIiIiIiJlTcFXREREREREypqCr4iIiIiIiJQ1BV8REREREREpawq+ImXKzH7HzHafwvv/j5ndMpFlKjYz+6GZ/XGpyyEiIqfGzDaa2e8c55xWMxsws9gkFeuUqF4WKS0FX5nyzOznZnbAzKpKXZZyNVZl7O6fdPc/LVWZToa7X+7ut53KZ5jZtWb2q3Gc97tm9ksz6zezDjP7hZldeSrXFhGZ6sxsu5kNR4Fzn5ndamb1E30ddz/H3X9+nHN2unu9u2cn+vqlpnr5ENXLMlEUfGVKM7OlwCsBByb1l5eZxSfzejJ9mNmbgG8DXwcWA/OBjwKvL2W5REQmyevdvR44H1gD3DD6BAv0d6ZMCtXLMh76hSRT3TuA3wC3Aod1lTGzGjP7rJntMLNeM/uVmdVEx15hZr82sx4z22Vm10b7f25mf1rwGYfdRTQzN7N3m9lzwHPRvi9En9FnZo+a2SsLzo9FXY+ej+4wPmpmS8zsJjP77Kjy3mNm7x/rhzSzs83sx2bWbWZbzOzN0f6LzWxvYTcuM/t9M9sQva4ys38xs/bo8S9HaxmPfrYVBdu3mtknzKwO+CGwMLqDP2BmC83sRjO7o+D8K6OuZz3R97iq4Nh2M/uQmW2I/i2+ZWbVRynHcjP7qZl1mVmnmX3DzJoLjp9vZo9H3+e3o8/6RHRslpl9P7qTeyB6vbjgvQf/ffP/tmb2z9G5L5jZ5QXnXmtm26LrvGBmb4t+pn8HXh59Dz1jlN+AzwH/4O63uHuvu+fc/Rfu/q6xfmYRkXLk7m2E+uNcOPg7+B/N7EFgCDjDzJrM7D/NbI+ZtUX1TmGd9i4z2xz9Lt5kZudH+7eb2Wui1xeZ2fqoHt5nZp+L9i+N6rZ4tL0wqmu7zWyrmb2r4Do3mtndZvb16FobzWzN0X421cuql6X8KPjKVPcO4BvR43fNbH7BsX8GLgB+C5gNfBjImdnphArj34C5wEuBJ07gmm8ALgZWR9vros+YDXwT+HZB5fEB4BrgCqAReCehsr8NuMaiu91m1gK8Jnr/YaIK7sfRsXnA1cCXzGy1uz8MDAKvLnjLWws+5/8DXhaV7yXARYxx5/1Y3H0QuBxoj7qM1bt7+6gyngncCfw14TtdC3zPzCoLTnszcBmwDHgxcO1RLmnAp4CFwCpgCXBjdJ1K4LuEGx2zo2v+fsF7K4CvAacDrcAw8MVj/HgXA1uAFuAzwH9aUAf8K3C5uzcQ/j/0hLtvBv4ceCj6HprH+MyzojJ/5xjXFREpe2a2hFD/PV6w+4+A64EGYAfh93kGWAGcB7wOyAehPyT8/n8HoQ69Euga41JfAL7g7o3AcuDuoxTpLmA3oX55E/BJMyusP6+MzmkG7uEo9YfqZdXLUp4UfGXKMrNXEH6R3u3ujwLPEyoXokD5TuB97t7m7ll3/7W7J6Nz7nf3O9097e5d7n4iwfdT7t7t7sMA7n5H9BkZd/8sUEX4JQuh8r7B3bd48GR07iNAL3BpdN7VwM/dfd8Y1/s9YLu7fy26xuPAfwF/GB2/kxCuMbMGwh8Zd0bH3gZ83N33u3sH8PeEPzom2luAH7j7j909TbjpUEOomPL+1d3b3b0b+B6h0j+Cu2+NPicZlflzwKuiwy8D4tFnpd39v4FHCt7b5e7/5e5D7t4P/GPBe8eyw92/Eo3/ug04jdD9CSAHnGtmNe6+x903jvO7mBM97xnn+SIi5eb/Ri1vvwJ+AXyy4Nit7r7R3TOEoHQF8NfuPuju+4HPE+pECHXoZ9x9XVSHbnX3HWNcLw2sMLMWdx9w99+MPiEK4ZcAf+vuI1G9fwshVOf9yt3XRnXC7YRgOhbVy6qXpQwp+MpU9sfAfe7eGW1/k0PdnVuAakIYHm3JUfaP167Cjair0Oaoq1AP0BRd/3jXug14e/T67YRKdiynAxdHXZV6omu8DVgQHf8m8Maoq9QbgccK/jBYSLijnrcj2jfRDruOu+cI39OignP2FrweAsac7MTM5pvZXRa6vPUBd3Do+1wItLm7F7xlV8F7a83sPyx0b+8Dfgk029Fn9DxYJncfil7WR3fT30K4i7zHzH5gZmcf7YcfJd8acdo4zxcRKTdvcPdmdz/d3f8yf6M4UliHng4kCL9n8/XbfxBaUWH89fV1wJnAM2a2zsx+b4xzFgLdUfjK28Gx66lqG3s+D9XLqpelDCn4ypRkYazum4FXWRhLsxd4P/ASM3sJ0AmMELo8jbbrKPshdE+qLdheMMY5B3+5WxjP++GoLLOiLja9hG5Bx7vWHcBVUXlXAf/3KOftAn4R/RGRf9S7+18AuPsmQuV2OYd3pwJoJ1TQea3RvrEMcfSf3Tm2w64TjadZArQd531j+WR0vRdF3dbezqHvcw+wKPr8vCUFrz9IaG2/OHrvb+eLdKKFcPd73f21hIryGeAr+UPHeesWwr/ZH5zoNUVEZoDRASkJtBTUb43ufk7B8aPVoYc+0P05d7+GEJg/DXwn6hpbqB2YHbXA5rVycvWU6mXVy1KGFHxlqnoDkCWMs31p9FgFPAC8I7qz+VXgcxYmfIiZ2cuju6/fAF5jZm82s7iZzTGzfPeeJwh3aWstTChx3XHK0UAYm9QBxM3so4RxSHm3AP9gZiujMSovNrM5AO6+mzA++Hbgv0bdES/0feBMM/sjM0tEjwutYJIKQqX6PkKF8u2C/XcCN5jZ3Ggc8UcJgXssTwBvjb6ryzi8K9I+YI6ZNR3lvXcD/8vMLjWzBKGiSwK/Psr5x9IADAC9ZrYI+JuCYw8R/t3fE/3bXUUYH1X43mGgx8xmAx87ievn725fFf3hlIzKk4sO7wMWjxondVB01/sDwN+Z2Z+YWaOZVViYUO3mkymPiEg5cvc9wH3AZwt+Vy43s3z9cwvwITO7IKpDV1iYp+MwZvZ2M5sb1f35yY1yhee4+y5CnfQpM6s2sxcT6vij1YnHonr5ENXLUjYUfGWq+mPgax7W6NubfxAmTHhb1DXpQ8BThHDZTbgLXOHuOwnjbT4Y7X+CQ+N4Pg+kCL9EbyOE5GO5F/gR8Czh7u4Ih3fj+hyh8rkP6AP+kzDGJu824EUcvZszUbes1xHGPLUTugF9mjCWOO9OQoX404Ku3wCfANYDG6Lv4rFo31jeR5jWP99l62ALtLs/E11jW9St67BuWe6+hXAH+N8Ire2vJyxnkTraz3UMf09YAqMX+AHw3wXXSRG6jV0XlfPthD9AktEp/0L4fjsJs33/6CSuD+F33wcI33c34bv9i+jYT4GNwF4z6xzrze7+HUKXrHdGn7GP8L3/z0mWR0SkXL0DqAQ2AQcIExCdBuDu3yaMCf0m0E+ol2aP8RmXARvNbIAw0dXVR7mZfA2wlPB7+bvAx9z9/hMtsOpl1ctSnuzwLvsiMpHM7LcJd3pPd/3HdlLM7GHg3939a6Uui4iIyEynelmmK7X4ihRJ1PXofcAtCr3jZ2avMrMFUZeqPyYswXCyd5BFRETkFKhelnIx1kx2InKKonFA64EngT8pcXGmm7MI3cfrgG3Am6JxYiIiIjL5VC9LWVBXZxERERERESlr6uosIiIiIiIiZU3BV0RERERERMpa2YzxbWlp8aVLl5a6GCIiUiYeffTRTnefW+pyTGeqm0VEZCKdSt1cNsF36dKlrF+/vtTFEBGRMmFmO0pdhulOdbOIiEykU6mb1dVZREREREREypqCr4iIiIiIiJQ1BV8REREREREpawq+IiIiIiIiUtYUfEVERERERKSsKfiKiIiIiIhIWVPwFRERERERkbKm4CsiIiIiIiJlTcFXREREREREypqCr4iIiIiIiJS1ogZfM7vMzLaY2VYz+8gYx1vN7Gdm9riZbTCzK8Y4PmBmHypmOUVERERERKR8FS34mlkMuAm4HFgNXGNmq0eddgNwt7ufB1wNfGnU8c8BPyxWGUVERERERKT8xYv42RcBW919G4CZ3QVcBWwqOMeBxuh1E9CeP2BmbwBeAAaLWMYZKZPL0Z/KMpDK0JdM0zOSoT+dIZPLTeh1KjDqKmM0VyVorErQUBmnvjJGVawCM5vQa4mIiMg0NtADzz8BG38Nvfvhd98JZ7y41KUSkTJSzOC7CNhVsL0buHjUOTcC95nZe4E64DUAZlYP/C3wWkDdnE+CuzOcyR4MuD3JNL0jGQbTGTI5J1Zh4JBxL2YpGBnO0TWcJmZgZuTcMaA2EaOxMkFTdZzGygT1lTHqK+NUKBCLiMhkyOUgl4VYHFT3TL5sFnZvgS3r4ZmHoLcz/FukRsLxOz8Jy14Mv/dn0DintGUVkbJQzOA7HtcAt7r7Z83s5cDtZnYuIRB/3t0HjtUyaGbXA9cDtLa2TkJxp54jWm+TGfpTGYbTWSoKwmbOR7+vmIH3SFkHCkJ2fyqE8vYBDobwrDuVsQoaKuM0VcVpqkpQXxmnoTJOVVzzsImIyARpfx7u+hT0d4e6qaICKuIQi0XP0SOegFgiPMcTkKiCeCUkKsPrRHV4rqwKx+tnwbmvCO+VI/V2Rq26D8KOTeH7TiXBox5n2cyhc9NJ2PoY/Nu74VVvgZe/Xt+riJySYv4GaQOWFGwvjvYVug64DMDdHzKzaqCF0DL8JjP7DNAM5MxsxN2/WPhmd78ZuBlgzZo1E5LksjnnkT0HqI3HDnbPbaiMUVnC7rmh9TZHfypzqPU2mWEwVdB6y5FhdnTYnIqcw8udzOZIDqfoHE6NaiW2qJU4TnN1/t8lTl1lrCxbid2d/lSG9oERepMZljfX0VJbWepiiYhMb7kcPHQP/OxOyKQO359LQebobz2uilgIvz/5Brz+L2Dl+adc3Gkvk4adm2HLOnjm4dCduaIihFo4/vedy4bHL74F638Eb3gvLD236MUWkfJUzOC7DlhpZssIgfdq4K2jztkJXArcamargGqgw91fmT/BzG4EBkaH3mJJZXPsHUjicIzgFaehMjHhwSuTyzGQytJf0Ho7ELXe2hRqvZ0shwf3EATzYTBWYbhDzp2qWAX1USBurIrTkIhTXxWnKja9Wondna7hNG39w7T1j5DO5cLPCOwdGKEuEWd1SwOn1VdpjLSIyIka6IFv/zO0bz089E6UXBZS2dBV9+7PwMIVIQC3LJr4a01lB/bB1sfh6V9B27OHui/n6/PsSXxmOgk9++GOfwg3FK64HhpmTWixRaT8FS34unvGzN4D3AvEgK+6+0Yz+ziw3t3vAT4IfMXM3k9o/LvWvfRNlBUWQtfRgxdHD15RS2T9Ubrn5ltvB6LP6o1abwcKW2/dyYz+FqZB6+1kGd1KPJLNMXKwldgwg1zOMTPqEjEaq+I0VyWor4paiRNTp5U4k3P2DybZ1T/MvoEk2Ng3MrIOfakM6/b0UBkzzp5TT2tj7cHWfhGR8TKzy4AvEOrmW9z9n0YdbwVuI/S4igEfcfe1ZpYAbgHOJ/z98HV3/9SkFv5kPf9ECL2pkRBQiy2dDC2d//4BOO9SePXboKau+NcthXQStm+ELY+Ex/BgGDN9sFU3PXHXyqTCmOCtj8Or3woX/a/QXVpEZBxsCuTMCbFmzRpfv379KX/OcDrLfS/sD6H3JIwOXvlW4mzUdfV4rbdSHMYxblZEgbihMk7lJLQSj2Sy7B1MsrN3mO6RFBVmJ9xqHzOjwuDM2fWc0VxLYpq1botMB2b2qLuvKXU5JlK01OCzhMkjdxN6Z13j7psKzrkZeNzdvxwtQ7jW3Zea2VuBK939ajOrJazS8Dvuvv1o15uouvmkZTPw46/D+nuL08o7HvFEaPV8zTvggteGLtHTmTt0tcNzj8GmB8N46Xji8FbdyZCoCq2+b3gftJ49edcVkZI6lbpZswRMsKx7aJIE8EOtxIdR6+2kO6lW4urEwdb7U20l7k9laO8fZlffCAPpDIaF/68QgviJyrqTdXimq59nuvpZ1lzLytn11MSn+R9UIlJsp7LUoAN1ZhYHaoAU0DcZhT4p3Xvgzk/Bgf2lC70QWjwzabjvVvj1/8BV755+41RTI/DCU2Gc7rPrD4Xc/PeaPZXB0ScpnYTuvfD1j8HZF8Fl10F98+SXQ0SmDQVfmfFG36zoS2XoS2Vo6x81ljien3E6QVNV/OCM02O1Ers73SNp2vpH2N0/TDqbw6GghX9ibnzkeyZsOzDEtp4hFtZXs6qlgYZK/actImM66aUGge8QQvIeoBZ4v7t3F7W0J2vDL+B7Xw6B0yd2jfqTlk7Cgb3wjU/A0nPgij+DWfNKXaqxuUPHbnju0TAD877toVU3OcJE1V8TJpOCzb8Jgfw1fwRrfnf6t6qLSFHor2ORoziilTiTYySTomPoUCtxNufE8l3ao1binpE0eweTk7BO8iG5qMBt/SO0D4zQUlPJqpYG5tRoJuiprnDcf86d+XWavExK7mhLDV5EmJpoITALeMDM7s+3HueVdKnB5DDcc1MIQfkxplNNOgnPPwk3vRcuugJe9Waoqil1qWBkKLTqbv5NCLyZVLhpkB+jW4pW3fHKZqJu7bfDI2vhDX8Fi88sdalEZIpR8BU5CYWtxJlRrcSlvBee70W/fyhF165u6itjrG5pYIHCVMllcs5AtCRZfyrDgZH0EeP+3aGhMsaa05pprEqUushSnk5lqcG3Aj9y9zSw38weBNYAhwXfYiw1OC7tz8Odn4Th/omdUKkY8sv0PLIWHrsfLr8OXvTbYamfyeIO+3YcGqu7f+fUbdUdr/QIdLbBrX8Hq18Ol70TahuP/z4RmREUfEUm0FT6UyHrTm8yw7r2HhIxY3VLA0saa6bMjNblyN0ZidbcLpy1fTCVIX1w1vYxegIUjPvvSWb46Y5OljfXsqqlkbhm7paJddJLDUb7X01oAa4DXgb8y2QV/KhyuTB29ud3lXYs78nIpMLj+/8Ov/pvuOo9xWupTEVLAnXshM0Pw9bHIBsF8Ow0aNU9EZkUbPw1PPMIvO5aOP81k3tTQUSmJAVfkTKXcSeTcZ7Y18eG/X2cNaeehfXV1CViagU+BQOpDD1Rq+3B1ttMWCal4hTX3M45bOsZYmffCOcvaOK0+uqJLr7MUKey1KCZ3QR8zcw2EibL/5q7byjRjxIM9MDd/z/seX76hd5C6SR07AotlSsvgMv/FBpnn9hnZLPQ1xXGEffsD5N7dewKz33dYUKqRCVgkBouyo8xpWTT4XHvV+GRH8DLr4QFy2DuktCyLSIzjpYzGuVUlzMSmepiUdjNuVMdD8s6zYpmsM4/tDzSkcaasMywoo/jjhnMqank/AXN1CY0YctkKsfljCZbUZczmuy1eSdLRSwsf3TJ74dHIpqrwR0Ge+HAvijY7g3BtqsNejtheCAEuopY+D7SKaZWP6QSS1SH9YUzKWicAwuXw5JVIQwvWFa+6yyLlBktZyQi45YtCGrDmRzDY03YVVGwrFPVoVBcO8NaibM5Z/9Qkt19w+yJJizLuhf8KVn8PyqzDh1DKX78QgerWupZMatO3dVlZsuk4f7bS7s2bzHlx/8++F145IfQsjAE3YFeqDCIJUIITiePnLF6qk7oNRWkRw697tkfHlvWh5sF6SRU18H800MYXrg8hOGmlhCWRaQsKPiKCDBqwq5cGB/cm8zQxggVBcs6VUfLOjVXJ2iM1jluqIqTKJPxU8lMjr2DI+zqG6ZzOEXFJLTqHo8T/n02dw7wQs8QF57WzGzN2C0zUfeeMIFVT4nX5p0M6WR47Ow9tC/H1J+4azrJd4cGGOoLs1pv3wiV1WG8sxm0LILFZ4Wx1wuWhe2Y/nwWmY70X66IHFMOyOWObCXeP5QibgZRK3FTVZzWxhoWNlRTm5hev1oGUhnaB0LY7U9mQst39CPnplBXwaw7g+ksD+zqYlFDDS+e1zjmOtIiZenJn4dJoKbS2rxSfjwHyaFD23u2hceTPwOirtJNc2HZi+AVb4TZC0pWVBE5MdPrr1MRmVIyBa3EPckM/Z39PN3ZT3U8xpKGahY21NBcFZ9y3aPdnQMF43VT2RBvD+b7qZN1x5R12N0/TPvACC+Z10hrY82U+45FJlT/gbA+b7nMOizTT6qgq3R+ArENP4ezXwavfqsCsMg0oOArIhMm30o6lM7ybPcgWw8MUWGwsKGaxQ01zK2tLNn41GzO6RhKsrt/mD0DydB9OOdTPeMeVS7qev7Evl629Qyx5rRmGir1K13KVDYdJm1S8JWpwnOQycHGB+GZ38BZF8Olb4XZp5W6ZCJyFPorSUSKIj8uNeuwo3eYtr4RHGdubRVLGmuYX1dVlG662ZwzmA7r6PYnM/Qk0/QlMwxlslSYjXtJoeki63BgJM1PtnewYlYdq+Y0hPWCRUSk+PIBeNOvYcvDcNaF8Oq3wxwFYJGpRsFXRCZFfoKovYNJOodS5HCaqhIHu0SfyFI97k4ym2MglaE/laU3maYnmWYwlSWVzR0MfqNbdHNlsnzbWHIOzx8YZEfvMGtOa2Z+XVWpiyQiMnPkA/Dmh2HLOjjzQrj0bTBnYalLJiIRBV8RmXT5EHxgJE1vMs3Tnf3UxGMsaaxhYX01TdG44Jz7wXDbn0rTM5KhL5VmOJ3FgQozPGpVPuzzy6xVd7yyDtlsjt+0dTO3torzFjRRE9favyIikya/HNUzD8Oz62HlBSEAtywqdclEZjwFXxEpqXxGHUxn2dI1wHPdg8QMzCy03hbMHD06zpZzC+6pyDrsG0xy37YO6isnNvhWxSo4c3Y9c2srNaGWiMjRFAbg5x6FlefDpW9XABYpIQVfEZkyCscF56dWLpw5WsYv/132Jid+MqCu4QPUxCtYPbeBRfXVCsAiIkfjubAE0jOPwHOPwYrzQgCeu7jUJSudgR7Yux32vgC9HWGN5MVnhonBVJ9IESn4iojICcm6M5DO8tieXp6s6GPVnHpOb6rVpFoiIkeTD8Bb1sHWx2HFS6MAvKTUJSueXBa694aA2/487HoG9u8Ma3HHE+H7yGbCGsnugMFpy+CMF8OSs2HRSqiuK/VPIWVEwVdERE5Kxp1M1nmqo4+nO/tZMauOFbPqijJbt4hIWTgYgNfD1idg+Uvg0j+CedM8AKdGYN+OEHJ3b4G2rWG941gcMEiPROE2kk0f/t68nZth97OQqIJ0EuqboXUVLHsRLD4rtJRXaO4KOTkKviIickqyDrjzXPcAz3UP0NpYy1lz6k9opm4RkRklH4CffRSefzK0ci5/6QRfxELLajwB8crDnxNVRz92rGDpDgMHQsDd80IIqvu2w2Bv+MxsJvxceSez9nYuC8mh8LqvC57+VWgpN4NsFua1hu+rdVVoFa5vPvFryIyk4CsiIhMiP7v29t4hdvQNsaCumlUt9TRVJUpbMBGRqSofgJ+LAvBEMsAqonGzNsb4WQ9B1h1yuVCWXDacVxELrbUHH4nwPNADuUzYTifD+Xn5sFoM6eSh13ueh73bYP294burrgtjhM94SXg+bTlUqOeRHEnBV0REJlTUAMyegRH2Do4wqzrB6pYGWmo0E7SIyJjcD+/+W0ruoaX2WK21mRKX1f1Q0B7sDS3Czz8Rgv45l8BV79FEWXIE3Q4REZGicMJyVV3DaX69+wD3b++krX8Y1zJUIiIy0TLp0DK88UF4+PulLo1MQQq+IiJSdFl3+lMZ1u/p5Yfb9rPtwCDZnAKwiIhMsHQS7r8jzJ4tUkDBV0REJk3WnZFMjqc6+ln7/D6e6epnJJM9/htFRETGK5OCb30G9u8qdUlkClHwFRGRSZd1J51znuka4Efb9vPjF/bzTFc/fcm0ukKLiMipSyfh6x+Fwb5Sl0SmCAVfEREpmZyHR38qyzOdA/xsRxc/fH4/T+7rpXMopRAsIiInyWFoAO74+9JPxiVTgmZ1FhGRKSEH4E426zzfM8SOvmFwmF9fxZKGGubVVRGv0CydIiIyTrkMdOyG7/4rvOkDmul5hlPwFRGRKSkTTX7V1j/CvsEkOXdmV1fS2ljDgvoqquOxEpdQRESmvEwKnl0HD/wX/PabSl0aKSEFXxERmfLyIbhzOMWBkTRP7HfqE3GWNFazsKGGhkpVZyIichTpJPzy2zB3Cay6uNSlkRLRGF8REZlWsu7kHPpSGTZ3DfDT7R2sfX4fG/b30jWsccEiIjKGTAr++/Ow54VSl0RKRMFXRESmrZxD1mEkk2PrgSEe3NXNA7u6Sl0sERGZitJJ+PrHoL+71CWRElDwFRGRspFxZzCtdYFFROQokkPw9RtDCJYZRcFXRERERERmhlwWDuyD73wWNDRmRlHwFRERERGRmSOTgm0b4CffKHVJZBIp+IqIiIiIyMySTsJvvgdPPVDqksgkUfAVEREREZGZJ5OC//ki7H621CWRSaDgKyIiIiIiM1MmBXd8HHo6Sl0SKTIFXxERERERmbmSw2GZo+RwqUsiRVTU4Gtml5nZFjPbamYfGeN4q5n9zMweN7MNZnZFtP8iM3siejxpZr9fzHKKiIiIiMgM5Tno64RvfRpyuVKXRoqkaMHXzGLATcDlwGrgGjNbPeq0G4C73f084GrgS9H+p4E17v5S4DLgP8wsXqyyioiIiIjIDJZJw65n4L5bS10SKZJitvheBGx1923ungLuAq4adY4DjdHrJqAdwN2H3D0T7a+OzhMRERERESmOdBIevQ8eu7/UJZEiKGbwXQTsKtjeHe0rdCPwdjPbDawF3ps/YGYXm9lG4CngzwuCsIiIiIiIyMRLJ2HtV2D7xlKXRLI0SScAACAASURBVCZYqSe3uga41d0XA1cAt5tZBYC7P+zu5wAXAv/bzKpHv9nMrjez9Wa2vqNDM7GJiIiIiMgpyqTgzn+E7j2lLolMoGIG3zZgScH24mhfoeuAuwHc/SFCt+aWwhPcfTMwAJw7+gLufrO7r3H3NXPnzp3AoouIiIiIyIyVSsJtH4ORwVKXRCZIMYPvOmClmS0zs0rC5FX3jDpnJ3ApgJmtIgTfjug98Wj/6cDZwPYillVERERERCTwHAz0wFc+DL/4Njz/JAwrBE9nRZsp2d0zZvYe4F4gBnzV3Tea2ceB9e5+D/BB4Ctm9n7CBFbXurub2SuAj5hZGsgBf+nuncUqq4iIiIiIyGGyaehqh198CxJVYfxvXRMsPhOWvggWr4T5SyGeKHVJZRyKukSQu68lTFpVuO+jBa83AZeM8b7bgduLWTYREREREZHjymUhORRe93fD5t/Ac49BRSyMB561AE5fHR6LVsLs06Ci1FMpyWhaG1dEREREROREZFKHXne1hcfTD4B7eMw/PbQKLzkrhOGGWaUrqwAKviIiIiIiIqcuNXLoddtz0L4VKmsgk4bKKli4HJa9GFpXwcIV6iI9yRR8RUREZhAzuwz4AmH+jVvc/Z9GHW8FbgOao3M+Eg1dwsxeDPwH0EiYg+NCdx9BRESO5H6oi/RwOkyQtX1jCLyZNMw7HVZeAMui8cKJqtKWt8wp+IqIiMwQZhYDbgJeC+wG1pnZPdGcG3k3AHe7+5fNbDVhro6l0WoLdwB/5O5PmtkcID3JP4KIyPSWzYQHwJ7nYd92ePj7oet0yyJYcQGc8SJYcjZUVpe0qOVGwVdERGTmuAjY6u7bAMzsLuAqoDD4OqFFF6AJaI9evw7Y4O5PArh716SUWESknBVOnLVvB+zfBet/COlUmCRrxXlwxkug9WyorittWac5BV8REZGZYxGwq2B7N3DxqHNuBO4zs/cCdcBrov1nAm5m9wJzgbvc/TOjL2Bm1wPXA7S2tk5o4UVEyp7nIDkcXnfuhs42ePz+EISb58Ly82D5S6B1NdQ2lLas04yCr4iIiBS6BrjV3T9rZi8Hbjezcwl/M7wCuBAYAn5iZo+6+08K3+zuNwM3A6xZs8Ynt+giIuXGDwXh7r3Q/SPY8PMQhBtmw/KXwtJzoGlumDm6YbbGCh+Fgq+IiMjM0QYsKdheHO0rdB1wGYC7P2Rm1UALoXX4l+7eCWBma4HzgZ8gIiKTpCAI93bAY/fD078CM8jlwljheCK0BjfMDoF41gJoagnbjXPCc11jWId4BlHwFRERmTnWASvNbBkh8F4NvHXUOTuBS4FbzWwVUA10APcCHzazWiAFvAr4/GQVXERExuKQGj58VzoJvUno7YTdzwIWwnAsin75CbaqakMAbmyBWfNCQM6H4/lLw7EyouArIiIyQ7h7xszeQwixMeCr7r7RzD4OrHf3e4APAl8xs/cTJrq61t0dOGBmnyOEZwfWuvsPSvOTiIjI+HloCc6kDt89MhAeXe3wAmAVkKgMz5kU1DfDGS+FFS+F088J29OYgq+IiMgMEq3Ju3bUvo8WvN4EXHKU995BWNJIRETKjecgVbA0e29nmFhr44OQTUNtI5zxYlhxfgjCjbNLV9aToOArIiIiIiIiY8t3pe7vhid/DpsfDl2la+pg2YsOBeHmuSUt5vEo+IqIiIiIiMj45IPwQA889QBsWRfWI66sgWXnwooLwkzTzfPCpFtThIKviIiIiIiInJx89+hMGjb+Gp59LHSbTlSGluCVF8CK88LM0iWk4CsiIiIiIiITI50Pwil45mHY+niYNfrd/1bSYlWU9OoiIiIiIiJSvjKp0BpcYgq+IiIiIiIiUtYUfEVERERERKSsKfiKiIiIiIhIWVPwFRERERERkbKm4CsiIiIiIiJlTcFXREREREREypqCr4iIiIiIiJQ1BV8REREREREpawq+IiIiIiIiUtYUfEVERERERKSsKfiKiIiIiIhIWVPwFRERERERkbKm4CsiIiIiIiJlTcFXREREREREypqCr4iIiIiIiJQ1BV8REREREREpawq+IiIiIiIiUtYUfEVERERERKSsKfiKiIiIiIhIWVPwFRERERERkbKm4CsiIiIiIiJlTcFXREREREREypqCr4iIiIiIiJQ1BV8REREREREpa0UNvmZ2mZltMbOtZvaRMY63mtnPzOxxM9tgZldE+19rZo+a2VPR86uLWU4REREREREpX/FifbCZxYCbgNcCu4F1ZnaPu28qOO0G4G53/7KZrQbWAkuBTuD17t5uZucC9wKLilVWERERERERKV/FbPG9CNjq7tvcPQXcBVw16hwHGqPXTUA7gLs/7u7t0f6NQI2ZVRWxrCIiIiIiIlKmihl8FwG7CrZ3c2Sr7Y3A281sN6G1971jfM4fAI+5e3L0ATO73szWm9n6jo6OiSm1iIiIiIiIlJVST251DXCruy8GrgBuN7ODZTKzc4BPA3821pvd/WZ3X+Pua+bOnTspBRYREREREZHppZjBtw1YUrC9ONpX6DrgbgB3fwioBloAzGwx8F3gHe7+fBHLKSIiIiIiImWsmMF3HbDSzJaZWSVwNXDPqHN2ApcCmNkqQvDtMLNm4AfAR9z9wSKWUURERERERMpc0YKvu2eA9xBmZN5MmL15o5l93MyujE77IPAuM3sSuBO41t09et8K4KNm9kT0mFessoqIiIiIiEj5KtpyRgDuvpYwaVXhvo8WvN4EXDLG+z4BfKKYZRMREREREZGZodSTW4mIiIiIiIgUlYKviIiIiIiIlDUFXxERERERESlrCr4iIiIiIiJS1hR8RUREZhAzu8zMtpjZVjP7yBjHW83sZ2b2uJltMLMrxjg+YGYfmrxSi4iInBoFXxERkRnCzGLATcDlwGrgGjNbPeq0GwhLEJ4HXA18adTxzwE/LHZZRUREJpKCr4iIyMxxEbDV3be5ewq4C7hq1DkONEavm4D2/AEzewPwArBxEsoqIiIyYRR8RUREZo5FwK6C7d3RvkI3Am83s93AWuC9AGZWD/wt8PfHuoCZXW9m681sfUdHx0SVW0RE5JQo+IqIiEiha4Bb3X0xcAVwu5lVEALx59194Fhvdveb3X2Nu6+ZO3du8UsrIiIyDvFSF0BEREQmTRuwpGB7cbSv0HXAZQDu/pCZVQMtwMXAm8zsM0AzkDOzEXf/YvGLLSIicmoUfEVERGaOdcBKM1tGCLxXA28ddc5O4FLgVjNbBVQDHe7+yvwJZnYjMKDQKyIi04W6OouIiMwQ7p4B3gPcC2wmzN680cw+bmZXRqd9EHiXmT0J3Alc6+5emhKLiIhMDLX4ioiIzCDuvpYwaVXhvo8WvN4EXHKcz7ixKIUTEREpErX4ioiIiIiISFlT8BUREREREZGypuArIiIiIiIiZU3BV0RERERERMqagq+IiIiIiIiUNQVfERERERERKWsKviIiIiIiIlLWFHxFRERERESkrB03+JrZe81s1mQURkRERERERGSixcdxznxgnZk9BnwVuNfdvbjFEhERkeMxs0XA6RTU5+7+y9KVSEREZGo6bvB19xvM7O+A1wF/AnzRzO4G/tPdny92AUVERORIZvZp4C3AJiAb7XZAwVdERGSU8bT44u5uZnuBvUAGmAV8x8x+7O4fLmYBRUREZExvAM5y92SpCyIiIjLVHTf4mtn7gHcAncAtwN+4e9rMKoDnAAVfERGRybcNSAAKviIiIscxnhbf2cAb3X1H4U53z5nZ7xWnWCIiInIcQ8ATZvYTCsKvu/9V6YokIiIyNY0n+P4Q6M5vmFkjsMrdH3b3zUUrmYiIiBzLPdFDREREjmM8wffLwPkF2wNj7BMREZFJ5O63mVklcGa0a4u7p0tZJhERkalqPMHXCpcviro4j2tSLBERESkOM/sd4DZgO2DAEjP7Yy1nJCIicqTxBNhtZvZXhFZegL8kTKghIiIipfNZ4HXuvgXAzM4E7gQuKGmpREREpqCKcZzz58BvAW3AbuBi4PpiFkpERESOK5EPvQDu/ixhlmcREREZ5bgtvu6+H7h6EsoiIiIi47fezG4B7oi23wasL2F5REREpqzxrONbDVwHnANU5/e7+zuLWC4RERE5tr8A3g3kly96APhS6YojIiIydY1njO/twDPA7wIfJ9xR1jJGIiIiJeTuSeBz0UNERESOYTzBd4W7/6GZXRUtnfBNwl1lERERmWRmdre7v9nMngJ89HF3f3EJiiUiIjKljSf45tcE7DGzc4G9wLziFUlERESO4X3R8++VtBQiIiLTyHhmdb7ZzGYBNwD3AJuATxe1VCIiIjImd98TvewEdrn7DqAKeAnQXrKCiYiITGHHbPE1swqgz90PAL8EzpiUUomIiMjx/BJ4ZXRz+j5gHfAWwlwcIiIiUuCYLb7ungM+fLIfbmaXmdkWM9tqZh8Z43irmf3MzB43sw1mdkW0f060f8DMvniy15fjc3e6B5Ls6hwk50cMFRMRkanL3H0IeCPwJXf/Q8IKDCIiIjLKeMb43m9mHwK+BQzmd7p797HeZGYx4CbgtcBuYJ2Z3ePumwpOuwG4292/bGargbXAUmAE+Dvg3OghEyCTzbH3wAjtB4Zp6x6ivXuY9u5hhlNZAGbXV/KyM1u4aOUcGmsTJS6tiIgch5nZywktvNdF+2IlLI+IiMiUNZ7g+5bo+d0F+5zjd3u+CNjq7tsAzOwu4CrCGOHCz2mMXjcRjU1y90HgV2a2Yhzlm5Lcna17B3hg0362tPXRUJNgVn0ls+sro+eqg9vNdZXEKmxCrz8wkqatKwTb9gPDtHUNsb93hFzUqFsZr2DBrGpeumwWC2fXUBmvYP3WbtY+1s6PHm/nnNZmXn5WC2cubKDCJrZsIiIyIf4a+N/Ad919o5mdAfysxGUSERGZko4bfN192Ul+9iJgV8H2buDiUefcCNxnZu8F6oDXnMgFzOx64HqA1tbWkyzmxEplcjy2rZsHNu1nz4ER6qpiXLhiDiPpLAcGUjzX3k/vUPqw9SfMoKk2cTAQHwrHlcyqC6/jsbF7pedyTkdfkvbuIdqiFtz27mH6htMHz2mqTbBwdg3ntjaxcHYtC2fX0NJQRcWosH3hijns7x3h4Wc7eeS5bp7a0aNWYBGRKcrdfwH8omB7G/BXpStR8XTd/k0yj7ZDNgvuuBOeAXLRNhx+bMznQ68tZtTOraVuQS3VzdXYBN+AFhGRqeW4wdfM3jHWfnf/+gRc/xrgVnf/bNRd63YzOzcaW3xc7n4zcDPAmjVrSjpA9cBAigef6eA3z3YylMxy2qwa3nJJK+edMZvK+OGhNZPN0TuUprs/xYHBFN0DyYOvt+0b4PFtqYMts3mNNYmDgXhWfSVDyQzt3cPsOTBMOhtOrjCY31zDmQsbWDi7JnrUUl89nob9YF5TNa+/cDGXn7+Qp3b08NCWzsNagX/rrBZWqhVYRKRkzOxf3P2vzex7jL2O75UlKFZR9XxvLem2TjCw8D+YhefC10d7Pmxf9P5sMkvHhg46NkCssoLa+XXULaijbn4tlfWVpf6RRURkgo0nEV1Y8LoauBR4DDhe8G0DlhRsL472FboOuAzA3R8ys2qgBdg/jnKVnLvzwv5BHti0n6d29ODAua3NvHL1XJbPrw+V7BjisQrmNFQxp6FqzOPZnNM7lOLAQIrugcOfd3YOsWFHD1WJChbOquG3zp7Lwlkh5M5vrj5qy/CJiscqOO+M2Zx3xmz2947wm2c7WVfYCnxWCxetUCuwiEgJ3B49/3NJSzGJlt99B9z0V5BOTujnZkYyDO4dZHDfEIN7B+nf1Q9Aoj4RheAQhGOVGjotIjLdjaer83sLt82sGbhrHJ+9DlhpZssIgfdq4K2jztlJCNK3mtkqQrDuGMdnl1Qmm+PxbQd4YPN+dncNU1MZ41XnzOOSVXOZXT92mD0RsQqLujxXsXyM4zn3cAN7klpd5zVVc+WFi7misBX40XZ+9Fg750ZjgdUKLCIyOdz90ejlemA430sqmlTy1CuhGSReHadpaRNNS5twd1J9KQb3DTK4d4i+7X30bO0Bg+rZ1dTNr6N+QR01c2qwmOo7EZHpZvx9YA8ZBI477tfdM2b2HuBewiyTX40m3/g4sN7d7wE+CHzFzN5P6K51rXsYqWNm2wkTX1Wa2RuA142aEXrS9Q2lefCZDh7a0snASIb5zdW86eVLuGD5bKoSk3c3uFQB84hW4C2drNvaxQa1AouIlMJPCHNjDETbNYT1fH+rZCWaxsyMqqYqqpqqmH3mbDznDHcNhxbhvUN0be6ia1MXFg9jg+sXhK7RlY2Vk3YjWkRETt54xvgWjiGqAFYDd4/nw919LWGJosJ9Hy14vQm45CjvXTqea0yGHR2hO/MTLxzAHVYvaeKVq+ey8rSGGVvZzWuq5sqLFnPFBQvZMKoV+EWnN/OyM1tYcVrDhM9WLSIiB1W7ez704u4DZlZbygKVE6sIAbd2bi1zXwTZVJah/UMHu0bvezyMyorXxKmbX0vd/Dqq51STqEtQMUHDjkREZOKMp8W3cAxRBtjh7ruLVJ4pI5PNsWFHD7/ctJ+dHUNUJyp4xap5XLKqhbmN1aUu3pQRj1Vw/hmzOX9UK/CT23swoK46TkNNnIaaBI01Cepr4jTWJGioSRzc31CToLYqpq7SIiInZtDMznf3xwDM7AJguMRlKluxyhgNixtoWNwAQHowHYXgQQbaB+nd3nfw3HhtnMr6ShL1CSrrK6msT5CInjVeWESkNMYTfHcCe9x9BMDMasxsqbtvL2rJSqRrIMm9T+zhV5s76RtOM7exit+/eDEXrpxD9SR2Z56O8q3Al5+/kI27etnbM0zfUIaBkTR9w2k6+pL0D6fJZI+cgLvCOCIMjw7M9dVxKmMVxGMVJGJGPFZBLGYKzCIyU/018G0zayfMXbwAeEtpizRzJOoSNC9vpnl5M+5OsidJsidJaiBFaiBNeiDNQNsA2WT2sPfFKmMHA3F4jl43JIhXx2dsTzIRkWIbT/D9NoePF8pG+y4c+/TpK5tz3vilX7O/P8nZixp5y+pWzlrUqGB1ghLxCl66bBYw64hj7s5IOkf/cDp6ZOgreJ3f3949TP9w+ohlncYSq7CDQTgRqyAevY7H7OD26Of88QXNNaxe0nRCSz6JiEwF7r7OzM4Gzop2bXH39LHeI8VhZlTPqqZ61pE9wrLpLOmBdBSGUwefh7uG6dvVd9iCVBazw1qHKxuqqD+tjkSd5s4QETlV4/lrP+7uqfyGu6fMrCwXuItVGB99/WraR5K0NKk7czGYGTWVMWoqY8w7znecc2comWVgOLQYD45kSGedTDZ38DmTddLRc9h/5L5UOsfgSOaw/emCc81g2bx6zm1t4pzWJnVlF5FpIRrP+wHgdHd/l5mtNLOz3P37pS6bHBJLxIjNio0Zij3npAfTBa3Eh1qLB/cO4lEPqapZVTQsaqBhUT1VzVVqFRYROQnjCb4dZnZlNAszZnYV0FncYpXOpavmc98L+xmjN65Msgoz6qtDF+cFs2om/PPdnd1dQzy9s5end/Zyz7o27lnXxvzmas5tbeLc1maWtNSqxV9EpqqvAY8CL4+22wg9shR8pwmrMCobKqlsOLI9wd1J9acYaBugv22Azqc76Xy6k0RtnPooBNfOq8U0iaSIyLiMJ/j+OfANM/titL0beEfxiiQyOcyMJS11LGmp4/LzF9LVn2Tjzl6e3tnDz57ax0827KOxJsHqJU2c29rEytMaSMQ1U6eITBnL3f0tZnYNgLsPmZoCy4aZUdVYRVVjFXNWzSEzkmGgPYTgnm09HHjuABWJCuoX1tOwqJ660+qIaS4SEZGjOm7wdffngZeZWX20PXCct4hMS3Maqvjtc+bx2+fMYzCZYfPuXjbu7OXxbd385tlOKuMVnLWokXNbm1i9uIk6jQsWkdJKmVkN0ShRM1sOJEtbJCmWeHWc5jOaaT6jmVwmx+DeQfrbBhhoH6BvRx9UQN28OuoXhSCcqNW4YBGRQuNZx/eTwGfcvSfangV80N1vKHbhREqlrirOmuVzWLN8Dplsjuf29IfW4F29PLWjhwqDZfPrD3aJntNQVeoii8jM8zHgR8ASM/sGcAlw7fHeZGaXAV8AYsAt7v5Po463ArcBzdE5H3H3tWb2WuCfgEogBfyNu/904n4cGa+KeMXBpZU85wx3DYcQ3NbPvkf3se/RfVTPqg4heHE9VU0aFywiMp4mq8vd/f/kN9z9gJldASj4yowQj1WwanETqxY38UZ3dnfmxwX38D+PtPE/j7SxoLmac1ubOXtRIw01cWqqYlQnYsRj6hotIhMv6tL8DPBG4GWE5Yze5+7HnIPDzGLATcBrCUOX1pnZPe6+qeC0G4C73f3LZrYaWAssJczv8Xp3bzezc4F7gUUT+5PJibIKo3ZuLbVza5n3krmk+lIHQ/DBccF1iYMtwbVzNS5YRGam8QTfmJlVuXsSwjq+gJq3ZEaqMKN1bh2tc+u44oIwLjgfgn/61F7u37D3sPMTMaOmMn5wJuvq6Hn060Pbh5+biJnu0ovIEdzdzWytu78I+MEJvPUiYKu7bwMws7uAq4DC4OtAY/S6CWiPrvl4wTkbgZrCvw+k9MyMqqYqqpqqaFk9h8xwhv72EIJ7tvZw4NkDWMyomVNDTUsNtS3hOVapscEiUv7GE3y/AfzEzL5GuKN8LaELlMiMN6ehiledM49XnTOPwZEMOzoGGUplGE5mGUllGU5nGU5Fr1NZBpMZuvqTDEfb2eMsVByrMM5e1MiVFy3SMksiMtpjZnahu687gfcsAnYVbO8GLh51zo3AfWb2XqAOeM0Yn/MHwGMKvVNbvCbOrOXNzFreTC6dY2DfIEP7hxjuHKZrcxddURVU1VRFzdxDQThRl9BNVxEpO+OZ3OrTZvYkoeJzQtem04tdMJHppq46zuolTeM+391JZz0KxZmDYTgfkodTWfqG0jzyXBef+e5mXnXOPF77kgVUadZOEQkuBt5uZtuBQcLN6f/H3p1HyXXW577//vbeNXVVV4+apZZtWfKIwUZ4DIMNdswMNwmxSUJgkZicACEkKwnJzSWEG3I4uUkIuXA5cYADYYghHMIywWADxowGbLCNLXmUB81SD2r13DX97h97q1Vqt+buru7q57PWXlW1h6q3S+p6+6l3cne/6DSf90bgU+7+D2Z2BfAZM7vQ3WsAZnYB8D+A62a62MxuAm4C6OnpOc2iyGwJUgHFta0U17YCUCvXGB8YZ6x3nPG+cYaeHmLwiUEgDsxTLcLLcmTbs+oeLSKL3olOS7uPOPT+GvAU8L/nrEQiS4SZkY6MdBRQPMbsmy+9aCX/de8u7nxwH/duG+DVm9dwyVkd+jZeRH75FK7ZBayre7w22VfvrcD1AO5+t5llgW5gv5mtBf4TeFOy6sOzuPvNwM0AmzdvPna3FmmYIBWQX5EnvyIPgNecyYOTjPWNM947zljfGMM7hgGwyMh15mhZFrcI57pzWjpJRBadowZfM9tE/K3vjcQTWnwBMHe/ep7KJiJAsSXFG190Blee282Xf7yTz33vaX70SC+vv2wda7tbGl08EZlnSRD9PeBs4EHgE+5eOcHL7wE2mtmZxIH3BuCN087ZDrwU+JSZnQdkgV4zayceT/wed//h6f8kspBYYGQ7smQ7srCxA4DyWJnxvnHGescY6xunb2t/3Axicffolu4c2c4sYTrEooAgMoIwIIiCIx6rtVhEFoJjtfg+AnwfeJW7PwFgZu+el1KJyLOcsbzAH776HH76eD+3/Ww3H/rqI1y2qZtXPH8VhazWaxRZQj4NlInr6JcD5wPvOpEL3b1iZu8gHrYUAp909y1m9n7gXne/Ffhj4F+TOt+BNyeTab2DOGy/18zemzzlde6+fzZ/OFk4Ui0pUj0pij3xXGfVcpWJ/gnGeuNxwgefHuJA0j36WCwwgsiSMBwk4bjucX1IjgLyy1vILcupZ5OIzKpjBd//g/ib4O+Y2TeAW4jHD4lIgwRmXL6pm+eub+f2+/fyg4f388DTB7j+4lVcee4yQn2rLrIUnJ/M5oyZfQL46clc7O63ES9RVL/vvXX3txKvCTz9ur8B/uZUCizNIUyF5Ffmya883D26PFqmVqklm1Or1vCpx/E+r9SoVZPjyX6v1KhOVClXy1OPaxXHa04fkOvO0X1BF/mVeQVgEZkVRw2+7v4V4Ctmlide6uAPgeVm9jHgP939jnkqo4hMk8tEvO6ytVx+Thdf+clO/vMnO7n70T5ef9k6Nq5ubXTxRGRulQ/dSVpwG1kWWcIsMNKt6Vl9zlqlxuCTB+l/pJ8d391JtiNL9wVdFNYUFIBF5LQExzvB3Ufd/fPu/mriSTDuA/5szksmIse1sj3H2647m7dccxalSo2P3f44n7rzSQaGtcKISBN7rpkNJdswcNGh+2Y21OjCiZyOIAro3NTB2a/cwKoXrKRarrLzB7t46htPcfCZIfw4ywCKiBzNic7qDIC7HyCeqfHmuSmOiJwsM+M569s5Z02R7z60j2/9Yi9bdx7kmues4JrnrCQdHff7LRFZRNxd0+lK07PQaN/QTtuZbQxtH6Jvaz+7795N30Mpus7rom19GxaqBVhETtxJBV8RWbjSUcC1z1vF5rO7+Oq9u7jj/r3c8/gAr7l0DRetb1cXMRERWXQsMNrOaKO4vsjwrhH6t/Sx56d76X2oj67zumg/s41AX/CKyAlQ8BVpMh2FNG96yZlceW43//njHXz6O09x9soCr7tsHas7c40unoiIyEkzM4prW2ldU2B0zyh9W/vZ97N99G3po+vcTjo2dBCkFIBF5Oj0CSHSpM5e2cofveY8fuXydewaGOcfb32YL/94B2OTJ7rcp4iIyMJiZhRWuQuWCAAAIABJREFUF1j/0h56rl5Hpi3D/vt7eeKrT9C3pY9qqdroIorIAqUWX5EmFgbGVect43lndvD1+3bzw0d6ufvRPnLpkCg0UmFAFBpRcps64namfTMfK2QjOgpp2lrSWlJJRETmnJmRX5EnvyLPeN84fVv76H2wj/5HBujY2EHnpg6irP7MFZHD9IkgsgTksxG/ekUPV2zq5mfbBihVapSrNSo1p1KpUa46lWqNUrnG6ESNStUPH68mjys1jjeXZmDQnk/TUUjTWTh0m5l63J5XMBYRkdmV686x7kXrmDgwQd/Wfvq39jPw6AAdZ7fTeW4nqVyq0UUUkQVAwVdkCVnT1cKarpZTutbdqTlxIK7WKFecSi2+HR4vc2C0xMBwiYGRSQ6Mlnh89zAHx8pHhGUzaMul6GhN05GPQ3Hn1P04KEehRmCIiMjJy3ZkWXvVGiaHJuPw+9gBDjw+SHF9kWx7hlQhTbqQIpVPaUIskSVIwVdEToiZERqEQQip6aupzDxpVqVa4+BYmYGREgdGklA8UmJgpMTT+0e5/6kDTF+SsZhL0V5IUcylaM2laM1FFHMpCrkUxVyU7EtpmSYREZlRpphh9eWr6b6wm/6HBxh6ZoiDTx084pwoF5EqpEgX0qTyKdKH7hdShJlQKyGINCEFXxGZM1EY0NWaoas1M+Pxas0ZGisfEYgPjJQ4MFqif3iSp/ePMjpRmbGLdSYVJIE4OhySsxGtLXX3k+CsVmQRkaUnXUiz6gUrWbl5BdXJKqWRMuWR0hG3o3tHqYwfOeljEAVJKE7VtRIfbi02DdkRWZQUfEWkYcLA6Ei6OB9NteaMTlQYHi8zNF5meDy+f/i2zN4DEzy2e5jxo8zmmUuHtOdTR4w3rr/NZyJ9uy8i0qTMjCgbxZNddT+7h1KtUqM8WqY0UqI8Uo6D8WiJyaESI7tH8fquSQaplhSFNQW6z+/SBFoii4h+W0VkQQsDo9iSotiSYs1xzq1Ua4cDcRKWh8fKDI1XGByNW5K37R1molw74rp0FNBxaFKuI8YcZ+gspGnNKRiLiDSrIArItGXItD27d5K7UxmvJIE4biUuDU1y4PEDDD45SOemTrrO7SRMTx8CJCILjYKviDSNKAyO24IMMD5ZYWC0xIHhUnw7UmJgOJ6Ua3vfKGOTR7YcR6EdDsbJ83e1ZljdkWNZW1YzVYuINCkzI9WSItWSomX54ckhJ4cm4+WTtvYz+MQBus7romNjhybNElnAFHxFZMnJZSLWZCLWdM48w/VEuRqPNU7GHdePQX5o+0FGJg6PB4tCY2V7ltWdLazuzLG6M8eajhy5jD5eRUSaVaaYYe1VaxgfmKD3F73sf6CXgccO0H1BN+1ntWkcsMgCpL/MRESmyaZCVnXkWNUx82zVpUqNvqEJdh8YZ/dAvG3dcZCfPt4/dU5HIc3qjhxrunKs7sixurOFztY0gbpMi4g0jVxnlp6XrGN0/xi9D/Sy99699D/Sz7LnLKPY06phMiILiIKviMhJSkdB0sLbAhvife7O0Hh5KgjvHhhn18AYW3cexJN5UTKpIAnBuakW4lUdOS3NJCKyyOWXt9Dysh5Gdo/Q+4s+dt+9m/6HMyy/aBn5VXkFYJEFQMFXRGQWmBltLWnaWtKct7Ztan+pUmPvgXF2HQrEB8a4d9sAk4/0JdfBsmLmcFfppJW4mEvpDyURkUXEzGhd00phVYGh7UP0PtjHju/tJLcsx/KLltGybObhNSIyPxR8RUTmUDoK6FmWp2dZfmpfzZ0DI6UkDI+xe2CcZ3pHuf+pA1Pn5DNRXTfpuIV4Rbsm0hIRWegsMNrOaKO4rsiBJwfp29LHM9/eTmF1nmUXLSPbnm10EUWWJAVfEZF5FpjR1ZqhqzXDRevbp/aPT1amxg0faiH+wSO9VKpxX+kwODSRVu6IybTymkhLRGTBsdDo3NhB+5ltDDx2gP6H+3nqG09TXF9k2YXdpFuPvQKBiMwu/bUkIrJA5DIRG1a2smFl69S+as3pPThxROvwwzuHuOeJgalz2vOpeDbpujDc1ZrRRFoiIgtAEAV0n99Fx9nt9D/cz8BjBxjaPkT7hna6L+gilUs1uogiS4KCr4jIAhYGxsqOHCs7cjx/Q+fU/qGxMrsPjLOrf2yqlfjhnUOHJ9KKApa1ZUlHAanQiMLDt4fvH9ofEIU2dVt/7hHHIqMlHZHPRupyLSJyksJ0yPLnLqdjUyf9W/o4sG2Qg08dpHNTB13ndRGmw0YXUaSpKfiKiCxCxZYUxZYU564pTu0rVWrsGzzcTbpvaJJytcZ4uUplokKlWqNSdcrTbk+WAflsRGsuojWXSraIYi5F4dBtNkWxJaIlE6nlWUSkTioXsXLzSjrP6aT3oT76Hx6g/5EBokxEmAmntqjufpiJpj0OCUKtCCByMuY0+JrZ9cCHgRD4uLt/cNrxHuDTQHtyznvc/bbk2J8DbwWqwB+4++1zWVYRkcUuHQWs686zrjt//JMT7k615pSrPi0Y147Yd+jx6GSF4fEyw+OHb/uGRhgaL88YogODQjZVF5Lj22Jyvz2fZm1XCykt6SQiS0y6Nc2aK1bTdV4nQ9uHqU5UqExWqU5WmTw4ydhElWqpetTrgyg4SlCOA3QqF5HKp0jlUwT6jBWZu+BrZiHwUeBaYCdwj5nd6u5b6077S+CL7v4xMzsfuA04I7l/A3ABsBr4lpltcvej//aLiMhJMzvU5Rni7x9PjbszWa4xNF6eFowP3x8ar7D3wDhD42VqdRk5Co31y/JsWFlgw4pW1i/Pa21jEVkysu3Zo8707DWnWq5SnYgDcaV06H6FainZl4Tl0lCJymQFrzz7S8gwG5LKp0gnQTiVT0+F4lQ+UuuxLAlz2eJ7KfCEuz8JYGa3AK8F6oOvA4f66bUBu5P7rwVucfdJ4CkzeyJ5vrvnsLwiInKKzIxsOiSbDlneduylOmrujE9WGZ4o03dwkm37Rti2d4RvPrCXO3wvYWD0dLfEQXhlK2csz5NJaeybiCw9FhhRJiI6idn7a5Ua1ckq5fEy5ZEy5dF4K42WGR+YYGjHcPwXeJ2ornV4KiAXksctKUzzOkgTmMvguwbYUfd4J3DZtHPeB9xhZu8E8sDL6q798bRr10x/ATO7CbgJoKenZ1YKLSIicyswI5+NJ8la2Z7jwmRJp/FSlaf2jbBt7zDb9o5w54P7+NYv9hEYrOtOWoRXFjhzeYGsJoEREZlREAUEUUAqn4LuZx/3mlMZr0yF4UPBuDxSYrx3jKHtlSODsSXBuCVFmA4IUiFhKr4N0sHU/fi27n46LodpngdZIBo9udWNwKfc/R/M7ArgM2Z24Yle7O43AzcDbN68+eRnaBERkQUjlw45f10b569rA2CiXOXp/aNTQfi7W/Zz54P7MIO1XS1sWBEH4bNWFMhpLWMRkRNigU217LbMcNxrTnmsrqU4aTWujFcoj1WolUvUylWq5dqzWo5nEiSBOEyF8W0SnoNUEI9Dbjnc0hzlIrUuy5yZy78UdgHr6h6vTfbVeytwPYC7321mWeLvpk7kWhERaWLZVMi5a4pTM1eXKjWe3h93i962d4TvP9zLXVv2Y8DqzhxnJV2jL6ib6VpERE6OBUa6kCZdSB/zPHfHq061XKNWrlIr16iWalOheGpfuUatdGhfjcp4hepQiVqp9uzJu4wjgvARXa8VjOU0zWXwvQfYaGZnEofWG4A3TjtnO/BS4FNmdh6QBXqBW4HPm9k/Ek9utRH46RyWVUREFrh0FLBpdZFNqw8H4e29o0kQHubuR/v4/tZerjqnm1+74FmjY0REZBaZGRZZPGN07tQiRa1aozxaOdzduq6leXTvKJXxyrQXnTkYp1oijUeW45qz4OvuFTN7B3A78VShn3T3LWb2fuBed78V+GPgX83s3cSdJd7s7g5sMbMvEk+EVQHerhmdRUSkXjoKOHtVK2evagVWUanW2N43Rvsp/gEmIiLzKwgDMsU0meLMrcu1ao3K2AzjkY8SjC00WtcUKPYUya/Ka7ZqOcKc/nWQrMl727R97627vxW46ijXfgD4wFyWT0REmkcUBvF4Xy2FJCLSFIIwIN2aJt2aZqYV6r165Hjk8YEJhncMM7R9mCAV0Lq2NQ7BK1rUEiwNn9xKRERERETkpFloU8EYoH0DrHz+Ckb3jTL0zBDDO4c5+NRBwkxIcV0rxfVFct05zTS9RCn4ioiIiIhIU7DAKKwqUFhVoFatMbInDsGDTx3kwBODRLmIYk+R4voi2Y6MQvASouArIiIiIiJNJwgDimtbKa5tpVquMrJrhKHtQww8PsDAowOkW1NxCO4pkmnLNLq4MscUfEVEREREpKmFqZC2M9poO6ON6mSVoZ3DDG0fom9rP31b+sm0Z5IQ3HrcpZxkcVLwFRERERGRJSPMhHRsaKdjQzuV8QpDO4YY2j5M7y966f1FL9muLG09RVp7WknlUo0urswSBV8REREREVmSolxE56ZOOjd1Uh4tM7R9iIPbh9h333723befdDFNtj1Dpi1DJrlN5VMaG7wIKfiKiIiIiMiSl8qn6Dqvi67zupgcmmR4xzDjAxOM908wtH146rwgCsi0pY8Iw5n2LFEmbGDp5XgUfEVEREREROpkihkyFxye8KparlI6WGLi4CSTgxNMDk4yvHOYwScPTp0TZaO6IJzctqUJQq0vvxAo+IqIiIiIiBxDmArJdefIdeem9rk7lYkKk4OTTB6cZHJwkomDk4w9PobXPD7JIF1IHw7CxTRRNiLMhkTZiCAVqNv0PFHwFREREREROUlmRiqXIpVLUVhVmNrvNac0UpoKxBODk0wcmGB4x/CznySAKJME4UxElA0Jp27DwyE5ExFmQoJIrcenSsFXRERERERkllhgcVfp4pFrA9cqNUrDJSqTVaoTlcO3E1Wqk/FtabhEZaKCV33G5w6i4HAQzoZEmRALDHfA/ei3tbiFmhmOH9p/6NYMopYUqZYUqXyU3MaPw2y4aFuoFXxFRESWEDO7HvgwEAIfd/cPTjveA3waaE/OeY+735Yc+3PgrUAV+AN3v30+yy4ispgFUUC2I3tC59YqNSoTFaoTVSpJKD4cluN95ZEy4/3jUAMsDtxY3BJ93NsgvrUAAguS/YAZXnPKI2XG9o1Rq9SOKJcFNhWGo0OBOJ8i1RLFt7kUFi7MYKzgKyIiskSYWQh8FLgW2AncY2a3uvvWutP+Eviiu3/MzM4HbgPOSO7fAFwArAa+ZWab3L06vz+FiEjzC6KAdCENheOfO5eqpSrl0TLlsTLl0UpyGz+e2DNCdeLZVUCUiw63FufjgJw7s4XcDM8/nxR8RURElo5LgSfc/UkAM7sFeC1QH3wdKCb324Ddyf3XAre4+yTwlJk9kTzf3fNRcBERmX9hOiRMh0dtqa5Va1TGKnXhuEw5eTzeP8HQzmGoQWclpeArIiIi82YNsKPu8U7gsmnnvA+4w8zeCeSBl9Vd++Np166Z/gJmdhNwE0BPT8+sFFpERBamIAxIt6ZJt6ZnPH5o5mtrXz7PJXs2TQsmIiIi9W4EPuXua4FXAJ8xsxP+e8Hdb3b3ze6+edmyZXNWSBERWfgOzXwdtcwcjOeTWnxFRESWjl3AurrHa5N99d4KXA/g7nebWRboPsFrRUREFiS1+IqIiCwd9wAbzexMM0sTT1Z167RztgMvBTCz84As0Jucd4OZZczsTGAj8NN5K7mIiMhpUIuviIjIEuHuFTN7B3A78VJFn3T3LWb2fuBed78V+GPgX83s3cQTXb3Z3R3YYmZfJJ4IqwK8XTM6i4jIYqHgKyIisoQka/LeNm3fe+vubwWuOsq1HwA+MKcFFBERmQPq6iwiIiIiIiJNTS2+IiIiIgtNKgNBCF4DM5gcb3SJREQWNQVfERERkYUiTEEQwFWvhytfC2EE2+6Hn30Ttt0HQQQlhWARkZOl4CsiIiLSaEEYbxdfAy+5EfLFw8c2bY63yXF49B6493bY9Vh8fnmycWUWEVlEFHxFREREGsUsbuXd+Hy47s3Qsfzo52ZycNGL4m30IGz5YRyCB/bE829Xy/NVahGRRUfBV0RERKQRUhlYfTa8/Hdg5Rknd22+DS59Rbwd2Ae/+G7cHXp8GKoVqGmlKRGRegq+IiIiIvMplYX2ZfDKm+CMC0//+TpWwIvfAC/6Ndj7NNx/J/zirjgAlyfB/fRfQ0RkkVPwFREREZkPqSzk8nEL77mXxd2cZ5MZrDoTVr0Vfvkt8MxWuO9b8PCPwQJNiiUiS5qCr4iIiMhcitLx9rLfhItfBmE4968ZBHDmhfFWKcMTP4d774CnHoxfvzQx92UQEVlAFHxFRERE5kKUiltaf+lX4IrXQDrTuHKce1m8TYzBIz+JJ8Xas00zQ4vIkqHgKyIiIjKbDi1NdMm18JI3QEvx+NfMl2wLPO/qeBsZPDwz9IG9mhlaRJqagq+IiIjIbDi0NNE5L4BrfzuewGohK7TDZa+Mt4G98OD3NDO0iDQtBV8RERGRU5XOxQExDGH9hfDS34AV6xtdqpPXufLwzND7noEHvgP3fyduAS6XwGuNLqGIyGlR8BURERE5EUEYr71bnoxbc8+4EM54Dqw7B9qXz/4szY1gFq8pvPItcav1jkfgvm/D1h8BppmhRWTRUvAVERGRha8R3W7T2XgNXDNYtQE2PA96zoXVZ8fHml0QwPrz4+3V/w22PQA//2Y8Q3QQKQSLyKKi4CsiIiILW6EDnn9dPAHTyCCMDcHEaLwkTxDG42rNAIdqFSqlk3+N+tbcYhesvwDOugjWnhN3A26G1tzTEUaw6fnxVpqAR++Bn90RtwhjmhRLRBY8BV8RERFZ2KIUvOJ3n72/VoOJERg9CKND8e3YUByOD/bBcH98f3w4CcqTEEVxa6XZ4VbklWcmrbnnwZqNkMnN78+32KSz8JwXxtvIIHzn8/DAd+MJsTQWWEQWKAVfERERWZyCIF4qqKUIJzKBcq0KY8NxOB49CK2d0LVarbmno9AOr/59uPJ1cNvHYftWrQssIguSgq+IiIgsDUEYB7VCe6NL0ny6VsNvvRe2Pwxf/RgM9kJ5otGlEhGZEszlk5vZ9Wb2qJk9YWbvmeH4h8zs/mR7zMwG6479DzN7KNl+fS7LKSIiIiKzoOc8+P0Pw+veAfn2eNy0iMgCMGctvmYWAh8FrgV2AveY2a3uvvXQOe7+7rrz3wlcnNx/JXAJ8DwgA9xlZl9396G5Ku9SFyRzgmhkjoiIiJwWM7jgKjjnUvjJ1+C7X4i7mVc0AZaINM5ctvheCjzh7k+6ewm4BXjtMc6/Efj35P75wPfcveLuo8AvgOvnsKxLWmDQmo7Y1FUgnwoJDEINdxIREZHTEaXgqtfBu/8VLrkWonTc3VxEpAHmMviuAXbUPd6Z7HsWM1sPnAncmex6ALjezFrMrBu4Glg3w3U3mdm9ZnZvb2/vrBZ+qQgNOrIpXtzTxfndrfzyWcu57szlPGdZkc5sisAgCpSCRURE5BTlCvGs3G//Z9h4CaTSjS6RiCxBC2VyqxuAL7l7FcDd7zCzFwA/AnqBu4FnrVzv7jcDNwNs3rzZ56+4zSE0WJ7PcNnqDoK6GS1bUiFndeQ5qyNPuVpj3+gkO4bH2T86iZlRqemtFhERkZPUsQJu/AvY+Rj8179A/25NgCUi82Yug+8ujmylXZvsm8kNwNvrd7j7B4APAJjZ54HH5qCMS1ZosLaY45IVbdgxlnFIhQFriznWFnPU3OkbK7FzeJzdIxNUa1BzRzFYRERETtjaTfC2v4dHfwpfuzleY1lLIInIHJvL4HsPsNHMziQOvDcAb5x+kpmdC3QQt+oe2hcC7e7eb2YXARcBd8xhWZeU0GBDR54LuluPGXqnC8xYns+wPJ/hYncOTlbYPTLBjqFxxitxg7wag0VEROS4zODcy2Dj8+He2+HOz0G1CpVSo0smIk1qzoKvu1fM7B3A7UAIfNLdt5jZ+4F73f3W5NQbgFvcvT4ypYDvJ6FsCPhNd6/MVVmXktDg/O5WNnYWTut5zIz2bIr2bIrzu1sZK1fZMzLO9qEJDk6WCTAqrhQsIiIixxBGcNkr4bkvge9+Ee75RjwDdO1ZI9xERE7LnI7xdffbgNum7XvvtMfvm+G6CeKZnWUWBQbPW9HG+raWWX/ullTIho4CGzoKlKs19o5OsmNonN6xSUwhWERERI4lm4dffgtc/mr44Vfgoe9DuQTVskKwiMyKhTK5lcyx0ODS1R2sKmTn/LVSYcC6Yo51ybjg3kPjgocnqLnGBYuIiMhRtHXDK34HXv5W2PMkPPQDePB78TjgWhWq6gAoIqdGwXcJCM24cm0Hy1oy8/7agRkr8hlW5DNcssIZnKywe3icHcMTTFSqGFBVChYREZF6ZrB6Q7xd+ybo3QEP/RB+cReMDILXFIJF5KQo+Da5VGC8cF0X7dlUo4uCmdGRTdGRTXHBsiKj5Qp7hifYPjTO0GSFwOa+S3RkBgbVmhMFRhgYpWoN0MRcIiIiC5IZLO+Ba3rgmhvjZZC2/Age+A4c7AVMk2KJyHEp+DYpI+5y/OKeLlrTC/OfOZ+KOLuzwNmdBUrTxgUHp7FecAAEgeFJt+pcFNKajmjPRrRmUrSmQwrpiFQQADBaOjw79XwFcBERETlFXavhRb8ab4P7YevdcP934kAcBFoaSURmtDATkZwWA7JRwIt7umlJhY0uzglJhwE9xRw9xRzVmtM7PsnOoQn2jEzgDtUZxgVPb71tSYW0ZeKZpgvpOOy2ROFxl2zKpyM2dhbYeCiAJyG4d7x0WgFcRERE5lj7crjytfE2PAAP/xjuvxP2PQNBqBAsIlMUfJtMYJBPhbxoXTeZKGh0cU5JGBgr81lW5rO4O4OTZXYNT7BzOA7BremQ9myKYjqiNRNRSEWkwtn5WdNhQE9bCz1tLc8K4JqYS0REZAFr7YRLXxFvo0Pw6E/hvm/D7icgSifrBCsIiyxVCr5NJDBoz6S4al3nVDfexS4eF5ymI5vmwmXFeX3t6QH8wESZ3cMT7BgeZ1LjgkVERBaufBEueVm8TYzBvqehbyfseQr2Pgn9e2ByDFJpcIfSJOirbZGmpuDbJEKD7lyGy9d0EAbH7torJ8/M6Myl6cyluXB5kZFShT0j8cRcw6UKAVBRfSkiIrLwZFtg/fnxVm9yHPp2xYF439Owe1v8ePQgpJKVMMoTcTAWkUVPwbcJhGasLmTYvKr9uONZZXYU6sYFT9aNC+4bn/1ZJQ2jqkpXRERkdmVysObseKtXKccTZfXthP3b467SvTtgaADCKB47XJqIl1QSkUVDwXeRC804oz3HRcuKCr0NkgkD1re1sL6thZo75Vnu/7xraJwHe4e03rGIiMh8iFKwYn28XXDV4f21KhzYHwfix+6FB+6KW4Or5YYVVUROnILvIhYanNOZ59zu1kYXRRKBGZlwdr+AOKsjTzoM+NneQYVfERGRRglC6FoVb+e8AK6+EX70FbjnG3EA1lrCIgtac8yAtMQYcei9aHlRoXeJWFvMccWaTkK16ouIiCwMhXa47s3wR5+Aq14P6ezhscEisuAo+C4SgcVhNxcFbOjI86Kebs5szze6WDKPluczvHBdJ5EmLxMREVk4cnm4+gb440/CS26AbF4BWGQBUlfnBSy0eGL9Qiqip5hjdWuWQlr/ZEtZZy7NS3q6+N72fso1rSksIiKyYGRycNXr4nWE7/sW3PUFKJfimaFFpOGUohaYKDBq7nRl06xry7EqnyEThY0uliwgxUyKa87o5q7t/UxWagq/InJSzOx64MNACHzc3T847fiHgKuThy3AcndvT479HfBK4h5j3wTe5a5p50WOkErH4ff518GD34Nvfy5eM7ikACzSSAq+C0CUjNtcWciwtphjeUtG3VnlmFpSEdes7+Z72/sZK1fRggoiciLMLAQ+ClwL7ATuMbNb3X3roXPc/d11578TuDi5fyVwFXBRcvgHwIuBu+al8CKLTRjB866Bi14MW38M3/4sjA4qAIs0iIJvAwQWT1CVCgLWFLOsLeTozKW0HJGclGwUcvX6br6/o5+hUoVZXkVJRJrTpcAT7v4kgJndArwW2HqU828E/iq570AWSJNUY8C+OS2tSDMIQrjwKrjgyngZpG99Fgb3qwu0yDxT8J0noRmOU0hFrCtmWV3I0ZrR2y+nJxUGvLinmx/tGmBgvKTljkTkeNYAO+oe7wQum+lEM1sPnAncCeDud5vZd4A9xMH3I+7+8NwWV6SJmMXLIG3aDE89CN/6DPTuiMcBa+CSyJxT8pploRlmUHPHiLukFtMRy/MZVhUyZDVeV2ZZGBhXre3knt2D7B2dUPgVkdlyA/Ald68CmNnZwHnA2uT4N83she7+/fqLzOwm4CaAnp6eeSyuyCJhBmddBDf9P7DjkbgFePfjYEEcgl0DmETmgoLvKTDisIFD1Z10GNCajmjLRBQzKVrTEa3pkHQYqPuyzIvAjEtXt3P/viG2D40p/IrI0ewC1tU9Xpvsm8kNwNvrHr8e+LG7jwCY2deBK4Ajgq+73wzcDLB582Z9Gokcy7pz4S1/A/uegT3b4GAf9O+GA/vi+2MHoVaDKB0H5loVypONLrXIoqTgO4NDYyVDAzOra70NKaZTtGUjiukUhXRIIR0RKNzKAmBmPG9FkUwU8PjAiMKviMzkHmCjmZ1JHHhvAN44/SQzOxfoAO6u270d+F0z++/E3wG/GPinOS+xyFKwYn28zWRiDIb64iA81AcH9sfheHAfDA/A2HDcWhxFgEG1DJXyvBZfZDFQ8J0mHQasLGRoicKp1ttCOiSj1ltZBMyM87tbyYQBD/UOKfyKyBHcvWJm7wBuJ17O6JPuvsXM3g/c6+63JqfeANwybamiLwHXAA8SD0j8hrt/dR5UmT2EAAAgAElEQVSLL7I0ZVsg2wPLjzJ0wD0Ovwd7k4DcD4N7oX9PPInW8AGYGI1nmQ4jwONgXK3M648h0mgKvtOEgXHFms5GF0PktGzoyJMOjJ/vO6jwKyJHcPfbgNum7XvvtMfvm+G6KvC2OS2ciJw8M8gX4231hpnPqVVhZPBwq/HBvrg7df/uODCPJMssRal4Fmp3qJTi60SahIKvSJNa19ZCKgr4ya5Bqq70KyIismQFIRS74o1zZj6nUo5bh+u7VffvgQN74scjg3HI1jrEskgp+Io0sZX5LL+0rpMf7higovArIiIiRxOloGN5vM2kVoVHfgrf/iwMDWgdYll0gkYXQETmVlcuzYt7ukgFGqMuIiIipygI4fwr4B0fgRveA6s3QipDPNedyMKnFl+RJaAtm+Ka9d3ctb2fUrWG2n5FRETklJjBhufG287H4M7Pw/aH48mytAaxLGBq8RVZIvLpiGvO6KYlFWKw4LfQjCgwfUg1udBsVv/fQDw7v4iIzIO1m+BN74Ob/h7Ou/zw5FgiC5BafEWWkFwUct2Zy6bWql7IxitVRkoVhksVBifLHJysMFauUq05YWC4o0m7FikjbjDozKU5v6uVjmxqVp9fvfpFRObZ8nXwhj+Jl0/6/pfgge8ytWySyAKh4CuyxJgZ4SIIBoV0RCEdsXLa/lK1NhWIhybjUDxSqjBRqRGYYQa1mqPOVgtPAGCwqpDlvK4CxczsBl4REWmw9uXw6t+Hq98IP7oV7rktXvW7Mtnokoko+IrI4pIOAzpzaTpz6SP219wZK1cZLlUYKVUYnChzsHRkK3FlMTR1N6HA4lbeM9pa2NhZoCWlbnAiIk2t0A7XvQle9Ktwz9fhB1+OZ4UuKwBL4yj4ikhTCMymWomnK1VrDE6U2Tk8zu6RCaq1OCgrBs+tMGmB39SZ56z2vMbeiogsNdkWeOGvwOWvhvvvhLu+EK8DrKWQpAEUfEWk6aXDgOX5DMvzGS525+Bkhd0jE+wYGme8UgVYFOOeF4somZjs3K4C69taCDXoVkRkaUul4QXXwyXXwtYfwZ2fg9GDcQgWmScKviKypJgZ7dkU7dkU53e3Mlausnt4nB3DExycLBNgVDRp1ikJzWhJhZzf3crqQgYzBV4REakThvCcF8KFvwSP/wx+8jXYvQ1K4xCl4yCsJZFkjij4isiS1pIKObuzwNmdBUrVGvtGJ9kxNE7v2CSmEHxcRjyGtz2b4oLuVrpyaQVeERE5NjPYtDneIG793b0Ndj0OTz8Ee5+OxwNHEZQmFYZlVij4iogk0mHAumKOdcUcNXd6x0rxuODhCWquccH1Di1JtCqf5dzuAm2aoVlERE5Vvg02XhJv/Hq8b/gA7NkGO5MwvO9pqJYhTMUtxPpiWk6Sgq+IyAwCM1bkM6zIZ7hkhTM4WZnqEj1RqWJAdQnWuYdmaF5fbGFTl2ZoFhGROdLaAa11rcLuMDwQtwzvfAye2RKH4Vot7kI9OQH6elqOQcFXROQ4zIyObIqObIoLlhUZLVfYOzLJRDIx1mwZr9TYM7LwWpdDMxynJQrpactphmYREZl/ZlDsirdzL433ucPBvqRl+DF4egsM9cH4SNw6HKUhSL6grVWhXELheOlS8BUROUn5VMSGjrn5+HR3BifL7BqeYOfQBBPVxsw6HZlRw2nPpOhpy7EqnyWn1l0REVlIzKB9Wbydd/mRx8olGB2EkWQbHYy7Tw/2xuF4+ACMDcHESDyGOExDEAAO1SpUSg35kWTuzGnwNbPrgQ8DIfBxd//gtOMfAq5OHrYAy929PTn2d8ArgQD4JvAud3XmF5HmFrcup+nIprlwWZHR0uGll4YmKwQ2dxNuRYHhDsvzadYVc6zIZ0gFatkVEZFFKJWG9uXxdjyliSMD8shg3K16sDe+LY3H55Qm4kBcKUGlHAfkIIhblYMArK7OdE+2anyeJuhquDkLvmYWAh8FrgV2AveY2a3uvvXQOe7+7rrz3wlcnNy/ErgKuCg5/APgxcBdc1VeEZGFKJ+O2NhZYGMy6/TeJAT3jpfiEHwaTcEBcdAOA2NNIcvaYpauXJpAszKLiMhSks5C58p4Oxm1Wjz79KFQXJqIQ/LUvrrAPDkG46NxC/PA3ngrjccB3V1rGs+DuWzxvRR4wt2fBDCzW4DXAluPcv6NwF8l9x3IAmnieVRSwL45LKuIyIKXDgN62lroaWuhWnN6xybZOTzBnpEJ3KF6AuOCQ4s/YHNRyLpijjWFLMVMpCWIRERETlYQQCYXb6dichz6d0PvDti/Ix6r3LcLRg7Es1cHQRyia7M7p8hSNZfBdw2wo+7xTuCymU40s/XAmcCdAO5+t5l9B9hDHHw/4u4Pz3DdTcBNAD09PbNaeBGRhSwMjJWFLCsLWdydAxPJuODhcSarcXeqQ43BUWDU3GnLpOgp5lhVyGo2ZhERkUbL5GD1hnirV63C4H7o2xkH4T3bYN92GNwXtzJHqWSyrsljP38YQRAlY5cBPL6+lnS/TqUglY3Lkc1DtgAtrdBSjCcGm00rz5jd5zsFC2VyqxuAL7l7FcDMzgbOA9Ymx79pZi909+/XX+TuNwM3A2zevFnjf0VkSTIzOnNpOnNpnrO8yEipwp6kS/Shlt0V+QwpzcQsIiKy8IUhdK2Kt3NecHi/ezwhV9+uOBTvewb2PBlP1HUovLYU463QFgfZbB5y+STY5g8H3HS2LhAvDXMZfHcB6+oer032zeQG4O11j18P/NjdRwDM7OvAFcD3Z7hWRETqFOrGBYuIiEiTMIN8W7ytP7/RpVl05jLm3wNsNLMzzSxNHG5vnX6SmZ0LdAB31+3eDrzYzCIzSxFPbPWsrs4iIiIiIiIixzNnwdfdK8A7gNuJQ+sX3X2Lmb3fzF5Td+oNwC3Tlir6ErANeBB4AHjA3b86V2UVERERERGR5jWnY3zd/Tbgtmn73jvt8ftmuK4KvG0uyyYiIiIiIiJLw9Ia0SwiIiIiIiJLjoKviIiIiIiINDUFXxEREREREWlqCr4iIiIiIiLS1BR8RUREREREpKkp+IqIiIiIiEhTU/AVERERERGRpqbgKyIiIiIiIk1NwVdERERERESamrl7o8swK8ysF3im0eVYxLqBvkYXognofZwdeh9nh97H07Pe3Zc1uhCLmerm06bf4dmh93F26H2cHXofT88p181NE3zl9JjZve6+udHlWOz0Ps4OvY+zQ++jyOKm3+HZofdxduh9nB16HxtHXZ1FRERERESkqSn4ioiIiIiISFNT8JVDbm50AZqE3sfZofdxduh9FFnc9Ds8O/Q+zg69j7ND72ODaIyviIiIiIiINDW1+IqIiIiIiEhTU/AVERERERGRpqbgu8SZ2Toz+46ZbTWzLWb2rkaXaTEzs9DM7jOz/2p0WRYrM2s3sy+Z2SNm9rCZXdHoMi1GZvbu5Hf6ITP7dzPLNrpMInJiVDfPLtXNp0918+xQ3dxYCr5SAf7Y3c8HLgfebmbnN7hMi9m7gIcbXYhF7sPAN9z9XOC56P08aWa2BvgDYLO7XwiEwA2NLZWInATVzbNLdfPpU918mlQ3N56C7xLn7nvc/efJ/WHiD7I1jS3V4mRma4FXAh9vdFkWKzNrA14EfALA3UvuPtjYUi1aEZAzswhoAXY3uDwicoJUN88e1c2nT3XzrFLd3EAKvjLFzM4ALgZ+0tiSLFr/BPwpUGt0QRaxM4Fe4H8l3dI+bmb5RhdqsXH3XcDfA9uBPcBBd7+jsaUSkVOhuvm0qW4+faqbZ4Hq5sZT8BUAzKwA/G/gD919qNHlWWzM7FXAfnf/WaPLsshFwCXAx9z9YmAUeE9ji7T4mFkH8FriP1ZWA3kz+83GlkpETpbq5tOjunnWqG6eBaqbG0/BVzCzFHHF+jl3/3Kjy7NIXQW8xsyeBm4BrjGzzza2SIvSTmCnux9q2fgScWUrJ+dlwFPu3uvuZeDLwJUNLpOInATVzbNCdfPsUN08O1Q3N5iC7xJnZkY8ZuNhd//HRpdnsXL3P3f3te5+BvFEBXe6u77FO0nuvhfYYWbnJLteCmxtYJEWq+3A5WbWkvyOvxRNRCKyaKhunh2qm2eH6uZZo7q5waJGF0Aa7irgt4AHzez+ZN9fuPttDSyTLG3vBD5nZmngSeAtDS7PouPuPzGzLwE/J54d9j7g5saWSkROgupmWWhUN58m1c2NZ+7e6DKIiIiIiIiIzBl1dRYREREREZGmpuArIiIiIiIiTU3BV0RERERERJqagq+IiIiIiIg0NQVfERERERERaWoKviJLhJmtNLNbzGybmf3MzG4zs02NLpeIiMhSpbpZZP5oHV+RJSBZKP0/gU+7+w3JvucCK4DHGlk2ERGRpUh1s8j8UvAVWRquBsru/j8P7XD3BxpYHhERkaVOdbPIPFJXZ5Gl4ULgZ40uhIiIiExR3SwyjxR8RUREREREpKkp+IosDVuA5ze6ECIiIjJFdbPIPFLwFVka7gQyZnbToR1mdpGZvbCBZRIREVnKVDeLzCMFX5ElwN0deD3wsmTJhC3Afwf2NrZkIiIiS5PqZpH5ZfHvnIiIiIiIiEhzUouviIiIiIiINDUFXxEREREREWlqCr4iIiIiIiLS1BR8RUREREREpKkp+IqIiIiIiEhTU/AVERERERGRpqbgKyIiIiIiIk1NwVdERERERESamoKviIiIiIiINDUFXxEREREREWlqCr4iIiIiIiLS1BR8RUREREREpKkp+IqIiIiIiEhTU/AVERERERGRpqbgKyIiIiIiIk1NwVdERERERESamoKviIiIiIiINDUFXxEREREREWlqCr4iJ8nMXmJmO+seP21mL5vnMpzya5pZj5mNmFk42+WaK2b2G2Z2R6PLISIic8/MtpjZS5L7Zmb/y8wOmNlPzeyFZvboCTzHgq83zOx9ZvbZ07j+62b227NZprmW/P1xVqPLIUuTgq8sakkAHE8+SPea2afMrNDoci0k00Oyu29394K7VxtZrpPh7p9z9+tO93nMzM3s7OOcs8rMPmFme8xs2MweMbO/NrP86b6+iMhiY2a/ZGY/MrODZjZgZj80sxfM5Wu6+wXuflfy8JeAa4G17n6pu3/f3c85gec4ot44kc//hWymkOzuL3f3TzeqTKci+fvjydN5juRvvb85zjlmZn9gZg+Z2aiZ7TSz/zCz55zOa8vipuArzeDV7l4AngdcDPx5g8sji5SZdQJ3AzngCndvJf6Dqx3Y0MiyiYjMNzMrAv8F/L9AJ7AG+Gtgch6LsR542t1H5/E1ZfH7MPAu4A+I/+9uAr4CvLKRhZLGUvCVpuHue4HbiQMwAGaWMbO/N7PtZrbPzP6nmeXqjr/WzO43syEz22Zm1yf732JmDyctfk+a2dtOpUzHev3k+V9Vd25kZr1mdkny+DVJd69BM7vLzM47ymsc8c1nfVdsM/sM0AN8NWkV/1MzOyP55jtKzlltZrcm3+Q/YWa/W/dc7zOzL5rZvyXvxRYz23yMn/fDZrYjeT9/ZmYvrDuWM7NPJ93VHk7KUt9l/D3Jv8GwmW01s9fXHXuzmf2g7rGb2e+Z2ePJ+/NRM7Pk2Nlm9t2kdaLPzL6Q7P9ecvkDyXvx6zP8CH8EDAO/6e5PA7j7Dnd/l7v/4mg/t4hIk9oE4O7/7u5Vdx939zsOfR4mn80/NLOPJJ+5j5jZSw9dbGZtdT1odpnZ31jdMBsz+926unZrXf33tJm9zMzeCnwcuCL53P5re/Zwo3Vm9uWk/uw3s4/Ule0Hyf1nff5b3BL46rrnSSV1xsUzvRFm9qrk74VBi1vAL0r2/5mZfWnauR82s39O7h+1jp12zRE/17T34XrgL4BfT8r/QHL8LjP7neR+YGZ/aWbPmNn+pN5uS44dqvd/2+K/R/rM7P+c8V88Pv+VZnZfUpfvMLP3TTv+puR1+s3s/7K6nmVmdqmZ3Z28T3uS/xvpumunWt4t/vvlo2b2teT/wE/MbENyzMzsQ8nPMmRmD5rZhWZ2E/AbwJ8m78VXZyj/RuDtwI3ufqe7T7r7WNIL4INH+7ml+Sn4StMws7XAy4En6nZ/kLjifh5wNvG31e9Nzr8U+DfgT4hb9F4EPJ1ctx94FVAE3gJ86FCFfJKO+vrAvwM31p37y0Cfu//czDYlx/8QWAbcRhxe05wEd/8tYDtJq7i7/90Mp90C7ARWA78K/K2ZXVN3/DXJOe3ArcBHjvGS9yQ/ayfweeA/zCybHPsr4AzgLOJW1N+cdu024IVAG3GLwmfNbNUxXutVwAuAi4A3EL9/AP83cAfQAawlbqnA3V+UHH9u8l58YYbnfBnwZXevHeN1RUSWiseAqsVfWr7czDpmOOcy4s/vbuLP+S9b3HsG4FNAhbj+uxi4DjgU1H4NeB/wJuK69jVAf/0Tu/sngN8D7k4+t/+q/ngSov8LeIa4fllDXF8x7Xlm+vz/N46sh14B7HH3+6Zfn4ThTwJvA7qAfwFuNbNM8nqvMLPWujK9gbgOhOPXscfl7t8A/hb4QlL+585w2puT7WrierbAs+vrXwLOAV4KvNeO8oU6MEr879JO3EL638zsdcnPdz7w/xGHz1XEdfaaumurwLuJ/z9ckbzW7x/jx7uBuM7vIP777QPJ/uuI/y7blLzGG4B+d78Z+Bzwd8l78epnPyUvBXa6+0+P8bqyBCn4SjP4ipkNAzuIA+tfQfxtIXAT8G53H3D3YeKK44bkurcCn3T3b7p7zd13ufsjAO7+NXff5rHvEgepF3ISTuD1Pw+8xsxaksdvJA67AL8OfC0pWxn4e+Lut1eeTBlOoIzrgKuAP3P3CXe/n/jb9TfVnfYDd78tGRP8GWCmChcAd/+su/e7e8Xd/wHIEFeyEFdaf+vuB9x9J/DP0679D3ffnfxbfAF4HLj0GMX/oLsPuvt24DscbukvE3eNW538TD846jM8Wxew5yTOFxFpWu4+RByWHPhXoDdpvVxRd9p+4J/cvZx8dj8KvDI55xXAH7r7qLvvBz7E4Trwd4jDyz1JXfuEuz9zkkW8lDhQ/knyGifzmf9Z4sBaTB7/FnEdN5ObgH9x958kLd+fJu7ufXlS5p8Dh3opXQOMuf//7N15nGR3Xe//16equnpfZssyk8ySzTCBLDIkKmIQRFHEXFBxAgoRrrkq248LeuHKDRgvP/zdK7uIhhh2CCEKRoyEhCxsCUkGss1Mltl7tszS+1pV53x+f3xPdVev0zPp6qqufj/ncR5VZ6v6VnVPn3rXd/MH5niNnS9vAD7q7rvcfYDQ7WuzJa27En+d1No/CjzKDNdzd7/X3R9PrsePET6bXJns/j3g3939h+6eI3yZ7yXnbnH3B5LPAXsIXxJcycy+6e4PunuBEGhLr+WtwIWAuft2d5/r9VnXcpmWgq/Ugv+S9MV8KeEP5Mpk+yqgCdiSNLnpAb6TbAc4m/At9RTJN9sPJE2TeggX75XTHTuLWZ/f3XcA24FXJ+H3dxj/hng14RtskmNjQrAv/VZ1PqwGiqG8aO+k5zlccn8IaJh0IR1jZu+x0GytN3m97Yy/b6sJr6Goc9K5byxpRtYDPJ/Z3/PJ5SoOavaXgAEPWmia/eZZHmOy44RvsEVEBEgCxzXufhbh7/Jq4OMlhxxwdy9Z35scsw6oAw6V/F3/J+C05LgZr8En4WxgbxKaToq7HwR+BPyumXUQWox9ZYbD1wHvLr6O5LWcTXidEK7dxRZcr2fitfxE19j5MuFzQ3I/A5R+STHTdXMCM7vCzO6x0Hy8l1DrPu213N2HKKmpN7MLzOzbFgYc7SN84X/S13J3v5tQY/1p4IiZ3VDyJcWJ6Fou01LwlZqR1Mx+nlA7CnAMGAYucveOZGn3MBAWhD/cUwYsSpou/UvyOKe7ewehqbGdZJFO9Pww3tz5KmBbEoYBDhIutMUyGeEie2Ca5xkkBOyiMybtd2Z2EFhebKKVWDvD88zKQn/evyTU7C5L3rdext+3Q4Smx0Vnl5y7jlCb8DZgRXLuE5z8e467H3b3P3H31YRmaf9gcx/J8y7gNWamv40iIpMkraI+TwjARWuSa1TRWsK1pZNQK7qy5BrY5u4XJcdNew0+SZ3A2pm+jJ2DLxCaO/8+oTn1TNe+TuBDJa+jw92b3L3YSusbwEuTLlevYTz4nsw1dsK1PGkyvapk/2zX8uJzrStZX0toZv7sCc6bzlcJXZvOdvd24B+Z4VpuYdySFSXnfgZ4Ejjf3dsIfZNP+loO4O6fdPcXAhsJTZ7/orjrBKd+DzjLZhmTRJYmfbiTWvNx4BVmdklSS/pZQv/c0wDMbI2ZFfuC/jPwx2b2cguDQqwxswuBLKGJ7lGgYGa/SehrclLm8PwQ+v78OvBnjF8oAW4hNBV7uZnVAe8mfID48TRP9QihudZyMzuD0C+41LOE/j7TlbEzecwPm1mDhcE63kJoAnayWgkX2aNAxsyuI/TbKn1N7zOzZWa2hhByi5oJF7KjEAYXY+IHqzkzs99PPnwAdCePW+yzO+N7kfhoUuYvJGG8+DP7aPLeiIgsGWZ2oZm9u/g3NWm6ezXwQMlhpwHvsDA41O8DzwNuT5qlfhf4iJm1JdfZc82s2Oz1RuA9ZvZCC84r/t09CQ8Sgtjfmllzch178QzHTvf3/1vAzxNG//3iLM/zWeBPk5pQS57rVcVA6+5HgXuBzwG73X17sv1krrFPE1pUvSq57r+f8FmktPzrZ/li9mvAu8xsg4VpHYt9gk+6NpxwPe9y9xEL46G8vmTfrYSWar9kYdyRDzIx2LYCfcBA8pnqz07h+TGzFyXvdx3hS4ER5ngtd/dnCP2Qv2Zh0LBs8v5vNrP3nkp5pDYo+EpNSS4+X2R8AKn/QRgs4YGkyc1dJH1OPQx68MeEPke9wH3AuqRJ0jsIQa2b8Af/tlMs0ozPn5ThEGH6nF8Cvl6y/SnCt9CfItQcv5owQFVumuf4EqGvzh7Ch4zJgzZ9GHh/0jzrPdOcfzVhUJCDwDeBD7j7XSf7Qgkjan+HcPHeS7hIlTZnvp4wwMduwvtwK8mUGO6+DfgI4b14FngBoQnaqXgR8BMzGyD83N7p43MGfpAQanvM7HWTT3T3LsLPIp88Rj/hm+NeJg6aJiKyFPQTBq/6iZkNEgLvE4QvY4t+ApxPuFZ9CPg9dy82fX0j4cvkbYTr6a0kTVDd/RvJ8V9NnudbhIER5ywZe+LVhMGz9hGuMdON2A/T/P1392FCC68NwL/O8jwPA39CaHrbTbgeXDPpsK8SBkj86qTtc7rGunsvYRCoGwk1woPJ6yn6RnJ73Mx+Ok0xbyJ8Hvg+4To7Arx9ptd0An8OXJ9cA68jfB4qlnNr8rg3E750GCD08y5OcfUewuemfsIXBtMNJDkXbcn53YTPFMeB/5vs+2dgY/Kz/NYM57+D8abSPYRm9a8BpowCLUuHTeyWISKyMMzsz4DN7j7boBciIlKlzOwa4L+6+y9XuiynKmmddIG7T55pQOYgqV3uITRt3l3p8ojMRjW+IrIgzOxMM3tx0tzt5wg1Bt+sdLlERGRpsjDt0luAGypdlsXEzF5tZk1m1kwYD+VxxqeDFKlaCr4islCyhBE9+4G7gX8j9MERERFZUGb2J4TuOP/p7t+vdHkWmasITbcPEpq5b3Y1IZVFQE2dRUREREREpKapxldERERERERq2qnOe1Z1Vq5c6evXr690MUREpEZs2bLlmLuvOvGRMhNdm0VEZD49l2tzzQTf9evX8/DDD1e6GCIiUiPMbG+ly7DY6dosIiLz6blcm9XUWURERERERGqagq+IiIiIiIjUNAVfERERERERqWkKviIiIiIiIlLTFHxFRERERESkpin4ioiIiIiISE1T8BUREREREZGapuArIiIiIiIiNU3BV0RERERERGqagq+IiIiIiIjUNAVfERERERERqWkKviIiIiIiIlLTMpUugIiIyHzJeY7YYxpSDZUuilS7XA+MHJnfx6xrg8Yz5vcxl5poBPID0LCy0iURkRqj4CsiIouGuzPkQ/TEPfRFfXRH3RyPj9MT9dAf95Mnz/LUcv6o/Y8qXVSpRrke2P8t2HEjdD0Mqez8Pn6ch9W/BZf+LbSdP7+PXWviPPQ9Db1boedROPYg9G0LX0ZYCtZcBZd+GFrPrXRJRaRGKPiKiEhViTyiL+6jN+6lN+qlK+7ieHScvqiPQR/EMNKkcZw8+Snn533qNlnC8n2w/zbYeSMcewBSdVAYCPvi0fl/vgP/BoduT4Lb/wst58z/cywmcQEGdoaA2/0YHH8Qep+A4UOQbgzHFAaBePwcB/Z/Ew7+O5z1GrjkQ9CyoRKlF5EaouArInISRuPRUNsY9817wEpbmrZUG+2pdhqtETOb18c/VbHH9Mf99Ma9DMaDOD6vjz/ogxwvHKcr7qI/7mfUR8mQwTCi5N9k020TGZMfgAP/DjtvgqM/KH/YLeURRBF0/ksIwWf/Llz8v6FlfXmft9I8hoHdSQ3u4yHg9jwOQ/shXQ+kkoBb8n+30D/L4xUgKsC+b4QQfPbr4JK/gea15X4lIlKjFHxFREq4OwM+QG/US2/cG5rSRsfpiXsYiAeIicmQmffwV1QMewBNqSY6Uh2sSK9gWXoZ7al22lPttKZaSVt6Xp8357mx19wb93I8Ok5X1EVf3MeIj5AmTcpSuM//6y5QmPJ+TleTKzKrwhAc/I8Qdp+9JzRjLgarcofd6RSD296vhxC8djNcfD00n73wZZlP7jC0D3qeCCH3+IPQ/SgM7QXLQiodfhZeGD+n8Bz+P4+9j1+Dzltg3evh4r+GprOe+2sRkSVFwVdElpyCF+iL++iJeuiNe+mKuuiKuuiNexn2YVLJv5iYAoUp5+fILUg5++N++uN+OgudpJN/jlOgQIM10JZqY1l6GSvSK2hPtdOR6kku/CsAACAASURBVKAt3Ua91U95rNK+sb1RLz1RD8fj43RH3QzEAxQojNWyFigQlzY7JITTMmV9kVMXjcDB/wxh9/BdSc1uBcPudIrBbc9XYN/NsO4NSXBbU+mSzc4dhg9Az9aSgPsIDO4BS0MqA4VhmNDyJU/ZGmN4HqI87P4S7P0qbHgjvOCD0HhmmZ5QRGqNgq+I1Bx3Z8RHxmove6Iejkch5PV7PznPzRryJq9Xg8lNfod9mOFomGejZ0mRIpP8OS9QIE2allQL7al28uTpjXoZ8qET941VLassBtEoHPou7PocHPoOWKYk7I5UtmyzKQa3PV+CvV+BDdfACz5Q+VGg3WHk2aSJ8hNh0K/un8HArrA/lYVoGOJJX/hVqrdB8X3c9XnY/UU4583w/Oug8fQKFUhEFgsFXxFZlEr7nU5ukjwYDwJMqCGt5aa0MfGEWuiYmO64m+64e8qx6hsri9bhu2DHZ+HAt5Ow21fpEp2aYoDc9TnY/QU49y3w/P8FDaeV/7lHjoWBpXq3QtcW6Pop9O8ItdLphvClwuQvD6Lh8pfrVBTfx53/HN7L866Fi/4KGlZVtlwiUrUUfEWkapX2O+2L+yb0Ox324bF+p7FP3yRZIU+kRgwfgnt/K4wQXCtt7otNsXd8NoS38/4ULvqf8zN/ba57vIly90/h+MPQ/0x4znRjCI2TA221NA0/WcVyP/OPsOMGOP+tcNF7oX5FZcslIlVHwVdE5qzghbEpZkr7xg74wLw+j+OMxCMUKFBHXXhu9TsVWbriXGhyG9dOS40xY8HtM7Djn+CCt8LG90L98hOfm++D3m1JDe7PoOuhMDduNAjppvB+RUOTnm9hxihYcMX38em/h2f+AX7uHbDxLyG7rLLlEpGqoeArImNK+8YWB0GaqW/sTNPMzLeFGkhKRKSiik2Mn/4UPP1p+Ln/Bzb+BWQ7wjRAvduTGtxHwkBTfU+F4JtpDFMoFQYnPV7vwr+GalB8H5/6BDz1KXjef4cL3w3Z9sqWS0QqTsFXZIlR31gRkSoWFYPbx+DpT0C6GXJdkGkKc+UWBpnQ1CWvv8nTKjbl3v538ORH4fSXw4rLoeP50P58aNkAlqpsGUVkQSn4itQ4d6cn7mFffh878js4VDiEYeobKyJSzYrBrViTm1+kg3lVWvF9PHAbHLw9fIEQR+A5aFoLyy6BFVdA+0UhFDetBbPKlllEykLBV6QGDcfDdBY62ZXbxd7CXgoewu2EkKu+sSIispR4YeIXCAM7w7L/35NAnA/NxpvXw7JLQw1xMRA3rlYgFlnkFHxFakDBCxwuHGZ3fje78rvoi/tIk1azZBERkRPxPORL+kT3PxWW/d8cHwUboOUcWPbzsGJTuF35i5BKV6bMInLSFHxFFiF3pyvuYm9uLzvyOzgSHRkLusU+uZNHQBYREZGTEOcmjoLdm0wRte8bYZRxA9a+Dja8EVa9WH2GRaqcgq/IIjEYD9KZ72RXfhf7CvuIPSYmHuuPq365IiIiCyAeGR89etdNsPfrYGlYtzmE4JVXKASLVCEFX5Eq1hf38cjII+zM72QwHiRFSs2XRUREqoXHUOgP93d+FvZ8GVL1sP71sP4PYcWL1DdYpEoo+IpUoZF4hAdGHuCJ0SdwfKzZsmp1RUREqpRHUBgABuCZf4Bdn4N0E2z4Q1j/Blh22dIOwVEu9J3u2Qo9j8Hxn8DwYWjfGGrJ2y8KS9PZS/t9krIpa/A1s1cCnwDSwI3u/reT9q8FvgB0JMe8191vT/a9D3gLEAHvcPc7yllWkWpQ8AI/G/kZD408NKEZs4iIiCwixRBcGICnPgk7/gkyrbDhj0JNcMcLajfcxXno3xH6Q/c8Dsd+Ar1PwMizYbAwSL4gSKaX6NsWppsqDiTmHuZZLh1Zu/0iaDyzdt8zWRBlC75mlgY+DbwC2A88ZGa3ufu2ksPeD9zi7p8xs43A7cD65P5m4CJgNXCXmV3g7koBUpNij9k+up0fjvyQghemnVtXREREFiEvQKEQ5mR+8uPw9KchuwzOeVOoCW7fWOkSnpo4goFd4wH3+E+g5wkYPgDpBiCVBNySwTaLzcKnPNakgcT6toel81/DY0Wjod90y7mw/DJY/iLoSAJxw2nlfJVSQ8pZ43s5sMPddwGY2c3AVUBp8HWgLbnfDhxM7l8F3Ozuo8BuM9uRPN79ZSyvyIJzd/YU9nDv0L0Mx8PqvysiIlLLPA9RHoaHYPvfhSBcvxLO+WNou7DSpTuBGAb3wvEHQ1PlwU5IZ8PAXoWhEPCLCgPz9JSjYSnqfTwse2+BdD1EI5Cqg9bzYfkLw7JuM2Q75uf5paaUM/iuATpL1vcDV0w65oPAd83s7UAz8Gsl5z4w6dw15SmmSGUcLhzmnqF76Iq6VMMrIiKy1MQ5IAdD+2Dbh8OgWNUuGgnhvahQoS/sS0fWjkeh+2dh2fOV8GXCb9wfatVFSlR6cKurgc+7+0fM7BeBL5nZ8+d6spldC1wLsHbt2jIVUWR+9UQ93Dd0H/sL+xV4RUREZGpTXzk10TAM7obvvhh+/ceq+ZUJyjnJ2AHg7JL1s5Jtpd4C3ALg7vcDDcDKOZ6Lu9/g7pvcfdOqVavmsegi828wHuSuwbv4ct+X2VvYq9ArIiIiMt/iXOh7fOdLIN9X6dJIFSln8H0ION/MNphZljBY1W2TjtkHvBzAzJ5HCL5Hk+M2m1m9mW0AzgceLGNZZREYiUco+OILiznP8ePhH/P53s/zZO5JIiK8OJKhiIiIiMyveBT6n4E7fwXyMwyoJUtO2Zo6u3vBzN4G3EGYqugmd99qZtcDD7v7bcC7gc+a2bsIA11d4+4ObDWzWwgDYRWAt2pE56Wn4AUOFQ6xJ7+HXfld9MXhW7tV6VWcV3ce6+rWsTK9EqvSoe0jj3hi9Al+PPJjYo9VwysiIiKyUOJR6HsS7roSfu37UNdS6RJJhZW1j28yJ+/tk7ZdV3J/G/DiGc79EPChcpZPqou70xV3sTe3lx35HRyJjpAmTZ78hBrSZ6NnORYd48GR0Ajg7MzZnJs9l7V1a2lJVf6PmruzI7+D+4buI+c5jdQsIiIiUgnxKPRuh+/9KvzavZBprnSJpIIqPbiVLHGD8SCd+U525nfSWegk8tAMOCJU8BdvJ4uSfwC7CrvoLHQSE9NojWzIbmBD3QbOypxFndUtyOsY9VF6o166424eHH6Q/rhfgVdERESk0uIR6H0CvvdyePndkGmqdImkQhR8ZUEVvMCBwgF253ezO7+bgXhgrFb3uSieP+ADPDH6BE+NPkWBAivSKzi37lzW161nVXoVKTu1bu3uzoAP0Bf10RP30BP1cCw6Rk/cw0A8QExMhgyOK/CKiIiIVJNoBLofhbtfAS+7CzKNlS6RVICCr5SVu3MsOsbefGi+fDQ6SobMhObLMfH8PidOjjAlwNHoKMej42wZ2YLjrMms4bzseazNrKUt3TbhvIIX6Iv76I176Y16OR4dpyvqoi/uY8iHSCX/Yqbvr1t8ThERERGpMvFImOv3nt+Al30X0g2VLpEsMAVfmXcD8QD78vvYmQvNlyGE22LT5IUOiHHyD2BvYS8HCgdwnHqr57T0aQzEA/R7PznPkSGDYROaUpc+joiIiIgsUtEwdD0M9/wm/Op3IF1f6RLJAlLwlecs57nQfDkXmi8P+zApUlXb5LdYWzvkQ+wp7Jmwr1rLLCIiIiLzIBqG4z+Be18FL70d0tlKl0gWiIKvnLTYY45ER9ib38vO/E6OR8fJkJlQkzvToFQiIiIiIhUVDcOxH8N9r4aXfhtSCzMYqlSWgq/MSV/Ux77CPnbkdnCgcGCsOXCx+a/6t4qIiIjIohENw9Efwn1XwZX/pvC7BCj4yrRGfZT9+f3syu9iT34Poz6KYdMO6iQiIiIisuhEQ3DkPvjB78FL/gVSika1TD9dAULz5cPR4dB8ObeT7rh7SvNlEREREZGaEg3B4bvgh6+DX/4GpNKVLpGUyalNaio1w915cvRJbuy9kW/1f4uHRx7meHycmFihV0SkBpnZK83sKTPbYWbvnWb/OjP7npk9Zmb3mtlZyfZLzex+M9ua7PuDhS+9iEgZRENw6A740WaINU5NrVLwXaLcnb35vXyh7wvcPXQ3wz5Mnrym7BERqWFmlgY+DfwmsBG42sw2Tjrs74AvuvvFwPXAh5PtQ8Ab3f0i4JXAx82sY2FKLiJSZtEQHLwdfvyH4Po8XIsUfJegZwvPckv/LXx74Nv0xr2awkdEZOm4HNjh7rvcPQfcDFw16ZiNwN3J/XuK+939aXd/Jrl/EDgCrFqQUouILIRoCA7cBve/SeG3BqmP7xLSG/Xy/eHvsy+/T4NUiYgsTWuAzpL1/cAVk455FHgt8AngNUCrma1w9+PFA8zsciAL7CxvcUVEFlg0BJ3/CpaGX/gcmFW6RDJPFHyXgKF4iPuH72d7bjsxMY5XukgiIlK93gP8vZldA3wfOADjk7Ob2ZnAl4A3uU+tEjGza4FrAdauXbsQ5RURmV/REOz7Rpji6PIbFH5rhIJvDct7ni0jW9gysgXHiVBnfRGRJe4AcHbJ+lnJtjFJM+bXAphZC/C77t6TrLcB/wH8lbs/MN0TuPsNwA0AmzZt0jetIrI4RUOw56uQ64azfxfaL4K2n4N0faVLJqdIwbcGxR6zdXQrPxr5EZFHatYsIiJFDwHnm9kGQuDdDLy+9AAzWwl0JbW57wNuSrZngW8SBr66dUFLLSJSCdEQdH4TDn03WR+GhtOhfSOsuAKWXRwCcev5oXZYqpqCbw1xd3bld3Hv0L2M+qgGrRIRkQncvWBmbwPuANLATe6+1cyuBx5299uAlwIfNjMnNHV+a3L664BfAVYkzaABrnH3RxbyNYiILKwYCv3jq8MHwnL4e5BpBhyiEWhcDR3PD4G44wUhELecq3mBq4iCb404WDjIPYP3aJRmERGZlbvfDtw+adt1JfdvBabU6Lr7l4Evl72AIiKLwqRAPLQvLIfuCIHYY4hGoeks6LgYVlweAnHHRdC8HkyT6yw0Bd9Frivq4t6hezlUOKQmzSIiIiIileQR5PvG1wd3h+Xg7ZBphDgCz0PzOui4JATi9otCbXHT2RpIq4wUfBepgXiAHw39iB35HUREGqlZRERERKRaeR7yJa0y+58Jy4HbIN0IcT6E5pYN0HEprEwCcftFoRm1AvFzpuC7yDxbeJafjfyMnfmdxMk/ERERERFZhOJcWIr6ngzL/n9NAvEoYKG/8LLLYMWmJBA/HxpOUyA+CQq+i0De8zyVe4otI1sYiAdUwysiIiIiUssmB+LeJ8Ky7xthSqVoBFIZaDkvTLNk8zmIlkHreeODdLWeVxOjViv4VrFj0TEeGXmEp3JPYZgGrRIRERERWcrikbBAqA3ueTQs8y4FmRYgTkatXhP6Ia/8hVDb3H4RtJyzqEatVvCtMgUvsCO3gy2jW+iJelS7KyIiIiIiCyyGQskgXUN7w3LoDsg0hf7IcS4MyNVxcTKNUxKIm9dV5ajVCr5Vojvq5tHRR9k2ug1AtbsiIiIiIlJdvDBx1OqBXWE58B/JqNWFcEzz+jBq9crLQw3x8k3QsLJixQYF34qKPGJ3fjdbRrZwNDqK4xqsSkREREREFpcpo1Y/HZYD/wapbAjCr3q8YsUDBd+K6Iv7eHzkcR7LPYa7q3ZXRERERERqT3GQrmio0iUpb/A1s1cCnwDSwI3u/reT9n8M+NVktQk4zd07kn3/B3gVkALuBN7p7ou2s2vsMXsLe/npyE85VDik2l0REREREZEFUrbga2Zp4NPAK4D9wENmdpu7byse4+7vKjn+7cBlyf1fAl4MXJzs/iFwJXBvucpbDpFHHI4Osye3hydyTxB5pNpdERERERGRBVbOGt/LgR3uvgvAzG4GrgK2zXD81cAHkvsONABZwIA64NkylnVeuDs9cQ/78vt4Jv8MhwuHSZMmT14jM4uIiIiIiFRIOYPvGqCzZH0/cMV0B5rZOmADcDeAu99vZvcAhwjB9+/dfXsZy3rKhuNhOgud7MztZF9hHwUvAFAg3EZElSyeiIiIiIjIklctg1ttBm519wjAzM4Dngecley/08xe4u4/KD3JzK4FrgVYu3btghS04AUOFQ6xJ7+HXfld9MV9Y7W6IiIiIiIiUn3KGXwPAGeXrJ+VbJvOZuCtJeuvAR5w9wEAM/tP4BeBCcHX3W8AbgDYtGlTWdoSuzvH4+Psy+1jR34HR6IjU5ova5AqERERERGR6lXO4PsQcL6ZbSAE3s3A6ycfZGYXAsuA+0s27wP+xMw+TGjqfCXw8TKWdYLBeJDOfCc786H5snsYgbnYbFnNl0VERERERBaPsgVfdy+Y2duAOwjTGd3k7lvN7HrgYXe/LTl0M3DzpKmKbgVeBjxOGOjqO+7+7+Uqa6nheJibem9S82UREREREZEaUdY+vu5+O3D7pG3XTVr/4DTnRcB/K2fZZhIRkSKl0CsiIiIiIlIjUpUugIiIiIiIiEg5KfiKiIiIiIhITVPwFRERERERkZqm4CsiIiIiIiI1TcFXREREREREapqCr4iIiIiIiNQ0BV8RERERERGpaQq+IiIiIiIiUtMUfEVERERERKSmKfiKiIiIiIhITVPwFRERERERkZqm4CsiIiIiIiI1TcFXREREREREapqCr4iIiIiIiNQ0BV8RERERERGpaQq+IiIiIiIiUtMUfEVERERERKSmKfiKiIiIiIhITVPwFRERERERkZqm4CsiIiIiIiI1TcFXREREREREapqCr4iIiIiIiNQ0BV8RERERERGpaQq+IiIiIiIiUtMUfEVERERERKSmKfiKiIiIiIhITVPwFRERERERkZqm4CsiIiIiIiI1razB18xeaWZPmdkOM3vvNPs/ZmaPJMvTZtZTsm+tmX3XzLab2TYzW1/OsoqIiIiIiEhtypTrgc0sDXwaeAWwH3jIzG5z923FY9z9XSXHvx24rOQhvgh8yN3vNLMWIC5XWUVERERERKR2lbPG93Jgh7vvcvcccDNw1SzHXw18DcDMNgIZd78TwN0H3H2ojGUVERERERGRGlXO4LsG6CxZ359sm8LM1gEbgLuTTRcAPWb2r2b2MzP7v0kN8uTzrjWzh83s4aNHj85z8UVERERERKQWVMvgVpuBW909StYzwEuA9wAvAs4Brpl8krvf4O6b3H3TqlWrFqqsIiIiIiIisoiUM/geAM4uWT8r2TadzSTNnBP7gUeSZtIF4FvAz5ellCIiIkvIHAaeXGdm3zOzx8zsXjM7q2Tfm8zsmWR508KWXERE5NSVM/g+BJxvZhvMLEsIt7dNPsjMLgSWAfdPOrfDzIrVuC8Dtk0+V0REROauZODJ3wQ2Alcn42qU+jvgi+5+MXA98OHk3OXAB4ArCON4fMDMli1U2UVERJ6LsgXfpKb2bcAdwHbgFnffambXm9nvlBy6GbjZ3b3k3IjQzPl7ZvY4YMBny1VWERGRJWIuA09uZHzMjXtK9v8GcKe7d7l7N3An8MoFKLOIiMhzVrbpjADc/Xbg9knbrpu0/sEZzr0TuLhshRMREVl6pht48opJxzwKvBb4BPAaoNXMVsxw7rSDVoqIiFSbahncSkRERKrDe4ArzexnwJWE8Tmi2U8ZpxkXRESkGin4ioiILB0nHHjS3Q+6+2vd/TLgr5JtPXM5NzlWMy6IiEjVUfAVERFZOk448KSZrTSz4ueD9wE3JffvAH7dzJYlg1r9erJNRESk6in4ioiILBFzHHjypcBTZvY0cDrwoeTcLuBvCOH5IeD6ZJuIiEjVK+vgViIiIlJdTjTwpLvfCtw6w7k3MV4DLCIismioxldERERERERqmoKviIiIiIiI1DQFXxEREREREalpCr4iIiIiIiJS0xR8RUREREREpKYp+IqIiIiIiEhNU/AVERERERGRmqbgKyIiIiIiIjVNwVdERERERERqmoKviIiIiIiI1DQFXxEREREREalpCr4iIiIiIiJS0xR8RUREREREpKYp+IqIiIiIiEhNU/AVERERERGRmqbgKyIiIiIiIjVNwVdERERERERqmoKviIjIImRmL6h0GURERBYLBV8REZHF6R/M7EEz+3Mza690YURERKqZgq+IiMgi5O4vAd4AnA1sMbOvmtkrKlwsERGRqqTgKyIiski5+zPA+4H/AVwJfNLMnjSz11a2ZCIiItVFwVdERGQRMrOLzexjwHbgZcCr3f15yf2PVbRwIiIiVaaswdfMXmlmT5nZDjN77zT7P2ZmjyTL02bWM2l/m5ntN7O/L2c5RUREFqFPAT8FLnH3t7r7TwHc/SChFlhEREQSmXI9sJmlgU8DrwD2Aw+Z2W3uvq14jLu/q+T4twOXTXqYvwG+X64yioiILGLfdPcvlW4ws3e6+ycmbxcREVnqylnjezmww913uXsOuBm4apbjrwa+VlwxsxcCpwPfLWMZRUREFqs3TrPtmoUuhIiIyGJQthpfYA3QWbK+H7hiugPNbB2wAbg7WU8BHwH+EPi1MpZRRERkUTGzq4HXAxvM7LaSXa1AV2VKJSIiUt3KGXxPxmbgVnePkvU/B2539/1mNuNJZnYtcC3A2rVry15IERGRKvBj4BCwkvAlcVE/8FhFSiQiIlLlyhl8DxDmFiw6K9k2nc3AW0vWfxF4iZn9OdACZM1swN0nDJDl7jcANwBs2rTJ56vgIiIi1crd9wJ7CddKERERmYNZg6+Z/ffZ9rv7R2fZ/RBwvpltIATezYSmWZOf40JgGXB/yeO+oWT/NcCmyaFXRERkKTKzH7r7L5tZP1D6pa8B7u5tFSqaiIhI1TpRjW/rqT6wuxfM7G3AHUAauMndt5rZ9cDD7l7sl7QZuNndVWMrIiJyAu7+y8ntKV+jRURElppZg6+7//VzeXB3vx24fdK26yatf/AEj/F54PPPpRwiIiK1xsx+Adjq7v3Jeiuw0d1/UtmSiYiIVJ8TNXX+5Gz73f0d81scERERmaPPAD9fsj44zbbacNt74GAKMvVQFyeLh9uZx8AUEREZc6KmzlsWpBQiIiJysqy0m5C7x2ZWLbM1zK9UGnLAQBa8NOk6ZJIAnI1LQnGypCtVYBERqTYnaur8hYUqiIiIiJyUXWb2DkItL4SpAHdVsDzl89v/H/zHtyE/CJFBLgX50sVgIANRauJ5qaRmeLpQnHHVFouILCFz+mbYzFYB/wPYCDQUt7v7y8pULhEREZndnwKfBN5PGN35eyRz29csIwTWTAREU/dHQCE1NRiPpKE/w4Skaw7NhfElozE2RURq2VybRH0F+DrwKsKF9k3A0XIVSkRERGbn7kcIMyNIURpIx1AfT93nhJrhYhgeTcNgBgbqws6GCFqSEJxV32ERkVoz1+C7wt3/2cze6e73AfeZ2UPlLJiIiIhMZWZ/6e7/x8w+xcR5fAENPDkjA7IO2WJtcT68e6OpEH4HM3CsAY4BdSUhuDFSCBYRqQFzDb755PaQmb0KOAgsL0+RREREZBbbktuHK1qKWmBAQwwNo7ByNNQIF2uBe7LQXQ8ph+Z8CMJNBQ2YJSKySM01+P5vM2sH3g18CmgD3lW2UomIiMhM/gD4NtDh7p+odGFqSp1DRz4sMSEED2bCwFn9WcChKRoPwnXqFywisljMKfi6+7eTu73Ar5avOCIiInICLzSz1cCbzeyLTGqI6+5dlSlWjUkBrYWwOGGArIEkBB9tDCOd1EehOXRLPvQrVpNoEZGqNddRnb8AvNPde5L1ZcBH3P3N5SyciIiITPGPhBGczwG2MDFuebJd5pMR+vo2RrBqNIwaPZDUBndloas+DKrVVEiaTkchFKdO+MgiIrJA5trU+eJi6AVw924zu6xMZRIREZEZuPsngU+a2Wfc/c8qXZ4lKRvD8lxYIhtvDj2cgf5i2k3mDy6G4IZYYVhEpILmGnxTZrbM3bsBzGz5SZwrIiIi88TM2ty9D/ir5Ho8gZo6L7C0Q1s+LAAFC82iR9JhxOjBDPRlk4M9NImuj0IgbohCOFYYFhEpu7mG148A95vZN5L13wc+VJ4iiYiIyCy+Cvw2oZmzo6bO1SXjYeCrlkJYd0IYHk2PB+KBacJwMQjXR+ovLCJSBnMd3OqLZvYw8LJk02vdfdts54iIiMj8c/ffTm43VLosMgdGGP25bpowPFYznIb+OuhNwrCVhOGmAjRqGiURkefqZJorLwcG3f1zZrbKzDa4++5yFUxERERmZmavAe52995kvQN4qbt/q7IlkxMqDcOtJWE4P6lmuDeZTxhPQnAyinRDpBphEZGTNNdRnT8AbAJ+DvgcUAd8GXhx+YomIiIis/iAu3+zuOLuPcn1WsF3MTIg65CdFIaH0zCUgaH0+AjSKQ+1wE0FaI6gTk2jRUROZK41vq8BLgN+CuDuB82stWylEhERkROZbkgkDTxZS4xQy9sUhfWIJAQny2BdmE84k0yl1FwIx6a9goUWEalOc71A5tzdzcwBzKy5jGUSERGRE3vYzD4KfDpZfythwCupVWlCbXCxRjhn4yF4oC4ZMCvpH1wMwg2aQklEBOYefG8xs38COszsT4A3AzeWr1giIiJyAm8H/hfwdUKj2DsJ4VeWiqxDNg8d+fAbMJIOTaKHMtCdhe76MFBWYzQehLNqFi0iS9NcR3X+OzN7BdBH6Od7nbvfWdaSiYiIyIzcfRB4r5k1J/dlKTNCwG2MYEUOYsb7Bg9l4FgDHAPS8Xjf4KZCmH5JRGQJmHNfoCTo3glgZikze4O7f6VsJRMREZEZmdkvEVpftQBrzewS4L+5+59XtmRSFVKUzCc8GkaMLu0f3J9MnZQtqQ1uVLNoEalds/55M7M2M3ufmf29mf26BW8DdgGvW5giioiIyDQ+BvwGcBzA3R8FfuVEJ5nZK83sfts/JwAAIABJREFUKTPbYWbvnWb/WjO7x8x+ZmaPmdlvJdvrzOwLZva4mW03s/fN8+uRcqpzaM/DmcNwzgCsHYCVI6HGtzcLB5phZyt0NoXRo0dSofm0iEiNOFGN75eAbuB+4L8C/5PQmOa/uPsjZS6biIiIzMLdO80mdNiMZjvezNKEwbBeAewHHjKz29x9W8lh7wducffPmNlG4HZgPfD7QL27v8DMmoBtZvY1d98zby9IFoYBDTE05GB50iy6OG3SYNIsGiAVj88d3FQI4VlEZJE6UfA9x91fAGBmNwKHgLXuPlL2komIiMhsOpPmzm5mdcA7ge0nOOdyYIe77wIws5uBq4DS4OtAW3K/HThYsr3ZzDJAI5AjjP0hi12K0Oe3OYJVo1Cw8b7Bg8mI0QB10XgQbiyEUaZFRBaJEwXffPGOu0dmtl+hV0REpCr8KfAJYA0hnN7BiUd1XgN0lqzvB66YdMwHge+a2duBZuDXku23EkLyIaAJeJe7d01+AjO7FrgWYO3atXN/NVI9Mg5thbA4kEuND5TVVxeaRuNhqqTivMFph9Q0t+ozLCJV4kTB9xIzK36ba0Bjsm6Au3vbzKeKiIhIubj7MeANZXjoq4HPu/tHzOwXgS+Z2fMJtcURsBpYBvzAzO4q1h6XlOsG4AaATZs2qW3sYmeEeYHrc+GnHpNMm5TUBndlmXV+JJshFE8blAm3dZpySUTm36zB193ViEVERKQKmdk5hBrfXyDUy91PqIXdNctpB4CzS9bPSraVegvwSgB3v9/MGoCVwOuB77h7HjhiZj8CNhEGvJSlIkWo5W2KYOVo+M2LDSILX4sU70++Ld7Pp2A0WfcZ0m0mhmWjYTAu1RiLyDyZ83RGp8LMXkm4KKeBG939byft/xjwq8lqE3Cau3eY2aXAZwh9jCLgQ+7+9XKWVUREZJH5KmGgqtck65uBrzG16XKph4DzzWwDIfBuJgTaUvuAlwOfN7PnAQ3A0WT7ywg1wM2EwP3x+XkpsmgZ4zW4J8uZGo4jg946ONoIXfXQkQuLqmJE5DkqW/Cdy8iR7v6ukuPfDlyWrA4Bb3T3Z8xsNbDFzO5w955ylVdERGSRaXL3L5Wsf9nM/mK2E9y9kExLeAchStzk7lvN7HrgYXe/DXg38Fkzexchmlzj7m5mnwY+Z2ZbCXHnc+7+WDlemCwRRuhPPHnepPZ86E/clYXjDdBdD+05WJZLjhcROXnlrPGdy8iRpa4GPgDg7k8XN7r7QTM7AqwCFHxFRESC/0zm4b2ZkBz+ALjdzJYDTDfwVLL9dsIURaXbriu5vw148TTnDRCmNBIpv6YImoZhZDTU/HZnoScLbXlYPqqplUTkpJUz+M5l5EgAzGwdsAG4e5p9lwNZYGcZyigiIrJYvS65vTa5LXaY3EwIwucseIlE5ltDDKuHw8jSXdnQDLq3DlrzYQ7i+rjSJRSRRaKsfXxPwmbgVnePSjea2ZnAl4A3ufuUv2yaMkFERJYaM3sR0OnuG5L1NwG/C+wBPjhTTa/IopaN4YwRWDEaan97s9CfheYkADdGJ34MEVnSyjlW3lxGjiwqDsgxxszagP8A/srdH5juJHe/wd03ufumVatWzUORRUREqt4/ATkAM/sV4MPAF4BekmmERGpWncNpo3DOQGjyPJyBzmbobILB9JTuwiIiReWs8Z3LyJGY2YWEmeHuL9mWBb4JfNHdby1jGUVERBabdEmt7h8AN7j7vwD/YmaPVLBcIgsn7WE6peWjoe9vdxYONEN9FLa1FDQXsIhMULYaX3cvAMWRI7cDtxRHjjSz3yk5dDNws7uXfkf3OuBXgGvM7JFkubRcZRUREVlE0mZW/OL65UwcH6NaujCJLIwUoanzhgE4bThMjXSoCfY2h77AqgEWkURZL5AnGjkyWf/gNOd9GfhyOcsmIiKySH0NuM/MjgHDwA8AzOw8QnNnkaUnBXTkw1RIA5kwEvSzjXA8mQu4PoK6ODSVVk2wyJKkb4ZFREQWEXf/kJl9DzgT+G5Ji6kU8PbKlUykChjQWghNnQczoQn0sYaSAzyE37p4+iVdqYKLSLkp+IpUqdhj8oU8dek6UqlyjkMnIovNdIM+uvvTlSiLSFUyQvhtKUDeIJ+auORSMJKBeNL1NR2P1wxPDsUZ1RaLLGYKviLzyN2J4ohcPke+kCdfyJMrhPtTtuUn7U+2FdcLUQEAw2huaqatqY225jZam1rDbXMrbU1tZOuyFX7VUguiOJrw+zf59zGXz5GPwhcxzY3NNDc009zYTFNDE5m0LiUiUsXqHOoiYJopjyKmhuJ8CobT0J9hQtK1kjDcXIC2fHnnRxGReaVPKyJzFMURQyNDDA4PMjgyyODw4MT1kbA+cZy26ZkZ2UyWurq6cJupoyHbQFtT24RtdZk6RvOj9A/20zfUx55DexjJjUx4rPpsPW1N40G4NBw3NTRh9ty/ni6GopmC+lxe88lKpVKkU+mxJZUO6ylLkU6XbJ/muJRV/pOIu5Mv5BnJjTAyOsJwbnjs/khuZOr93AjuPuU1pVInfr3T7UulUpjZ+Jct+dyEn1++kJ8QbKP41OfArM/WjwXh5oaJobi4TV/QiEhVShNqeRviqfucqbXFuWQZTPoPL8tBe05NpEUWAQVfESBfyIfgOjw0FmqLYXZwOCyTAydAJp0Z+2C/euVqGusbqa+rHwut2boQYEuDbLYuSzqVPuVAmsvn6B/qp2+wj77BvrH7R7uPsvvg7gkhNJ1K09LUQltz21g4bmpoolAozF4bPY+hqBLMbEoYPKkwOcP+0uMwGM2NzhxqcyPE8TQfpAg/l4b6BhqyYWlrbqOhvoGUpYjiiCiOiKN47H4UR8RxWM8VckTR+HrpviiOpv0SIp1Kj/8e1oXfw+aGZupaxn9HS39PJ/ze1o3vy6Qz4/9XJn0JVPz/c7T76LT/V4rPOTkQtzS20NHSQUtTy7x8SSMiMm8MyDpkJ9UWO6FGuKs+9B/uSgbQ6siF5tAiUpUUfIU4jukd7K3aWhl3Z3h0mKGRIQpRYcKH/NIP/RO2TwoGk/cXQ8VIboTBkUFy+dyU562vqx/7gL6yY+WEmqzibV2mbsE/rGfrsqxoX8GK9hVT9sVxzMDwwIRAXLx/+Nhh8lF+yjnFUFQadJoamsi2TBOCMtMHpfnug+zuxB7P+nM8lZ/35O2FQmHiOZPC5lxrsrOZbAix9Q20NLawsmPlWKhtyDaEL0Sy9TRmG2mobyCTzpTt9yb2eOw1ujt1mboQ1OdJti5Lti7LstZlMx5TiAohGM/QMqL7aDfDI8N4yTwj6XSajpYOOlo6WNa6jI7WDjpaO2hralMfdxGpLgY0RdA0BCOpEHy7krmE2/OwbDQ0rxaRqqLgu0QVogIHjh5gz8E97H1271jwa8g2jDWZnXw7X81mpxPHMf1D/WNhbXJoK/Z3PRkpS52wKWhbcxtnrjhzSi1Uc0Mzmczi++9RfE1tzW1T9rk7I7kRhkaGJoTWdFrts2ZS7LM9Fq5Lwrjj1Gfracg2zGuwfK5SliKVTlW0320mnZnx97AojmOGR4fpH+qnZ6CHnv6wHO46zM4DO8eOS1mK9pb2EIRbQhhe1rqM9uZ2/e6KSOU1xLB6ODR/7spCT11Y2vKhGXT99C1/RGThLb5P9nLK8oU8+4/sZ/eh3XQe7iQf5cnWZVl3+jrOXHnm2IfQvsE+jnQfYdeBXRNrZFJpWptaQxAu6Ufa1tRGS1PLCT9o5/I5+ob6Qn/V0nA71Mfg0ODU52pupbWpldWrVtPW1EZzYzOZdGbavo3TNUdVs8mJzIzG+kYa6xsrXZRFw8zIpDPhd7uu0qWpLalUKnzR1NjMGSvOmLAvV8jR299Ld3/3WCg+3nuc3Qd3jx1jGK3NraF2OAnExXCsn5WILLhsDGeMwIrRUPPbm4W+ujCq9PLR6fsQi8iCUvCtcbl8jn3P7mPPoT10HukkiiIasg2cs+Yc1q9ez+qVq2esrZqpFrZ/qJ9Dxw5NqYVtbmieUEscxdFYyO0b6mM0Nzrh+GLt8unLTqf1rIWrXRaR6pbNZFm1bBWrlq2asL0QFegd6J1QQ9w90M2+Z/dNaJZ+2TmX8eaXvHmhiy0iEpo4nzYKy3PQkw3LQB00JQG4MdKUSCIVouBbg0ZyI+w7vI/dh3Zz4OgB4jimqb6JC86+gA2rN3DG8jPm1GculQpNDNtb2qfsKzabLW2OXBx5uPNIJ8Ojw5gZLY0ttDa1suHMDZqGR0Sek0w6M23/9jiO6RvsG6shXtO+pkIlFBFJZBxWjob+vr1J/9/9zdAQhQDcXFAAFllgCr41Ynh0mL2H9rL70G4OHjuIu9PS2MLG9RtZf+Z6Tl9++rzWoJY2mz19+elT9hcKBVKplAalEZGyS6VSY02dAVqttcIlEhFJpAm1vx250PS5ux4ONoWRopfnoDWvACyyQBR8F7HB4UH2HNrD7kO7efb4szhOW3MbLzj3BWw4cwMrO1ZWrLnwYhwYSkRERKQsUkBHPoz63J8JI0EfboRj9aEGuC0fjhGRslE6WQSKzYqLTYn7BvvYf2Q/R7qPANDR2sGlF1zK+jPXs7xtufrGioiIiFQjA9oK0FqAwUwYCfpIIxxtgLo4NJHOJLfp4nrJNn3EEzllCr5VYrr5V0tv84WJ86+uaFvBCy98IevPXD/rfJoiIiIiUmWMMOJzcwGG09BfBwWDKAVDmXB/upSbniUUjwVmBWSR6Sj4LqBcITdlKp/i7cDwwIRRSdOpNC1NLbQ1t3HGijPGpg0qTvFTyTk6RURERGQeGNAUhaWUA5GFAFxITbxfvB21sH1Kyk3Cb10cRpmuiycuqjmWJUrpqYyiKOLpzqd5pvMZ+gb7GMmNTNhfn62nramNVctWce6ac8dGO25r1nQ+IiIiIkuWMV6jyyxzAE8OyKW3+VRSm5xhQtK1yWF40rr6GkuNUvAtg0JU4Mm9T/LYjscYGhliRfsK1p25bsI8ta3NrdTX1Ve6qCIiIiKyWM0lIDshBOeTMJxPQS65HcqA28SDM9PUEhcDsppRyyKm4DuP8oU82/ds5/GdjzM8OswZK87gysuuZPXK1aq9FREREZGFZ0A2hizADE2qi4E4b+OheDAT+hxPebwkAKcdUpNup9s2dotCs1SUgu88yOVzbN29lSd2PcFobpQ1q9Zw6QWXcuaKMytdNBERERGR6Y3VGEfQGE3dHzMxFMdJv+LibZQE5eI2ny3ZJuF3QlCeZpCu4jY1uZZ5puD7HIzkRti6aytbd20lV8hx9ulnc9n5l3Ha8tMqXTQRERERkecmBdTHYZmLmImheHJQHtuXHJfLhPXpAnNquhGrNcWTnDoF31MwPDrM4zsfZ/vu7eSjPOvOWMdlF1zGyo6VlS6aiIiIiEhlpEgCq5/w0DFOCMGFaQbpKg7cNdcpnuo8TBHVVFCNsUyh4HsShkaGeGzHY2zfu50oijhnzTlcev6lLG9bXumiiYiIiIgsPsZ4/+B6mNIPuWguUzwNp6A3G/ohNxXG50o+mSAuNUvBdw4GhgZ4dMejPL3vaWKPOW/NeVxywSV0tHRUumgiIiIiIrVvriNYD6VhsA4GMuEWh4ZoPARnYzWPXqIUfGfRN9jHo888yjOdzwBw/trzueS8S2hrbqtwyUREREREZAIDmqOwrCIMvDWQgYE6ONYAxwhTMzXnQxBujBSClxAF32n0DPSw5ekt7Diwg5SluHDdhVx83sW0NLVUumgiIiIiInIiRjIwVw5W5EJz6IFMmKapNws99aE/cnMBWvKhaXS60oWWclLwLVGIC1z3g+u4c8+dZNIZLtpwERefdzFNDU2VLpqIiIiIiJyqjENHPiwxYcCsYhDuT5pEN0XjQbhO/YJrjYJviUwqQ9rSXHbeZWw8dyON9Y2VLpKIiMj/z96dx0l2l/e9/zy1dPf07JuW2UfSCCEJoQ2xSCAZCbSAJUgCkTDOxXHg+l7DTTB2ArkECIkd4iR27ASHiwnGxjYKxjaWsbCMAw7BZtGONCMEo5HQjDSSZuuevWe66rl/nNPTNa0eaZauru7qz3te51VVZ6l6urqnT3/rtxxJ0kSqUHR1njNcjAs+WB0Nwdv6iqWnHBc8qxwX7KWTpj2D7xgfveqj/O7g7zLMcKdLkSRJktROQTHWd1YDlg4V44L3la3BO3sop5ouukX3NIru0z3N0ftVA/F00dbgGxE3AL9B0WP+M5n5iTHbfx34ifJhP3BaZi4ot/0fwIfLbf82M3+3nbVKkiRJmuF6mtBzCBYeKq6sNFQtwvDI7d4aNFouElxpjobh3kZ5fNNLKE1BbQu+EVEFPgm8AdgC3B0Rd2TmhpF9MvP9Lfu/D7ikvL8I+ChwOUUHhHvLY3e1q15JkjRFVWdDswG1OUAFhvdxzGt9StJEqVKM++1vAIdH1w9HGYYrcKha3O6pF5NmHTl2JAy3tA73NJxAq4Pa2eJ7BbAxMzcBRMTtwC3AhmPsfxtF2AW4HvhaZu4sj/0acAPwhTbWK0mSpqK+JfD2PbBnIww+DAMPwY7vFfcPPAPVck6O4b0Un5dLUhvVEmpjAnECjTg6DB+qwGAdckwgriXUWm6refQ6u0+3RTuD73Jgc8vjLcArx9sxIlYDa4Gvv8Cxy8c57j3AewBWrVp16hVLkqSpqVKD+ecVy6p/MLq+cQj2/BAG18OuB8tAvAGGtpWBOMsWYgOxpDYKRgPx7JYeKUnRQjzSVfpQpQjIwxU4GEd3m249qFYu4wXlkW2VYwTkbF2imMU64/geN8snrJQBfOS29X4wLYP5VJnc6lbgS5l5Qv2WMvPTwKcBLr/8cs9okiTNNNUeWHBhsaz+h6PrGwdh9w9gYD0MPFAG4h/AoZ1Q7QeaBmJJ7RcUl0aqH2Pi3JFgPFwpbhst94crcLgCBwKa4wTkaAmiY8PsKUleONm+QCiuMM66hL7mKdZ06toZfJ8CVrY8XlGuG8+twM+POfaaMcf+zQTWJkmSulm1DxZeXCz81Oj64X0w+EjRQjy4HnY9AHt+BPufKlqVKz3QPAyN/R0rXdIMciQYv0j7X5Pnh+LhKJaR56lQhOFgtDV4vMet+z3vccvrNcsgPnLber/1drhy9OPxLO3879R2Bt+7gXURsZYiyN4KvGPsThFxHrAQ+HbL6ruAX4mIheXjNwIfamOtkiRpJqjNhsWXF0urbMKBp4sQvLvsOj3wIOx5DA4+C5XeIhg3hqB5sDO1S5q5KhTh9cUC8kSpUrTW1k+wV0wyGtJbw/C8+W0o8sS0Lfhm5nBEvJcixFaBz2bm+oj4OHBPZt5R7norcHtmZsuxOyPi31CEZ4CPj0x0JUmSNOGiAv0riuX0nzh6W3MY9j9ZBOI9Pyom1xp4CPY+VnadnlUcP3wA8vD4zy9JM0EwGppbh5LMrneooFFtHeObmXcCd45Z95Exjz92jGM/C3y2bcVJkiQdj0oN5pxVLNxw9LbGEOzdVATip++Ex3+3aD1uHupIqZKk8Y03jZgkSZKOR7UX5r8UVtwMV3wKbt4EZ/1MMcY4Ot/CIUkqGHwlSZImyqwziwD8kz+CtT9tAJakKcLgK0mSNNH6V8Cr/ju8+VFY844yAE+Vq0hK0sxj8JUkSWqX2avg1Z+DNz0Cq95uAJakDjH4SpIktducNXDlH8BND8PKv28AlqRJZvCVJEmaLHPPhqtuhxsfhOU3l5dCqna6KknqegZfSZKkyTbvXHjdH8MN98GyN49eC1iS1Bb+hpUkSeqU+efB1V+G6++GM28sArB/nknShPM3qyRJM0hE3BARj0bExoj44DjbV0XENyLi/oj4fkTc1LLtooj4dkSsj4iHIqJvcqvvYgsugGu+Am/8Dpz5hjIAR6erkqSuYfCVJGmGiIgq8EngRuB84LaIOH/Mbh8GvpiZlwC3Ar9VHlsDfh/4ucy8ALgGODxJpc8cCy+Cn/hLeOPfwRnXQaUXarM7XZUkTXsGX0mSZo4rgI2ZuSkzDwG3A7eM2SeBeeX9+cDT5f03At/PzAcBMnNHZjYmoeaZaeHF8Pq/grdsgUt/HRa9ogjB1f5OVyZJ05LBV5KkmWM5sLnl8ZZyXauPAe+MiC3AncD7yvXnAhkRd0XEfRHxz8d7gYh4T0TcExH3bNu2bWKrn4n6lsA574YbvgdveRIu/U+w8FJDsCSdIIOvJElqdRvwucxcAdwEfD4iKkANuAr4qfL2rRFx7diDM/PTmXl5Zl6+dOnSyay7+/WdBut+Dm68F255Ai7+BCy4CCp95ZhgSdKxGHwlSZo5ngJWtjxeUa5r9bPAFwEy89tAH7CEonX4m5m5PTP3U7QGX9r2ijW+WWfAS94HNz0IN2+Ei34Z5l9QhmDnHJOksQy+kiTNHHcD6yJibUT0UExedceYfZ4ErgWIiJdSBN9twF3AyyKiv5zo6mpgw6RVrmPrXw4vfT+86WH4yUfhZR+HuS8pAnDFECxJYPCVJGnGyMxh4L0UIfYRitmb10fExyPi5nK3DwDvjogHgS8A78rCLuDXKMLzA8B9mfkXk/9V6AXNXgXn/xL85A/gTRvgZR+BOecUXaErvZ2uTpI6ptbpAiRJ0uTJzDspuim3rvtIy/0NwJXHOPb3KS5ppOlgzlq44EPFsvtH8OPb4cn/AXsegwio9EDjADQPdbpSSWo7g68kSVK3m7cOXvaviiUTDjwFA+th8GHY8T3Y9SDsewKiCpUaDB+A9DLNkrqHwVeSJGkmiYD+FcWy7PrR9dmEfU/C4HoYKAPxwIOw/8midTiqMLwfcrhztUvSSTL4SpIkCaICc9YUy/I3ja5vNorW4MGHWwLxQ7B/C1R7izBMszM1S9JxMvhKkiTp2CpVmHt2say4ZXR9cxgGvg/ffAscfA6aQ52rUZJehLM6S5Ik6cRVarDo0uJawgsvgWp/pyuSpGMy+EqSJOnk9SyEN3wTVr3N8CtpyjL4SpIk6dRU6vCq34GX/evimsGSNMUYfCVJknTqIuD8X4Srvgi12Z2uRpKOYvCVJEnSxFn+ZnjD30HvEoh6p6uRJMDgK0mSpIm28CK46SGYdy5U+zpdjSS1N/hGxA0R8WhEbIyIDx5jn7dHxIaIWB8Rf9iy/lfLdY9ExG9GRLSzVkmSJE2gWWfA9XfDGddB1a7PkjqrbcE3IqrAJ4EbgfOB2yLi/DH7rAM+BFyZmRcA/6xc/xrgSuAi4ELgFcDV7apVkiRJbVCbBa/7M3jJe53xWVJHtbPF9wpgY2ZuysxDwO3ALWP2eTfwyczcBZCZz5XrE+gDeoBeoA4828ZaJUmS1A5RgYs/AVd8yhmfJXVMO4PvcmBzy+Mt5bpW5wLnRsTfRsR3IuIGgMz8NvANYGu53JWZj7SxVkmSJLXT2p+G138N6vOLMCxJk6jTv3VqwDrgGuA24LcjYkFEnAO8FFhBEZZfHxGvHXtwRLwnIu6JiHu2bds2iWVLkiTphC29Em68H/pXQqW309VImkHaGXyfAla2PF5Rrmu1BbgjMw9n5uPADymC8FuB72Tm3szcC3wVePXYF8jMT2fm5Zl5+dKlS9vyRUiSJGkCzVkLNz4Ii1/huF9Jk6adwfduYF1ErI2IHuBW4I4x+3yZorWXiFhC0fV5E/AkcHVE1CKiTjGxlV2dJUmSukHPfLj2G7D6NsOvpEnRtuCbmcPAe4G7KELrFzNzfUR8PCJuLne7C9gRERsoxvT+UmbuAL4EPAY8BDwIPJiZf96uWiVJkjTJKjV41Wfg5b/ipFeS2q7WzifPzDuBO8es+0jL/QR+oVxa92kA/2c7a5MkSdIUcN4/hXnnwrfeBsP7KS7uIUkTq9OTW0mSJGmmW3YjvPE70LsUoq3tMpJmKIOvJEmSOm/BhfCmh4pxv7U5UJsLRKerktQl/EhNkiRJU0PfafCa34PmYXjm6/D478KWP4OowvCeTlcnaRoz+EqSJGlqqdRh2fXF0jgEz/w1bPodePoviq7QhmBJJ8jgK00xQVCnDsAwwyRJnTqHONThyiRJ6oBqDyy/qVgaQ7D1riIEb73LECzpuBl8pQ6olv+SZJhheqOXuZW5LKwsZHF1MQuqC5hfmc/8ynxqUeOp4ad44vATbDq8ib3NvVSpcpjDnf4yJEmaXNVeWHFzsQwfgK1/CY99tmgRrtQNwZKOyeArtUmdOkHQoEGSzI7ZzKvOY3F1MYsqi5hfLYLtvMo8qlF9wedaXV/N6vpqruZq9jX3sXl4M5sObeLJ4SdpZPH8DRqT9JVJkjQF1GbByrcWy/D+ohv0Y5+FZ78BlR5DsKSjGHylCVCnTpMms2IWp9VOY3FlMQurC4+E2/7oJ2JiZqacXZnNeT3ncV7PeWQmO5s7+fGhH/PY4cd4tvHskdbg9DqI00aFCkGwsLKQ2gRexiNJDuQB9jX3ARzVy8CfD0ldpdYPq95WLIf3wlNfgcc+A9u+ZQiWBBh8pZNSoUKVopV2ZW0lZ/Wcxar6KuZW5k5qHRHB4upiFs9azKWzLqWRDbYOb+WJw0/w2OHH2N3cTY2a44OnqDp1KlHhZT0v46K+i9r285NZBODB5iCDjUEGm4Nsb2xnV2MXe3Mvh/IQNWoEwTDDNGm2pQ5JmhT1ObDm1mI5vLsIwc99E3beC3t+CI2DUJ0FzUPQONDpaiVNEoOvdJx66GGYYZZWl3JO/RxW1VextLp0wlpyJ0I1qqyor2BFfQVXcRUHmgfYMryFTYc28cTwEwznMFBMmqXOGGndXVpdymV9l7G2vvZFu7qfqoigP/rpr/RzZu3M520/nIfZ3dx9JBTvaOxgZ2Mnu5u72Z/7qVKlQoUmTX92JE0v9Xmw5h3FMmJoBwyuL5Yd9xaBeO/G4hJK1b5iAq3mwc7VLKktDL7SMdSp06DBnMoczqqfxZr6GpbXlk9oV9R2m1WZxbqedazrWUdcKO3sAAAgAElEQVRmMtAcYPPwZg40J/YT7sN5mB2NHQw0B9jb3EuTJjVqR8Ye24LIkZm6L+y9kIt6L2JBdUGHKxpVj3rRc6C6+HnbmtlkX+5jsDHIQHOALYe3sPHwxiOtw5I07fQuhtNeVyzrWtYffA4GHi4C8c57YOf9RSAmodJbtBQ3hzpVtaRTNH3+gpfarEqVIKhEhdW11ZxVP4uV9ZXMrszudGkTIiJYWF3IwurCtr/WUA4daT0c24I4lENHutU2yn/dKgiqVFlYXchlfZdxdv3safXBCUAlKsyNucytzGUFK7iw90KGcogfDP2A+4bu40DzgDOMS+oOfafBGa8vlhGZcGBr2UL8MOy4G3Y9AHsfB5pFd2lJ08L0+gtMmkAj18tt0OD06umcXT+b1T2rWVRZNKW6L09HvdHLabXTOI3TnretkQ32NPccCcU7GzvZ2djJQHOAfc19R8LidJ6EqU6dJHlpz0u5uO9iFlUXdbqkCdUbvby87+Vc1HsRWxtbuf/g/Tx++HFbgSV1nwjoX1YsZ75hdP2BZ+Chj8PjvwNpAJamA4OvZpSRcbrzKvM4u342a+prOKN2xrRrhZvOqlFlQXXBuF19x07CNNAcYHtjOwONAfbkHg7n4SnbWjwS2OdV5nFZ32Ws61lHPeqdLqutIoJltWUsm7OMA80DbBjawP1D93MoD9kKLKm7zToDrvgtuPDD8NBH4Yk/gOYwpL/7pKnKv/bV1Wrlj3gtaqypreGsnrNYUVvBrMqsDlem8RzPJEyDzUF2N3Yz0Bw40lrcyUmYRsYyn9tzLhf3Xsxptee3cs8EsyqzuGzWZVzadylbhrdw38H72Dy8GWBKfUAhSROqfxm88rfhwo/A9z8KT34Bmg0DsDQFGXwFjE68M91baUYuM9SkybLaMs6un82q+ioWVBbYfbkL1KPOkuoSllSXPG9bM5vszb1HxhbvauxiR2MHg81B9jb3tiV8za3M5bLeyziv9zx6omfCn386ighW1leysr6Sfc19PDz0MA8OPchwDk/73y+SdEyzV8KrPwsXfRQe/DBs/lLZAuzwD2mqMPjOYCMtVWvqa7ik9xL2537uPXgv2xvbSXLazMQ70n15YWUhZ/eczer6as6onkElKp0uTZOoEhXmxTzmVeaxkpXP2z6cEz9euEbND1RewOzKbF4565W8ou8VPDn8JPcevJetw1sBW4EldanZq+E1n4e9H4cH/1/Y8qcGYGmKMPjOQHXq1KPOJb2XcEHvBUd1+13Xs45djV08OPQgG4Y2AFOvFXgksPdGL2vra1lbX8uK+gp6o7fTpWkKcxx351Siwpr6GtbU17CnuYfvH/w+Dx16iGY2p9zvF0maEHPWwpV/CHs2wgMfgqe/UlwnOP3QT+oU/xKcIapUAVhRW8GlfZeysrbymC1VC6sLuab/Gq6adRUbD23k3qF7GWgM0KDRkRl2R7ovJ8ny2nLO6TmHVbVVzKvOm/RaJJ2auZW5XNl/Ja+a9SoeP/w49w/dz7bhbQwzfGTismGGp02PE0l6QXPPgdf+Eex+FB74IGz9SwOw1CEG3y5Xp041qlzcezEX9l54QtekrUWN83rP47ze89je2M4DBx/g0UOPEkRbW2lGLjM0zDCLq4s5p34Oq+urWVpdavdlqUtUo8o5PedwTs85ABzKQ0fGZ+9u7j7q2s8H8kBHJi6TpAkz7yXwuj+FwQ3wwL+AZ/4nNIbAD/mkSWPw7UIjrbtn1s7k0r5LWV1bfcqBcUl1CdfNvo6r+6/m0UOPcu/Be49MGDQRrcB16jRp0h/9rO0pui8vry3v+svBSCr0RA9La0tZytLnbWtmk73NvQw0BxhsDjLQKC9zVV77OckjQyAMxZKmtPnnw9V/DgMPwf3/HJ77XwZgaZIYfCdBjRpn1s5ksDnIvuY+gCNdd4eZuAl36tSJCC7quYiX9b2MeZWJ7wpcjzoX9l7Ihb0X8uzws9x/8H42Ht54pHvi8RppvQmCFfUVnFM/h5X1lcypzJnwmiVNb5WoMK86b9zhDZnJwTxYXPu5vP6z4/0lTXkLXgY/8VXY9QDc90uw/W8hE5oHO12Z1LUMvm1Wo8Y1s67hgr4LgOf/kTbQGGBHYwe7GrvYk3s4lIeOjHNrlP9eyEh4XFpdymV9l7G2vpZqVCfjS+P02uncMOcGhnKIR4Ye4b6h+zjYPDhuN+jW7sunVU/jnPo5rKqvYkl1ibPiSjppEcGsmMWsyizO4IxOlyNJJ2bhxXDt12DX9+G5b8LOe2DnfbD3McgmVHuhcRCaQ52uVJr2DL5tVKPG+b3nHwm98OJ/pA3nMLubuxloDDxvnNv+3E+l/AeQJBf0XsDLe1/OguqCSfu6xuqNXi7uu5iX976crY2t3HfwPp44/ARVqgwzzNzKXM6qn8Wa+hqW1ZY5u64kSVKrhRcVy4hMOPgMDK6HgYdhx91F6/DeTRABlR5oHIDmoc7VLE0zJpA2qVLltOppXD3r6hM6rhY1FlUXsai66HnbMpO9uZfdjd0M5RCr6qumVIiMCJbVlrFszjIONA+wdXgrZ9TOoL/S3+nSJEmSpo8ImHVmsZxx3ej6TNi/pQjEg+thx3dh14Ow7wmIGlRqMHwA0kvFSWNNndTUZWbFLG6ec/OEzkIcEcyNucytzJ2w52yXWZVZnNVzVqfLkCRJ6h4RMHtlsSy7YXR9NmHfkzD48GgL8cCDxbpqD0QVhvdDOgGgZi6DbxvUqfP35v49eitOsCJJkqQ2iwrMWVMsy988ur7ZgH2Pt3SZ/m4xo/T+LVDtAyowvA9eZE4ZqRu0NfhGxA3AbwBV4DOZ+Ylx9nk78DEggQcz8x3l+lXAZ4CV5babMvOJdtY7EWrUuGnOTSysLux0KZIkSZrJKlWYe06xrLhldH1zuJhAa+DhIgjv+F7RWnxgK1RnFfsM78PLLKmbtC34RkQV+CTwBmALcHdE3JGZG1r2WQd8CLgyM3dFxGktT/F7wC9n5tciYg7T4H9ejRqv7Hsla+prOl2KJEmSNL5KDea9pFhW/f3R9Y1DsOdHZQvx94sW4sENcPC5IhA3DthdWtNWO1t8rwA2ZuYmgIi4HbgF2NCyz7uBT2bmLoDMfK7c93yglplfK9fvbWOdE6JGjbX1tVzWd1mnS5EkSZJOXLUHFlxQLKvfPrq+MVS0CK//BDz9FWgehrR7tKaXiZt56fmWA5tbHm8p17U6Fzg3Iv42Ir5Tdo0eWT8QEX8SEfdHxH8oW5CnpAoV5lfm88bZb/SatJIkSeou1V5YdBm89o/gxgdg+c3FGOGp++e59DztDL7HowasA64BbgN+OyIWlOtfC/wi8ArgLOBdYw+OiPdExD0Rcc+2bdsmq+bn6Yke3jr3rVPq0kKSJEnShJv3Enjdn8AN98Kym4ou0BN4FROpXdr5U/oUxcRUI1aU61ptAe7IzMOZ+TjwQ4ogvAV4IDM3ZeYw8GXg0rEvkJmfzszLM/PypUuXtuWLeDE1arxlzluYXZndkdeXJOlERMQNEfFoRGyMiA+Os31VRHyj7HH1/Yi4aZzteyPiFyevaklTzvzz4eo74PrvwRnXl5NiGYA1dbXzp/NuYF1ErI2IHuBW4I4x+3yZorWXiFhC0cV5U3nsgogYSbOv5+ixwVNCjRrX9l/L6bXTO12KJEkvqmXiyRuB84Hbynk1Wn0Y+GJmXkJx7v6tMdt/Dfhqu2uVNE0suBB+4k5447fhjGvLAOzQP009bQu+ZUvte4G7gEcoTqLrI+LjEXFzudtdwI6I2AB8A/ilzNyRmQ2Kbs7/MyIeovjf89vtqvVk1KhxUe9FnNd7XqdLkSTpeB2ZeDIzDwEjE0+2SmBeeX8+8PTIhoh4C/A4sH4SapU0nSx8Obz+r+AN/xtOuwaq/RiANZW0dVBqZt4J3Dlm3Uda7ifwC+Uy9tivARe1s76TVaXKmbUzuWrWVZ0uRZKkEzHexJOvHLPPx4C/ioj3AbOB6wDKSwv+C4rLFB6zm3NEvAd4D8CqVasmqm5J08Wiy+C6r8P278H9vwg774XG/k5XJdkR/0QFwezKbN40503O4CxJ6ka3AZ/LzBXATcDnI6JCEYh//cUuMTgV5t+QNAUsuQLe8E14/V/DkleXLcBS5zgN8QmqU+etc95Kb/R2uhRJkk7U8Uw8+bPADQCZ+e2I6AOWULQM/4OI+FVgAdCMiIOZ+V/bX7akaWvpq+GNfwfPfQvu/wAMrIfGvk5XpRnIFt8TUKPGm+e8mQXVBZ0uRZKkk3E8E08+CVwLEBEvBfqAbZn52sxck5lrgP8M/IqhV9JxO+0quP67cM1XYOElUPWKKJpcBt/jVKPGlbOuZGV95YvvLEnSFHScE09+AHh3RDwIfAF4VzknhySdutOvKa4BfPWfwfI3Q/8qiBrU5kBtLsYTtYtdnY9DjRrretbx8t6Xd7oUSZJOyXFMPLkBuPJFnuNjbSlO0swQUVz66Ixri8fNw7DnRzC4HnY9BDu+W9w/+Gx5eSRgeC/FpPPSyTH4vogKFRZVF3Ft/7VOZiVJkiRNtEod5p9fLKveNrq+MQR7fggDD8PA92HH92BwAwxtLyfLasLwPgzEOh4G3xfRG73cMucWqlHtdCmSJEnSzFHthQUvKxZuG10/fAB2/6BsIX6gCMS7fwCHdkFlGsSb6IFsTL1JvqJWfqDQKOqbKNmE+vyJe76TNA1+MjqnRo23znkr/RWnX5ckSZKmhNosWHRJsax95+j64X1lC/AUlk3Yv7loxd71AOy8G3Y/Cod3Q60fmpMRiKtQmw00oXEQ+pfD/AthyauK2/nnQ88ET+Zbmzuxz3cyJXS6gKmoSZMaNa6ffT1La16DUJIkSZryarPLQDfFzToDFr/i6HWHBotu3IPrYdf9sPOeIhAP7y+CfvMwNA6c4AtVRt+PxoHidedfCIuvgAUXwfwLYO4506OVfALMjK/yBDVpcnnf5ZzTc06nS5EkSZLU7XrmF9c8Xvrqo9cf2lVc+3hwPey6D3bcU0wE1hwqJv5qHipabWtziv0b+6HvdJh3PixpDbjnQrVn8r+uKcTgO0Z/9HNd/3Wc33N+p0uRJEmSNJP1LCyugXzaVUevP7i9CMOD6+HAVpj/0iLgznsJVPs6U+sUZ/AdoxIVLui9oNNlSJIkSdL4+pZA39Vw+tWdrmTa8ArRkiRJkqSuZvCVJEmSJHU1g68kSZIkqasZfCVJkiRJXc3gK0mSJEnqagZfSZIkSVJXM/hKkiRJkrqawVeSJEmS1NUMvpIkSZKkrmbwlSRJkiR1NYOvJEmSJKmrGXwlSZIkSV0tMrPTNUyIiNgG/LjTdUxjS4DtnS6iC/g+Tgzfx4nh+3hqVmfm0k4XMZ15bj5l/h+eGL6PE8P3cWL4Pp6akz43d03w1amJiHsy8/JO1zHd+T5ODN/HieH7KE1v/h+eGL6PE8P3cWL4PnaOXZ0lSZIkSV3N4CtJkiRJ6moGX434dKcL6BK+jxPD93Fi+D5K05v/hyeG7+PE8H2cGL6PHeIYX0mSJElSV7PFV5IkSZLU1Qy+kiRJkqSuZvCd4SJiZUR8IyI2RMT6iPinna5pOouIakTcHxFf6XQt01VELIiIL0XEDyLikYh4dadrmo4i4v3l/+mHI+ILEdHX6ZokHR/PzRPLc/Op89w8MTw3d5bBV8PABzLzfOBVwM9HxPkdrmk6+6fAI50uYpr7DeAvM/M84OX4fp6wiFgO/D/A5Zl5IVAFbu1sVZJOgOfmieW5+dR5bj5Fnps7z+A7w2Xm1sy8r7y/h+IX2fLOVjU9RcQK4E3AZzpdy3QVEfOB1wH/HSAzD2XmQGermrZqwKyIqAH9wNMdrkfScfLcPHE8N586z80TynNzBxl8dURErAEuAb7b2Uqmrf8M/HOg2elCprG1wDbgd8puaZ+JiNmdLmq6ycyngP8IPAlsBQYz8686W5Wkk+G5+ZR5bj51npsngOfmzjP4CoCImAP8MfDPMnN3p+uZbiLizcBzmXlvp2uZ5mrApcB/y8xLgH3ABztb0vQTEQuBWyj+WFkGzI6Id3a2KkknynPzqfHcPGE8N08Az82dZ/AVEVGnOLH+QWb+SafrmaauBG6OiCeA24HXR8Tvd7akaWkLsCUzR1o2vkRxstWJuQ54PDO3ZeZh4E+A13S4JkknwHPzhPDcPDE8N08Mz80dZvCd4SIiKMZsPJKZv9bpeqarzPxQZq7IzDUUExV8PTP9FO8EZeYzwOaIeEm56lpgQwdLmq6eBF4VEf3l//FrcSISadrw3DwxPDdPDM/NE8Zzc4fVOl2AOu5K4KeBhyLigXLdv8zMOztYk2a29wF/EBE9wCbgZzpcz7STmd+NiC8B91HMDns/8OnOViXpBHhu1lTjufkUeW7uvMjMTtcgSZIkSVLb2NVZkiRJktTVDL6SJEmSpK5m8JUkSZIkdTWDryRJkiSpqxl8JUmSJEldzeArzRARcUZE3B4Rj0XEvRFxZ0Sc2+m6JEmaqTw3S5PH6/hKM0B5ofQ/BX43M28t170cOB34YSdrkyRpJvLcLE0ug680M/wEcDgzPzWyIjMf7GA9kiTNdJ6bpUlkV2dpZrgQuLfTRUiSpCM8N0uTyOArSZIkSepqBl9pZlgPXNbpIiRJ0hGem6VJZPCVZoavA70R8Z6RFRFxUUS8toM1SZI0k3luliaRwVeaATIzgbcC15WXTFgP/Dvgmc5WJknSzOS5WZpcUfyfkyRJkiSpO9niK0mSJEnqagZfSZIkSVJXM/hKkiRJkrqawVeSJEmS1NUMvpIkSZKkrmbwlSRJkiR1NYOvJEmSJKmrGXwlSZIkSV3N4CtJkiRJ6moGX0mSJElSVzP4SpIkSZK6msFXkiRJktTVDL6SJEmSpK5m8JUkSZIkdTWDryRJkiSpqxl8JUmSJEldzeArSZIkSepqBl9JkiRJUlcz+EpdJiLeFRHfOoXjPxUR/2oia2q3iFgfEdd0ug5JkiRNTQZfTWkR8UREHIiIvS3LsnLbpyPi0YhoRsS7XuR5VkTEH0fE9ogYjIiHX+yYmWC8kJyZP5eZ/6ZTNZ2MzLwgM//mVJ4jIj4WEb9/HPu9IyLuKX8Wt0bEVyPiqlN5bUmSJLWXwVfTwU9m5pyW5ely/YPA/w3cdxzP8XlgM7AaWAz8NPDsRBYZEbWJfD5NPRHxC8B/Bn4FOB1YBfwWcEsn65IkSdILM/hq2srMT2bm/wQOHsfurwA+l5n7MnM4M+/PzK+ObIyIqyLi7yJiICI2j7QGR8T8iPi9iNgWET+OiA9HRKXc9q6I+NuI+PWI2AF8rFz/jyPikYjYFRF3RcTqYxUVEa9qed0HR7rrRsQ/jIh7xuz7/oi448XqGnPMmojI1lAeEX8TEf8kIl4KfAp4ddl6OVBu/1xE/NuW/d8dERsjYmdE3DHS4l5uy4j4uYj4Ufk1fDIi4hhf6xUR8e1yv60R8V8joqdl+xvLFvzBiPitiPhfEfFPym1nR8TXI2JH2Wr/BxGxoOXYJyLiuvL+xyLii+X7s6fsBn15y77/IiKeKrc9GhHXRsQNwL8E/mH5Xjw4Tv3zgY8DP5+Zf1L+LB3OzD/PzF8a9xssSZKkKcHgq5niO8AnI+LWiFjVuqEMpl8F/guwFLgYeKDc/F+A+cBZwNXAPwJ+puXwVwKbKFr/fjkibqEIUH+vfK7/DXxhvIIiYjnwF8C/BRYBvwj8cUQsBf4ceElErGs55B3AHx5nXS8qMx8Bfg74dtmSvmDsPhHxeuDfAW8HzgR+DNw+Zrc3U3ywcFG53/XHeMkG8H5gCfBq4FqKFnsiYgnwJeBDFC3yjwKvaS2lrGMZ8FJgJeUHDcdwc1nnAuAO4L+Wr/MS4L3AKzJzblnrE5n5lxStuP+jfC9ePs5zvhroA/70BV5XkiRJU5DBV9PBl8tWwoGI+PJJPsfbKELovwIej4gHIuIV5bZ3AH+dmV8oW/B2ZOYDEVEFbgU+lJl7MvMJ4D9RdJMe8XRm/peyFfkARZD8d5n5SGYOU4Spi4/R6vtO4M7MvDMzm5n5NeAe4KbM3A/8GXAbQBmAzwPuOM66JspPAZ/NzPsyc4gimL46Ita07POJzBzIzCeBb1B8cPA8mXlvZn6nfK+eAP4/itAOcBOwvmxJHQZ+E3im5diNmfm1zBzKzG3Ar7UcO55vle9rg6Kb+0iQbQC9wPkRUc/MJzLzseN8LxYD28v6JEmSNI0YfDUdvCUzF5TLW07mCTJzV2Z+MDMvoGidfYAiUAdF6+F44WcJUKdo5RzxY2B5y+PNY45ZDfzGSFAHdlK0Vi7n+VYDb2sJ9QPAVRQtq1C07t5W3n8H8OUyEB9PXRNlWevrZOZeYMeY13qm5f5+YM54TxQR50bEVyLimYjYTfGhwJKW1znyXmZmAltajj09Im4vuyjvBn6/5djxjK2pLyJqmbkR+GcUrcXPlc+5bLwnGMcOYEk4lluSJGnaMfhqxsnM7cB/pAhbiygC19nj7LodOEwRUEesAp5qfboxx2wG/s+WoL4gM2dl5t+N8/ybgc+P2Xd2Zn6i3P41YGlEXEwRgEe6OR9PXSP2lbf9LevOeIH6x3q69XUiYjZFy+d4r/Vi/hvwA2BdZs6j6BI+Mh54K7Ci5XWi9TFFSE7gZeWx72w59oRk5h9m5lUUX1cC/35k04sc+m1gCDipD18kSZLUOQZfTVsR0RMRfRQBqB4RfeNN8FTu++8j4sKIqEXEXOD/AjZm5g7gD4DrIuLt5fbFEXFx2U32ixRjd+eW3ZV/gaK18Vg+BXwoIi4oX3d+RLztGPv+PvCTEXF9RFTL+q+JiBUAmXkY+CPgP1AE9K+V64+7rrJb8FPAO8vX+MccHfKfBVa0TjI1xheAn4mIiyOilyKAfrfsqnyi5gK7gb0RcR7F92DEXwAvi4i3lC2qP8/RAX0usBcYLMdGn9RkUhHxkoh4ffm1HAQOAM1y87PAmmP9DGXmIPARirHib4mI/oioR8SNEfGrJ1OPJEmSJofBV9PZX1EEl9cAny7vv+4Y+/ZTTEo0QDEZ1WqKCZAox6beBHyAomvyA4yOCX0fRavpJuBbFK2unz1WQZn5pxQtiLeXXXIfBm48xr6bKS6D8y+BbRQtwL/E0f8v/xC4DvijMWNLT6Sud5fPuwO4AGhtff46sB54JiK2j1PjX1OMi/5jilbZsynGF5+MX6Tosr0H+G3gf7S8znaKcdi/WtZ5PsV456Fyl38NXAoMUoTkPznJGnqBT1C0mj8DnEYxbhmKDxkAdkTEuJfIysz/RPEhw4cZ/Z69FzjZseeSJEmaBFEMpZOkqaNsdd0C/FRmfqPT9UiSJGl6s8VX0pRQdvleUHZDHhn/+50OlyVJkqQuYPCVNFW8mmJ27e3AT1LM5n2gsyVJM0dEfDYinouIh4+xPSLiNyNiY0R8PyIunewaJUk6WXZ1liRJRMTrKCaR+73MvHCc7TdRzC9wE/BK4Dcy85WTW6UkSSfHFl9JkkRmfpNigr9juYUiFGdmfgdYEBFnvsD+kiRNGbVOFzBRlixZkmvWrOl0GZKkLnHvvfduz8ylna5jCllOMZP5iC3luq2tO0XEe4D3AMyePfuy8847b9IKlCR1t1M5N3dN8F2zZg333HNPp8uQJHWJiPhxp2uYjjLz0xSXmOPyyy9Pz82SpIlyKudmuzpLkqTj8RSwsuXxinKdJElTnsFXkiQdjzuAf1TO7vwqYDAzt77YQZIkTQVd09VZkiSdvIj4AnANsCQitgAfBeoAmfkp4E6KGZ03AvuBn+lMpZIknTiDryRJIjNve5HtCfz8JJUjSdKEsquzJEmSJKmrGXwlSZIkSV3N4CtJkiRJ6moGX0mSJElSVzP4SpIkSZK6msFXkiRJktTVDL6SJEmSpK5m8JUkSZIkdbVapwuQpBmr0YCDh2BoqLgFOH0xVKudrUuSJKnLGHylqerwMGx5BrbthAVzYeF8mD8XeuqdrkwvJhMOHYaDQ6PB9sAQ7D8IBw7C0KFie7MJ1QpEQJbHrv8RrF0Bq5dD3V/RkiRJE8G/qqSpZnAPPL4Fnt0BATSasGMAak8X93vqRRBevLC4nTsbKo5amFTNhN17YN+BItweOFiE2oNDRaA9PFx8Tyoxun+zOf5zNcZZ/9jmYll5Bpy1Evp62/e1SJIkzQAGX2kqaDTg6W3w+OYiPI0XhoYbxe3QoSIUb9tVtBQ2mzB7FixeAIvmw/x5MMugNKEyYc++4gOIZ7fD4N7ivacMtDnOMc0mHCPrvqiRkPzkVtj8TNH9+ZzVMKf/JJ9QkiRpZjP46uRkFkHs4FARxIYOFV06D5RdOQ8OFa1eULRaEkffP+qWMkS03o63XxQBb+H8oqVzdv9oi9p0tXc/PPEUPP1s8Xi8wHssrS2Ie/cXy5Zni+9NpVJ0i16yABbMg/lzHDd6IjKL1twdA8WHDAODQBTrj9Vy2646MuGZ7UUdi+bDutXF91SSJEnHzeCro2XC8HA5LvEYgXakKycUASsYDQTjtXxNpIHd8Mw2oKWlc+F8WDivCMP9s0bD81TVbBYhZtPmIqzmBL5vjcboa+zYBTsHijGkjWbRXXbR/GLp74NaDWrV0duZ3l16/8Ei6D63o3jfMovvy2QG3WMZCcDbd8GuweJDn3PXwJKFU//nXZIkaQow+M50B4fguZ1Fi+O+A+MH2hdqhRwJWpOpMU5L50iLaVJ0B100v2gVWzC3CHxTIRwcOAg/fho2by3qnIz3bqRlfuT1nzpYfHAw8n6MBKpmFusqlSIoV6tFGK7XRpeeOtTqZVguA3N95LYOfT1T430+XgeHYOdgEXR3DBTv08iY6qms0YTde+H+DdDbA+vWwBlLp3/vB0mSpDYy+M40mUVQfHY7PP1cEYYinv/HficC7alorX/33mKpPlN8vRHFBFCLy/GvC+7tCQIAACAASURBVOYWgWEyjLTSbdpctFZDETI76VjBLrP4vjcawOFjHz/yoUhryB0JzvNmw6IFRQv8VJuB+tDhIuhu21F8Tw4dhqhMv5/1EY1m0Ur98I/gkcfgnFWw4gy7tEuSJI3D4DsTNLMYo7h1WzFWsNEYbeUD2t8/uUNaA83A7mKpVYuuq5UqzJtTBLSe+tFdfkdu6y1dgE+0JXPoUDEp0Y+fKgLKdA1X43mhrr+7dhdLrVp83bUaLJhThOEFc2He3GJbu2QWLbn7DhQf8OzZW0xKtf9g0aJbGRt0u+D7MvJhxaOPww+fgDUrYM2yohVekiRJgMG3ew03ilatrc8V14Eluit8nayRbr/N4WIc586BootoxNHhNim7AZfjb6uVIjS1dvOt1aCn7OY7Ep6r1WL87vbyPZ8K40M7YeR9Pny4mH16e8tY496eohv64vK6xCdzOabhBuzbPxpwd++BveWlhSKK7+l4lxDq5v8DIy35mzYXy9hLIY10e280xr9tvX+4HMc/PDy6bWR7BJyxpFgWzJte3dslSdKMZfDtJkOHivGKTz0Hg7undzfOydRMXrTVu9EslpEx0OMZCdBHuhJ3aUv6yWgda3xwqBhn/NyO0csx9fcVk5QtKsPw7Fmj++4tA+7ultbbRmM0LI/9Gc88+csIdYOjLoW0tWzlbo52+z+ytBxz5IOecnkxTzxV9GgI4LTFcOZpxeW0qjN8gjRJkjRlGXyns5FLroyM1913oAhfR4KXoXdSHU+A1qjW1th9B4pl6zZgJIDR0nrbfP7YaD/UeWFHZqVujFk3QT+jI+//088VH2I0m0WX9mWnwWmL7GotSZKmFIPvdNNoFGMon9tRjNc9PAy0jNdtGLw0jdl6Oz2NtOZv31X8fnqoWUx0tux0OH0xzOrrbH2SJGnGM/hOdc0mDO4pxkg+tx327C+6Ew7b2iVpChr58GJwb/H76tFNxTjjZacVl12a0++4YEmSNOkMvlNNZjGOcfuuolV3cE8xVrfZHO2iaOiVNB2MdGfffxAe2wyPbykmgDtjCZy5tBjXbQiWJEmTwODbaZnFH4U7dhWzAe8aLNfTMgbSoCtpmssshmI0msXEW089ByQsXQRLFsLs/mJSs8m6xrYkSZpRDL6dcHAIdgwULbo7BkYno5qpl76RNPOMdIl+ZntxybWRGb4jijHBc2cX19qe01+E4v4+W4clSdJJM/iOZ3BPSwhtvezHmEuAtD4e+wfZyOOR1Xv2w7YdxTVNDx8ec9kbSZrBjvpdmMUlrPbuL0JxtVJOcpZFa/Cc/iIQz51dtBDP7i+uoS1JkvQCDL5jDR2Cv7v/5P+QeqFJlY+asdbZlyXpBbVe/xmK3jIHh4o5EKrl7+hms/h93T9rNBAvnFfclyRJKhl8x8qEirMmS9KU1vpB4uHhoqfO4J7i93dfD1x9RedqkyRJU06l0wVIkjRhms3R65pLkiSVDL6SJEmSpK5m8JUkSZIkdTWDryRJkiSpqxl8JUmSJEldzeArSZIkSepqBl9JkiRJUlcz+EqSJEmSuprBV5IkSZLU1Qy+kiRJkqSuZvCVJEmSJHU1g68kSZIkqasZfCVJkiRJXc3gK0mSJEnqagZfSZIkSVJXM/hKkiRJkrqawVeSJEmS1NXaGnwj4oaIeDQiNkbEB8fZvioivhER90fE9yPippZtHyqPezQirm9nnZIkSZKk7lVr1xNHRBX4JPAGYAtwd0TckZkbWnb7MPDFzPxvEXE+cCewprx/K3ABsAz464g4NzMb7apXkiRJktSd2tniewWwMTM3ZeYh4HbgljH7JDCvvD8feLq8fwtwe2YOZebjwMby+SRJkiRJOiHtDL7Lgc0tj7eU61p9DHhnRGyhaO193wkcK0mSJEnSi+r05Fa3AZ/LzBXATcDnI+K4a4qI90TEPRFxz7Zt29pWpCRJkiRp+mpn8H0KWNnyeEW5rtXPAl8EyMxvA33AkuM8lsz8dGZenpmXL126dAJLlyRJkiR1i3YG37uBdRGxNiJ6KCarumPMPk8C1wJExEspgu+2cr9bI6I3ItYC64DvtbFWSZIkSVKXatuszpk5HBHvBe4CqsBnM3N9RHwcuCcz7wA+APx2RLyfYqKrd2VmAusj4ovABmAY+HlndJYkSZIknYy2BV+AzLyTYtKq1nUfabm/AbjyGMf+MvDL7axPkiRJktT9Oj25lSRJkiRJbWXwlSRJRMQNEfFoRGyMiA+Os31VRHwjIu6PiO9HxE2dqFOSpJNh8JUkaYaLiCrwSeBG4Hzgtog4f8xuHwa+mJmXUExY+VuTW6UkSSfP4CtJkq4ANmbmpsw8BNwO3DJmnwTmlffnA09PYn2SJJ0Sg68kSVoObG55vKVc1+pjwDsjYgvFxJXvG++JIuI9EXFPRNyzbdu2dtQqSdIJM/hKkqTjcRvwucxcAdwEfD4invd3RGZ+OjMvz8zLly5dOulFSpI0HoOvJEl6CljZ8nhFua7VzwJfBMjMbwN9wJJJqU6SpFNk8JUkSXcD6yJibUT0UExedceYfZ4ErgWIiJdSBF/7MkuSpgWDryRJM1xmDgPvBe4CHqGYvXl9RHw8Im4ud/sA8O6IeBD4AvCuzMzOVCxJ0ompdboASZLUeZl5J8WkVa3rPtJyfwNw5WTXJUnSRLDFV5IkSZLU1Qy+kiRJkqSuZvCVJEmSJHU1g68kSZIkqasZfCVJkiRJXc3gK0mSJEnqagZfSZIkSVJXM/hKkiRJkrqawVeSJEmS1NUMvpIkSZKkrmbwlSRJkiR1NYOvJEmSJKmrGXwlSZIkSV3N4CtJkiRJ6moGX0mSJElSVzP4SpIkSZK6msFXkiRJktTVDL6SJEmSpK5m8JUkSZIkdTWDryRJkiSpqxl8JUmSJEldzeArSZIkSepqBl9JkiRJUlcz+EqSJEmSuprBV5IkSZLU1Qy+kiRJkqSuZvCVJEmSJHU1g68kSZIkqasZfCVJkiRJXc3gK0mSJEnqagZfSZIkSVJXM/hKkiRJkrqawVeSJEmS1NUMvpIkSZKkrmbwlSRJkiR1NYOvJEmSJKmrGXwlSZIkSV3N4CtJkiRJ6moGX0mSJElSVzP4SpIkSZK6msFXkiRJktTVDL6SJEmSpK5m8JUkSZIkdTWDryRJkiSpqxl8JUmSJEldzeArSZIkSepqBl9JkiRJUlcz+EqSJEmSuprBV5IkSZLU1Qy+kiRJkqSu1tbgGxE3RMSjEbExIj44zvZfj4gHyuWHETHQsu1XI2J9RDwSEb8ZEdHOWiVJkiRJ3anWrieOiCrwSeANwBbg7oi4IzM3jOyTme9v2f99wCXl/dcAVwIXlZu/BVwN/E276pUkSZIkdad2tvheAWzMzE2ZeQi4HbjlBfa/DfhCeT+BPqAH6AXqwLNtrFWSJEmS1KXaGXyXA5tbHm8p1z1PRKwG1gJfB8jMbwPfALaWy12Z+Ugba5UkSZIkdampMrnVrcCXMrMBEBHnAC8FVlCE5ddHxGvHHhQR74mIeyLinm3btk1qwZIkSZKk6aGdwfcpYGXL4xXluvHcymg3Z4C3At/JzL2ZuRf4Kv9/e3ceJOld33f8/e2ee2fvXa2O3dVhBEIIZGC5g7ENToA4kMSOI9lOjIuykrLBGLBTOE6Awkkl2AnETohjcRgf2DKWbWpjy+ADKKccIJK4JQwWQhIrDl27s8fsnP3NH79ndmZHc/TOTHdP97xfVY/meZ5+uve7j2a259O/C563+EmZeXNmHsnMI/v379+gsiVJ2npWm5CyuuaHIuLuavLJ3213jZIkrVUrg+/twNURcWVEDFDC7dHFF0XENcBu4BMLTj8AvCgi+iKinzKxlV2dJUlqgQUTUr4MuBa4MSKuXXTN1cDPAy/IzKcAP9P2QiVJWqOWBd/MnAFeA3yEElo/mJl3RcTbIuIVCy69AbglM3PBuVuBrwJfAD4HfC4z/3erapUkaYtrZkLKnwDelZnHATLzoTbXKEnSmrVsOSOAzLwNuG3RuTcvOn7rEs+bBf5VK2uTJEnnLDUh5XMWXfNEgIj4G6AOvDUzP7z4hSLiJuAmgMOHD7ekWEmSLtRmmdxKkiRtbn3A1cB3U5YgfHdE7Fp8kfNvSJI2I4OvJElqZkLKY8DRzJzOzK8BX6EEYUmSNj2DryRJamZCyg9RWnuJiH2Urs/3trNISZLWyuArSdIW1+SElB8BHo2Iu4GPAT+XmY92pmJJki5MSye3kiRJ3WG1CSmr1RfeUG2SJHUVW3wlSZIkST3N4CtJkiRJ6mkGX0mSJElSTzP4SpIkSZJ6msFXkiRJktTTDL6SJEmSpJ62avCNiJGI+PcR8e7q+OqI+P7WlyZJkiRJ0vo10+L7G8Ak8Lzq+EHgP7SsIkmSJEmSNlAzwfc7MvOXgGmAzBwHoqVVSZIkSZK0QZoJvlMRMQwkQER8B6UFWJIkSZKkTa+viWveAnwYOBQRHwBeALyqlUVJkiRJkrRRVgy+ERHA3wL/FHgupYvz6zLzkTbUJkmSJEnSuq0YfDMzI+K2zHwq8KdtqkmSJEmSpA3TzBjfT0fEs1peiSRJkiRJLdDMGN/nAD8SEfcDZyjdnTMzn9bSyiRJkiRJ2gDNBN9/0PIqJEmSJElqkVW7Omfm/cAu4B9V267qnCRJkiRJm96qwTciXgd8ALio2n4nIl7b6sIkSZIkSdoIzXR1fjXwnMw8AxARbwc+Afz3VhYmSZIkSdJGaGZW5wBmFxzPVuckSZIkSdr0mmnx/Q3gUxHxx9XxPwbe27qSJEmSJEnaOKsG38x8R0R8HPh71akfz8zPtLQqSZIkSZI2yKrBNyKeC9yVmZ+ujndExHMy81Mtr06SJEmSpHVqZozvrwGnFxyfrs5JkiRJkrTpNTW5VWbm3EFmNmhubLAkSZIkSR3XTPC9NyJ+OiL6q+11wL2tLkySJEmSpI3QTPD918DzgQer7TnATa0sSpIkSZKkjdLMrM4PATe0oRZJkiRJkjbcsi2+EfETEXF1tR8R8b6IGIuIz0fEM9pXoiRJkiRJa7dSV+fXAfdV+zcC1wNXAW8AfqW1ZUmSJEmStDFWCr4zmTld7X8/8FuZ+Whm/iWwrfWlSZIkSZK0fisF30ZEXBIRQ8CLgb9c8Nhwa8uSJEmSJGljrDS51ZuBO4A6cDQz7wKIiBfhckaSJEmSpC6xbPDNzD+JiMuB7Zl5fMFDdwD/vOWVSZIkSZK0AVZczigzZ4Dji86daWlFkiRJkiRtoJXG+EqSJEmS1PUMvpIkSZKknram4BsR12x0IZIkSZIktcJaW3z/fEOrkCRJkiSpRZad3CoifnW5h4BdrSlHkiRJkqSNtdKszj8OvBGYXOKxG1tTjiRJkiRJG2ul4Hs78MXM/L+LH4iIt7asIkmSJEmSNtBKwfcHgYmlHsjMK1tTjiRJkiRJG2ulya1GM3O8bZVIkiRJktQCKwXfD83tRMQftqEWSZIkSZI23ErBNxbsX9XqQiRJkiRJaoWVgm8usy9JkiRJUtdYaXKr6yPiJKXld7japzrOzNzR8urUMpkwMw4Tj8HEcZg5C0N7YGQ/DOyEiNVfQ5IkSZK6wbLBNzPr7SxErTc7BZPHS9CdOA6NqZJu+0aS/m0w/i04842gPpgM74eRi6B/1BAsSZIkqbut1OKrLpezMDlWQu7kcZg+UxJsrS8Z3A1Du8vXvqFyfWMWJh5Jxh+C0w/C6WNB30gysh+GL4L+kQ7+ZSRJkiRpjQy+PSQTps/Mt+pOjgGNgEgGd8KOK5Oh3cu34tbqMHKgbLPTcPbhEoJP3g8n7w/6t5cQPHIR1Afb/teTJEmSpDUx+Ha52akq5D4GEyfO7748egkM7k4Gd5VQeyHq/TB6adlmJuHsQyUEj90bjN1bgvTwARjZB7X+FvzFJEmSJGmDGHxbaHYaTt0P4w+V46iVjeprxKJzC46XOxc1IOZbds91X+5f1H15A1tk+wZh+6GyTY8nZx8qf6cTXwlO/F1pRR45AEN7LzxgS5IkSVKrGXxbIGfh1INw6oGyP7wfan2QjdIdORtAozpulLG1i8/NbeQyM0s12X15o/WPQP8VsP1ymD5dWoHPPgSPPRZELRnaV2aGHtpThfQOyrlFuLJsObfP/HGt3vk6JUmSJLWWwXcDZcL4t+HkfTA7GQztSXZeBf3b1vOaJaGdH4ahNtDZ1tUIGNhetp1XwdRYFYIfhrMPBdGX9A8vWAB6iVWhFwfRcw8vs4L0qkH2vNdq8lOASPqGoG+kbP3D8/t1u3BL2kIi4qXArwB14D2Z+Z+Xue4HgFuBZ2XmHW0sUZKkNWtp8F3tTTQi3gl8T3U4AlyUmbuqxw4D7wEOUaLMyzPzvlbWux4Tj8HY12D6dNA/mux+UmmJXa8IIDZ3q2QEDO4q264nwMTx5OzDZfxxsKglOs5/3uJzxILDJZ638DmLn3/ecWQ5vcr1s5NlDePpak3jhS3stf6krwrC/SOc2+8bdoknSb0lIurAu4DvA44Bt0fE0cy8e9F124HXAZ9qf5WSJK1dy4JvM2+imfn6Bde/Fnj6gpf4LeA/ZuZfRMQo0GhVresxdRrG7oXJ40F9KNnz5LIG7lYNRlGD4b1l6zaZMDuRTI/DzPiCQPwojH9rYWKvAvFwFYhH5ved6EtSl3o2cE9m3gsQEbcArwTuXnTdLwJvB36uveVJkrQ+rWzxbfZNdM6NwFuqa68F+jLzLwAy83QL61yTmUk4+bXStTn6YOdVyehlm7tlViuL4FygZVFwb0wn02erQDxeAvHM2ce3EkdfPm4ysrmJzFjqfDVZ2ZLnq68D2+fXWpakFrkM+PqC42PAcxZeEBHPAA5l5p9GhMFXktRVWhl8V30TnRMRlwNXAh+tTj0ROBERf1Sd/0vgTZk527pym9OYKZNWnXoQSBg9CDsO29LX62r9MNgPgzvOP58JM2fzXAvxzMQyE5Ul5Mwy56vrlx2XHMnIRbD9cGlVlqR2i4ga8A7gVU1cexNwE8Dhw4dbW5gkSU3aLJNb3QDcuiDY9gEvpHR9fgD4fcqb7XsXPqmdb67ZgNPfKMsTNWaCkYuSHVfaErfVRVQzXa8zkGaW/ywMxDTKBy3j34Yz3yxfh/eVADywfSOql6RzHqTMqTHnYHVuznbgOuDjUcbyXAwcjYhXLJ7gKjNvBm4GOHLkyMLpCiVJ6phWBt/V3kQXugH4qQXHx4DPLugm/SHguSwKvu14c82Es4+UcbyzE8HgrmTnVWnw0IZaaRKzge0l7J5+sGxnHwkGdyc7DsPAzq07nlzShroduDoirqS8V98A/PDcg5k5BuybO46IjwM/66zOkqRu0crgu+Kb6JyIuAbYDXxi0XN3RcT+zHwY+F6g7W+uk2Mw9lWYOhX0bUv2XpdlfVqDhtqsPgA7r4Tth+D0N5LTx+DhzwUDO5Lth/H7UtK6ZOZMRLwG+AhlJYb3ZeZdEfE24I7MPNrZCiVJWp+WBd8LeBO9Abglc3711sycjYifBf4qSp+qO4F3t6rWxabHk7F7YOLRoDaQ7H5iMnKxwUKdV+srY8pHL4PxbyWnvg6PfjHo31YC8FaeUVzS+mTmbcBti869eZlrv7sdNUmStFFaOsa3mTfRzHzrMs/9C+BpLStuqT9ztsGJP7mPM3eWLqc7rkhGD0Kt3s4qpNXV6iX8brsExh9KTj0Aj30p6Lsv2X4IRg44w7gkSZI0Z7NMbrUpRL3G7Mkptl1aWtXqA52uSFpZ1GDbxSXonn2kBODjXwlO3l8+tNl2iR/cSJIkSQbfRfbecDXxf+6AhhNRqntEwMj+Muvz5PHk5AMw9tXgVBWARy91yS1JkiRtXQbfRaLmAEl1r4gy0dXQHpgcKy3AJ+8LTn09Gb20rDttTwZJkiRtNQZfqUcN7oTBp8LU6RKAT30dTh0rLcO1/mqt4NU02fEh6uU16/1QG5j/Wuu3q7UkSZI6z+Ar9biBUdh7LUyPl/A78ch8nl2yf0MznR4WXZMzkI2lnxi1PBeCzwvGi/br1TVOyiVJkqSNZvCVtoj+EdjzJOBJrXn9xmzSmIbGFMxOQ2MaZqc479zsFEyfLvvkMkG5L+eD8dzWNx+Ma33z5+v9EH0u4SRJkqSVGXwlbYhaverWPLT6tZmQszkfjBeG5AX7sxMwfWrloAx5Xkh+XGheEJzrA6WV2aAsSZK0tRh8JbVdRGmprTX5L1AmZCPPBePG9Hyr8uJz0+Pzx0v32y5dr+eCcH2Q848HoDZYtSbb7VqSJKknGHwlbXoR1QRaTbYoQxWWZ/K8gDw7WVqTZ6eq7tdTMHVquZCc82OPq4B8XjgemB+bHHVbkSVJkjYzg6+knhQB0d/c+sWZ0JgqXa+XCsezUzB9phwv2YocWcYeD0C9b37irrnxyIu7X9cHbE2WJElqJ4OvpC0vomrRHQS2L39dJjSm5wNyY+bxXa0bc92tx1bqbl3Ndr1obHJ9EOrD0DdUtvqQLcmSJEkbweArSU2KmO/qzOjq12dCY6a5sckzZ0uYPn8Sr6ReheC+4RKE+xYE42ZasyVJkmTwlaSWiahacpsMqJkwO5klBE/AzEQJxDMTcPYRaEyf3/wbfTnfOrwgEPcNl9Zju1NLkiQVBl9J2iQi5sPrUhozycxEFYrPzgfj6TNw9lEe11p8bummnDuzjGUfePzjtapL9lxX7PoQ9A1W+4N2zZYkSZuTwVeSukStDwZGWbKb9Vxr8cKW4tmpx18Xj9tZxjKPN6arFuhHH98CDVnGKc+1Qi8MyNW+rdCSJKkTDL6S1AMWthYPtunPzNlkZnK+W/bsgv3JE9WY5UUJujaQ8y3EQ9A/DAM7S/dsW4slSVKrGHwlSWsSdegfKdtSslGNWa4C8ewE5/anTsHsI5zrnl0fTAZ3weAuGNpdzbAtSZK0QQy+kqSWiFo1C/Xw0o9nwszZZPIETB6HiUdh/NslCPcNJ4O7OReGm50gTJIkaSkGX0lSR0TMtxiPXlqC8PSZZPJ46So9/m04840yO1f/6HwIHtwFtXqnq5ckSd3E4CtJ2hQiyuRdA6Ow/VDpKj11ar5F+PSDcPpYQCQD26sQvBsGdzhpliRJWpnBV5K0KUUNBneWjcshZ2HyZGkRnjgBpx6AUw8E1JLBHSUED+2C/oFcddJqSZK0tRh8JUldIepl4quh3bATaMzA5InSIjxxAk5+LTgJjFw2y54Xd7paSZK0mRh8JUldqdYHw/vKBjA7VUJwfbv9niVJ0vkMvpKknlAfgJGLgCGDryRJOp+/HUiSJEmSeprBV5IkSZLU0wy+kiRJkqSeZvCVJEmSJPU0g68kSZIkqacZfCVJkiRJPc3gK0mSJEnqaQZfSZIkSVJPM/hKkiRJknqawVeSJEmS1NMMvpIkSZKknmbwlSRJkiT1NIOvJEmSJKmnGXwlSZIkST3N4CtJkiRJ6mkGX0mSJElSTzP4SpIkSZJ6msFXkiRJktTTDL6SJEmSpJ5m8JUkSZIk9TSDryRJkiSppxl8JUmSJEk9zeArSZIkSeppBl9JkiRJUk8z+EqSJEmSeprBV5IkSZLU0wy+kiRJkqSeZvCVJEmSJPU0g68kSZIkqacZfCVJkiRJPc3gK0mSiIiXRsSXI+KeiHjTEo+/ISLujojPR8RfRcTlnahTkqS1MPhKkrTFRUQdeBfwMuBa4MaIuHbRZZ8BjmTm04BbgV9qb5WSJK2dwVeSJD0buCcz783MKeAW4JULL8jMj2XmeHX4SeBgm2uUJGnNDL6SJOky4OsLjo9V55bzauDPWlqRJEkbqK/TBUiSpO4RET8KHAFetMzjNwE3ARw+fLiNlUmStDxbfCVJ0oPAoQXHB6tz54mIlwC/ALwiMyeXeqHMvDkzj2Tmkf3797ekWEmSLlRLg28TM0S+MyI+W21fiYgTix7fERHHIuJ/tLJOSZK2uNuBqyPiyogYAG4Aji68ICKeDvw6JfQ+1IEaJUlas5Z1dV4wQ+T3UcYK3R4RRzPz7rlrMvP1C65/LfD0RS/zi8Bft6pGSZIEmTkTEa8BPgLUgfdl5l0R8Tbgjsw8CvwyMAr8QUQAPJCZr+hY0ZIkXYBWjvE9N0MkQETMzRB59zLX3wi8Ze4gIp4JHAA+TBlLJEmSWiQzbwNuW3TuzQv2X9L2oiRJ2iCt7Orc9AyREXE5cCXw0eq4BvxX4GdbWJ8kSZIkaQvYLJNb3QDcmpmz1fFPArdl5rGVnhQRN0XEHRFxx8MPP9zyIiVJkiRJ3aeVXZ2bmiGycgPwUwuOnwe8MCJ+kjKeaCAiTmfmeRNkZebNwM0AR44cyY0qXJIkSZLUO1oZfM/NEEkJvDcAP7z4ooi4BtgNfGLuXGb+yILHXwUcWRx6JUmSJElqRsu6OmfmDDA3Q+SXgA/OzRAZEQtngbwBuCUzbbGVJEmSJG24Vrb4rjpDZHX81lVe4/3A+ze4NEmSJEnSFrFZJreSJEmSJKklDL6SJEmSpJ5m8FVRq0FfvdNVSJIkSdKGa+kYX3WJWg327ITDl8LYSXh0DE6dgUyoBczMrv4akiRJkrRJGXy3unoNLt4PT30iRMCBveV8JkxMwthpOHESHhuD02cgKdfNGoYlSZIkdQeD71ZWq8EVB+Hqy0uYXSgChofKdvG+ci4Tzk6UMHz8JDx2As6Mzz93ttHe+iVJkiSpCQbfrapWg2uugssvbf45ETAyXLZL9pdzmXDmLIydKmH4+Fg5rtWANAxLkiRJ6jiD71ZUq8F3XgMH9q3/tSJgdKRslx0o5xpZWoLHTpUgfPwkjE+UPzcTGh0Ow7UotTQapda548Qu3JIkSVIPMvhuNfU6HLmuTGbVKrWA7dvKdvDicq7RgNNVGH5srIwbPjtZxhg3WhiG++pV2M7SbXvHKOzaPl/fQD9MTZcu3OMTMH621HnmbBnjPD1dQnFEeR1bsCVJIFXVmwAAD9VJREFUkqSuY/DdSvr74DnXl8DXbrVaCZ07RuHQJeXcbKNMmHUuDJ8qYXMtYbheLcXUaMBgP4xug107YMe2sj8y9PhxzHMGB8q2a8fjH2tUk3ydrULx+ESZ8Xp8AiYnSwtxrQ5BawO8JEmSpDUz+G4FESXYPff60uq5WdRrsHN72Q5XY41nZ+HkGTh5qiyrNHYKJqeqMNwAorQozzZKa+62kdKCu2O0BPptI+XajVKLEppHhmDvrsc/PjtbQvBci/GZ8VL/mbMwM1ONdcYu1JIkSVIHGXx7Xa2akOo515duvZtdvQ67d5Tt8svKuZlZOHm6hOB6rQTc0W2lBbvT6vX5btOLzcyWIHx6vLQSnzxd9qemSisxGIglSZKkNtgEyUEtU6uV1tBnXldaR7tVX72MSW7luORW6KvPt2gv1GiUFuHT4/NdvU+Pw8RU+aBibp3k7EzZkiRJUq8x+Paqeg3274Hrr5nvbqvNoVZb0Eq8f/58ZukuvTAQnxqHiYkyfliSJEnSmhh8e1G9ViaQuuaq5Sd00uYTAduGy3Zg7/z5Eyfhji+WrtNpAJYkSZIulE2BvaZWgydcDk/+DkNvr9i1A/7eM0sgtvVekiRJumD+Ft1LajW47mq46lCnK9FGGxqE5z+jtARv5KzVkiRJ0hbgb9C9ol6DZz4FLjvQ6UrUKvVaGbN99RW2/EqSJEkXwN+ee0FfvSxXtG93pytRq0XAlQfhSJfP1C1JkiS1kcG3m0WUtXmf/4zHL5mj3rZ3F7zgmTAyVJZAkiRJkrQsg2+3qgUMD85PeqStZ2SohN+9u+36LEmSJK3A35a7Tb1eQu++PaWld3Cg0xWpk/rqZWz3VYcMv5IkSdIyXMd3s6vXodGA4SE4sA8u2gO7thtyNC8Crr4cdo7CZ78Es41OVyRJkiRtKgbfzaYWJchElMmqDuwr4zlt2dVqLtpbegH8v8/D9DQ0stMVSZIkSZuCwXcz6KuXVrod2+Di/SXwbt9Wwq90IUZH4IVH4NN3wdgpW38lSZIkDL6dUa9BAv19pZXuor2wZ6fL02hj9PfBs58Gf/s1eOAbpau8tBnN9W4Bv08lSVJLGXzbIYBaHTJh947Sqrt/dxm3K7VCBDz5qjLu9wtfMVRoc5mbpO/QJXDZAfjWI/DVByAb5UNBSZKkDWbwbYW5oNtolC7L+/eUoLtzh2uuqr0uvQi2jcAdX4DpmfLhi9QJ9Vr5/rtoLxy+tPRymWvtfcLhEoDv+jt49IQf1EiSpA1n8N0oc+N0t4/A/r1lQqpdO8ove1In7Rwt6z3f8UU4PW6oUPvMdWXeNgxXXAYX74O+Zd52hgfhyHXwyPHSS2F62jHqkiRpwxh812ou6I6OlCWG9u426GrzGhyA531naVH75sMGCrVWvV7+LTx4MRy6GEaGm3/uvt3womfBfcfgngfK7OT2VJAkSetk8G3WkkF3e/kFT+oGtRo89Unle/eR4zA1BVPTMDUDMzMwM1sCRq1WtdRVz2ukrcRa3dykfRfteXxX5gtVq8FVh+HSA3D3PfDwcb8HJUnSuhh8l5I5H3S3DZcxafvmui4bdNXlLr2obEuZbZQuptMzVSienj+emILJqSowz4XlmfKcViy9NbemdVImPXJd4s2nFkCUDwQvv7RM3LeRs9MPDcIznlLG/X7hy+X70d4KkiRpDQy+iw0MwJOuLJNS7Tboaoup16A+WAJHszJLa/FGm56Bycn5wD0xCeNny9fJKnxnY76F2oDcPnNdmQ9dAgcPXFhX5rXYuwu+61lw34Nwz/12f5YkSRfM4LtYLeDKg52uQuoeEWXt4I3W3wcjqyz5NTtbheIqHE9OwdkJGJ+YD8jTcwG5Pv8cXZharXR9j1qZpf7wxbB7HV2Z11rDVYeq2Z/vgYcfs/uzJElqmsFXUveq10tr42otjrOzJQyfPA3HT8LxMThztoQpsvPdZyPmJ8bbLGOq6/XygcGO7WU25n27S5fmdobdpQwOwDOuhcfG4PNfLl3vO/3/T5IkbXoGX0m9r14vwxe2bysthlC6yp4eb28YrtdLy+lsowTd4aFS047REt77++DUmVLLiVOl1bpea08Ynltnd3AQDuwtLbu7d27emer37Czdn+9/EP7uPrs/S5KkFRl8JW1NEcuH4bFTcOIkPHayjCu+kDBcq5UhE40szxkahG0jZT3lbSNlwrxtw8uvZ7tnZ5koCsqfd+o0jJ0+PwzXqpC6njA8N3lYRBlDe6Bq1R0cWPtrttvc0JRLL4K7vwoPPbo5WsslSdKmY/CVpDkLw/DBi8u5RsKZKgwfP1m2hWG40SiT4m0bnn/uyHDpFjzQv76uwfVamU1+147Vw3AzLcNzs9XvGJ3vvrx9W+e7L6/X4AA8/cnl/83nvzy/FJckSVLF4CtJK6mtEIZrVXflWhuT1rJh+EwVzhe3DFfBfK778p6dvTtb/e4d8F1HyrJHkiRJCxh8JelCzYXhzaJeg13by7YwDJ8+U1pDL2R5qm4X0V3dtSVJUlsYfCWpF9VrsHN7p6uQJEnaFDbpdJ2SJEmSJG0Mg68kSZIkqacZfCVJkiRJPc3gK0mSJEnqaQZfSZIkSVJPM/hKkiRJknqawVeSJEmS1NMMvpIkSZKknmbwlSRJRMRLI+LLEXFPRLxpiccHI+L3q8c/FRFXtL9KSZLWxuArSdIWFxF14F3Ay4BrgRsj4tpFl70aOJ6ZTwDeCby9vVVKkrR2Bl9JkvRs4J7MvDczp4BbgFcuuuaVwG9W+7cCL46IaGONkiStmcFXkiRdBnx9wfGx6tyS12TmDDAG7G1LdZIkrVNfpwvYKHfeeecjEXF/p+voYvuARzpdRA/wPm4M7+PG8D6uz+WdLqAbRcRNwE3V4WREfLGT9fQIf5bXz3u4ft7D9fMert+T1vrEngm+mbm/0zV0s4i4IzOPdLqObud93Bjex43hfdQFeBA4tOD4YHVuqWuORUQfsBN4dPELZebNwM3g9+BG8T6un/dw/byH6+c9XL+IuGOtz7WrsyRJuh24OiKujIgB4Abg6KJrjgI/Vu3/IPDRzMw21ihJ0pr1TIuvJElam8yciYjXAB8B6sD7MvOuiHgbcEdmHgXeC/x2RNwDPEYJx5IkdQWDr+bc3OkCeoT3cWN4HzeG91FNy8zbgNsWnXvzgv0J4J9d4Mv6PbgxvI/r5z1cP+/h+nkP12/N9zDspSRJkiRJ6mWO8ZUkSZIk9TSD7xYXEYci4mMRcXdE3BURr+t0Td0sIuoR8ZmI+JNO19KtImJXRNwaEX8bEV+KiOd1uqZuFBGvr36mvxgRvxcRQ52uSb0rIl4aEV+OiHsi4k1LPD4YEb9fPf6piLii/VVubk3cwzdU79Wfj4i/igiX21rCavdxwXU/EBEZEc6wu0gz9zAifmjB746/2+4aN7smfp4PV79/f6b6mX55J+rcrCLifRHx0HLL4UXxq9X9/XxEPKOZ1zX4agZ4Y2ZeCzwX+KmIuLbDNXWz1wFf6nQRXe5XgA9n5jXA9Xg/L1hEXAb8NHAkM6+jTFbkRERqiYioA+8CXgZcC9y4xPvIq4HjmfkE4J3A29tb5ebW5D38DOVn+mnArcAvtbfKza/J+0hEbKe8X3+qvRVufs3cw4i4Gvh54AWZ+RTgZ9pe6CbW5PfhvwM+mJlPp7w//8/2VrnpvR946QqPvwy4utpuAn6tmRc1+G5xmfnNzPx0tX+KEjIu62xV3SkiDgL/EHhPp2vpVhGxE/guyuyxZOZUZp7obFVdqw8YrtZbHQG+0eF61LueDdyTmfdm5hRwC/DKRde8EvjNav9W4MUREW2scbNb9R5m5scyc7w6/CRlrWWdr5nvRYBfpHz4MtHO4rpEM/fwJ4B3ZeZxgMx8qM01bnbN3MMEdlT7O/E9+jyZ+deU1QOW80rgt7L4JLArIi5Z7XUNvjqn6nr2dPwEdK3+G/BvgEanC+liVwIPA79Rdf95T0Rs63RR3SYzHwT+C/AA8E1gLDP/vLNVqYddBnx9wfExHv8B6rlrMnMGGAP2tqW67tDMPVzo1cCftbSi7rTqfay6RB7KzD9tZ2FdpJnvxScCT4yIv4mIT0bESi1zW1Ez9/CtwI9GxDHKbPqvbU9pPeNC/80EDL6qRMQo8IfAz2TmyU7X020i4vuBhzLzzk7X0uX6gGcAv1Z1/zkDLDtGS0uLiN2UT0OvBC4FtkXEj3a2KkkbofpZPgL8cqdr6TYRUQPeAbyx07V0uT5KF9PvBm4E3h0RuzpaUfe5EXh/Zh4EXk5ZI91c1mLeYBER/ZTQ+4HM/KNO19OlXgC8IiLuo3Rp+d6I+J3OltSVjgHHMnOu18GtlCCsC/MS4GuZ+XBmTgN/BDy/wzWpdz0IHFpwfLA6t+Q1Vff7ncCjbamuOzRzD4mIlwC/ALwiMyfbVFs3We0+bgeuAz5evV8/FzjqBFfnaeZ78RhwNDOnM/NrwFcoQVhFM/fw1cAHATLzE8AQsK8t1fWGpv7NXMzgu8VVY6zeC3wpM9/R6Xq6VWb+fGYezMwrKJMUfDQzbWG7QJn5LeDrEfGk6tSLgbs7WFK3egB4bkSMVD/jL8ZJwtQ6twNXR8SVETFA+Tfw6KJrjgI/Vu3/IOXfyGxjjZvdqvcwIp4O/Dol9Dqmcmkr3sfMHMvMfZl5RfV+/UnK/byjM+VuSs38PH+I0tpLROyjdH2+t51FbnLN3MMHKO/NRMSTKcH34bZW2d2OAv+ymt35uZQhXd9c7Ul9ra9Lm9wLgH8BfCEiPlud+7eZeVsHa9LW9lrgA9Wbxb3Aj3e4nq6TmZ+KiFuBT1Nmbv8McHNnq1KvysyZiHgN8BHKDOLvy8y7IuJtwB2ZeZTyAetvR8Q9lAlLnGV8gSbv4S8Do8AfVPOCPZCZr+hY0ZtQk/dRK2jyHn4E+PsRcTcwC/xcZtqDo9LkPXwjpYv46ykTXb3KDwPnRcTvUT5c2VeNg34L0A+Qmf+LMi765cA9wDhN/q4Y3mNJkiRJUi+zq7MkSZIkqacZfCVJkiRJPc3gK0mSJEnqaQZfSZIkSVJPM/hKkiRJknqawVfaIiLi4oi4JSK+GhF3RsRtEfHETtclSZIktZrr+EpbQJRFH/8Y+M3MvKE6dz1wAPhKJ2uTJEmSWs3gK20N3wNMV4t+A5CZn+tgPZIkSVLb2NVZ2hquA+7sdBGSJElSJxh8JUmSJEk9zeArbQ13Ac/sdBGSJElSJxh8pa3ho8BgRNw0dyIinhYRL+xgTZIkSVJbGHylLSAzE/gnwEuq5YzuAv4T8K3OViZJkiS1XpTfhyVJkiRJ6k22+EqSJEmSeprBV5IkSZLU0wy+kiRJkqSeZvCVJEmSJPU0g68kSZIkqacZfCVJkiRJPc3gK0mSJEnqaQZfSZIkSVJP+//tgmoXpEwwmAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1152x1440 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<Figure size 1152x1440 with 6 Axes>,\n",
       " array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f810a1fd438>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7f810a1ba198>],\n",
       "        [<matplotlib.axes._subplots.AxesSubplot object at 0x7f810a1e75f8>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7f810a193ba8>],\n",
       "        [<matplotlib.axes._subplots.AxesSubplot object at 0x7f810a14e198>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7f810a0fa748>]],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_performance(smv_lin_rbf_perf_metrics[smv_lin_rbf_perf_metrics['kernel'] == 'rbf'], param='C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show model configuration with highest accuracy and f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>kernel</th>\n",
       "      <th>accuracy_avg</th>\n",
       "      <th>precision_avg</th>\n",
       "      <th>recall_avg</th>\n",
       "      <th>specificity_avg</th>\n",
       "      <th>f1_score_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.0</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.810326</td>\n",
       "      <td>0.822935</td>\n",
       "      <td>0.713557</td>\n",
       "      <td>0.883239</td>\n",
       "      <td>0.763184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.810062</td>\n",
       "      <td>0.826171</td>\n",
       "      <td>0.708359</td>\n",
       "      <td>0.886693</td>\n",
       "      <td>0.761739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.5</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.809537</td>\n",
       "      <td>0.824026</td>\n",
       "      <td>0.709889</td>\n",
       "      <td>0.884621</td>\n",
       "      <td>0.761567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     C kernel  accuracy_avg  precision_avg  recall_avg  specificity_avg  \\\n",
       "7  2.0    rbf      0.810326       0.822935    0.713557         0.883239   \n",
       "3  1.0    rbf      0.810062       0.826171    0.708359         0.886693   \n",
       "5  1.5    rbf      0.809537       0.824026    0.709889         0.884621   \n",
       "\n",
       "   f1_score_avg  \n",
       "7      0.763184  \n",
       "3      0.761739  \n",
       "5      0.761567  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smv_lin_rbf_perf_metrics.nlargest(3, 'accuracy_avg')[['C', 'kernel',\n",
    "                                          'accuracy_avg',\n",
    "                                          'precision_avg',\n",
    "                                          'recall_avg',\n",
    "                                          'specificity_avg',\n",
    "                                          'f1_score_avg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>kernel</th>\n",
       "      <th>accuracy_avg</th>\n",
       "      <th>precision_avg</th>\n",
       "      <th>recall_avg</th>\n",
       "      <th>specificity_avg</th>\n",
       "      <th>f1_score_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.0</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.810326</td>\n",
       "      <td>0.822935</td>\n",
       "      <td>0.713557</td>\n",
       "      <td>0.883239</td>\n",
       "      <td>0.763184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.810062</td>\n",
       "      <td>0.826171</td>\n",
       "      <td>0.708359</td>\n",
       "      <td>0.886693</td>\n",
       "      <td>0.761739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.5</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.809537</td>\n",
       "      <td>0.824026</td>\n",
       "      <td>0.709889</td>\n",
       "      <td>0.884621</td>\n",
       "      <td>0.761567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     C kernel  accuracy_avg  precision_avg  recall_avg  specificity_avg  \\\n",
       "7  2.0    rbf      0.810326       0.822935    0.713557         0.883239   \n",
       "3  1.0    rbf      0.810062       0.826171    0.708359         0.886693   \n",
       "5  1.5    rbf      0.809537       0.824026    0.709889         0.884621   \n",
       "\n",
       "   f1_score_avg  \n",
       "7      0.763184  \n",
       "3      0.761739  \n",
       "5      0.761567  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smv_lin_rbf_perf_metrics.nlargest(3, 'f1_score_avg')[['C', 'kernel',\n",
    "                                          'accuracy_avg',\n",
    "                                          'precision_avg',\n",
    "                                          'recall_avg',\n",
    "                                          'specificity_avg',\n",
    "                                          'f1_score_avg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "smv_lin_rbf_perf_metrics.to_csv('SVM_kernel=lin-rbf_BERT.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with random forest based model by griding the following values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf_params = {\n",
    "        'n_estimators': list(range(25, 725, 25)),\n",
    "        'max_depth': [100, 200]\n",
    "        #'max_depth': [None] + list(range(50, 500, 50))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters {'max_depth': 100, 'n_estimators': 25}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 50}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 75}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 100}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 125}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 150}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 175}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 200}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 225}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 250}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 275}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 300}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 325}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 350}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 375}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 400}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 425}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 450}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 475}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 500}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 525}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 550}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 575}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 600}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 625}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 650}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 675}\n",
      "Training with parameters {'max_depth': 100, 'n_estimators': 700}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 25}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 50}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 75}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 100}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 125}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 150}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 175}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 200}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 225}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 250}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 275}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 300}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 325}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 350}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 375}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 400}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 425}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 450}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 475}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 500}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 525}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 550}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 575}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 600}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 625}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 650}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 675}\n",
      "Training with parameters {'max_depth': 200, 'n_estimators': 700}\n"
     ]
    }
   ],
   "source": [
    "rand_for_perf_metrics = []\n",
    "\n",
    "for param in ParameterGrid(rdf_params):\n",
    "    \n",
    "    meta = {\n",
    "        'n_estimators': param['n_estimators'],\n",
    "        'max depth': param['max_depth'],\n",
    "        'accuracy_fold': [],\n",
    "        'precision_fold': [],\n",
    "        'recall_fold': [],\n",
    "        'specificity_fold': [],\n",
    "        'f1_score_fold': []\n",
    "    }\n",
    "    \n",
    "    print('Training with parameters {}'.format(param))\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X_train_vect, y_train):\n",
    "    \n",
    "        # Split train and test partition\n",
    "        X_train_part = X_train_vect[train_index]\n",
    "        y_train_part = y_train[train_index].values\n",
    "        \n",
    "        X_test_part = X_train_vect[test_index]\n",
    "        y_test_part = y_train[test_index].values\n",
    "        \n",
    "        # Fit Random Forest model\n",
    "        rand_forest = RandomForestClassifier(n_estimators=param['n_estimators'],\n",
    "                                             max_depth=param['max_depth'])\n",
    "        rand_forest.fit(X_train_part, y_train_part)\n",
    "        \n",
    "        # Predict over test\n",
    "        y_pred = rand_forest.predict(X_test_part)\n",
    "        \n",
    "        # Meassure performance metrics\n",
    "        meta['accuracy_fold'].append(metrics.accuracy_score(y_test_part, y_pred))\n",
    "        meta['precision_fold'].append(metrics.precision_score(y_test_part, y_pred))\n",
    "        meta['recall_fold'].append(metrics.recall_score(y_test_part, y_pred))\n",
    "        #  Exchange class 1 and 0 so measure recall for class 0 (specificity)\n",
    "        meta['specificity_fold'].append(metrics.recall_score(np.abs(y_test_part-1), np.abs(y_pred-1)))\n",
    "        meta['f1_score_fold'].append(metrics.f1_score(y_test_part, y_pred))\n",
    "    \n",
    "    # Compute global average metrics\n",
    "    meta['accuracy_avg'] = np.mean(meta['accuracy_fold'])\n",
    "    meta['accuracy_std'] = np.std(meta['accuracy_fold'])\n",
    "    \n",
    "    meta['precision_avg'] = np.mean(meta['precision_fold'])\n",
    "    meta['precision_std'] = np.std(meta['precision_fold'])\n",
    "    \n",
    "    meta['recall_avg'] = np.mean(meta['recall_fold'])\n",
    "    meta['recall_std'] = np.std(meta['recall_fold'])\n",
    "    \n",
    "    meta['specificity_avg'] = np.mean(meta['specificity_fold'])\n",
    "    meta['specificity_std'] = np.std(meta['specificity_fold'])\n",
    "    \n",
    "    meta['f1_score_avg'] = np.mean(meta['f1_score_fold'])\n",
    "    meta['f1_score_std'] = np.std(meta['f1_score_fold'])\n",
    "    \n",
    "    rand_for_perf_metrics.append(meta)\n",
    "\n",
    "rand_for_perf_metrics = pd.DataFrame(rand_for_perf_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_for_perf_metrics = pd.DataFrame(rand_for_perf_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_for_perf_metrics.to_csv('Random_Forest_BERT_experiments2.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron\n",
    "\n",
    "Let's try with a single neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import GlorotUniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_MLP_network(input_dim, layers_dim: tuple, dropout=0.1, lr=1e-4, inner_act_func='relu', out_act_func='sigmoid', seed=10):\n",
    "    \n",
    "    if isinstance(dropout, (float, int)):\n",
    "        dropout = (dropout,) * len(layers_dim)\n",
    "    \n",
    "    # Input\n",
    "    input_layer = Input(shape=(input_dim,), dtype='float32')\n",
    "    prev_lay = input_layer\n",
    "    \n",
    "    # Add intern fully-connected layers\n",
    "    for i in range(len(layers_dim)):\n",
    "        fully_lay = Dense(layers_dim[i], activation='relu',\n",
    "                          kernel_initializer=GlorotUniform(seed=seed))(prev_lay)\n",
    "        drop_layer = Dropout(dropout[i])(fully_lay)\n",
    "        prev_lay = drop_layer\n",
    "    \n",
    "    # Add classification output layer\n",
    "    output_layer = Dense(1, activation='sigmoid',\n",
    "                        kernel_initializer=GlorotUniform(seed=seed))(prev_lay)\n",
    "    \n",
    "    # Builad & compile model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    model.optimizer.lr = lr\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare and grid following configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mulspace_limits(begin, end, factor):\n",
    "    x = begin\n",
    "    \n",
    "    while x <= end:\n",
    "        yield x\n",
    "        x *= factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration with only 1 hidden layer\n",
    "one_hidden_lay_conf = [[x] for x in mulspace_limits(32, X_train_vect.shape[1], 2)]\n",
    "\n",
    "# Configuration with 2 hidden layers\n",
    "two_hidden_lay_conf = [[x[0], y] for x in one_hidden_lay_conf[1:] for y in mulspace_limits(32, x[0], 2)]\n",
    "\n",
    "# Grid parameters\n",
    "params = {\n",
    "        'lay_conf': one_hidden_lay_conf + two_hidden_lay_conf,\n",
    "        'lr': [1e-4],\n",
    "        'dropout': [0.1],\n",
    "        'max_epochs': [300],\n",
    "        'batch_size': [100],\n",
    "        'seed': [123456]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lay_conf': [[32],\n",
       "  [64],\n",
       "  [128],\n",
       "  [256],\n",
       "  [512],\n",
       "  [64, 32],\n",
       "  [64, 64],\n",
       "  [128, 32],\n",
       "  [128, 64],\n",
       "  [128, 128],\n",
       "  [256, 32],\n",
       "  [256, 64],\n",
       "  [256, 128],\n",
       "  [256, 256],\n",
       "  [512, 32],\n",
       "  [512, 64],\n",
       "  [512, 128],\n",
       "  [512, 256],\n",
       "  [512, 512]],\n",
       " 'lr': [0.0001],\n",
       " 'dropout': [0.1],\n",
       " 'max_epochs': [300],\n",
       " 'batch_size': [100],\n",
       " 'seed': [123456]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_MLP_exp(X, y, params, val_split=0.1, test_size=0.2):\n",
    "\n",
    "    mlp_perf_metrics = []\n",
    "\n",
    "    # Split train and test sets in a single one-shot validation\n",
    "    X_train_part, X_test_part, y_train_part, y_test_part = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "    for param in ParameterGrid(params):\n",
    "\n",
    "        meta = {\n",
    "            'n_layers': len(param['lay_conf']), #if isinstance(conf, list) else 1\n",
    "            'layer_conf': param['lay_conf'],\n",
    "            'lr': param['lr'],\n",
    "            'dropout': param['dropout'],\n",
    "            'batch_size': param['batch_size']\n",
    "        }\n",
    "\n",
    "        print('Training with parameters {}'.format(param))\n",
    "\n",
    "        # Fit MLP\n",
    "        mlp_model = build_MLP_network(input_dim=X.shape[1],\n",
    "                                      layers_dim=param['lay_conf'],\n",
    "                                      dropout=param['dropout'], lr=param['lr'],\n",
    "                                      seed=param['seed'],\n",
    "                                      inner_act_func=param['inner_act_func'] if 'inner_act_func' in param else 'relu',\n",
    "                                      out_act_func=param['out_act_func'] if 'out_act_func' in param else 'sigmoid')\n",
    "        mlp_model.summary()\n",
    "        hist = mlp_model.fit(X_train_part, y_train_part,\n",
    "                             batch_size=param['batch_size'],\n",
    "                             epochs=param['max_epochs'],\n",
    "                              validation_split=val_split,\n",
    "                              callbacks=[EarlyStopping(monitor='val_loss', patience=5,\n",
    "                                                       restore_best_weights=True,\n",
    "                                                      min_delta=1e-7)])\n",
    "\n",
    "        # Add train & val metrics\n",
    "        meta['accuracy_train'] = hist.history['acc'][-1]\n",
    "        meta['accuracy_val'] = hist.history['val_acc'][-1]\n",
    "        meta['loss_train'] = hist.history['loss'][-1]\n",
    "        meta['loss_val'] = hist.history['val_loss'][-1]\n",
    "        meta['epochs'] = len(hist.history['val_loss'])\n",
    "\n",
    "        # Predict over test\n",
    "        y_pred = mlp_model.predict(X_test_part)\n",
    "        y_pred[y_pred >= 0.5] = 1\n",
    "        y_pred[y_pred < 0.5] = 0\n",
    "        y_pred = y_pred.squeeze().astype('int8')\n",
    "\n",
    "        # Meassure performance metrics\n",
    "        meta['accuracy'] = metrics.accuracy_score(y_test_part, y_pred)\n",
    "        meta['precision'] = metrics.precision_score(y_test_part, y_pred)\n",
    "        meta['recall'] = metrics.recall_score(y_test_part, y_pred)\n",
    "        #  Exchange class 1 and 0 so measure recall for class 0 (specificity)\n",
    "        meta['specificity'] = metrics.recall_score(np.abs(y_test_part-1), np.abs(y_pred-1))\n",
    "        meta['f1_score'] = metrics.f1_score(y_test_part, y_pred)\n",
    "\n",
    "        mlp_perf_metrics.append(meta)\n",
    "\n",
    "    mlp_perf_metrics = pd.DataFrame(mlp_perf_metrics)\n",
    "    return mlp_perf_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_45 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 32)                24608     \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 24,641\n",
      "Trainable params: 24,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 5ms/step - loss: 0.6453 - acc: 0.6313 - val_loss: 0.5793 - val_acc: 0.7274\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5779 - acc: 0.7227 - val_loss: 0.5384 - val_acc: 0.7537\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.5451 - acc: 0.7444 - val_loss: 0.5111 - val_acc: 0.7898\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.5199 - acc: 0.7677 - val_loss: 0.4918 - val_acc: 0.7882\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.5035 - acc: 0.7721 - val_loss: 0.4771 - val_acc: 0.7915\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4914 - acc: 0.7800 - val_loss: 0.4684 - val_acc: 0.7964\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4803 - acc: 0.7860 - val_loss: 0.4566 - val_acc: 0.7980\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4706 - acc: 0.7893 - val_loss: 0.4503 - val_acc: 0.7980\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4641 - acc: 0.7951 - val_loss: 0.4428 - val_acc: 0.8030\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4598 - acc: 0.8002 - val_loss: 0.4380 - val_acc: 0.8046\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4553 - acc: 0.8013 - val_loss: 0.4345 - val_acc: 0.8144\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4494 - acc: 0.8009 - val_loss: 0.4308 - val_acc: 0.8210\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4448 - acc: 0.8093 - val_loss: 0.4266 - val_acc: 0.8161\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4421 - acc: 0.8079 - val_loss: 0.4247 - val_acc: 0.8227\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4379 - acc: 0.8113 - val_loss: 0.4214 - val_acc: 0.8243\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4383 - acc: 0.8082 - val_loss: 0.4202 - val_acc: 0.8309\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4344 - acc: 0.8135 - val_loss: 0.4177 - val_acc: 0.8259\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4289 - acc: 0.8130 - val_loss: 0.4178 - val_acc: 0.8210\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4271 - acc: 0.8132 - val_loss: 0.4144 - val_acc: 0.8243\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4244 - acc: 0.8174 - val_loss: 0.4136 - val_acc: 0.8309\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4238 - acc: 0.8170 - val_loss: 0.4123 - val_acc: 0.8292\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.4221 - acc: 0.8163 - val_loss: 0.4121 - val_acc: 0.8259\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.4212 - acc: 0.8154 - val_loss: 0.4107 - val_acc: 0.8259\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4195 - acc: 0.8188 - val_loss: 0.4120 - val_acc: 0.8243\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4193 - acc: 0.8190 - val_loss: 0.4090 - val_acc: 0.8309\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4147 - acc: 0.8214 - val_loss: 0.4084 - val_acc: 0.8309\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4157 - acc: 0.8199 - val_loss: 0.4081 - val_acc: 0.8309\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4130 - acc: 0.8217 - val_loss: 0.4069 - val_acc: 0.8292\n",
      "Epoch 29/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4119 - acc: 0.8214 - val_loss: 0.4055 - val_acc: 0.8325\n",
      "Epoch 30/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4106 - acc: 0.8245 - val_loss: 0.4047 - val_acc: 0.8325\n",
      "Epoch 31/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.4099 - acc: 0.8256 - val_loss: 0.4051 - val_acc: 0.8292\n",
      "Epoch 32/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.4069 - acc: 0.8230 - val_loss: 0.4045 - val_acc: 0.8342\n",
      "Epoch 33/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4078 - acc: 0.8258 - val_loss: 0.4054 - val_acc: 0.8292\n",
      "Epoch 34/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4066 - acc: 0.8265 - val_loss: 0.4038 - val_acc: 0.8292\n",
      "Epoch 35/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4045 - acc: 0.8223 - val_loss: 0.4050 - val_acc: 0.8292\n",
      "Epoch 36/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4018 - acc: 0.8256 - val_loss: 0.4041 - val_acc: 0.8309\n",
      "Epoch 37/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4032 - acc: 0.8245 - val_loss: 0.4020 - val_acc: 0.8358\n",
      "Epoch 38/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4009 - acc: 0.8285 - val_loss: 0.4055 - val_acc: 0.8292\n",
      "Epoch 39/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3990 - acc: 0.8298 - val_loss: 0.4061 - val_acc: 0.8276\n",
      "Epoch 40/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3979 - acc: 0.8280 - val_loss: 0.4014 - val_acc: 0.8374\n",
      "Epoch 41/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.3980 - acc: 0.8290 - val_loss: 0.4033 - val_acc: 0.8325\n",
      "Epoch 42/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3946 - acc: 0.8301 - val_loss: 0.4009 - val_acc: 0.8374\n",
      "Epoch 43/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3964 - acc: 0.8289 - val_loss: 0.4016 - val_acc: 0.8391\n",
      "Epoch 44/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3943 - acc: 0.8334 - val_loss: 0.4004 - val_acc: 0.8440\n",
      "Epoch 45/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3941 - acc: 0.8316 - val_loss: 0.4024 - val_acc: 0.8342\n",
      "Epoch 46/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3929 - acc: 0.8316 - val_loss: 0.4001 - val_acc: 0.8440\n",
      "Epoch 47/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3920 - acc: 0.8332 - val_loss: 0.3994 - val_acc: 0.8407\n",
      "Epoch 48/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3913 - acc: 0.8309 - val_loss: 0.3998 - val_acc: 0.8424\n",
      "Epoch 49/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3869 - acc: 0.8301 - val_loss: 0.4004 - val_acc: 0.8374\n",
      "Epoch 50/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3894 - acc: 0.8343 - val_loss: 0.3992 - val_acc: 0.8424\n",
      "Epoch 51/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3888 - acc: 0.8331 - val_loss: 0.3991 - val_acc: 0.8407\n",
      "Epoch 52/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3851 - acc: 0.8329 - val_loss: 0.4010 - val_acc: 0.8374\n",
      "Epoch 53/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3859 - acc: 0.8349 - val_loss: 0.3988 - val_acc: 0.8424\n",
      "Epoch 54/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3838 - acc: 0.8342 - val_loss: 0.4002 - val_acc: 0.8342\n",
      "Epoch 55/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3848 - acc: 0.8365 - val_loss: 0.3986 - val_acc: 0.8391\n",
      "Epoch 56/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3848 - acc: 0.8352 - val_loss: 0.4000 - val_acc: 0.8374\n",
      "Epoch 57/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3835 - acc: 0.8351 - val_loss: 0.4036 - val_acc: 0.8358\n",
      "Epoch 58/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3824 - acc: 0.8340 - val_loss: 0.3985 - val_acc: 0.8309\n",
      "Epoch 59/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3817 - acc: 0.8356 - val_loss: 0.3998 - val_acc: 0.8358\n",
      "Epoch 60/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3796 - acc: 0.8367 - val_loss: 0.3995 - val_acc: 0.8325\n",
      "Epoch 61/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.3807 - acc: 0.8351 - val_loss: 0.3985 - val_acc: 0.8391\n",
      "Epoch 62/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3795 - acc: 0.8385 - val_loss: 0.4012 - val_acc: 0.8309\n",
      "Epoch 63/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3788 - acc: 0.8384 - val_loss: 0.4004 - val_acc: 0.8292\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_46 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 64)                49216     \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 49,281\n",
      "Trainable params: 49,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 5ms/step - loss: 0.6662 - acc: 0.6004 - val_loss: 0.6087 - val_acc: 0.7340\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5859 - acc: 0.7241 - val_loss: 0.5543 - val_acc: 0.7504\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5430 - acc: 0.7495 - val_loss: 0.5223 - val_acc: 0.7734\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5156 - acc: 0.7623 - val_loss: 0.5000 - val_acc: 0.7865\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5000 - acc: 0.7794 - val_loss: 0.4853 - val_acc: 0.7833\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4838 - acc: 0.7873 - val_loss: 0.4706 - val_acc: 0.8013\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4748 - acc: 0.7907 - val_loss: 0.4599 - val_acc: 0.8046\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4662 - acc: 0.7966 - val_loss: 0.4529 - val_acc: 0.8013\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4581 - acc: 0.8009 - val_loss: 0.4455 - val_acc: 0.8062\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4528 - acc: 0.8020 - val_loss: 0.4405 - val_acc: 0.8095\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4454 - acc: 0.8090 - val_loss: 0.4378 - val_acc: 0.8062\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4433 - acc: 0.8095 - val_loss: 0.4330 - val_acc: 0.8210\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4385 - acc: 0.8104 - val_loss: 0.4298 - val_acc: 0.8177\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4322 - acc: 0.8137 - val_loss: 0.4263 - val_acc: 0.8243\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4291 - acc: 0.8157 - val_loss: 0.4233 - val_acc: 0.8259\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4273 - acc: 0.8128 - val_loss: 0.4207 - val_acc: 0.8292\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4241 - acc: 0.8163 - val_loss: 0.4195 - val_acc: 0.8259\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4213 - acc: 0.8163 - val_loss: 0.4246 - val_acc: 0.8112\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4205 - acc: 0.8163 - val_loss: 0.4177 - val_acc: 0.8259\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4170 - acc: 0.8227 - val_loss: 0.4165 - val_acc: 0.8243\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4155 - acc: 0.8186 - val_loss: 0.4152 - val_acc: 0.8243\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4115 - acc: 0.8225 - val_loss: 0.4115 - val_acc: 0.8358\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4100 - acc: 0.8236 - val_loss: 0.4115 - val_acc: 0.8358\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4094 - acc: 0.8234 - val_loss: 0.4132 - val_acc: 0.8227\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4081 - acc: 0.8207 - val_loss: 0.4131 - val_acc: 0.8227\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4041 - acc: 0.8276 - val_loss: 0.4101 - val_acc: 0.8276\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4054 - acc: 0.8247 - val_loss: 0.4085 - val_acc: 0.8358\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4029 - acc: 0.8267 - val_loss: 0.4082 - val_acc: 0.8342\n",
      "Epoch 29/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3993 - acc: 0.8267 - val_loss: 0.4088 - val_acc: 0.8325\n",
      "Epoch 30/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4003 - acc: 0.8292 - val_loss: 0.4097 - val_acc: 0.8391\n",
      "Epoch 31/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.3974 - acc: 0.8267 - val_loss: 0.4071 - val_acc: 0.8342\n",
      "Epoch 32/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3976 - acc: 0.8265 - val_loss: 0.4072 - val_acc: 0.8358\n",
      "Epoch 33/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.3936 - acc: 0.8294 - val_loss: 0.4060 - val_acc: 0.8374\n",
      "Epoch 34/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3930 - acc: 0.8323 - val_loss: 0.4056 - val_acc: 0.8391\n",
      "Epoch 35/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3926 - acc: 0.8301 - val_loss: 0.4073 - val_acc: 0.8440\n",
      "Epoch 36/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3912 - acc: 0.8314 - val_loss: 0.4060 - val_acc: 0.8391\n",
      "Epoch 37/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3898 - acc: 0.8325 - val_loss: 0.4056 - val_acc: 0.8407\n",
      "Epoch 38/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3880 - acc: 0.8321 - val_loss: 0.4040 - val_acc: 0.8391\n",
      "Epoch 39/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3887 - acc: 0.8307 - val_loss: 0.4045 - val_acc: 0.8391\n",
      "Epoch 40/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3856 - acc: 0.8336 - val_loss: 0.4048 - val_acc: 0.8407\n",
      "Epoch 41/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3853 - acc: 0.8345 - val_loss: 0.4056 - val_acc: 0.8342\n",
      "Epoch 42/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3847 - acc: 0.8373 - val_loss: 0.4036 - val_acc: 0.8325\n",
      "Epoch 43/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3840 - acc: 0.8325 - val_loss: 0.4052 - val_acc: 0.8374\n",
      "Epoch 44/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3825 - acc: 0.8327 - val_loss: 0.4032 - val_acc: 0.8259\n",
      "Epoch 45/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3827 - acc: 0.8331 - val_loss: 0.4044 - val_acc: 0.8456\n",
      "Epoch 46/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3814 - acc: 0.8376 - val_loss: 0.4056 - val_acc: 0.8407\n",
      "Epoch 47/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3794 - acc: 0.8384 - val_loss: 0.4029 - val_acc: 0.8342\n",
      "Epoch 48/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3767 - acc: 0.8384 - val_loss: 0.4049 - val_acc: 0.8358\n",
      "Epoch 49/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3771 - acc: 0.8374 - val_loss: 0.4021 - val_acc: 0.8391\n",
      "Epoch 50/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.3758 - acc: 0.8389 - val_loss: 0.4020 - val_acc: 0.8259\n",
      "Epoch 51/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3748 - acc: 0.8382 - val_loss: 0.4052 - val_acc: 0.8210\n",
      "Epoch 52/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3750 - acc: 0.8384 - val_loss: 0.4028 - val_acc: 0.8407\n",
      "Epoch 53/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3728 - acc: 0.8413 - val_loss: 0.4020 - val_acc: 0.8358\n",
      "Epoch 54/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3717 - acc: 0.8384 - val_loss: 0.4037 - val_acc: 0.8309\n",
      "Epoch 55/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3718 - acc: 0.8407 - val_loss: 0.4042 - val_acc: 0.8358\n",
      "Epoch 56/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3700 - acc: 0.8418 - val_loss: 0.4037 - val_acc: 0.8407\n",
      "Epoch 57/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3680 - acc: 0.8398 - val_loss: 0.4040 - val_acc: 0.8374\n",
      "Epoch 58/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3684 - acc: 0.8393 - val_loss: 0.4038 - val_acc: 0.8374\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_42\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_47 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 6ms/step - loss: 0.6063 - acc: 0.6970 - val_loss: 0.5575 - val_acc: 0.7471\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5444 - acc: 0.7480 - val_loss: 0.5121 - val_acc: 0.7816\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5088 - acc: 0.7734 - val_loss: 0.4850 - val_acc: 0.7964\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4874 - acc: 0.7856 - val_loss: 0.4686 - val_acc: 0.7980\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4710 - acc: 0.7915 - val_loss: 0.4570 - val_acc: 0.8046\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4614 - acc: 0.7978 - val_loss: 0.4493 - val_acc: 0.8013\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4507 - acc: 0.7997 - val_loss: 0.4385 - val_acc: 0.8161\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4439 - acc: 0.8061 - val_loss: 0.4325 - val_acc: 0.8177\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4409 - acc: 0.8055 - val_loss: 0.4303 - val_acc: 0.8259\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4329 - acc: 0.8144 - val_loss: 0.4257 - val_acc: 0.8227\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4297 - acc: 0.8134 - val_loss: 0.4214 - val_acc: 0.8210\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4268 - acc: 0.8137 - val_loss: 0.4194 - val_acc: 0.8276\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4227 - acc: 0.8185 - val_loss: 0.4166 - val_acc: 0.8309\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4163 - acc: 0.8210 - val_loss: 0.4163 - val_acc: 0.8309\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4142 - acc: 0.8210 - val_loss: 0.4194 - val_acc: 0.8210\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4135 - acc: 0.8190 - val_loss: 0.4117 - val_acc: 0.8358\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4107 - acc: 0.8210 - val_loss: 0.4099 - val_acc: 0.8358\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4071 - acc: 0.8254 - val_loss: 0.4123 - val_acc: 0.8243\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4031 - acc: 0.8261 - val_loss: 0.4078 - val_acc: 0.8374\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3993 - acc: 0.8285 - val_loss: 0.4080 - val_acc: 0.8259\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3990 - acc: 0.8269 - val_loss: 0.4068 - val_acc: 0.8374\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3982 - acc: 0.8274 - val_loss: 0.4071 - val_acc: 0.8276\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3986 - acc: 0.8250 - val_loss: 0.4055 - val_acc: 0.8342\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3932 - acc: 0.8323 - val_loss: 0.4062 - val_acc: 0.8292\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3912 - acc: 0.8301 - val_loss: 0.4053 - val_acc: 0.8292\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3902 - acc: 0.8327 - val_loss: 0.4074 - val_acc: 0.8227\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3859 - acc: 0.8312 - val_loss: 0.4037 - val_acc: 0.8292\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3878 - acc: 0.8318 - val_loss: 0.4056 - val_acc: 0.8292\n",
      "Epoch 29/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3846 - acc: 0.8367 - val_loss: 0.4039 - val_acc: 0.8292\n",
      "Epoch 30/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3821 - acc: 0.8321 - val_loss: 0.4021 - val_acc: 0.8292\n",
      "Epoch 31/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3790 - acc: 0.8356 - val_loss: 0.4019 - val_acc: 0.8309\n",
      "Epoch 32/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3796 - acc: 0.8369 - val_loss: 0.4036 - val_acc: 0.8342\n",
      "Epoch 33/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3780 - acc: 0.8362 - val_loss: 0.4037 - val_acc: 0.8342\n",
      "Epoch 34/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3752 - acc: 0.8373 - val_loss: 0.4025 - val_acc: 0.8194\n",
      "Epoch 35/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.3729 - acc: 0.8378 - val_loss: 0.4027 - val_acc: 0.8309\n",
      "Epoch 36/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.3712 - acc: 0.8398 - val_loss: 0.4022 - val_acc: 0.8276\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_48 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 197,121\n",
      "Trainable params: 197,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 6ms/step - loss: 0.6413 - acc: 0.6243 - val_loss: 0.5729 - val_acc: 0.7488\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5472 - acc: 0.7504 - val_loss: 0.5177 - val_acc: 0.7718\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5033 - acc: 0.7763 - val_loss: 0.4894 - val_acc: 0.7816\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4790 - acc: 0.7889 - val_loss: 0.4628 - val_acc: 0.7898\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4626 - acc: 0.7982 - val_loss: 0.4493 - val_acc: 0.7997\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4504 - acc: 0.8022 - val_loss: 0.4411 - val_acc: 0.8128\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4429 - acc: 0.8093 - val_loss: 0.4334 - val_acc: 0.8194\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4361 - acc: 0.8135 - val_loss: 0.4272 - val_acc: 0.8243\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4297 - acc: 0.8155 - val_loss: 0.4221 - val_acc: 0.8259\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4258 - acc: 0.8143 - val_loss: 0.4234 - val_acc: 0.8177\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4190 - acc: 0.8176 - val_loss: 0.4154 - val_acc: 0.8325\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4163 - acc: 0.8223 - val_loss: 0.4127 - val_acc: 0.8309\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4128 - acc: 0.8227 - val_loss: 0.4195 - val_acc: 0.8210\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4075 - acc: 0.8227 - val_loss: 0.4088 - val_acc: 0.8358\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4060 - acc: 0.8248 - val_loss: 0.4117 - val_acc: 0.8292\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4030 - acc: 0.8281 - val_loss: 0.4103 - val_acc: 0.8276\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4001 - acc: 0.8234 - val_loss: 0.4080 - val_acc: 0.8292\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3960 - acc: 0.8296 - val_loss: 0.4056 - val_acc: 0.8342\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3933 - acc: 0.8290 - val_loss: 0.4099 - val_acc: 0.8358\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3905 - acc: 0.8329 - val_loss: 0.4077 - val_acc: 0.8309\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3887 - acc: 0.8298 - val_loss: 0.4046 - val_acc: 0.8276\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3864 - acc: 0.8331 - val_loss: 0.4061 - val_acc: 0.8391\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3842 - acc: 0.8351 - val_loss: 0.4053 - val_acc: 0.8391\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3811 - acc: 0.8352 - val_loss: 0.4040 - val_acc: 0.8342\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3789 - acc: 0.8352 - val_loss: 0.4043 - val_acc: 0.8374\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3777 - acc: 0.8371 - val_loss: 0.4133 - val_acc: 0.8210\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3743 - acc: 0.8398 - val_loss: 0.4030 - val_acc: 0.8325\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3732 - acc: 0.8373 - val_loss: 0.4022 - val_acc: 0.8292\n",
      "Epoch 29/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3706 - acc: 0.8391 - val_loss: 0.4037 - val_acc: 0.8358\n",
      "Epoch 30/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3676 - acc: 0.8402 - val_loss: 0.4029 - val_acc: 0.8309\n",
      "Epoch 31/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3668 - acc: 0.8394 - val_loss: 0.4035 - val_acc: 0.8292\n",
      "Epoch 32/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3662 - acc: 0.8442 - val_loss: 0.4043 - val_acc: 0.8276\n",
      "Epoch 33/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3631 - acc: 0.8431 - val_loss: 0.4025 - val_acc: 0.8325\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_49 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 394,241\n",
      "Trainable params: 394,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 6ms/step - loss: 0.6075 - acc: 0.6663 - val_loss: 0.5335 - val_acc: 0.7734\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5107 - acc: 0.7785 - val_loss: 0.4791 - val_acc: 0.7947\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4732 - acc: 0.7909 - val_loss: 0.4538 - val_acc: 0.8079\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4575 - acc: 0.8020 - val_loss: 0.4376 - val_acc: 0.8095\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4438 - acc: 0.8110 - val_loss: 0.4269 - val_acc: 0.8210\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4336 - acc: 0.8128 - val_loss: 0.4238 - val_acc: 0.8243\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4266 - acc: 0.8174 - val_loss: 0.4210 - val_acc: 0.8128\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4216 - acc: 0.8197 - val_loss: 0.4111 - val_acc: 0.8325\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4172 - acc: 0.8203 - val_loss: 0.4095 - val_acc: 0.8309\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4118 - acc: 0.8280 - val_loss: 0.4073 - val_acc: 0.8325\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4080 - acc: 0.8280 - val_loss: 0.4061 - val_acc: 0.8358\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4030 - acc: 0.8280 - val_loss: 0.4044 - val_acc: 0.8292\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3994 - acc: 0.8263 - val_loss: 0.4038 - val_acc: 0.8309\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3973 - acc: 0.8296 - val_loss: 0.4018 - val_acc: 0.8325\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3935 - acc: 0.8349 - val_loss: 0.4014 - val_acc: 0.8325\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3897 - acc: 0.8336 - val_loss: 0.4009 - val_acc: 0.8325\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3868 - acc: 0.8323 - val_loss: 0.4057 - val_acc: 0.8243\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3841 - acc: 0.8332 - val_loss: 0.3991 - val_acc: 0.8309\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3816 - acc: 0.8362 - val_loss: 0.4178 - val_acc: 0.8062\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3782 - acc: 0.8365 - val_loss: 0.3990 - val_acc: 0.8374\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3738 - acc: 0.8416 - val_loss: 0.4005 - val_acc: 0.8374\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3723 - acc: 0.8378 - val_loss: 0.3998 - val_acc: 0.8374\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3706 - acc: 0.8405 - val_loss: 0.3975 - val_acc: 0.8325\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3668 - acc: 0.8422 - val_loss: 0.3972 - val_acc: 0.8391\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3651 - acc: 0.8436 - val_loss: 0.3999 - val_acc: 0.8342\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3623 - acc: 0.8438 - val_loss: 0.3994 - val_acc: 0.8309\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3585 - acc: 0.8433 - val_loss: 0.3979 - val_acc: 0.8292\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3564 - acc: 0.8486 - val_loss: 0.4014 - val_acc: 0.8292\n",
      "Epoch 29/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3553 - acc: 0.8440 - val_loss: 0.4021 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [64, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_50 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 64)                49216     \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 51,329\n",
      "Trainable params: 51,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 7ms/step - loss: 0.7006 - acc: 0.5359 - val_loss: 0.6453 - val_acc: 0.7011\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.6181 - acc: 0.6993 - val_loss: 0.5761 - val_acc: 0.7504\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5704 - acc: 0.7311 - val_loss: 0.5353 - val_acc: 0.7619\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5353 - acc: 0.7502 - val_loss: 0.5045 - val_acc: 0.7701\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.5123 - acc: 0.7659 - val_loss: 0.4836 - val_acc: 0.7750\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4937 - acc: 0.7814 - val_loss: 0.4659 - val_acc: 0.7931\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.4819 - acc: 0.7760 - val_loss: 0.4541 - val_acc: 0.7980\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4676 - acc: 0.7938 - val_loss: 0.4459 - val_acc: 0.8144\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4605 - acc: 0.7918 - val_loss: 0.4392 - val_acc: 0.8128\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4526 - acc: 0.8030 - val_loss: 0.4334 - val_acc: 0.8161\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4460 - acc: 0.8041 - val_loss: 0.4300 - val_acc: 0.8128\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4405 - acc: 0.8097 - val_loss: 0.4244 - val_acc: 0.8079\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4329 - acc: 0.8168 - val_loss: 0.4214 - val_acc: 0.8128\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4296 - acc: 0.8128 - val_loss: 0.4194 - val_acc: 0.8128\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4281 - acc: 0.8152 - val_loss: 0.4135 - val_acc: 0.8210\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4239 - acc: 0.8126 - val_loss: 0.4125 - val_acc: 0.8243\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4213 - acc: 0.8163 - val_loss: 0.4091 - val_acc: 0.8259\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4208 - acc: 0.8155 - val_loss: 0.4082 - val_acc: 0.8276\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4149 - acc: 0.8212 - val_loss: 0.4065 - val_acc: 0.8292\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4122 - acc: 0.8230 - val_loss: 0.4050 - val_acc: 0.8292\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4120 - acc: 0.8214 - val_loss: 0.4040 - val_acc: 0.8292\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4051 - acc: 0.8241 - val_loss: 0.4041 - val_acc: 0.8325\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4056 - acc: 0.8259 - val_loss: 0.4017 - val_acc: 0.8309\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4013 - acc: 0.8309 - val_loss: 0.4045 - val_acc: 0.8243\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3986 - acc: 0.8274 - val_loss: 0.4002 - val_acc: 0.8325\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3982 - acc: 0.8270 - val_loss: 0.4000 - val_acc: 0.8342\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3972 - acc: 0.8281 - val_loss: 0.4001 - val_acc: 0.8325\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3968 - acc: 0.8318 - val_loss: 0.4079 - val_acc: 0.8243\n",
      "Epoch 29/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3926 - acc: 0.8307 - val_loss: 0.3985 - val_acc: 0.8342\n",
      "Epoch 30/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3935 - acc: 0.8307 - val_loss: 0.4011 - val_acc: 0.8259\n",
      "Epoch 31/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3877 - acc: 0.8360 - val_loss: 0.3996 - val_acc: 0.8374\n",
      "Epoch 32/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3884 - acc: 0.8334 - val_loss: 0.3980 - val_acc: 0.8391\n",
      "Epoch 33/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3884 - acc: 0.8358 - val_loss: 0.3982 - val_acc: 0.8374\n",
      "Epoch 34/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3828 - acc: 0.8371 - val_loss: 0.3972 - val_acc: 0.8374\n",
      "Epoch 35/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3826 - acc: 0.8382 - val_loss: 0.3975 - val_acc: 0.8424\n",
      "Epoch 36/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3787 - acc: 0.8394 - val_loss: 0.3986 - val_acc: 0.8374\n",
      "Epoch 37/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3809 - acc: 0.8394 - val_loss: 0.3978 - val_acc: 0.8342\n",
      "Epoch 38/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3760 - acc: 0.8396 - val_loss: 0.3971 - val_acc: 0.8309\n",
      "Epoch 39/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3731 - acc: 0.8438 - val_loss: 0.3986 - val_acc: 0.8259\n",
      "Epoch 40/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3727 - acc: 0.8394 - val_loss: 0.3976 - val_acc: 0.8342\n",
      "Epoch 41/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3711 - acc: 0.8404 - val_loss: 0.3990 - val_acc: 0.8243\n",
      "Epoch 42/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3660 - acc: 0.8467 - val_loss: 0.4026 - val_acc: 0.8259\n",
      "Epoch 43/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3676 - acc: 0.8431 - val_loss: 0.3980 - val_acc: 0.8243\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [64, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_51 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 64)                49216     \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_123 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 53,441\n",
      "Trainable params: 53,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 6ms/step - loss: 0.6715 - acc: 0.5975 - val_loss: 0.6253 - val_acc: 0.7143\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5998 - acc: 0.7084 - val_loss: 0.5569 - val_acc: 0.7406\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.5508 - acc: 0.7426 - val_loss: 0.5131 - val_acc: 0.7833\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5205 - acc: 0.7612 - val_loss: 0.4823 - val_acc: 0.7882\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4949 - acc: 0.7741 - val_loss: 0.4630 - val_acc: 0.7898\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4758 - acc: 0.7838 - val_loss: 0.4464 - val_acc: 0.7964\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4619 - acc: 0.7927 - val_loss: 0.4376 - val_acc: 0.8079\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4538 - acc: 0.8008 - val_loss: 0.4283 - val_acc: 0.8144\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4478 - acc: 0.7993 - val_loss: 0.4222 - val_acc: 0.8177\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4422 - acc: 0.8086 - val_loss: 0.4169 - val_acc: 0.8161\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4344 - acc: 0.8126 - val_loss: 0.4206 - val_acc: 0.8210\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4273 - acc: 0.8163 - val_loss: 0.4114 - val_acc: 0.8276\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4242 - acc: 0.8165 - val_loss: 0.4071 - val_acc: 0.8276\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4197 - acc: 0.8174 - val_loss: 0.4101 - val_acc: 0.8259\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4139 - acc: 0.8236 - val_loss: 0.4021 - val_acc: 0.8325\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4146 - acc: 0.8212 - val_loss: 0.4011 - val_acc: 0.8358\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4096 - acc: 0.8248 - val_loss: 0.4028 - val_acc: 0.8259\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4074 - acc: 0.8223 - val_loss: 0.3995 - val_acc: 0.8309\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4010 - acc: 0.8278 - val_loss: 0.3969 - val_acc: 0.8374\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4011 - acc: 0.8298 - val_loss: 0.3955 - val_acc: 0.8358\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3991 - acc: 0.8305 - val_loss: 0.3943 - val_acc: 0.8358\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3968 - acc: 0.8307 - val_loss: 0.3935 - val_acc: 0.8374\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3918 - acc: 0.8290 - val_loss: 0.3950 - val_acc: 0.8440\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3935 - acc: 0.8294 - val_loss: 0.3946 - val_acc: 0.8374\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3889 - acc: 0.8356 - val_loss: 0.3964 - val_acc: 0.8292\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3853 - acc: 0.8329 - val_loss: 0.3914 - val_acc: 0.8342\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3810 - acc: 0.8391 - val_loss: 0.3923 - val_acc: 0.8325\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3845 - acc: 0.8311 - val_loss: 0.3912 - val_acc: 0.8292\n",
      "Epoch 29/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3793 - acc: 0.8374 - val_loss: 0.3912 - val_acc: 0.8342\n",
      "Epoch 30/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3760 - acc: 0.8409 - val_loss: 0.3922 - val_acc: 0.8309\n",
      "Epoch 31/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3751 - acc: 0.8394 - val_loss: 0.3941 - val_acc: 0.8358\n",
      "Epoch 32/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3746 - acc: 0.8413 - val_loss: 0.3917 - val_acc: 0.8342\n",
      "Epoch 33/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3706 - acc: 0.8431 - val_loss: 0.3940 - val_acc: 0.8292\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [128, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_52 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_126 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 102,593\n",
      "Trainable params: 102,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 7ms/step - loss: 0.6691 - acc: 0.5999 - val_loss: 0.6018 - val_acc: 0.7323\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5767 - acc: 0.7263 - val_loss: 0.5362 - val_acc: 0.7635\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.5260 - acc: 0.7643 - val_loss: 0.4956 - val_acc: 0.7783\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4992 - acc: 0.7719 - val_loss: 0.4738 - val_acc: 0.7931\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4783 - acc: 0.7843 - val_loss: 0.4645 - val_acc: 0.7833\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4676 - acc: 0.7893 - val_loss: 0.4435 - val_acc: 0.8013\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4550 - acc: 0.7968 - val_loss: 0.4353 - val_acc: 0.8095\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4491 - acc: 0.8024 - val_loss: 0.4289 - val_acc: 0.8161\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4425 - acc: 0.8081 - val_loss: 0.4235 - val_acc: 0.8144\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4384 - acc: 0.8088 - val_loss: 0.4196 - val_acc: 0.8144\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4324 - acc: 0.8108 - val_loss: 0.4195 - val_acc: 0.8079\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4226 - acc: 0.8176 - val_loss: 0.4148 - val_acc: 0.8161\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.4207 - acc: 0.8186 - val_loss: 0.4112 - val_acc: 0.8210\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4138 - acc: 0.8214 - val_loss: 0.4084 - val_acc: 0.8259\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4147 - acc: 0.8212 - val_loss: 0.4073 - val_acc: 0.8210\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4085 - acc: 0.8221 - val_loss: 0.4056 - val_acc: 0.8227\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4061 - acc: 0.8238 - val_loss: 0.4031 - val_acc: 0.8210\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4039 - acc: 0.8252 - val_loss: 0.4030 - val_acc: 0.8276\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3990 - acc: 0.8278 - val_loss: 0.4037 - val_acc: 0.8194\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3983 - acc: 0.8296 - val_loss: 0.4011 - val_acc: 0.8243\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3940 - acc: 0.8265 - val_loss: 0.4010 - val_acc: 0.8259\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3921 - acc: 0.8316 - val_loss: 0.4022 - val_acc: 0.8227\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3889 - acc: 0.8340 - val_loss: 0.3987 - val_acc: 0.8227\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3893 - acc: 0.8345 - val_loss: 0.4000 - val_acc: 0.8259\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3802 - acc: 0.8374 - val_loss: 0.3998 - val_acc: 0.8259\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3785 - acc: 0.8374 - val_loss: 0.4003 - val_acc: 0.8292\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3759 - acc: 0.8358 - val_loss: 0.3995 - val_acc: 0.8259\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3742 - acc: 0.8385 - val_loss: 0.3975 - val_acc: 0.8292\n",
      "Epoch 29/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3717 - acc: 0.8400 - val_loss: 0.3970 - val_acc: 0.8243\n",
      "Epoch 30/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3660 - acc: 0.8420 - val_loss: 0.3984 - val_acc: 0.8292\n",
      "Epoch 31/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3620 - acc: 0.8444 - val_loss: 0.4010 - val_acc: 0.8227\n",
      "Epoch 32/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3630 - acc: 0.8429 - val_loss: 0.3981 - val_acc: 0.8292\n",
      "Epoch 33/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3607 - acc: 0.8433 - val_loss: 0.3993 - val_acc: 0.8243\n",
      "Epoch 34/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3585 - acc: 0.8491 - val_loss: 0.3991 - val_acc: 0.8292\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [128, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_53 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_127 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_128 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_129 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 7ms/step - loss: 0.6544 - acc: 0.6138 - val_loss: 0.5971 - val_acc: 0.7438\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5702 - acc: 0.7340 - val_loss: 0.5358 - val_acc: 0.7488\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5251 - acc: 0.7592 - val_loss: 0.4889 - val_acc: 0.7849\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4931 - acc: 0.7794 - val_loss: 0.4642 - val_acc: 0.7915\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4722 - acc: 0.7882 - val_loss: 0.4512 - val_acc: 0.7833\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4565 - acc: 0.7955 - val_loss: 0.4408 - val_acc: 0.8046\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4452 - acc: 0.8051 - val_loss: 0.4282 - val_acc: 0.8112\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4387 - acc: 0.8075 - val_loss: 0.4263 - val_acc: 0.8161\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4337 - acc: 0.8113 - val_loss: 0.4183 - val_acc: 0.8177\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4278 - acc: 0.8139 - val_loss: 0.4151 - val_acc: 0.8227\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4201 - acc: 0.8188 - val_loss: 0.4115 - val_acc: 0.8227\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4151 - acc: 0.8185 - val_loss: 0.4092 - val_acc: 0.8227\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4103 - acc: 0.8250 - val_loss: 0.4088 - val_acc: 0.8177\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4063 - acc: 0.8221 - val_loss: 0.4047 - val_acc: 0.8309\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4015 - acc: 0.8225 - val_loss: 0.4053 - val_acc: 0.8194\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4033 - acc: 0.8250 - val_loss: 0.4031 - val_acc: 0.8292\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3977 - acc: 0.8267 - val_loss: 0.4038 - val_acc: 0.8276\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3917 - acc: 0.8287 - val_loss: 0.4027 - val_acc: 0.8227\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3874 - acc: 0.8309 - val_loss: 0.4040 - val_acc: 0.8227\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3808 - acc: 0.8411 - val_loss: 0.4027 - val_acc: 0.8243\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3816 - acc: 0.8349 - val_loss: 0.4039 - val_acc: 0.8259\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3845 - acc: 0.8362 - val_loss: 0.4014 - val_acc: 0.8292\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3762 - acc: 0.8363 - val_loss: 0.4040 - val_acc: 0.8243\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3756 - acc: 0.8351 - val_loss: 0.4011 - val_acc: 0.8276\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3689 - acc: 0.8413 - val_loss: 0.4022 - val_acc: 0.8243\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3702 - acc: 0.8393 - val_loss: 0.4086 - val_acc: 0.8177\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3674 - acc: 0.8415 - val_loss: 0.4029 - val_acc: 0.8259\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3629 - acc: 0.8424 - val_loss: 0.4021 - val_acc: 0.8194\n",
      "Epoch 29/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3594 - acc: 0.8436 - val_loss: 0.4064 - val_acc: 0.8227\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_54 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_130 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_131 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_132 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 1s 6ms/step - loss: 0.6545 - acc: 0.6214 - val_loss: 0.5911 - val_acc: 0.7389\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5671 - acc: 0.7289 - val_loss: 0.5221 - val_acc: 0.7619\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5098 - acc: 0.7666 - val_loss: 0.4761 - val_acc: 0.7997\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4792 - acc: 0.7833 - val_loss: 0.4519 - val_acc: 0.7964\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4605 - acc: 0.7949 - val_loss: 0.4355 - val_acc: 0.8128\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4467 - acc: 0.8042 - val_loss: 0.4277 - val_acc: 0.8079\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4376 - acc: 0.8046 - val_loss: 0.4194 - val_acc: 0.8144\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4300 - acc: 0.8135 - val_loss: 0.4165 - val_acc: 0.8161\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4249 - acc: 0.8166 - val_loss: 0.4107 - val_acc: 0.8243\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4163 - acc: 0.8207 - val_loss: 0.4100 - val_acc: 0.8210\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4127 - acc: 0.8207 - val_loss: 0.4062 - val_acc: 0.8276\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4106 - acc: 0.8241 - val_loss: 0.4053 - val_acc: 0.8292\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4059 - acc: 0.8269 - val_loss: 0.4039 - val_acc: 0.8276\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3975 - acc: 0.8269 - val_loss: 0.4045 - val_acc: 0.8309\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3974 - acc: 0.8300 - val_loss: 0.3996 - val_acc: 0.8325\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3940 - acc: 0.8320 - val_loss: 0.3987 - val_acc: 0.8309\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3894 - acc: 0.8309 - val_loss: 0.4001 - val_acc: 0.8243\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3837 - acc: 0.8362 - val_loss: 0.4006 - val_acc: 0.8342\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3797 - acc: 0.8351 - val_loss: 0.4052 - val_acc: 0.8243\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3784 - acc: 0.8382 - val_loss: 0.3985 - val_acc: 0.8276\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3736 - acc: 0.8433 - val_loss: 0.4007 - val_acc: 0.8259\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3715 - acc: 0.8418 - val_loss: 0.3982 - val_acc: 0.8259\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3669 - acc: 0.8416 - val_loss: 0.3981 - val_acc: 0.8292\n",
      "Epoch 24/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3633 - acc: 0.8416 - val_loss: 0.4035 - val_acc: 0.8243\n",
      "Epoch 25/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3599 - acc: 0.8435 - val_loss: 0.4002 - val_acc: 0.8177\n",
      "Epoch 26/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3610 - acc: 0.8477 - val_loss: 0.3993 - val_acc: 0.8194\n",
      "Epoch 27/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3529 - acc: 0.8502 - val_loss: 0.4027 - val_acc: 0.8243\n",
      "Epoch 28/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3439 - acc: 0.8539 - val_loss: 0.4005 - val_acc: 0.8227\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_55 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 8ms/step - loss: 0.6696 - acc: 0.6105 - val_loss: 0.5829 - val_acc: 0.7422\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5635 - acc: 0.7340 - val_loss: 0.5128 - val_acc: 0.7701\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5144 - acc: 0.7630 - val_loss: 0.4713 - val_acc: 0.7980\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4792 - acc: 0.7893 - val_loss: 0.4526 - val_acc: 0.8013\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4609 - acc: 0.7900 - val_loss: 0.4302 - val_acc: 0.8177\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4463 - acc: 0.8039 - val_loss: 0.4200 - val_acc: 0.8161\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4365 - acc: 0.8082 - val_loss: 0.4133 - val_acc: 0.8276\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4279 - acc: 0.8139 - val_loss: 0.4091 - val_acc: 0.8342\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4222 - acc: 0.8146 - val_loss: 0.4048 - val_acc: 0.8342\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4150 - acc: 0.8185 - val_loss: 0.4047 - val_acc: 0.8243\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4096 - acc: 0.8210 - val_loss: 0.4015 - val_acc: 0.8325\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4042 - acc: 0.8259 - val_loss: 0.3980 - val_acc: 0.8342\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4006 - acc: 0.8252 - val_loss: 0.3969 - val_acc: 0.8374\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3992 - acc: 0.8300 - val_loss: 0.3971 - val_acc: 0.8374\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3976 - acc: 0.8290 - val_loss: 0.3967 - val_acc: 0.8259\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3924 - acc: 0.8305 - val_loss: 0.4003 - val_acc: 0.8259\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3887 - acc: 0.8329 - val_loss: 0.3945 - val_acc: 0.8259\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3793 - acc: 0.8405 - val_loss: 0.3955 - val_acc: 0.8325\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3809 - acc: 0.8349 - val_loss: 0.3946 - val_acc: 0.8259\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3767 - acc: 0.8393 - val_loss: 0.4006 - val_acc: 0.8309\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3729 - acc: 0.8435 - val_loss: 0.3970 - val_acc: 0.8325\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3728 - acc: 0.8407 - val_loss: 0.3952 - val_acc: 0.8292\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [256, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_56 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_85 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_86 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_138 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 1s 6ms/step - loss: 0.6711 - acc: 0.5950 - val_loss: 0.5742 - val_acc: 0.7455\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.5577 - acc: 0.7356 - val_loss: 0.5046 - val_acc: 0.7816\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5005 - acc: 0.7756 - val_loss: 0.4635 - val_acc: 0.7882\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4773 - acc: 0.7869 - val_loss: 0.4410 - val_acc: 0.8030\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4568 - acc: 0.7973 - val_loss: 0.4261 - val_acc: 0.8161\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4422 - acc: 0.8051 - val_loss: 0.4189 - val_acc: 0.8144\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4350 - acc: 0.8084 - val_loss: 0.4157 - val_acc: 0.8194\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4290 - acc: 0.8155 - val_loss: 0.4090 - val_acc: 0.8227\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4188 - acc: 0.8154 - val_loss: 0.4065 - val_acc: 0.8276\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4125 - acc: 0.8223 - val_loss: 0.4017 - val_acc: 0.8325\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4073 - acc: 0.8239 - val_loss: 0.4015 - val_acc: 0.8391\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4028 - acc: 0.8245 - val_loss: 0.3981 - val_acc: 0.8325\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3998 - acc: 0.8250 - val_loss: 0.4010 - val_acc: 0.8309\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3917 - acc: 0.8311 - val_loss: 0.3984 - val_acc: 0.8309\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3893 - acc: 0.8352 - val_loss: 0.3958 - val_acc: 0.8292\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3847 - acc: 0.8334 - val_loss: 0.3951 - val_acc: 0.8342\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3835 - acc: 0.8332 - val_loss: 0.3957 - val_acc: 0.8358\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3807 - acc: 0.8351 - val_loss: 0.3975 - val_acc: 0.8358\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3721 - acc: 0.8405 - val_loss: 0.4070 - val_acc: 0.8210\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3686 - acc: 0.8407 - val_loss: 0.3987 - val_acc: 0.8342\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3665 - acc: 0.8438 - val_loss: 0.4033 - val_acc: 0.8259\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [256, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_57 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_140 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_141 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 229,889\n",
      "Trainable params: 229,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 6ms/step - loss: 0.6394 - acc: 0.6417 - val_loss: 0.5472 - val_acc: 0.7635\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.5326 - acc: 0.7526 - val_loss: 0.4792 - val_acc: 0.7865\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4851 - acc: 0.7789 - val_loss: 0.4441 - val_acc: 0.7997\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4633 - acc: 0.7942 - val_loss: 0.4248 - val_acc: 0.8144\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4446 - acc: 0.8064 - val_loss: 0.4131 - val_acc: 0.8177\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4344 - acc: 0.8097 - val_loss: 0.4062 - val_acc: 0.8276\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4224 - acc: 0.8196 - val_loss: 0.4076 - val_acc: 0.8177\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4168 - acc: 0.8201 - val_loss: 0.4006 - val_acc: 0.8210\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4110 - acc: 0.8163 - val_loss: 0.3992 - val_acc: 0.8276\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4054 - acc: 0.8228 - val_loss: 0.3971 - val_acc: 0.8292\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4008 - acc: 0.8236 - val_loss: 0.3933 - val_acc: 0.8342\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3914 - acc: 0.8289 - val_loss: 0.3926 - val_acc: 0.8325\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3913 - acc: 0.8307 - val_loss: 0.4082 - val_acc: 0.8259\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3852 - acc: 0.8340 - val_loss: 0.3917 - val_acc: 0.8391\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3796 - acc: 0.8349 - val_loss: 0.3937 - val_acc: 0.8309\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3759 - acc: 0.8354 - val_loss: 0.3946 - val_acc: 0.8276\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3712 - acc: 0.8411 - val_loss: 0.3955 - val_acc: 0.8325\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3685 - acc: 0.8440 - val_loss: 0.3915 - val_acc: 0.8292\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3627 - acc: 0.8446 - val_loss: 0.4023 - val_acc: 0.8210\n",
      "Epoch 20/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3578 - acc: 0.8473 - val_loss: 0.3940 - val_acc: 0.8292\n",
      "Epoch 21/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3514 - acc: 0.8462 - val_loss: 0.3953 - val_acc: 0.8259\n",
      "Epoch 22/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3491 - acc: 0.8504 - val_loss: 0.4023 - val_acc: 0.8194\n",
      "Epoch 23/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3423 - acc: 0.8515 - val_loss: 0.4032 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_58 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_89 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_90 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_144 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 6ms/step - loss: 0.6505 - acc: 0.6110 - val_loss: 0.5621 - val_acc: 0.7668\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5390 - acc: 0.7468 - val_loss: 0.4862 - val_acc: 0.7767\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4872 - acc: 0.7756 - val_loss: 0.4452 - val_acc: 0.7947\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4590 - acc: 0.7946 - val_loss: 0.4279 - val_acc: 0.8062\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4395 - acc: 0.8053 - val_loss: 0.4152 - val_acc: 0.8259\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4264 - acc: 0.8090 - val_loss: 0.4113 - val_acc: 0.8227\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4203 - acc: 0.8155 - val_loss: 0.4060 - val_acc: 0.8210\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4139 - acc: 0.8185 - val_loss: 0.4025 - val_acc: 0.8407\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4047 - acc: 0.8261 - val_loss: 0.4021 - val_acc: 0.8309\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4012 - acc: 0.8248 - val_loss: 0.4132 - val_acc: 0.8210\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3937 - acc: 0.8307 - val_loss: 0.4071 - val_acc: 0.8243\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3878 - acc: 0.8331 - val_loss: 0.3991 - val_acc: 0.8440\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3832 - acc: 0.8298 - val_loss: 0.4010 - val_acc: 0.8424\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3783 - acc: 0.8334 - val_loss: 0.4039 - val_acc: 0.8325\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3691 - acc: 0.8376 - val_loss: 0.4002 - val_acc: 0.8276\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3704 - acc: 0.8405 - val_loss: 0.4003 - val_acc: 0.8358\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3604 - acc: 0.8469 - val_loss: 0.4014 - val_acc: 0.8342\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [512, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_59 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_145 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_91 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_146 (Dense)            (None, 32)                16416     \n",
      "_________________________________________________________________\n",
      "dropout_92 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_147 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 410,177\n",
      "Trainable params: 410,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 8ms/step - loss: 0.6237 - acc: 0.6468 - val_loss: 0.5367 - val_acc: 0.7521\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.5184 - acc: 0.7659 - val_loss: 0.4723 - val_acc: 0.7865\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4763 - acc: 0.7885 - val_loss: 0.4483 - val_acc: 0.7816\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4544 - acc: 0.8002 - val_loss: 0.4222 - val_acc: 0.8177\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4421 - acc: 0.8075 - val_loss: 0.4251 - val_acc: 0.7931\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4284 - acc: 0.8146 - val_loss: 0.4115 - val_acc: 0.8243\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4243 - acc: 0.8157 - val_loss: 0.4115 - val_acc: 0.8259\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4114 - acc: 0.8256 - val_loss: 0.4003 - val_acc: 0.8325\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4075 - acc: 0.8301 - val_loss: 0.4033 - val_acc: 0.8309\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4011 - acc: 0.8250 - val_loss: 0.4151 - val_acc: 0.8177\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3963 - acc: 0.8296 - val_loss: 0.3964 - val_acc: 0.8325\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3919 - acc: 0.8360 - val_loss: 0.3983 - val_acc: 0.8259\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3862 - acc: 0.8338 - val_loss: 0.3989 - val_acc: 0.8243\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3801 - acc: 0.8338 - val_loss: 0.3986 - val_acc: 0.8259\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3794 - acc: 0.8384 - val_loss: 0.4014 - val_acc: 0.8243\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3731 - acc: 0.8402 - val_loss: 0.4044 - val_acc: 0.8259\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [512, 64], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_60 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_148 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_93 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dropout_94 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_150 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 426,625\n",
      "Trainable params: 426,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 7ms/step - loss: 0.6338 - acc: 0.6442 - val_loss: 0.5567 - val_acc: 0.7389\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5302 - acc: 0.7577 - val_loss: 0.4827 - val_acc: 0.7865\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4801 - acc: 0.7889 - val_loss: 0.4446 - val_acc: 0.7997\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4530 - acc: 0.7978 - val_loss: 0.4301 - val_acc: 0.7997\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4396 - acc: 0.8073 - val_loss: 0.4183 - val_acc: 0.8276\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4283 - acc: 0.8143 - val_loss: 0.4092 - val_acc: 0.8374\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4186 - acc: 0.8165 - val_loss: 0.4053 - val_acc: 0.8374\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4094 - acc: 0.8223 - val_loss: 0.4063 - val_acc: 0.8342\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4014 - acc: 0.8298 - val_loss: 0.4014 - val_acc: 0.8325\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3964 - acc: 0.8311 - val_loss: 0.4026 - val_acc: 0.8276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3916 - acc: 0.8320 - val_loss: 0.4068 - val_acc: 0.8177\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3849 - acc: 0.8352 - val_loss: 0.3999 - val_acc: 0.8374\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3827 - acc: 0.8354 - val_loss: 0.4040 - val_acc: 0.8276\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3782 - acc: 0.8362 - val_loss: 0.3967 - val_acc: 0.8259\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3710 - acc: 0.8425 - val_loss: 0.3998 - val_acc: 0.8309\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3628 - acc: 0.8444 - val_loss: 0.3989 - val_acc: 0.8342\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3597 - acc: 0.8458 - val_loss: 0.3980 - val_acc: 0.8342\n",
      "Epoch 18/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3562 - acc: 0.8478 - val_loss: 0.4089 - val_acc: 0.8227\n",
      "Epoch 19/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3487 - acc: 0.8517 - val_loss: 0.4028 - val_acc: 0.8259\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_61 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_95 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_96 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 7ms/step - loss: 0.6307 - acc: 0.6437 - val_loss: 0.5393 - val_acc: 0.7586\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.5150 - acc: 0.7645 - val_loss: 0.4666 - val_acc: 0.7882\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4648 - acc: 0.7933 - val_loss: 0.4336 - val_acc: 0.8161\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4416 - acc: 0.8075 - val_loss: 0.4239 - val_acc: 0.8112\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4312 - acc: 0.8135 - val_loss: 0.4231 - val_acc: 0.8062\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4216 - acc: 0.8223 - val_loss: 0.4076 - val_acc: 0.8309\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4083 - acc: 0.8230 - val_loss: 0.4006 - val_acc: 0.8276\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4012 - acc: 0.8309 - val_loss: 0.3978 - val_acc: 0.8440\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3935 - acc: 0.8343 - val_loss: 0.3969 - val_acc: 0.8407\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3874 - acc: 0.8289 - val_loss: 0.3985 - val_acc: 0.8259\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3813 - acc: 0.8352 - val_loss: 0.3971 - val_acc: 0.8259\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3795 - acc: 0.8396 - val_loss: 0.3961 - val_acc: 0.8374\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3741 - acc: 0.8435 - val_loss: 0.3991 - val_acc: 0.8342\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3638 - acc: 0.8429 - val_loss: 0.3967 - val_acc: 0.8309\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3576 - acc: 0.8456 - val_loss: 0.3994 - val_acc: 0.8325\n",
      "Epoch 16/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3478 - acc: 0.8539 - val_loss: 0.4060 - val_acc: 0.8309\n",
      "Epoch 17/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3446 - acc: 0.8528 - val_loss: 0.4027 - val_acc: 0.8276\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_62 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_154 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_97 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_155 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_98 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_156 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "55/55 [==============================] - 1s 7ms/step - loss: 0.6113 - acc: 0.6710 - val_loss: 0.5274 - val_acc: 0.7471\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4979 - acc: 0.7730 - val_loss: 0.4707 - val_acc: 0.7685\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4582 - acc: 0.7957 - val_loss: 0.4277 - val_acc: 0.8095\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4363 - acc: 0.8068 - val_loss: 0.4197 - val_acc: 0.8210\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4224 - acc: 0.8155 - val_loss: 0.4074 - val_acc: 0.8276\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4126 - acc: 0.8199 - val_loss: 0.4028 - val_acc: 0.8374\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4045 - acc: 0.8232 - val_loss: 0.4013 - val_acc: 0.8374\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3952 - acc: 0.8296 - val_loss: 0.4086 - val_acc: 0.8243\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3938 - acc: 0.8294 - val_loss: 0.4078 - val_acc: 0.8259\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3823 - acc: 0.8338 - val_loss: 0.3993 - val_acc: 0.8374\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3702 - acc: 0.8420 - val_loss: 0.4041 - val_acc: 0.8292\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3649 - acc: 0.8435 - val_loss: 0.4152 - val_acc: 0.8177\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3612 - acc: 0.8475 - val_loss: 0.4095 - val_acc: 0.8177\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3517 - acc: 0.8517 - val_loss: 0.4056 - val_acc: 0.8292\n",
      "Epoch 15/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3421 - acc: 0.8542 - val_loss: 0.4128 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': 100, 'dropout': 0.1, 'lay_conf': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_63 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_157 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_99 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_158 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_100 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_159 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 656,897\n",
      "Trainable params: 656,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 1s 7ms/step - loss: 0.5992 - acc: 0.6822 - val_loss: 0.5057 - val_acc: 0.7734\n",
      "Epoch 2/300\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4881 - acc: 0.7834 - val_loss: 0.4388 - val_acc: 0.8079\n",
      "Epoch 3/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4528 - acc: 0.7993 - val_loss: 0.4192 - val_acc: 0.8194\n",
      "Epoch 4/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4329 - acc: 0.8124 - val_loss: 0.4100 - val_acc: 0.8177\n",
      "Epoch 5/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4134 - acc: 0.8234 - val_loss: 0.4051 - val_acc: 0.8325\n",
      "Epoch 6/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.4063 - acc: 0.8243 - val_loss: 0.4063 - val_acc: 0.8243\n",
      "Epoch 7/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3977 - acc: 0.8292 - val_loss: 0.3997 - val_acc: 0.8259\n",
      "Epoch 8/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3906 - acc: 0.8312 - val_loss: 0.4067 - val_acc: 0.8259\n",
      "Epoch 9/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3862 - acc: 0.8316 - val_loss: 0.3955 - val_acc: 0.8374\n",
      "Epoch 10/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3718 - acc: 0.8400 - val_loss: 0.4008 - val_acc: 0.8309\n",
      "Epoch 11/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3724 - acc: 0.8369 - val_loss: 0.3992 - val_acc: 0.8276\n",
      "Epoch 12/300\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.3566 - acc: 0.8475 - val_loss: 0.3991 - val_acc: 0.8309\n",
      "Epoch 13/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3474 - acc: 0.8559 - val_loss: 0.4111 - val_acc: 0.8259\n",
      "Epoch 14/300\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3395 - acc: 0.8575 - val_loss: 0.4052 - val_acc: 0.8194\n"
     ]
    }
   ],
   "source": [
    "mlp_perf_metrics1=make_MLP_exp(X_train_vect, y_train.values, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.838351</td>\n",
       "      <td>0.829228</td>\n",
       "      <td>0.378801</td>\n",
       "      <td>0.400439</td>\n",
       "      <td>63</td>\n",
       "      <td>0.806303</td>\n",
       "      <td>0.820467</td>\n",
       "      <td>0.700920</td>\n",
       "      <td>0.885189</td>\n",
       "      <td>0.755997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.839263</td>\n",
       "      <td>0.837438</td>\n",
       "      <td>0.368426</td>\n",
       "      <td>0.403811</td>\n",
       "      <td>58</td>\n",
       "      <td>0.806303</td>\n",
       "      <td>0.812609</td>\n",
       "      <td>0.711656</td>\n",
       "      <td>0.877153</td>\n",
       "      <td>0.758790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.839810</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.371238</td>\n",
       "      <td>0.402153</td>\n",
       "      <td>36</td>\n",
       "      <td>0.812869</td>\n",
       "      <td>0.829443</td>\n",
       "      <td>0.708589</td>\n",
       "      <td>0.890930</td>\n",
       "      <td>0.764268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.843094</td>\n",
       "      <td>0.832512</td>\n",
       "      <td>0.363093</td>\n",
       "      <td>0.402502</td>\n",
       "      <td>33</td>\n",
       "      <td>0.808930</td>\n",
       "      <td>0.836127</td>\n",
       "      <td>0.688650</td>\n",
       "      <td>0.898967</td>\n",
       "      <td>0.755257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[512]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.844007</td>\n",
       "      <td>0.821018</td>\n",
       "      <td>0.355342</td>\n",
       "      <td>0.402125</td>\n",
       "      <td>29</td>\n",
       "      <td>0.807617</td>\n",
       "      <td>0.816578</td>\n",
       "      <td>0.710123</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.759639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>[64, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.843094</td>\n",
       "      <td>0.824302</td>\n",
       "      <td>0.367635</td>\n",
       "      <td>0.397961</td>\n",
       "      <td>43</td>\n",
       "      <td>0.809586</td>\n",
       "      <td>0.832721</td>\n",
       "      <td>0.694785</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.757525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.843094</td>\n",
       "      <td>0.829228</td>\n",
       "      <td>0.370643</td>\n",
       "      <td>0.394023</td>\n",
       "      <td>33</td>\n",
       "      <td>0.807617</td>\n",
       "      <td>0.829358</td>\n",
       "      <td>0.693252</td>\n",
       "      <td>0.893226</td>\n",
       "      <td>0.755221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.849115</td>\n",
       "      <td>0.829228</td>\n",
       "      <td>0.358455</td>\n",
       "      <td>0.399094</td>\n",
       "      <td>34</td>\n",
       "      <td>0.807617</td>\n",
       "      <td>0.828154</td>\n",
       "      <td>0.694785</td>\n",
       "      <td>0.892078</td>\n",
       "      <td>0.755630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.843642</td>\n",
       "      <td>0.822660</td>\n",
       "      <td>0.359424</td>\n",
       "      <td>0.406357</td>\n",
       "      <td>29</td>\n",
       "      <td>0.807617</td>\n",
       "      <td>0.821109</td>\n",
       "      <td>0.703988</td>\n",
       "      <td>0.885189</td>\n",
       "      <td>0.758051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.853859</td>\n",
       "      <td>0.822660</td>\n",
       "      <td>0.343919</td>\n",
       "      <td>0.400544</td>\n",
       "      <td>28</td>\n",
       "      <td>0.810243</td>\n",
       "      <td>0.830601</td>\n",
       "      <td>0.699387</td>\n",
       "      <td>0.893226</td>\n",
       "      <td>0.759367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.840723</td>\n",
       "      <td>0.829228</td>\n",
       "      <td>0.372808</td>\n",
       "      <td>0.395154</td>\n",
       "      <td>22</td>\n",
       "      <td>0.814183</td>\n",
       "      <td>0.843575</td>\n",
       "      <td>0.694785</td>\n",
       "      <td>0.903559</td>\n",
       "      <td>0.761985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.843824</td>\n",
       "      <td>0.825944</td>\n",
       "      <td>0.366488</td>\n",
       "      <td>0.403302</td>\n",
       "      <td>21</td>\n",
       "      <td>0.808273</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.713190</td>\n",
       "      <td>0.879449</td>\n",
       "      <td>0.761047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.851487</td>\n",
       "      <td>0.821018</td>\n",
       "      <td>0.342340</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>23</td>\n",
       "      <td>0.802364</td>\n",
       "      <td>0.808436</td>\n",
       "      <td>0.705521</td>\n",
       "      <td>0.874856</td>\n",
       "      <td>0.753481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.846926</td>\n",
       "      <td>0.834154</td>\n",
       "      <td>0.360357</td>\n",
       "      <td>0.401359</td>\n",
       "      <td>17</td>\n",
       "      <td>0.810243</td>\n",
       "      <td>0.815652</td>\n",
       "      <td>0.719325</td>\n",
       "      <td>0.878301</td>\n",
       "      <td>0.764466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.840175</td>\n",
       "      <td>0.825944</td>\n",
       "      <td>0.373114</td>\n",
       "      <td>0.404376</td>\n",
       "      <td>16</td>\n",
       "      <td>0.810243</td>\n",
       "      <td>0.835490</td>\n",
       "      <td>0.693252</td>\n",
       "      <td>0.897819</td>\n",
       "      <td>0.757754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 64]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.851669</td>\n",
       "      <td>0.825944</td>\n",
       "      <td>0.348749</td>\n",
       "      <td>0.402754</td>\n",
       "      <td>19</td>\n",
       "      <td>0.807617</td>\n",
       "      <td>0.823423</td>\n",
       "      <td>0.700920</td>\n",
       "      <td>0.887486</td>\n",
       "      <td>0.757249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.852764</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.344633</td>\n",
       "      <td>0.402671</td>\n",
       "      <td>17</td>\n",
       "      <td>0.808273</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.720859</td>\n",
       "      <td>0.873708</td>\n",
       "      <td>0.762987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.854224</td>\n",
       "      <td>0.821018</td>\n",
       "      <td>0.342069</td>\n",
       "      <td>0.412827</td>\n",
       "      <td>15</td>\n",
       "      <td>0.814839</td>\n",
       "      <td>0.832734</td>\n",
       "      <td>0.710123</td>\n",
       "      <td>0.893226</td>\n",
       "      <td>0.766556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.857508</td>\n",
       "      <td>0.819376</td>\n",
       "      <td>0.339545</td>\n",
       "      <td>0.405185</td>\n",
       "      <td>14</td>\n",
       "      <td>0.809586</td>\n",
       "      <td>0.822064</td>\n",
       "      <td>0.708589</td>\n",
       "      <td>0.885189</td>\n",
       "      <td>0.761120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_layers  layer_conf      lr  dropout  batch_size  accuracy_train  \\\n",
       "0          1        [32]  0.0001      0.1         100        0.838351   \n",
       "1          1        [64]  0.0001      0.1         100        0.839263   \n",
       "2          1       [128]  0.0001      0.1         100        0.839810   \n",
       "3          1       [256]  0.0001      0.1         100        0.843094   \n",
       "4          1       [512]  0.0001      0.1         100        0.844007   \n",
       "5          2    [64, 32]  0.0001      0.1         100        0.843094   \n",
       "6          2    [64, 64]  0.0001      0.1         100        0.843094   \n",
       "7          2   [128, 32]  0.0001      0.1         100        0.849115   \n",
       "8          2   [128, 64]  0.0001      0.1         100        0.843642   \n",
       "9          2  [128, 128]  0.0001      0.1         100        0.853859   \n",
       "10         2   [256, 32]  0.0001      0.1         100        0.840723   \n",
       "11         2   [256, 64]  0.0001      0.1         100        0.843824   \n",
       "12         2  [256, 128]  0.0001      0.1         100        0.851487   \n",
       "13         2  [256, 256]  0.0001      0.1         100        0.846926   \n",
       "14         2   [512, 32]  0.0001      0.1         100        0.840175   \n",
       "15         2   [512, 64]  0.0001      0.1         100        0.851669   \n",
       "16         2  [512, 128]  0.0001      0.1         100        0.852764   \n",
       "17         2  [512, 256]  0.0001      0.1         100        0.854224   \n",
       "18         2  [512, 512]  0.0001      0.1         100        0.857508   \n",
       "\n",
       "    accuracy_val  loss_train  loss_val  epochs  accuracy  precision    recall  \\\n",
       "0       0.829228    0.378801  0.400439      63  0.806303   0.820467  0.700920   \n",
       "1       0.837438    0.368426  0.403811      58  0.806303   0.812609  0.711656   \n",
       "2       0.827586    0.371238  0.402153      36  0.812869   0.829443  0.708589   \n",
       "3       0.832512    0.363093  0.402502      33  0.808930   0.836127  0.688650   \n",
       "4       0.821018    0.355342  0.402125      29  0.807617   0.816578  0.710123   \n",
       "5       0.824302    0.367635  0.397961      43  0.809586   0.832721  0.694785   \n",
       "6       0.829228    0.370643  0.394023      33  0.807617   0.829358  0.693252   \n",
       "7       0.829228    0.358455  0.399094      34  0.807617   0.828154  0.694785   \n",
       "8       0.822660    0.359424  0.406357      29  0.807617   0.821109  0.703988   \n",
       "9       0.822660    0.343919  0.400544      28  0.810243   0.830601  0.699387   \n",
       "10      0.829228    0.372808  0.395154      22  0.814183   0.843575  0.694785   \n",
       "11      0.825944    0.366488  0.403302      21  0.808273   0.815789  0.713190   \n",
       "12      0.821018    0.342340  0.403203      23  0.802364   0.808436  0.705521   \n",
       "13      0.834154    0.360357  0.401359      17  0.810243   0.815652  0.719325   \n",
       "14      0.825944    0.373114  0.404376      16  0.810243   0.835490  0.693252   \n",
       "15      0.825944    0.348749  0.402754      19  0.807617   0.823423  0.700920   \n",
       "16      0.827586    0.344633  0.402671      17  0.808273   0.810345  0.720859   \n",
       "17      0.821018    0.342069  0.412827      15  0.814839   0.832734  0.710123   \n",
       "18      0.819376    0.339545  0.405185      14  0.809586   0.822064  0.708589   \n",
       "\n",
       "    specificity  f1_score  \n",
       "0      0.885189  0.755997  \n",
       "1      0.877153  0.758790  \n",
       "2      0.890930  0.764268  \n",
       "3      0.898967  0.755257  \n",
       "4      0.880597  0.759639  \n",
       "5      0.895522  0.757525  \n",
       "6      0.893226  0.755221  \n",
       "7      0.892078  0.755630  \n",
       "8      0.885189  0.758051  \n",
       "9      0.893226  0.759367  \n",
       "10     0.903559  0.761985  \n",
       "11     0.879449  0.761047  \n",
       "12     0.874856  0.753481  \n",
       "13     0.878301  0.764466  \n",
       "14     0.897819  0.757754  \n",
       "15     0.887486  0.757249  \n",
       "16     0.873708  0.762987  \n",
       "17     0.893226  0.766556  \n",
       "18     0.885189  0.761120  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_perf_metrics1.to_csv('mlp_perf_metrics1.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the best architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.854224</td>\n",
       "      <td>0.821018</td>\n",
       "      <td>0.342069</td>\n",
       "      <td>0.412827</td>\n",
       "      <td>15</td>\n",
       "      <td>0.814839</td>\n",
       "      <td>0.832734</td>\n",
       "      <td>0.710123</td>\n",
       "      <td>0.893226</td>\n",
       "      <td>0.766556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.840723</td>\n",
       "      <td>0.829228</td>\n",
       "      <td>0.372808</td>\n",
       "      <td>0.395154</td>\n",
       "      <td>22</td>\n",
       "      <td>0.814183</td>\n",
       "      <td>0.843575</td>\n",
       "      <td>0.694785</td>\n",
       "      <td>0.903559</td>\n",
       "      <td>0.761985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.839810</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.371238</td>\n",
       "      <td>0.402153</td>\n",
       "      <td>36</td>\n",
       "      <td>0.812869</td>\n",
       "      <td>0.829443</td>\n",
       "      <td>0.708589</td>\n",
       "      <td>0.890930</td>\n",
       "      <td>0.764268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.853859</td>\n",
       "      <td>0.822660</td>\n",
       "      <td>0.343919</td>\n",
       "      <td>0.400544</td>\n",
       "      <td>28</td>\n",
       "      <td>0.810243</td>\n",
       "      <td>0.830601</td>\n",
       "      <td>0.699387</td>\n",
       "      <td>0.893226</td>\n",
       "      <td>0.759367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.846926</td>\n",
       "      <td>0.834154</td>\n",
       "      <td>0.360357</td>\n",
       "      <td>0.401359</td>\n",
       "      <td>17</td>\n",
       "      <td>0.810243</td>\n",
       "      <td>0.815652</td>\n",
       "      <td>0.719325</td>\n",
       "      <td>0.878301</td>\n",
       "      <td>0.764466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_layers  layer_conf      lr  dropout  batch_size  accuracy_train  \\\n",
       "17         2  [512, 256]  0.0001      0.1         100        0.854224   \n",
       "10         2   [256, 32]  0.0001      0.1         100        0.840723   \n",
       "2          1       [128]  0.0001      0.1         100        0.839810   \n",
       "9          2  [128, 128]  0.0001      0.1         100        0.853859   \n",
       "13         2  [256, 256]  0.0001      0.1         100        0.846926   \n",
       "\n",
       "    accuracy_val  loss_train  loss_val  epochs  accuracy  precision    recall  \\\n",
       "17      0.821018    0.342069  0.412827      15  0.814839   0.832734  0.710123   \n",
       "10      0.829228    0.372808  0.395154      22  0.814183   0.843575  0.694785   \n",
       "2       0.827586    0.371238  0.402153      36  0.812869   0.829443  0.708589   \n",
       "9       0.822660    0.343919  0.400544      28  0.810243   0.830601  0.699387   \n",
       "13      0.834154    0.360357  0.401359      17  0.810243   0.815652  0.719325   \n",
       "\n",
       "    specificity  f1_score  \n",
       "17     0.893226  0.766556  \n",
       "10     0.903559  0.761985  \n",
       "2      0.890930  0.764268  \n",
       "9      0.893226  0.759367  \n",
       "13     0.878301  0.764466  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics1.nlargest(5, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.854224</td>\n",
       "      <td>0.821018</td>\n",
       "      <td>0.342069</td>\n",
       "      <td>0.412827</td>\n",
       "      <td>15</td>\n",
       "      <td>0.814839</td>\n",
       "      <td>0.832734</td>\n",
       "      <td>0.710123</td>\n",
       "      <td>0.893226</td>\n",
       "      <td>0.766556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.846926</td>\n",
       "      <td>0.834154</td>\n",
       "      <td>0.360357</td>\n",
       "      <td>0.401359</td>\n",
       "      <td>17</td>\n",
       "      <td>0.810243</td>\n",
       "      <td>0.815652</td>\n",
       "      <td>0.719325</td>\n",
       "      <td>0.878301</td>\n",
       "      <td>0.764466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.839810</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.371238</td>\n",
       "      <td>0.402153</td>\n",
       "      <td>36</td>\n",
       "      <td>0.812869</td>\n",
       "      <td>0.829443</td>\n",
       "      <td>0.708589</td>\n",
       "      <td>0.890930</td>\n",
       "      <td>0.764268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.852764</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.344633</td>\n",
       "      <td>0.402671</td>\n",
       "      <td>17</td>\n",
       "      <td>0.808273</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.720859</td>\n",
       "      <td>0.873708</td>\n",
       "      <td>0.762987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.840723</td>\n",
       "      <td>0.829228</td>\n",
       "      <td>0.372808</td>\n",
       "      <td>0.395154</td>\n",
       "      <td>22</td>\n",
       "      <td>0.814183</td>\n",
       "      <td>0.843575</td>\n",
       "      <td>0.694785</td>\n",
       "      <td>0.903559</td>\n",
       "      <td>0.761985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_layers  layer_conf      lr  dropout  batch_size  accuracy_train  \\\n",
       "17         2  [512, 256]  0.0001      0.1         100        0.854224   \n",
       "13         2  [256, 256]  0.0001      0.1         100        0.846926   \n",
       "2          1       [128]  0.0001      0.1         100        0.839810   \n",
       "16         2  [512, 128]  0.0001      0.1         100        0.852764   \n",
       "10         2   [256, 32]  0.0001      0.1         100        0.840723   \n",
       "\n",
       "    accuracy_val  loss_train  loss_val  epochs  accuracy  precision    recall  \\\n",
       "17      0.821018    0.342069  0.412827      15  0.814839   0.832734  0.710123   \n",
       "13      0.834154    0.360357  0.401359      17  0.810243   0.815652  0.719325   \n",
       "2       0.827586    0.371238  0.402153      36  0.812869   0.829443  0.708589   \n",
       "16      0.827586    0.344633  0.402671      17  0.808273   0.810345  0.720859   \n",
       "10      0.829228    0.372808  0.395154      22  0.814183   0.843575  0.694785   \n",
       "\n",
       "    specificity  f1_score  \n",
       "17     0.893226  0.766556  \n",
       "13     0.878301  0.764466  \n",
       "2      0.890930  0.764268  \n",
       "16     0.873708  0.762987  \n",
       "10     0.903559  0.761985  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics1.nlargest(5, 'f1_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the best architectures, let's try to grid other values for `batch_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_arch1 = [[512, 256],\n",
    "             [256, 256],\n",
    "             [128],\n",
    "             [512, 128],\n",
    "             [256, 32],\n",
    "             [128, 128],\n",
    "             [256, 256]]\n",
    "\n",
    "# Grid parameters\n",
    "params2 = {\n",
    "        'lay_conf': best_arch1,\n",
    "        'lr': [1e-4],\n",
    "        'dropout': [0.1],\n",
    "        'max_epochs': [300],\n",
    "        'batch_size': [None, 1, 10, 20, 50, 70, 200, 350, 500, 700],\n",
    "        'seed': [123456]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters {'batch_size': None, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_64 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_160 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_101 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_102 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_162 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "172/172 [==============================] - 1s 2ms/step - loss: 0.5502 - acc: 0.7285 - val_loss: 0.4642 - val_acc: 0.7997\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4465 - acc: 0.8044 - val_loss: 0.4363 - val_acc: 0.8128\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4259 - acc: 0.8141 - val_loss: 0.4282 - val_acc: 0.8243\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4136 - acc: 0.8186 - val_loss: 0.4273 - val_acc: 0.8144\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4011 - acc: 0.8261 - val_loss: 0.4303 - val_acc: 0.8128\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3886 - acc: 0.8318 - val_loss: 0.4196 - val_acc: 0.8161\n",
      "Epoch 7/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3862 - acc: 0.8311 - val_loss: 0.4315 - val_acc: 0.8161\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3674 - acc: 0.8396 - val_loss: 0.4212 - val_acc: 0.8194\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3602 - acc: 0.8438 - val_loss: 0.4618 - val_acc: 0.8112\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3489 - acc: 0.8488 - val_loss: 0.4532 - val_acc: 0.8079\n",
      "Epoch 11/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3399 - acc: 0.8573 - val_loss: 0.4311 - val_acc: 0.8062\n",
      "Training with parameters {'batch_size': None, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_60\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_65 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_163 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_103 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_164 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_104 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_165 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "172/172 [==============================] - 1s 2ms/step - loss: 0.5802 - acc: 0.7021 - val_loss: 0.4827 - val_acc: 0.7915\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4667 - acc: 0.7909 - val_loss: 0.4545 - val_acc: 0.8079\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4394 - acc: 0.8048 - val_loss: 0.4364 - val_acc: 0.8210\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4223 - acc: 0.8188 - val_loss: 0.4350 - val_acc: 0.8144\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4133 - acc: 0.8172 - val_loss: 0.4288 - val_acc: 0.8144\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4036 - acc: 0.8245 - val_loss: 0.4231 - val_acc: 0.8259\n",
      "Epoch 7/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3971 - acc: 0.8289 - val_loss: 0.4222 - val_acc: 0.8210\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3899 - acc: 0.8316 - val_loss: 0.4177 - val_acc: 0.8292\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3803 - acc: 0.8376 - val_loss: 0.4188 - val_acc: 0.8210\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3731 - acc: 0.8402 - val_loss: 0.4184 - val_acc: 0.8276\n",
      "Epoch 11/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3628 - acc: 0.8431 - val_loss: 0.4329 - val_acc: 0.8112\n",
      "Epoch 12/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3563 - acc: 0.8500 - val_loss: 0.4298 - val_acc: 0.8144\n",
      "Epoch 13/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3467 - acc: 0.8515 - val_loss: 0.4232 - val_acc: 0.8194\n",
      "Training with parameters {'batch_size': None, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_61\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_66 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_166 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_105 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_167 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "172/172 [==============================] - 1s 2ms/step - loss: 0.5727 - acc: 0.7283 - val_loss: 0.5126 - val_acc: 0.7865\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.4962 - acc: 0.7752 - val_loss: 0.4742 - val_acc: 0.7947\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.4691 - acc: 0.7885 - val_loss: 0.4599 - val_acc: 0.7997\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.4513 - acc: 0.8024 - val_loss: 0.4553 - val_acc: 0.8062\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.4411 - acc: 0.8046 - val_loss: 0.4449 - val_acc: 0.8079\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4319 - acc: 0.8108 - val_loss: 0.4464 - val_acc: 0.8046\n",
      "Epoch 7/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.4267 - acc: 0.8135 - val_loss: 0.4363 - val_acc: 0.8112\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.4204 - acc: 0.8174 - val_loss: 0.4313 - val_acc: 0.8292\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.4126 - acc: 0.8234 - val_loss: 0.4428 - val_acc: 0.8046\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.4108 - acc: 0.8239 - val_loss: 0.4290 - val_acc: 0.8259\n",
      "Epoch 11/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.4067 - acc: 0.8228 - val_loss: 0.4262 - val_acc: 0.8276\n",
      "Epoch 12/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4027 - acc: 0.8243 - val_loss: 0.4271 - val_acc: 0.8210\n",
      "Epoch 13/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3989 - acc: 0.8259 - val_loss: 0.4358 - val_acc: 0.8062\n",
      "Epoch 14/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3968 - acc: 0.8280 - val_loss: 0.4220 - val_acc: 0.8227\n",
      "Epoch 15/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3922 - acc: 0.8300 - val_loss: 0.4219 - val_acc: 0.8210\n",
      "Epoch 16/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3898 - acc: 0.8318 - val_loss: 0.4228 - val_acc: 0.8128\n",
      "Epoch 17/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3855 - acc: 0.8318 - val_loss: 0.4214 - val_acc: 0.8194\n",
      "Epoch 18/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3826 - acc: 0.8343 - val_loss: 0.4194 - val_acc: 0.8161\n",
      "Epoch 19/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3792 - acc: 0.8343 - val_loss: 0.4190 - val_acc: 0.8194\n",
      "Epoch 20/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3784 - acc: 0.8371 - val_loss: 0.4191 - val_acc: 0.8227\n",
      "Epoch 21/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3746 - acc: 0.8367 - val_loss: 0.4182 - val_acc: 0.8177\n",
      "Epoch 22/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3726 - acc: 0.8415 - val_loss: 0.4170 - val_acc: 0.8210\n",
      "Epoch 23/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3712 - acc: 0.8400 - val_loss: 0.4185 - val_acc: 0.8161\n",
      "Epoch 24/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3669 - acc: 0.8391 - val_loss: 0.4187 - val_acc: 0.8177\n",
      "Epoch 25/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3647 - acc: 0.8422 - val_loss: 0.4261 - val_acc: 0.8079\n",
      "Epoch 26/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3623 - acc: 0.8456 - val_loss: 0.4237 - val_acc: 0.8144\n",
      "Epoch 27/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3604 - acc: 0.8433 - val_loss: 0.4166 - val_acc: 0.8161\n",
      "Epoch 28/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3587 - acc: 0.8438 - val_loss: 0.4179 - val_acc: 0.8194\n",
      "Epoch 29/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3568 - acc: 0.8438 - val_loss: 0.4196 - val_acc: 0.8210\n",
      "Epoch 30/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3526 - acc: 0.8486 - val_loss: 0.4167 - val_acc: 0.8095\n",
      "Epoch 31/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3512 - acc: 0.8486 - val_loss: 0.4191 - val_acc: 0.8161\n",
      "Epoch 32/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3486 - acc: 0.8491 - val_loss: 0.4249 - val_acc: 0.8112\n",
      "Training with parameters {'batch_size': None, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_62\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_67 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_168 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_106 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_169 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_107 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_170 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "172/172 [==============================] - 1s 2ms/step - loss: 0.5657 - acc: 0.7176 - val_loss: 0.4739 - val_acc: 0.7882\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4588 - acc: 0.7935 - val_loss: 0.4603 - val_acc: 0.7980\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4304 - acc: 0.8123 - val_loss: 0.4311 - val_acc: 0.8194\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4199 - acc: 0.8174 - val_loss: 0.4268 - val_acc: 0.8144\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4080 - acc: 0.8243 - val_loss: 0.4254 - val_acc: 0.8161\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3980 - acc: 0.8290 - val_loss: 0.4269 - val_acc: 0.8144\n",
      "Epoch 7/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3912 - acc: 0.8301 - val_loss: 0.4148 - val_acc: 0.8194\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3791 - acc: 0.8376 - val_loss: 0.4148 - val_acc: 0.8161\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3725 - acc: 0.8398 - val_loss: 0.4140 - val_acc: 0.8210\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3595 - acc: 0.8440 - val_loss: 0.4194 - val_acc: 0.8259\n",
      "Epoch 11/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3488 - acc: 0.8491 - val_loss: 0.4177 - val_acc: 0.8210\n",
      "Epoch 12/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3436 - acc: 0.8529 - val_loss: 0.4183 - val_acc: 0.8194\n",
      "Epoch 13/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3277 - acc: 0.8591 - val_loss: 0.4398 - val_acc: 0.8112\n",
      "Epoch 14/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3177 - acc: 0.8661 - val_loss: 0.4252 - val_acc: 0.8079\n",
      "Training with parameters {'batch_size': None, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_68 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_171 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_108 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_172 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_109 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_173 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "172/172 [==============================] - 1s 2ms/step - loss: 0.6045 - acc: 0.6829 - val_loss: 0.5101 - val_acc: 0.7865\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4971 - acc: 0.7701 - val_loss: 0.4618 - val_acc: 0.8128\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4598 - acc: 0.7986 - val_loss: 0.4439 - val_acc: 0.8112\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4420 - acc: 0.8039 - val_loss: 0.4468 - val_acc: 0.8062\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4289 - acc: 0.8132 - val_loss: 0.4283 - val_acc: 0.8227\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4169 - acc: 0.8121 - val_loss: 0.4270 - val_acc: 0.8161\n",
      "Epoch 7/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 0s 1ms/step - loss: 0.4092 - acc: 0.8196 - val_loss: 0.4222 - val_acc: 0.8194\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4025 - acc: 0.8272 - val_loss: 0.4363 - val_acc: 0.8161\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3934 - acc: 0.8294 - val_loss: 0.4194 - val_acc: 0.8210\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3894 - acc: 0.8290 - val_loss: 0.4191 - val_acc: 0.8161\n",
      "Epoch 11/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3848 - acc: 0.8356 - val_loss: 0.4280 - val_acc: 0.8128\n",
      "Epoch 12/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3783 - acc: 0.8354 - val_loss: 0.4180 - val_acc: 0.8161\n",
      "Epoch 13/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3766 - acc: 0.8387 - val_loss: 0.4158 - val_acc: 0.8194\n",
      "Epoch 14/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3672 - acc: 0.8415 - val_loss: 0.4159 - val_acc: 0.8210\n",
      "Epoch 15/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3637 - acc: 0.8411 - val_loss: 0.4255 - val_acc: 0.8144\n",
      "Epoch 16/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3609 - acc: 0.8489 - val_loss: 0.4173 - val_acc: 0.8194\n",
      "Epoch 17/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3509 - acc: 0.8509 - val_loss: 0.4156 - val_acc: 0.8210\n",
      "Epoch 18/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3470 - acc: 0.8508 - val_loss: 0.4216 - val_acc: 0.8144\n",
      "Epoch 19/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3444 - acc: 0.8570 - val_loss: 0.4204 - val_acc: 0.8144\n",
      "Epoch 20/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3331 - acc: 0.8568 - val_loss: 0.4183 - val_acc: 0.8079\n",
      "Epoch 21/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3280 - acc: 0.8590 - val_loss: 0.4249 - val_acc: 0.8128\n",
      "Epoch 22/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3245 - acc: 0.8575 - val_loss: 0.4296 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': None, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_64\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_69 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_174 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_110 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_175 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_111 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_176 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "172/172 [==============================] - 1s 2ms/step - loss: 0.5997 - acc: 0.6946 - val_loss: 0.5130 - val_acc: 0.7849\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.4901 - acc: 0.7780 - val_loss: 0.4612 - val_acc: 0.8062\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4570 - acc: 0.7971 - val_loss: 0.4484 - val_acc: 0.8095\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4408 - acc: 0.8046 - val_loss: 0.4462 - val_acc: 0.8030\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.4257 - acc: 0.8154 - val_loss: 0.4344 - val_acc: 0.8112\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4175 - acc: 0.8166 - val_loss: 0.4288 - val_acc: 0.8144\n",
      "Epoch 7/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4079 - acc: 0.8228 - val_loss: 0.4298 - val_acc: 0.8046\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4059 - acc: 0.8245 - val_loss: 0.4283 - val_acc: 0.8112\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3976 - acc: 0.8238 - val_loss: 0.4254 - val_acc: 0.8128\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3919 - acc: 0.8276 - val_loss: 0.4211 - val_acc: 0.8161\n",
      "Epoch 11/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3894 - acc: 0.8303 - val_loss: 0.4276 - val_acc: 0.8095\n",
      "Epoch 12/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3845 - acc: 0.8363 - val_loss: 0.4227 - val_acc: 0.8128\n",
      "Epoch 13/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3768 - acc: 0.8351 - val_loss: 0.4175 - val_acc: 0.8194\n",
      "Epoch 14/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3719 - acc: 0.8385 - val_loss: 0.4142 - val_acc: 0.8210\n",
      "Epoch 15/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3645 - acc: 0.8449 - val_loss: 0.4271 - val_acc: 0.8112\n",
      "Epoch 16/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3638 - acc: 0.8438 - val_loss: 0.4174 - val_acc: 0.8194\n",
      "Epoch 17/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3559 - acc: 0.8497 - val_loss: 0.4280 - val_acc: 0.8095\n",
      "Epoch 18/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3499 - acc: 0.8497 - val_loss: 0.4182 - val_acc: 0.8144\n",
      "Epoch 19/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3442 - acc: 0.8539 - val_loss: 0.4205 - val_acc: 0.8144\n",
      "Training with parameters {'batch_size': None, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_65\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_70 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_177 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_112 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_178 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_179 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "172/172 [==============================] - 1s 2ms/step - loss: 0.5874 - acc: 0.6982 - val_loss: 0.4883 - val_acc: 0.7882\n",
      "Epoch 2/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.4717 - acc: 0.7891 - val_loss: 0.4504 - val_acc: 0.7980\n",
      "Epoch 3/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4417 - acc: 0.8044 - val_loss: 0.4341 - val_acc: 0.8112\n",
      "Epoch 4/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.4226 - acc: 0.8137 - val_loss: 0.4380 - val_acc: 0.8144\n",
      "Epoch 5/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.4164 - acc: 0.8183 - val_loss: 0.4267 - val_acc: 0.8259\n",
      "Epoch 6/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.4044 - acc: 0.8210 - val_loss: 0.4229 - val_acc: 0.8210\n",
      "Epoch 7/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3979 - acc: 0.8236 - val_loss: 0.4200 - val_acc: 0.8194\n",
      "Epoch 8/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3885 - acc: 0.8325 - val_loss: 0.4194 - val_acc: 0.8243\n",
      "Epoch 9/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3787 - acc: 0.8318 - val_loss: 0.4178 - val_acc: 0.8243\n",
      "Epoch 10/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3733 - acc: 0.8384 - val_loss: 0.4283 - val_acc: 0.8177\n",
      "Epoch 11/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3660 - acc: 0.8444 - val_loss: 0.4237 - val_acc: 0.8128\n",
      "Epoch 12/300\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 0.3553 - acc: 0.8482 - val_loss: 0.4190 - val_acc: 0.8243\n",
      "Epoch 13/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3505 - acc: 0.8458 - val_loss: 0.4278 - val_acc: 0.8161\n",
      "Epoch 14/300\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 0.3364 - acc: 0.8564 - val_loss: 0.4228 - val_acc: 0.8243\n",
      "Training with parameters {'batch_size': 1, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_66\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_71 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_180 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_114 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_181 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_115 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_182 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5481/5481 [==============================] - 10s 2ms/step - loss: 0.4828 - acc: 0.7718 - val_loss: 0.4462 - val_acc: 0.7964\n",
      "Epoch 2/300\n",
      "5481/5481 [==============================] - 10s 2ms/step - loss: 0.4262 - acc: 0.8126 - val_loss: 0.4251 - val_acc: 0.8112\n",
      "Epoch 3/300\n",
      "5481/5481 [==============================] - 9s 2ms/step - loss: 0.4110 - acc: 0.8176 - val_loss: 0.4310 - val_acc: 0.8144\n",
      "Epoch 4/300\n",
      "5481/5481 [==============================] - 9s 2ms/step - loss: 0.3941 - acc: 0.8303 - val_loss: 0.4219 - val_acc: 0.8194\n",
      "Epoch 5/300\n",
      "5481/5481 [==============================] - 8s 2ms/step - loss: 0.3748 - acc: 0.8331 - val_loss: 0.4521 - val_acc: 0.8210\n",
      "Epoch 6/300\n",
      "5481/5481 [==============================] - 9s 2ms/step - loss: 0.3521 - acc: 0.8478 - val_loss: 0.4511 - val_acc: 0.8128\n",
      "Epoch 7/300\n",
      "5481/5481 [==============================] - 9s 2ms/step - loss: 0.3303 - acc: 0.8584 - val_loss: 0.4699 - val_acc: 0.8030\n",
      "Epoch 8/300\n",
      "5481/5481 [==============================] - 9s 2ms/step - loss: 0.3083 - acc: 0.8644 - val_loss: 0.4781 - val_acc: 0.8144\n",
      "Epoch 9/300\n",
      "5481/5481 [==============================] - 9s 2ms/step - loss: 0.2840 - acc: 0.8783 - val_loss: 0.4756 - val_acc: 0.8079\n",
      "Training with parameters {'batch_size': 1, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_67\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_72 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_183 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_116 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_184 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_117 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_185 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5481/5481 [==============================] - 8s 1ms/step - loss: 0.4858 - acc: 0.7763 - val_loss: 0.4427 - val_acc: 0.7964\n",
      "Epoch 2/300\n",
      "5481/5481 [==============================] - 8s 1ms/step - loss: 0.4279 - acc: 0.8113 - val_loss: 0.4191 - val_acc: 0.8161\n",
      "Epoch 3/300\n",
      "5481/5481 [==============================] - 8s 1ms/step - loss: 0.4138 - acc: 0.8190 - val_loss: 0.4107 - val_acc: 0.8079\n",
      "Epoch 4/300\n",
      "5481/5481 [==============================] - 8s 2ms/step - loss: 0.3939 - acc: 0.8281 - val_loss: 0.4194 - val_acc: 0.8079\n",
      "Epoch 5/300\n",
      "5481/5481 [==============================] - 8s 2ms/step - loss: 0.3826 - acc: 0.8300 - val_loss: 0.4424 - val_acc: 0.8194\n",
      "Epoch 6/300\n",
      "5481/5481 [==============================] - 8s 2ms/step - loss: 0.3601 - acc: 0.8458 - val_loss: 0.4178 - val_acc: 0.8128\n",
      "Epoch 7/300\n",
      "5481/5481 [==============================] - 8s 2ms/step - loss: 0.3462 - acc: 0.8506 - val_loss: 0.4258 - val_acc: 0.8144\n",
      "Epoch 8/300\n",
      "5481/5481 [==============================] - 8s 2ms/step - loss: 0.3261 - acc: 0.8604 - val_loss: 0.4546 - val_acc: 0.8128\n",
      "Training with parameters {'batch_size': 1, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_68\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_73 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_186 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_118 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_187 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5481/5481 [==============================] - 8s 1ms/step - loss: 0.4942 - acc: 0.7707 - val_loss: 0.4370 - val_acc: 0.8144\n",
      "Epoch 2/300\n",
      "5481/5481 [==============================] - 7s 1ms/step - loss: 0.4358 - acc: 0.8092 - val_loss: 0.4270 - val_acc: 0.8095\n",
      "Epoch 3/300\n",
      "5481/5481 [==============================] - 7s 1ms/step - loss: 0.4184 - acc: 0.8194 - val_loss: 0.4218 - val_acc: 0.8227\n",
      "Epoch 4/300\n",
      "5481/5481 [==============================] - 7s 1ms/step - loss: 0.4054 - acc: 0.8234 - val_loss: 0.4240 - val_acc: 0.8095\n",
      "Epoch 5/300\n",
      "5481/5481 [==============================] - 7s 1ms/step - loss: 0.3949 - acc: 0.8332 - val_loss: 0.4187 - val_acc: 0.8177\n",
      "Epoch 6/300\n",
      "5481/5481 [==============================] - 7s 1ms/step - loss: 0.3850 - acc: 0.8307 - val_loss: 0.4580 - val_acc: 0.7833\n",
      "Epoch 7/300\n",
      "5481/5481 [==============================] - 8s 1ms/step - loss: 0.3764 - acc: 0.8347 - val_loss: 0.4136 - val_acc: 0.8194\n",
      "Epoch 8/300\n",
      "5481/5481 [==============================] - 8s 1ms/step - loss: 0.3666 - acc: 0.8418 - val_loss: 0.4149 - val_acc: 0.8210\n",
      "Epoch 9/300\n",
      "5481/5481 [==============================] - 7s 1ms/step - loss: 0.3625 - acc: 0.8385 - val_loss: 0.4156 - val_acc: 0.8177\n",
      "Epoch 10/300\n",
      "5481/5481 [==============================] - 8s 1ms/step - loss: 0.3516 - acc: 0.8446 - val_loss: 0.4231 - val_acc: 0.8128\n",
      "Epoch 11/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5481/5481 [==============================] - 9s 2ms/step - loss: 0.3403 - acc: 0.8535 - val_loss: 0.5006 - val_acc: 0.8030\n",
      "Epoch 12/300\n",
      "5481/5481 [==============================] - 9s 2ms/step - loss: 0.3350 - acc: 0.8571 - val_loss: 0.4217 - val_acc: 0.8161\n",
      "Training with parameters {'batch_size': 1, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_69\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_74 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_188 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_119 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_189 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_120 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_190 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5481/5481 [==============================] - 9s 2ms/step - loss: 0.4876 - acc: 0.7685 - val_loss: 0.4483 - val_acc: 0.8062\n",
      "Epoch 2/300\n",
      "5481/5481 [==============================] - 9s 2ms/step - loss: 0.4301 - acc: 0.8132 - val_loss: 0.4282 - val_acc: 0.8112\n",
      "Epoch 3/300\n",
      "5481/5481 [==============================] - 9s 2ms/step - loss: 0.4126 - acc: 0.8170 - val_loss: 0.4186 - val_acc: 0.8128\n",
      "Epoch 4/300\n",
      "5481/5481 [==============================] - 9s 2ms/step - loss: 0.3958 - acc: 0.8227 - val_loss: 0.4639 - val_acc: 0.8161\n",
      "Epoch 5/300\n",
      "5481/5481 [==============================] - 9s 2ms/step - loss: 0.3832 - acc: 0.8278 - val_loss: 0.4253 - val_acc: 0.8079\n",
      "Epoch 6/300\n",
      "5481/5481 [==============================] - 9s 2ms/step - loss: 0.3611 - acc: 0.8420 - val_loss: 0.4704 - val_acc: 0.7964\n",
      "Epoch 7/300\n",
      "5481/5481 [==============================] - 10s 2ms/step - loss: 0.3468 - acc: 0.8451 - val_loss: 0.4324 - val_acc: 0.8112\n",
      "Epoch 8/300\n",
      "5481/5481 [==============================] - 9s 2ms/step - loss: 0.3247 - acc: 0.8564 - val_loss: 0.4475 - val_acc: 0.7980\n",
      "Training with parameters {'batch_size': 1, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_70\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_75 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_191 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_121 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_192 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_122 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_193 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5481/5481 [==============================] - 9s 2ms/step - loss: 0.5036 - acc: 0.7654 - val_loss: 0.4360 - val_acc: 0.8177\n",
      "Epoch 2/300\n",
      "5481/5481 [==============================] - 7s 1ms/step - loss: 0.4344 - acc: 0.8092 - val_loss: 0.4272 - val_acc: 0.8161\n",
      "Epoch 3/300\n",
      "5481/5481 [==============================] - 8s 1ms/step - loss: 0.4183 - acc: 0.8113 - val_loss: 0.4325 - val_acc: 0.8030\n",
      "Epoch 4/300\n",
      "5481/5481 [==============================] - 8s 2ms/step - loss: 0.4047 - acc: 0.8199 - val_loss: 0.4233 - val_acc: 0.8030\n",
      "Epoch 5/300\n",
      "5481/5481 [==============================] - 8s 1ms/step - loss: 0.3918 - acc: 0.8270 - val_loss: 0.4232 - val_acc: 0.8210\n",
      "Epoch 6/300\n",
      "5481/5481 [==============================] - 8s 2ms/step - loss: 0.3771 - acc: 0.8385 - val_loss: 0.4251 - val_acc: 0.8112\n",
      "Epoch 7/300\n",
      "5481/5481 [==============================] - 8s 1ms/step - loss: 0.3644 - acc: 0.8429 - val_loss: 0.4266 - val_acc: 0.8095\n",
      "Epoch 8/300\n",
      "5481/5481 [==============================] - 8s 1ms/step - loss: 0.3510 - acc: 0.8473 - val_loss: 0.4509 - val_acc: 0.7980\n",
      "Epoch 9/300\n",
      "5481/5481 [==============================] - 8s 1ms/step - loss: 0.3400 - acc: 0.8535 - val_loss: 0.4589 - val_acc: 0.8112\n",
      "Epoch 10/300\n",
      "5481/5481 [==============================] - 8s 1ms/step - loss: 0.3240 - acc: 0.8604 - val_loss: 0.4384 - val_acc: 0.8128\n",
      "Training with parameters {'batch_size': 1, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_71\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_76 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_194 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_123 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_195 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_124 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_196 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5481/5481 [==============================] - 9s 2ms/step - loss: 0.4954 - acc: 0.7694 - val_loss: 0.4459 - val_acc: 0.7997\n",
      "Epoch 2/300\n",
      "5481/5481 [==============================] - 9s 2ms/step - loss: 0.4337 - acc: 0.8062 - val_loss: 0.4223 - val_acc: 0.8112\n",
      "Epoch 3/300\n",
      "5481/5481 [==============================] - 8s 1ms/step - loss: 0.4139 - acc: 0.8210 - val_loss: 0.4193 - val_acc: 0.8112\n",
      "Epoch 4/300\n",
      "5481/5481 [==============================] - 8s 1ms/step - loss: 0.4028 - acc: 0.8258 - val_loss: 0.4151 - val_acc: 0.8177\n",
      "Epoch 5/300\n",
      "5481/5481 [==============================] - 8s 1ms/step - loss: 0.3876 - acc: 0.8320 - val_loss: 0.4177 - val_acc: 0.8194\n",
      "Epoch 6/300\n",
      "5481/5481 [==============================] - 8s 1ms/step - loss: 0.3764 - acc: 0.8358 - val_loss: 0.4193 - val_acc: 0.8144\n",
      "Epoch 7/300\n",
      "5481/5481 [==============================] - 8s 2ms/step - loss: 0.3618 - acc: 0.8404 - val_loss: 0.4149 - val_acc: 0.8128\n",
      "Epoch 8/300\n",
      "5481/5481 [==============================] - 8s 1ms/step - loss: 0.3451 - acc: 0.8535 - val_loss: 0.4406 - val_acc: 0.8030\n",
      "Epoch 9/300\n",
      "5481/5481 [==============================] - 8s 1ms/step - loss: 0.3335 - acc: 0.8537 - val_loss: 0.4592 - val_acc: 0.8062\n",
      "Epoch 10/300\n",
      "5481/5481 [==============================] - 8s 1ms/step - loss: 0.3185 - acc: 0.8619 - val_loss: 0.4264 - val_acc: 0.7947\n",
      "Epoch 11/300\n",
      "5481/5481 [==============================] - 9s 2ms/step - loss: 0.3026 - acc: 0.8688 - val_loss: 0.4434 - val_acc: 0.8079\n",
      "Epoch 12/300\n",
      "5481/5481 [==============================] - 9s 2ms/step - loss: 0.2847 - acc: 0.8796 - val_loss: 0.5096 - val_acc: 0.7849\n",
      "Training with parameters {'batch_size': 1, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_72\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_77 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_197 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_125 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_198 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_126 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_199 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5481/5481 [==============================] - 11s 2ms/step - loss: 0.4897 - acc: 0.7719 - val_loss: 0.4385 - val_acc: 0.7980\n",
      "Epoch 2/300\n",
      "5481/5481 [==============================] - 10s 2ms/step - loss: 0.4302 - acc: 0.8137 - val_loss: 0.4208 - val_acc: 0.8161\n",
      "Epoch 3/300\n",
      "5481/5481 [==============================] - 10s 2ms/step - loss: 0.4103 - acc: 0.8176 - val_loss: 0.4177 - val_acc: 0.8177\n",
      "Epoch 4/300\n",
      "5481/5481 [==============================] - 10s 2ms/step - loss: 0.3975 - acc: 0.8234 - val_loss: 0.4246 - val_acc: 0.8144\n",
      "Epoch 5/300\n",
      "5481/5481 [==============================] - 10s 2ms/step - loss: 0.3824 - acc: 0.8329 - val_loss: 0.4191 - val_acc: 0.8144\n",
      "Epoch 6/300\n",
      "5481/5481 [==============================] - 10s 2ms/step - loss: 0.3631 - acc: 0.8371 - val_loss: 0.4290 - val_acc: 0.8095\n",
      "Epoch 7/300\n",
      "5481/5481 [==============================] - 10s 2ms/step - loss: 0.3448 - acc: 0.8486 - val_loss: 0.4321 - val_acc: 0.8062\n",
      "Epoch 8/300\n",
      "5481/5481 [==============================] - 10s 2ms/step - loss: 0.3262 - acc: 0.8597 - val_loss: 0.4548 - val_acc: 0.7947\n",
      "Training with parameters {'batch_size': 10, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_73\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_78 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_200 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_127 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_201 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_128 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_202 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.5127 - acc: 0.7590 - val_loss: 0.4478 - val_acc: 0.8062\n",
      "Epoch 2/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.4337 - acc: 0.8112 - val_loss: 0.4731 - val_acc: 0.7947\n",
      "Epoch 3/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.4148 - acc: 0.8126 - val_loss: 0.4216 - val_acc: 0.8161\n",
      "Epoch 4/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.3976 - acc: 0.8223 - val_loss: 0.4180 - val_acc: 0.8243\n",
      "Epoch 5/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.3873 - acc: 0.8298 - val_loss: 0.4300 - val_acc: 0.8177\n",
      "Epoch 6/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.3727 - acc: 0.8371 - val_loss: 0.4221 - val_acc: 0.8177\n",
      "Epoch 7/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.3604 - acc: 0.8425 - val_loss: 0.4383 - val_acc: 0.8161\n",
      "Epoch 8/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.3434 - acc: 0.8491 - val_loss: 0.4286 - val_acc: 0.8161\n",
      "Epoch 9/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.3257 - acc: 0.8612 - val_loss: 0.4239 - val_acc: 0.8161\n",
      "Training with parameters {'batch_size': 10, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_74\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_79 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_203 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_129 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_204 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_130 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_205 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.5333 - acc: 0.7396 - val_loss: 0.4910 - val_acc: 0.7767\n",
      "Epoch 2/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.4432 - acc: 0.8030 - val_loss: 0.4321 - val_acc: 0.8177\n",
      "Epoch 3/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.4223 - acc: 0.8130 - val_loss: 0.4306 - val_acc: 0.8095\n",
      "Epoch 4/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.4100 - acc: 0.8214 - val_loss: 0.4229 - val_acc: 0.8194\n",
      "Epoch 5/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3941 - acc: 0.8287 - val_loss: 0.4282 - val_acc: 0.8194\n",
      "Epoch 6/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.3858 - acc: 0.8334 - val_loss: 0.4168 - val_acc: 0.8243\n",
      "Epoch 7/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3727 - acc: 0.8356 - val_loss: 0.4209 - val_acc: 0.8259\n",
      "Epoch 8/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3637 - acc: 0.8393 - val_loss: 0.4151 - val_acc: 0.8259\n",
      "Epoch 9/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3554 - acc: 0.8466 - val_loss: 0.4130 - val_acc: 0.8227\n",
      "Epoch 10/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3378 - acc: 0.8519 - val_loss: 0.4499 - val_acc: 0.8030\n",
      "Epoch 11/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3251 - acc: 0.8582 - val_loss: 0.4292 - val_acc: 0.8144\n",
      "Epoch 12/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3089 - acc: 0.8699 - val_loss: 0.4492 - val_acc: 0.8194\n",
      "Epoch 13/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.2992 - acc: 0.8719 - val_loss: 0.4430 - val_acc: 0.8112\n",
      "Epoch 14/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.2796 - acc: 0.8818 - val_loss: 0.4515 - val_acc: 0.8128\n",
      "Training with parameters {'batch_size': 10, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_75\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_80 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_206 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_131 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_207 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.5403 - acc: 0.7482 - val_loss: 0.4910 - val_acc: 0.7997\n",
      "Epoch 2/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.4660 - acc: 0.7916 - val_loss: 0.4519 - val_acc: 0.8046\n",
      "Epoch 3/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.4413 - acc: 0.8059 - val_loss: 0.4435 - val_acc: 0.8112\n",
      "Epoch 4/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.4301 - acc: 0.8101 - val_loss: 0.4337 - val_acc: 0.8194\n",
      "Epoch 5/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.4220 - acc: 0.8150 - val_loss: 0.4288 - val_acc: 0.8292\n",
      "Epoch 6/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.4134 - acc: 0.8181 - val_loss: 0.4342 - val_acc: 0.8095\n",
      "Epoch 7/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.4046 - acc: 0.8256 - val_loss: 0.4257 - val_acc: 0.8194\n",
      "Epoch 8/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.4022 - acc: 0.8243 - val_loss: 0.4276 - val_acc: 0.8112\n",
      "Epoch 9/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3942 - acc: 0.8281 - val_loss: 0.4281 - val_acc: 0.8112\n",
      "Epoch 10/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3896 - acc: 0.8300 - val_loss: 0.4196 - val_acc: 0.8144\n",
      "Epoch 11/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3867 - acc: 0.8290 - val_loss: 0.4196 - val_acc: 0.8177\n",
      "Epoch 12/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3811 - acc: 0.8371 - val_loss: 0.4231 - val_acc: 0.8177\n",
      "Epoch 13/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3784 - acc: 0.8343 - val_loss: 0.4194 - val_acc: 0.8161\n",
      "Epoch 14/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3733 - acc: 0.8405 - val_loss: 0.4230 - val_acc: 0.8144\n",
      "Epoch 15/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3688 - acc: 0.8405 - val_loss: 0.4212 - val_acc: 0.8112\n",
      "Epoch 16/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3646 - acc: 0.8413 - val_loss: 0.4182 - val_acc: 0.8046\n",
      "Epoch 17/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3627 - acc: 0.8400 - val_loss: 0.4181 - val_acc: 0.8177\n",
      "Epoch 18/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3583 - acc: 0.8413 - val_loss: 0.4191 - val_acc: 0.8161\n",
      "Epoch 19/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3545 - acc: 0.8440 - val_loss: 0.4165 - val_acc: 0.8128\n",
      "Epoch 20/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3488 - acc: 0.8498 - val_loss: 0.4178 - val_acc: 0.8128\n",
      "Epoch 21/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3463 - acc: 0.8511 - val_loss: 0.4181 - val_acc: 0.8161\n",
      "Epoch 22/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3429 - acc: 0.8497 - val_loss: 0.4195 - val_acc: 0.8177\n",
      "Epoch 23/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3397 - acc: 0.8497 - val_loss: 0.4217 - val_acc: 0.8095\n",
      "Epoch 24/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3343 - acc: 0.8588 - val_loss: 0.4194 - val_acc: 0.8112\n",
      "Training with parameters {'batch_size': 10, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_76\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_81 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_208 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_132 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_209 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_133 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_210 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.5159 - acc: 0.7508 - val_loss: 0.4592 - val_acc: 0.8079\n",
      "Epoch 2/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.4397 - acc: 0.8079 - val_loss: 0.4417 - val_acc: 0.7997\n",
      "Epoch 3/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.4169 - acc: 0.8190 - val_loss: 0.4191 - val_acc: 0.8259\n",
      "Epoch 4/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.4021 - acc: 0.8234 - val_loss: 0.4277 - val_acc: 0.8079\n",
      "Epoch 5/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.3956 - acc: 0.8298 - val_loss: 0.4292 - val_acc: 0.8144\n",
      "Epoch 6/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.3746 - acc: 0.8374 - val_loss: 0.4353 - val_acc: 0.8062\n",
      "Epoch 7/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.3682 - acc: 0.8398 - val_loss: 0.4263 - val_acc: 0.8128\n",
      "Epoch 8/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.3549 - acc: 0.8466 - val_loss: 0.4170 - val_acc: 0.8177\n",
      "Epoch 9/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.3402 - acc: 0.8502 - val_loss: 0.4207 - val_acc: 0.8227\n",
      "Epoch 10/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.3263 - acc: 0.8628 - val_loss: 0.4488 - val_acc: 0.8079\n",
      "Epoch 11/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.3099 - acc: 0.8683 - val_loss: 0.4405 - val_acc: 0.8013\n",
      "Epoch 12/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.2982 - acc: 0.8736 - val_loss: 0.4478 - val_acc: 0.8144\n",
      "Epoch 13/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.2748 - acc: 0.8805 - val_loss: 0.4554 - val_acc: 0.7997\n",
      "Training with parameters {'batch_size': 10, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_77\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_82 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_211 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_134 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_212 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_135 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_213 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.5506 - acc: 0.7285 - val_loss: 0.4737 - val_acc: 0.8013\n",
      "Epoch 2/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.4616 - acc: 0.7957 - val_loss: 0.4695 - val_acc: 0.7931\n",
      "Epoch 3/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.4313 - acc: 0.8126 - val_loss: 0.4308 - val_acc: 0.8177\n",
      "Epoch 4/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.4199 - acc: 0.8221 - val_loss: 0.4257 - val_acc: 0.8161\n",
      "Epoch 5/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.4092 - acc: 0.8223 - val_loss: 0.4215 - val_acc: 0.8194\n",
      "Epoch 6/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3996 - acc: 0.8254 - val_loss: 0.4211 - val_acc: 0.8128\n",
      "Epoch 7/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3955 - acc: 0.8259 - val_loss: 0.4149 - val_acc: 0.8177\n",
      "Epoch 8/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3864 - acc: 0.8320 - val_loss: 0.4169 - val_acc: 0.8128\n",
      "Epoch 9/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549/549 [==============================] - 1s 2ms/step - loss: 0.3794 - acc: 0.8373 - val_loss: 0.4293 - val_acc: 0.8128\n",
      "Epoch 10/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3718 - acc: 0.8385 - val_loss: 0.4662 - val_acc: 0.8046\n",
      "Epoch 11/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3657 - acc: 0.8478 - val_loss: 0.4205 - val_acc: 0.8161\n",
      "Epoch 12/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3584 - acc: 0.8477 - val_loss: 0.4246 - val_acc: 0.8144\n",
      "Training with parameters {'batch_size': 10, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_78\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_83 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_214 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_136 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_215 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_137 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_216 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.5454 - acc: 0.7336 - val_loss: 0.4638 - val_acc: 0.8030\n",
      "Epoch 2/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.4550 - acc: 0.7960 - val_loss: 0.4434 - val_acc: 0.8112\n",
      "Epoch 3/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.4312 - acc: 0.8110 - val_loss: 0.4418 - val_acc: 0.7997\n",
      "Epoch 4/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.4198 - acc: 0.8134 - val_loss: 0.4259 - val_acc: 0.8128\n",
      "Epoch 5/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.4077 - acc: 0.8247 - val_loss: 0.4304 - val_acc: 0.8128\n",
      "Epoch 6/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.4009 - acc: 0.8239 - val_loss: 0.4319 - val_acc: 0.8161\n",
      "Epoch 7/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3931 - acc: 0.8325 - val_loss: 0.4190 - val_acc: 0.8161\n",
      "Epoch 8/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3840 - acc: 0.8329 - val_loss: 0.4189 - val_acc: 0.8144\n",
      "Epoch 9/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3737 - acc: 0.8394 - val_loss: 0.4202 - val_acc: 0.8161\n",
      "Epoch 10/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3660 - acc: 0.8429 - val_loss: 0.4156 - val_acc: 0.8177\n",
      "Epoch 11/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3591 - acc: 0.8444 - val_loss: 0.4191 - val_acc: 0.8177\n",
      "Epoch 12/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3506 - acc: 0.8473 - val_loss: 0.4256 - val_acc: 0.8062\n",
      "Epoch 13/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3424 - acc: 0.8500 - val_loss: 0.4204 - val_acc: 0.8144\n",
      "Epoch 14/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.3311 - acc: 0.8555 - val_loss: 0.4225 - val_acc: 0.8062\n",
      "Epoch 15/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3223 - acc: 0.8599 - val_loss: 0.4341 - val_acc: 0.8194\n",
      "Training with parameters {'batch_size': 10, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_79\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_84 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_217 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_138 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_218 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_139 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_219 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 0.5239 - acc: 0.7451 - val_loss: 0.4644 - val_acc: 0.8062\n",
      "Epoch 2/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.4436 - acc: 0.8017 - val_loss: 0.4598 - val_acc: 0.8013\n",
      "Epoch 3/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.4225 - acc: 0.8121 - val_loss: 0.4332 - val_acc: 0.8128\n",
      "Epoch 4/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.4096 - acc: 0.8214 - val_loss: 0.4217 - val_acc: 0.8177\n",
      "Epoch 5/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3964 - acc: 0.8298 - val_loss: 0.4300 - val_acc: 0.8095\n",
      "Epoch 6/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3827 - acc: 0.8331 - val_loss: 0.4160 - val_acc: 0.8243\n",
      "Epoch 7/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3757 - acc: 0.8320 - val_loss: 0.4619 - val_acc: 0.7947\n",
      "Epoch 8/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3660 - acc: 0.8420 - val_loss: 0.4268 - val_acc: 0.8062\n",
      "Epoch 9/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3545 - acc: 0.8484 - val_loss: 0.4297 - val_acc: 0.8194\n",
      "Epoch 10/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3391 - acc: 0.8529 - val_loss: 0.4238 - val_acc: 0.8161\n",
      "Epoch 11/300\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 0.3294 - acc: 0.8595 - val_loss: 0.4257 - val_acc: 0.8144\n",
      "Training with parameters {'batch_size': 20, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_80\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_85 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_220 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_140 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_221 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_141 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_222 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 0.5322 - acc: 0.7415 - val_loss: 0.4539 - val_acc: 0.8128\n",
      "Epoch 2/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.4416 - acc: 0.8031 - val_loss: 0.4343 - val_acc: 0.8194\n",
      "Epoch 3/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.4199 - acc: 0.8174 - val_loss: 0.4217 - val_acc: 0.8259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.4084 - acc: 0.8227 - val_loss: 0.4192 - val_acc: 0.8194\n",
      "Epoch 5/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3979 - acc: 0.8252 - val_loss: 0.4170 - val_acc: 0.8227\n",
      "Epoch 6/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3831 - acc: 0.8320 - val_loss: 0.4153 - val_acc: 0.8309\n",
      "Epoch 7/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3756 - acc: 0.8415 - val_loss: 0.4120 - val_acc: 0.8325\n",
      "Epoch 8/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3641 - acc: 0.8398 - val_loss: 0.4222 - val_acc: 0.8161\n",
      "Epoch 9/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3484 - acc: 0.8484 - val_loss: 0.4638 - val_acc: 0.8062\n",
      "Epoch 10/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3367 - acc: 0.8515 - val_loss: 0.4435 - val_acc: 0.8046\n",
      "Epoch 11/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3285 - acc: 0.8604 - val_loss: 0.4358 - val_acc: 0.8177\n",
      "Epoch 12/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3070 - acc: 0.8701 - val_loss: 0.4591 - val_acc: 0.8161\n",
      "Training with parameters {'batch_size': 20, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_81\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_86 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_223 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_142 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_224 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_143 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_225 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 0.5599 - acc: 0.7188 - val_loss: 0.4721 - val_acc: 0.7931\n",
      "Epoch 2/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.4547 - acc: 0.7973 - val_loss: 0.4417 - val_acc: 0.8144\n",
      "Epoch 3/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.4297 - acc: 0.8137 - val_loss: 0.4288 - val_acc: 0.8210\n",
      "Epoch 4/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.4173 - acc: 0.8172 - val_loss: 0.4284 - val_acc: 0.8161\n",
      "Epoch 5/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.4043 - acc: 0.8212 - val_loss: 0.4247 - val_acc: 0.8210\n",
      "Epoch 6/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3951 - acc: 0.8311 - val_loss: 0.4558 - val_acc: 0.8062\n",
      "Epoch 7/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3926 - acc: 0.8289 - val_loss: 0.4215 - val_acc: 0.8243\n",
      "Epoch 8/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3810 - acc: 0.8349 - val_loss: 0.4205 - val_acc: 0.8144\n",
      "Epoch 9/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3739 - acc: 0.8338 - val_loss: 0.4160 - val_acc: 0.8177\n",
      "Epoch 10/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3642 - acc: 0.8418 - val_loss: 0.4285 - val_acc: 0.8112\n",
      "Epoch 11/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3503 - acc: 0.8486 - val_loss: 0.4303 - val_acc: 0.8144\n",
      "Epoch 12/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3438 - acc: 0.8489 - val_loss: 0.4373 - val_acc: 0.7980\n",
      "Epoch 13/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3287 - acc: 0.8593 - val_loss: 0.4297 - val_acc: 0.8210\n",
      "Epoch 14/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3242 - acc: 0.8630 - val_loss: 0.4311 - val_acc: 0.8161\n",
      "Training with parameters {'batch_size': 20, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_82\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_87 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_226 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_144 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_227 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 0.5594 - acc: 0.7307 - val_loss: 0.4962 - val_acc: 0.7849\n",
      "Epoch 2/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.4837 - acc: 0.7843 - val_loss: 0.4635 - val_acc: 0.8046\n",
      "Epoch 3/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.4549 - acc: 0.8028 - val_loss: 0.4494 - val_acc: 0.8128\n",
      "Epoch 4/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.4427 - acc: 0.8110 - val_loss: 0.4534 - val_acc: 0.8079\n",
      "Epoch 5/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.4305 - acc: 0.8143 - val_loss: 0.4491 - val_acc: 0.8046\n",
      "Epoch 6/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.4253 - acc: 0.8155 - val_loss: 0.4357 - val_acc: 0.8144\n",
      "Epoch 7/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.4174 - acc: 0.8183 - val_loss: 0.4317 - val_acc: 0.8144\n",
      "Epoch 8/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.4118 - acc: 0.8223 - val_loss: 0.4293 - val_acc: 0.8161\n",
      "Epoch 9/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.4063 - acc: 0.8234 - val_loss: 0.4328 - val_acc: 0.8030\n",
      "Epoch 10/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.4005 - acc: 0.8243 - val_loss: 0.4235 - val_acc: 0.8292\n",
      "Epoch 11/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3964 - acc: 0.8261 - val_loss: 0.4231 - val_acc: 0.8161\n",
      "Epoch 12/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3937 - acc: 0.8252 - val_loss: 0.4211 - val_acc: 0.8177\n",
      "Epoch 13/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3908 - acc: 0.8269 - val_loss: 0.4196 - val_acc: 0.8227\n",
      "Epoch 14/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3872 - acc: 0.8365 - val_loss: 0.4258 - val_acc: 0.8128\n",
      "Epoch 15/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3822 - acc: 0.8345 - val_loss: 0.4167 - val_acc: 0.8194\n",
      "Epoch 16/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3796 - acc: 0.8351 - val_loss: 0.4235 - val_acc: 0.8128\n",
      "Epoch 17/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3746 - acc: 0.8351 - val_loss: 0.4286 - val_acc: 0.8079\n",
      "Epoch 18/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3735 - acc: 0.8391 - val_loss: 0.4233 - val_acc: 0.8095\n",
      "Epoch 19/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3706 - acc: 0.8373 - val_loss: 0.4166 - val_acc: 0.8161\n",
      "Epoch 20/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3677 - acc: 0.8400 - val_loss: 0.4182 - val_acc: 0.8128\n",
      "Epoch 21/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3672 - acc: 0.8380 - val_loss: 0.4178 - val_acc: 0.8177\n",
      "Epoch 22/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3619 - acc: 0.8400 - val_loss: 0.4166 - val_acc: 0.8194\n",
      "Epoch 23/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3580 - acc: 0.8442 - val_loss: 0.4185 - val_acc: 0.8177\n",
      "Epoch 24/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3559 - acc: 0.8422 - val_loss: 0.4226 - val_acc: 0.8144\n",
      "Epoch 25/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3531 - acc: 0.8442 - val_loss: 0.4180 - val_acc: 0.8128\n",
      "Epoch 26/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3522 - acc: 0.8466 - val_loss: 0.4158 - val_acc: 0.8161\n",
      "Epoch 27/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3474 - acc: 0.8482 - val_loss: 0.4288 - val_acc: 0.8095\n",
      "Epoch 28/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3457 - acc: 0.8500 - val_loss: 0.4174 - val_acc: 0.8194\n",
      "Epoch 29/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3429 - acc: 0.8475 - val_loss: 0.4270 - val_acc: 0.8128\n",
      "Epoch 30/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3407 - acc: 0.8524 - val_loss: 0.4184 - val_acc: 0.8161\n",
      "Epoch 31/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3391 - acc: 0.8586 - val_loss: 0.4205 - val_acc: 0.8128\n",
      "Training with parameters {'batch_size': 20, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_83\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_88 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_228 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_145 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_229 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_146 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_230 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 0.5447 - acc: 0.7327 - val_loss: 0.4738 - val_acc: 0.7915\n",
      "Epoch 2/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.4477 - acc: 0.7997 - val_loss: 0.4456 - val_acc: 0.8013\n",
      "Epoch 3/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.4234 - acc: 0.8134 - val_loss: 0.4242 - val_acc: 0.8276\n",
      "Epoch 4/300\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 0.4112 - acc: 0.8239 - val_loss: 0.4218 - val_acc: 0.8210\n",
      "Epoch 5/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3992 - acc: 0.8289 - val_loss: 0.4289 - val_acc: 0.8161\n",
      "Epoch 6/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3876 - acc: 0.8343 - val_loss: 0.4233 - val_acc: 0.8144\n",
      "Epoch 7/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3754 - acc: 0.8365 - val_loss: 0.4212 - val_acc: 0.8194\n",
      "Epoch 8/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3679 - acc: 0.8407 - val_loss: 0.4181 - val_acc: 0.8161\n",
      "Epoch 9/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3615 - acc: 0.8427 - val_loss: 0.4421 - val_acc: 0.8161\n",
      "Epoch 10/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3446 - acc: 0.8511 - val_loss: 0.4236 - val_acc: 0.8259\n",
      "Epoch 11/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3303 - acc: 0.8577 - val_loss: 0.4235 - val_acc: 0.8227\n",
      "Epoch 12/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3215 - acc: 0.8623 - val_loss: 0.4267 - val_acc: 0.8227\n",
      "Epoch 13/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3057 - acc: 0.8741 - val_loss: 0.4293 - val_acc: 0.8128\n",
      "Training with parameters {'batch_size': 20, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_84\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_89 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_231 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_147 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_232 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_148 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_233 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 0.5791 - acc: 0.7081 - val_loss: 0.4897 - val_acc: 0.7898\n",
      "Epoch 2/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.4730 - acc: 0.7860 - val_loss: 0.4484 - val_acc: 0.8128\n",
      "Epoch 3/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.4414 - acc: 0.8048 - val_loss: 0.4334 - val_acc: 0.8227\n",
      "Epoch 4/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.4261 - acc: 0.8155 - val_loss: 0.4307 - val_acc: 0.8161\n",
      "Epoch 5/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.4140 - acc: 0.8225 - val_loss: 0.4299 - val_acc: 0.8046\n",
      "Epoch 6/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.4097 - acc: 0.8192 - val_loss: 0.4206 - val_acc: 0.8194\n",
      "Epoch 7/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.4021 - acc: 0.8234 - val_loss: 0.4199 - val_acc: 0.8177\n",
      "Epoch 8/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3949 - acc: 0.8296 - val_loss: 0.4238 - val_acc: 0.8079\n",
      "Epoch 9/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3873 - acc: 0.8312 - val_loss: 0.4204 - val_acc: 0.8079\n",
      "Epoch 10/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3833 - acc: 0.8394 - val_loss: 0.4250 - val_acc: 0.8112\n",
      "Epoch 11/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3751 - acc: 0.8405 - val_loss: 0.4238 - val_acc: 0.8128\n",
      "Epoch 12/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3737 - acc: 0.8433 - val_loss: 0.4155 - val_acc: 0.8161\n",
      "Epoch 13/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3628 - acc: 0.8433 - val_loss: 0.4150 - val_acc: 0.8177\n",
      "Epoch 14/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3592 - acc: 0.8451 - val_loss: 0.4168 - val_acc: 0.8194\n",
      "Epoch 15/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3533 - acc: 0.8504 - val_loss: 0.4186 - val_acc: 0.8161\n",
      "Epoch 16/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3482 - acc: 0.8528 - val_loss: 0.4411 - val_acc: 0.8144\n",
      "Epoch 17/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3415 - acc: 0.8540 - val_loss: 0.4267 - val_acc: 0.8112\n",
      "Epoch 18/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3307 - acc: 0.8623 - val_loss: 0.4256 - val_acc: 0.8161\n",
      "Training with parameters {'batch_size': 20, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_85\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_90 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_234 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_149 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_235 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_150 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_236 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/275 [==============================] - 1s 2ms/step - loss: 0.5758 - acc: 0.7157 - val_loss: 0.5031 - val_acc: 0.7750\n",
      "Epoch 2/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.4699 - acc: 0.7904 - val_loss: 0.4539 - val_acc: 0.8046\n",
      "Epoch 3/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.4450 - acc: 0.7993 - val_loss: 0.4391 - val_acc: 0.8112\n",
      "Epoch 4/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.4311 - acc: 0.8101 - val_loss: 0.4373 - val_acc: 0.8128\n",
      "Epoch 5/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.4210 - acc: 0.8194 - val_loss: 0.4343 - val_acc: 0.8079\n",
      "Epoch 6/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.4097 - acc: 0.8221 - val_loss: 0.4241 - val_acc: 0.8112\n",
      "Epoch 7/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3983 - acc: 0.8285 - val_loss: 0.4324 - val_acc: 0.8030\n",
      "Epoch 8/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3969 - acc: 0.8248 - val_loss: 0.4190 - val_acc: 0.8177\n",
      "Epoch 9/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3897 - acc: 0.8321 - val_loss: 0.4419 - val_acc: 0.8030\n",
      "Epoch 10/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3857 - acc: 0.8300 - val_loss: 0.4251 - val_acc: 0.8095\n",
      "Epoch 11/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3768 - acc: 0.8394 - val_loss: 0.4178 - val_acc: 0.8144\n",
      "Epoch 12/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3704 - acc: 0.8415 - val_loss: 0.4172 - val_acc: 0.8161\n",
      "Epoch 13/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3655 - acc: 0.8404 - val_loss: 0.4230 - val_acc: 0.8095\n",
      "Epoch 14/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3558 - acc: 0.8451 - val_loss: 0.4160 - val_acc: 0.8161\n",
      "Epoch 15/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3502 - acc: 0.8489 - val_loss: 0.4203 - val_acc: 0.8194\n",
      "Epoch 16/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3430 - acc: 0.8537 - val_loss: 0.4163 - val_acc: 0.8177\n",
      "Epoch 17/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3361 - acc: 0.8528 - val_loss: 0.4149 - val_acc: 0.8112\n",
      "Epoch 18/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3281 - acc: 0.8575 - val_loss: 0.4205 - val_acc: 0.8161\n",
      "Epoch 19/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3202 - acc: 0.8626 - val_loss: 0.4277 - val_acc: 0.8161\n",
      "Epoch 20/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3151 - acc: 0.8683 - val_loss: 0.4628 - val_acc: 0.8013\n",
      "Epoch 21/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3040 - acc: 0.8741 - val_loss: 0.4343 - val_acc: 0.8095\n",
      "Epoch 22/300\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 0.2997 - acc: 0.8719 - val_loss: 0.4304 - val_acc: 0.8062\n",
      "Training with parameters {'batch_size': 20, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_86\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_91 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_237 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_151 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_238 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_152 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_239 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 0.5580 - acc: 0.7203 - val_loss: 0.5136 - val_acc: 0.7553\n",
      "Epoch 2/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.4552 - acc: 0.8009 - val_loss: 0.4423 - val_acc: 0.8095\n",
      "Epoch 3/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.4333 - acc: 0.8024 - val_loss: 0.4396 - val_acc: 0.8095\n",
      "Epoch 4/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.4193 - acc: 0.8154 - val_loss: 0.4263 - val_acc: 0.8144\n",
      "Epoch 5/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.4041 - acc: 0.8248 - val_loss: 0.4241 - val_acc: 0.8144\n",
      "Epoch 6/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3979 - acc: 0.8256 - val_loss: 0.4288 - val_acc: 0.8128\n",
      "Epoch 7/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3902 - acc: 0.8296 - val_loss: 0.4576 - val_acc: 0.8013\n",
      "Epoch 8/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3828 - acc: 0.8316 - val_loss: 0.4205 - val_acc: 0.8227\n",
      "Epoch 9/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3717 - acc: 0.8393 - val_loss: 0.4177 - val_acc: 0.8243\n",
      "Epoch 10/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3626 - acc: 0.8380 - val_loss: 0.4148 - val_acc: 0.8243\n",
      "Epoch 11/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3554 - acc: 0.8464 - val_loss: 0.4156 - val_acc: 0.8243\n",
      "Epoch 12/300\n",
      "275/275 [==============================] - 0s 2ms/step - loss: 0.3430 - acc: 0.8488 - val_loss: 0.4354 - val_acc: 0.8128\n",
      "Epoch 13/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3376 - acc: 0.8539 - val_loss: 0.4216 - val_acc: 0.8194\n",
      "Epoch 14/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3206 - acc: 0.8659 - val_loss: 0.4386 - val_acc: 0.8177\n",
      "Epoch 15/300\n",
      "275/275 [==============================] - 0s 1ms/step - loss: 0.3067 - acc: 0.8692 - val_loss: 0.4450 - val_acc: 0.8161\n",
      "Training with parameters {'batch_size': 50, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_87\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_92 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_240 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_153 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_241 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_154 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_242 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "110/110 [==============================] - 1s 3ms/step - loss: 0.5729 - acc: 0.7084 - val_loss: 0.4804 - val_acc: 0.7947\n",
      "Epoch 2/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4626 - acc: 0.7946 - val_loss: 0.4422 - val_acc: 0.8112\n",
      "Epoch 3/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4342 - acc: 0.8099 - val_loss: 0.4320 - val_acc: 0.8144\n",
      "Epoch 4/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4198 - acc: 0.8183 - val_loss: 0.4332 - val_acc: 0.8062\n",
      "Epoch 5/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4061 - acc: 0.8243 - val_loss: 0.4270 - val_acc: 0.8128\n",
      "Epoch 6/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3970 - acc: 0.8267 - val_loss: 0.4276 - val_acc: 0.8194\n",
      "Epoch 7/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3864 - acc: 0.8343 - val_loss: 0.4242 - val_acc: 0.8161\n",
      "Epoch 8/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3778 - acc: 0.8332 - val_loss: 0.4278 - val_acc: 0.8194\n",
      "Epoch 9/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3697 - acc: 0.8402 - val_loss: 0.4225 - val_acc: 0.8112\n",
      "Epoch 10/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3609 - acc: 0.8433 - val_loss: 0.4301 - val_acc: 0.8112\n",
      "Epoch 11/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3506 - acc: 0.8506 - val_loss: 0.4198 - val_acc: 0.8259\n",
      "Epoch 12/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3399 - acc: 0.8515 - val_loss: 0.4466 - val_acc: 0.8177\n",
      "Epoch 13/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3358 - acc: 0.8579 - val_loss: 0.4372 - val_acc: 0.8095\n",
      "Epoch 14/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3214 - acc: 0.8606 - val_loss: 0.4389 - val_acc: 0.8177\n",
      "Epoch 15/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3077 - acc: 0.8699 - val_loss: 0.4464 - val_acc: 0.8144\n",
      "Epoch 16/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3009 - acc: 0.8737 - val_loss: 0.4359 - val_acc: 0.8292\n",
      "Training with parameters {'batch_size': 50, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_88\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_93 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_243 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_155 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_244 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_156 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_245 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "110/110 [==============================] - 1s 2ms/step - loss: 0.6078 - acc: 0.6679 - val_loss: 0.5181 - val_acc: 0.7800\n",
      "Epoch 2/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4912 - acc: 0.7838 - val_loss: 0.4698 - val_acc: 0.8046\n",
      "Epoch 3/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4523 - acc: 0.8006 - val_loss: 0.4506 - val_acc: 0.8112\n",
      "Epoch 4/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4329 - acc: 0.8088 - val_loss: 0.4354 - val_acc: 0.8194\n",
      "Epoch 5/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4198 - acc: 0.8144 - val_loss: 0.4379 - val_acc: 0.8144\n",
      "Epoch 6/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4093 - acc: 0.8221 - val_loss: 0.4310 - val_acc: 0.8161\n",
      "Epoch 7/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4071 - acc: 0.8252 - val_loss: 0.4237 - val_acc: 0.8292\n",
      "Epoch 8/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3956 - acc: 0.8276 - val_loss: 0.4474 - val_acc: 0.8112\n",
      "Epoch 9/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3872 - acc: 0.8278 - val_loss: 0.4228 - val_acc: 0.8210\n",
      "Epoch 10/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3805 - acc: 0.8316 - val_loss: 0.4189 - val_acc: 0.8194\n",
      "Epoch 11/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3759 - acc: 0.8400 - val_loss: 0.4177 - val_acc: 0.8292\n",
      "Epoch 12/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3675 - acc: 0.8429 - val_loss: 0.4185 - val_acc: 0.8177\n",
      "Epoch 13/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3588 - acc: 0.8402 - val_loss: 0.4162 - val_acc: 0.8227\n",
      "Epoch 14/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3520 - acc: 0.8469 - val_loss: 0.4217 - val_acc: 0.8161\n",
      "Epoch 15/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3438 - acc: 0.8513 - val_loss: 0.4253 - val_acc: 0.8112\n",
      "Epoch 16/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3359 - acc: 0.8588 - val_loss: 0.4318 - val_acc: 0.8112\n",
      "Epoch 17/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3268 - acc: 0.8617 - val_loss: 0.4295 - val_acc: 0.8112\n",
      "Epoch 18/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3206 - acc: 0.8586 - val_loss: 0.4270 - val_acc: 0.8177\n",
      "Training with parameters {'batch_size': 50, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_89\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_94 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_246 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_157 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_247 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.5872 - acc: 0.7075 - val_loss: 0.5298 - val_acc: 0.7750\n",
      "Epoch 2/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.5120 - acc: 0.7721 - val_loss: 0.4912 - val_acc: 0.7849\n",
      "Epoch 3/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4796 - acc: 0.7911 - val_loss: 0.4725 - val_acc: 0.7980\n",
      "Epoch 4/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.4603 - acc: 0.7971 - val_loss: 0.4577 - val_acc: 0.8079\n",
      "Epoch 5/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.4472 - acc: 0.8041 - val_loss: 0.4492 - val_acc: 0.8095\n",
      "Epoch 6/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.4412 - acc: 0.8086 - val_loss: 0.4481 - val_acc: 0.8095\n",
      "Epoch 7/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4332 - acc: 0.8104 - val_loss: 0.4480 - val_acc: 0.8062\n",
      "Epoch 8/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.4260 - acc: 0.8124 - val_loss: 0.4368 - val_acc: 0.8161\n",
      "Epoch 9/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.4211 - acc: 0.8188 - val_loss: 0.4333 - val_acc: 0.8276\n",
      "Epoch 10/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.4177 - acc: 0.8172 - val_loss: 0.4413 - val_acc: 0.8046\n",
      "Epoch 11/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.4141 - acc: 0.8201 - val_loss: 0.4314 - val_acc: 0.8095\n",
      "Epoch 12/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4084 - acc: 0.8252 - val_loss: 0.4276 - val_acc: 0.8276\n",
      "Epoch 13/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.4063 - acc: 0.8238 - val_loss: 0.4265 - val_acc: 0.8325\n",
      "Epoch 14/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.4026 - acc: 0.8269 - val_loss: 0.4255 - val_acc: 0.8243\n",
      "Epoch 15/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3995 - acc: 0.8261 - val_loss: 0.4238 - val_acc: 0.8210\n",
      "Epoch 16/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3960 - acc: 0.8269 - val_loss: 0.4225 - val_acc: 0.8276\n",
      "Epoch 17/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3947 - acc: 0.8309 - val_loss: 0.4229 - val_acc: 0.8161\n",
      "Epoch 18/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3918 - acc: 0.8300 - val_loss: 0.4253 - val_acc: 0.8112\n",
      "Epoch 19/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3896 - acc: 0.8309 - val_loss: 0.4197 - val_acc: 0.8210\n",
      "Epoch 20/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3874 - acc: 0.8300 - val_loss: 0.4196 - val_acc: 0.8210\n",
      "Epoch 21/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3851 - acc: 0.8327 - val_loss: 0.4290 - val_acc: 0.8095\n",
      "Epoch 22/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3810 - acc: 0.8320 - val_loss: 0.4216 - val_acc: 0.8128\n",
      "Epoch 23/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3782 - acc: 0.8367 - val_loss: 0.4185 - val_acc: 0.8177\n",
      "Epoch 24/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3774 - acc: 0.8373 - val_loss: 0.4186 - val_acc: 0.8144\n",
      "Epoch 25/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3756 - acc: 0.8369 - val_loss: 0.4221 - val_acc: 0.8128\n",
      "Epoch 26/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3728 - acc: 0.8394 - val_loss: 0.4169 - val_acc: 0.8177\n",
      "Epoch 27/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3716 - acc: 0.8420 - val_loss: 0.4218 - val_acc: 0.8112\n",
      "Epoch 28/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3697 - acc: 0.8393 - val_loss: 0.4168 - val_acc: 0.8177\n",
      "Epoch 29/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3673 - acc: 0.8415 - val_loss: 0.4162 - val_acc: 0.8194\n",
      "Epoch 30/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3640 - acc: 0.8429 - val_loss: 0.4198 - val_acc: 0.8128\n",
      "Epoch 31/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3615 - acc: 0.8415 - val_loss: 0.4188 - val_acc: 0.8177\n",
      "Epoch 32/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3616 - acc: 0.8424 - val_loss: 0.4172 - val_acc: 0.8177\n",
      "Epoch 33/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3602 - acc: 0.8456 - val_loss: 0.4166 - val_acc: 0.8210\n",
      "Epoch 34/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3593 - acc: 0.8455 - val_loss: 0.4176 - val_acc: 0.8144\n",
      "Training with parameters {'batch_size': 50, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_90\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_95 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_248 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_158 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_249 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_159 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_250 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "110/110 [==============================] - 1s 2ms/step - loss: 0.5866 - acc: 0.6917 - val_loss: 0.4943 - val_acc: 0.7800\n",
      "Epoch 2/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4752 - acc: 0.7895 - val_loss: 0.4616 - val_acc: 0.7997\n",
      "Epoch 3/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4432 - acc: 0.7999 - val_loss: 0.4553 - val_acc: 0.7964\n",
      "Epoch 4/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4242 - acc: 0.8154 - val_loss: 0.4365 - val_acc: 0.8112\n",
      "Epoch 5/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4096 - acc: 0.8230 - val_loss: 0.4233 - val_acc: 0.8243\n",
      "Epoch 6/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4028 - acc: 0.8236 - val_loss: 0.4202 - val_acc: 0.8276\n",
      "Epoch 7/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3967 - acc: 0.8285 - val_loss: 0.4194 - val_acc: 0.8243\n",
      "Epoch 8/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3886 - acc: 0.8303 - val_loss: 0.4211 - val_acc: 0.8161\n",
      "Epoch 9/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3789 - acc: 0.8343 - val_loss: 0.4157 - val_acc: 0.8227\n",
      "Epoch 10/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3734 - acc: 0.8376 - val_loss: 0.4189 - val_acc: 0.8227\n",
      "Epoch 11/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3643 - acc: 0.8431 - val_loss: 0.4206 - val_acc: 0.8177\n",
      "Epoch 12/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3570 - acc: 0.8460 - val_loss: 0.4210 - val_acc: 0.8144\n",
      "Epoch 13/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3508 - acc: 0.8477 - val_loss: 0.4168 - val_acc: 0.8194\n",
      "Epoch 14/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3373 - acc: 0.8559 - val_loss: 0.4262 - val_acc: 0.8144\n",
      "Training with parameters {'batch_size': 50, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_91\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_96 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_251 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_160 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_252 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_161 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_253 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "110/110 [==============================] - 1s 2ms/step - loss: 0.6296 - acc: 0.6575 - val_loss: 0.5327 - val_acc: 0.7701\n",
      "Epoch 2/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.5091 - acc: 0.7707 - val_loss: 0.4788 - val_acc: 0.7964\n",
      "Epoch 3/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4732 - acc: 0.7913 - val_loss: 0.4521 - val_acc: 0.8079\n",
      "Epoch 4/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4500 - acc: 0.8013 - val_loss: 0.4420 - val_acc: 0.8046\n",
      "Epoch 5/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4324 - acc: 0.8115 - val_loss: 0.4350 - val_acc: 0.8112\n",
      "Epoch 6/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4267 - acc: 0.8108 - val_loss: 0.4312 - val_acc: 0.8161\n",
      "Epoch 7/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4127 - acc: 0.8197 - val_loss: 0.4273 - val_acc: 0.8128\n",
      "Epoch 8/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4125 - acc: 0.8207 - val_loss: 0.4323 - val_acc: 0.8046\n",
      "Epoch 9/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4033 - acc: 0.8254 - val_loss: 0.4282 - val_acc: 0.8030\n",
      "Epoch 10/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3974 - acc: 0.8265 - val_loss: 0.4202 - val_acc: 0.8177\n",
      "Epoch 11/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3916 - acc: 0.8325 - val_loss: 0.4187 - val_acc: 0.8144\n",
      "Epoch 12/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3881 - acc: 0.8301 - val_loss: 0.4178 - val_acc: 0.8161\n",
      "Epoch 13/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3851 - acc: 0.8351 - val_loss: 0.4174 - val_acc: 0.8161\n",
      "Epoch 14/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3809 - acc: 0.8318 - val_loss: 0.4179 - val_acc: 0.8144\n",
      "Epoch 15/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3747 - acc: 0.8378 - val_loss: 0.4156 - val_acc: 0.8227\n",
      "Epoch 16/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3684 - acc: 0.8442 - val_loss: 0.4218 - val_acc: 0.8128\n",
      "Epoch 17/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3661 - acc: 0.8438 - val_loss: 0.4170 - val_acc: 0.8161\n",
      "Epoch 18/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3600 - acc: 0.8446 - val_loss: 0.4247 - val_acc: 0.8095\n",
      "Epoch 19/300\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3563 - acc: 0.8442 - val_loss: 0.4188 - val_acc: 0.8194\n",
      "Epoch 20/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3557 - acc: 0.8506 - val_loss: 0.4203 - val_acc: 0.8144\n",
      "Training with parameters {'batch_size': 50, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_92\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_97 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_254 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_162 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_255 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_163 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_256 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "110/110 [==============================] - 1s 2ms/step - loss: 0.6283 - acc: 0.6554 - val_loss: 0.5501 - val_acc: 0.7668\n",
      "Epoch 2/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.5186 - acc: 0.7637 - val_loss: 0.4765 - val_acc: 0.8013\n",
      "Epoch 3/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4729 - acc: 0.7896 - val_loss: 0.4538 - val_acc: 0.8062\n",
      "Epoch 4/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4498 - acc: 0.8004 - val_loss: 0.4432 - val_acc: 0.8112\n",
      "Epoch 5/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4359 - acc: 0.8090 - val_loss: 0.4366 - val_acc: 0.8144\n",
      "Epoch 6/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4264 - acc: 0.8134 - val_loss: 0.4322 - val_acc: 0.8144\n",
      "Epoch 7/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4214 - acc: 0.8130 - val_loss: 0.4287 - val_acc: 0.8112\n",
      "Epoch 8/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4138 - acc: 0.8223 - val_loss: 0.4386 - val_acc: 0.8046\n",
      "Epoch 9/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4093 - acc: 0.8210 - val_loss: 0.4384 - val_acc: 0.8046\n",
      "Epoch 10/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4004 - acc: 0.8272 - val_loss: 0.4287 - val_acc: 0.8128\n",
      "Epoch 11/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3960 - acc: 0.8243 - val_loss: 0.4257 - val_acc: 0.8095\n",
      "Epoch 12/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3929 - acc: 0.8281 - val_loss: 0.4187 - val_acc: 0.8210\n",
      "Epoch 13/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3869 - acc: 0.8327 - val_loss: 0.4200 - val_acc: 0.8161\n",
      "Epoch 14/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3816 - acc: 0.8351 - val_loss: 0.4207 - val_acc: 0.8210\n",
      "Epoch 15/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3773 - acc: 0.8362 - val_loss: 0.4421 - val_acc: 0.7997\n",
      "Epoch 16/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3761 - acc: 0.8424 - val_loss: 0.4242 - val_acc: 0.8144\n",
      "Epoch 17/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3672 - acc: 0.8405 - val_loss: 0.4155 - val_acc: 0.8227\n",
      "Epoch 18/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3611 - acc: 0.8440 - val_loss: 0.4227 - val_acc: 0.8144\n",
      "Epoch 19/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3561 - acc: 0.8477 - val_loss: 0.4250 - val_acc: 0.8144\n",
      "Epoch 20/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3502 - acc: 0.8478 - val_loss: 0.4225 - val_acc: 0.8144\n",
      "Epoch 21/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3460 - acc: 0.8517 - val_loss: 0.4365 - val_acc: 0.8046\n",
      "Epoch 22/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3382 - acc: 0.8562 - val_loss: 0.4207 - val_acc: 0.8128\n",
      "Training with parameters {'batch_size': 50, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_93\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_98 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_257 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_164 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_258 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_165 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_259 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "110/110 [==============================] - 1s 2ms/step - loss: 0.6112 - acc: 0.6643 - val_loss: 0.5123 - val_acc: 0.7734\n",
      "Epoch 2/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4915 - acc: 0.7727 - val_loss: 0.4664 - val_acc: 0.8079\n",
      "Epoch 3/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4524 - acc: 0.8015 - val_loss: 0.4416 - val_acc: 0.8062\n",
      "Epoch 4/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4307 - acc: 0.8119 - val_loss: 0.4488 - val_acc: 0.8062\n",
      "Epoch 5/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4217 - acc: 0.8154 - val_loss: 0.4276 - val_acc: 0.8243\n",
      "Epoch 6/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4116 - acc: 0.8214 - val_loss: 0.4252 - val_acc: 0.8210\n",
      "Epoch 7/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4027 - acc: 0.8280 - val_loss: 0.4226 - val_acc: 0.8227\n",
      "Epoch 8/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3986 - acc: 0.8228 - val_loss: 0.4211 - val_acc: 0.8276\n",
      "Epoch 9/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3899 - acc: 0.8318 - val_loss: 0.4213 - val_acc: 0.8210\n",
      "Epoch 10/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3826 - acc: 0.8347 - val_loss: 0.4166 - val_acc: 0.8276\n",
      "Epoch 11/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3773 - acc: 0.8376 - val_loss: 0.4168 - val_acc: 0.8177\n",
      "Epoch 12/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3678 - acc: 0.8427 - val_loss: 0.4220 - val_acc: 0.8210\n",
      "Epoch 13/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3634 - acc: 0.8435 - val_loss: 0.4163 - val_acc: 0.8194\n",
      "Epoch 14/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3553 - acc: 0.8478 - val_loss: 0.4298 - val_acc: 0.8095\n",
      "Epoch 15/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3517 - acc: 0.8467 - val_loss: 0.4190 - val_acc: 0.8177\n",
      "Epoch 16/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3409 - acc: 0.8504 - val_loss: 0.4238 - val_acc: 0.8144\n",
      "Epoch 17/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3356 - acc: 0.8542 - val_loss: 0.4185 - val_acc: 0.8194\n",
      "Epoch 18/300\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3259 - acc: 0.8571 - val_loss: 0.4225 - val_acc: 0.8194\n",
      "Training with parameters {'batch_size': 70, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_94\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_99 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_260 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_166 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_261 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_167 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_262 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "79/79 [==============================] - 1s 3ms/step - loss: 0.5912 - acc: 0.6844 - val_loss: 0.4990 - val_acc: 0.7882\n",
      "Epoch 2/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4772 - acc: 0.7840 - val_loss: 0.4540 - val_acc: 0.7997\n",
      "Epoch 3/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4433 - acc: 0.8037 - val_loss: 0.4459 - val_acc: 0.8112\n",
      "Epoch 4/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4271 - acc: 0.8144 - val_loss: 0.4309 - val_acc: 0.8177\n",
      "Epoch 5/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4185 - acc: 0.8141 - val_loss: 0.4251 - val_acc: 0.8276\n",
      "Epoch 6/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4046 - acc: 0.8243 - val_loss: 0.4414 - val_acc: 0.8062\n",
      "Epoch 7/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3971 - acc: 0.8289 - val_loss: 0.4183 - val_acc: 0.8194\n",
      "Epoch 8/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3847 - acc: 0.8351 - val_loss: 0.4319 - val_acc: 0.8161\n",
      "Epoch 9/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3815 - acc: 0.8376 - val_loss: 0.4213 - val_acc: 0.8144\n",
      "Epoch 10/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3695 - acc: 0.8373 - val_loss: 0.4168 - val_acc: 0.8177\n",
      "Epoch 11/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3650 - acc: 0.8402 - val_loss: 0.4168 - val_acc: 0.8227\n",
      "Epoch 12/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3507 - acc: 0.8497 - val_loss: 0.4281 - val_acc: 0.8177\n",
      "Epoch 13/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3433 - acc: 0.8526 - val_loss: 0.4318 - val_acc: 0.8144\n",
      "Epoch 14/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3372 - acc: 0.8537 - val_loss: 0.4279 - val_acc: 0.8128\n",
      "Epoch 15/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3267 - acc: 0.8613 - val_loss: 0.4304 - val_acc: 0.8194\n",
      "Epoch 16/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3194 - acc: 0.8646 - val_loss: 0.4404 - val_acc: 0.8194\n",
      "Training with parameters {'batch_size': 70, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_95\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_100 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_263 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_168 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_264 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_169 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_265 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 0.6274 - acc: 0.6468 - val_loss: 0.5438 - val_acc: 0.7652\n",
      "Epoch 2/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.5124 - acc: 0.7652 - val_loss: 0.4702 - val_acc: 0.7964\n",
      "Epoch 3/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4669 - acc: 0.7929 - val_loss: 0.4492 - val_acc: 0.8128\n",
      "Epoch 4/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4444 - acc: 0.7997 - val_loss: 0.4404 - val_acc: 0.8128\n",
      "Epoch 5/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4289 - acc: 0.8128 - val_loss: 0.4403 - val_acc: 0.8177\n",
      "Epoch 6/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4154 - acc: 0.8165 - val_loss: 0.4344 - val_acc: 0.8128\n",
      "Epoch 7/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4122 - acc: 0.8223 - val_loss: 0.4335 - val_acc: 0.8128\n",
      "Epoch 8/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4039 - acc: 0.8216 - val_loss: 0.4279 - val_acc: 0.8144\n",
      "Epoch 9/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3997 - acc: 0.8234 - val_loss: 0.4408 - val_acc: 0.8095\n",
      "Epoch 10/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3912 - acc: 0.8329 - val_loss: 0.4237 - val_acc: 0.8210\n",
      "Epoch 11/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3865 - acc: 0.8351 - val_loss: 0.4216 - val_acc: 0.8243\n",
      "Epoch 12/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3781 - acc: 0.8349 - val_loss: 0.4235 - val_acc: 0.8194\n",
      "Epoch 13/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3747 - acc: 0.8376 - val_loss: 0.4276 - val_acc: 0.8128\n",
      "Epoch 14/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3704 - acc: 0.8373 - val_loss: 0.4324 - val_acc: 0.8095\n",
      "Epoch 15/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3607 - acc: 0.8451 - val_loss: 0.4199 - val_acc: 0.8243\n",
      "Epoch 16/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3533 - acc: 0.8475 - val_loss: 0.4197 - val_acc: 0.8227\n",
      "Epoch 17/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3482 - acc: 0.8515 - val_loss: 0.4242 - val_acc: 0.8161\n",
      "Epoch 18/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3375 - acc: 0.8540 - val_loss: 0.4375 - val_acc: 0.8079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3342 - acc: 0.8548 - val_loss: 0.4274 - val_acc: 0.8194\n",
      "Epoch 20/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3260 - acc: 0.8588 - val_loss: 0.4364 - val_acc: 0.7997\n",
      "Epoch 21/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3187 - acc: 0.8615 - val_loss: 0.4463 - val_acc: 0.8079\n",
      "Training with parameters {'batch_size': 70, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_96\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_101 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_266 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_170 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_267 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.6016 - acc: 0.6940 - val_loss: 0.5478 - val_acc: 0.7619\n",
      "Epoch 2/300\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5285 - acc: 0.7601 - val_loss: 0.5010 - val_acc: 0.7865\n",
      "Epoch 3/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4938 - acc: 0.7800 - val_loss: 0.4783 - val_acc: 0.7931\n",
      "Epoch 4/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4725 - acc: 0.7896 - val_loss: 0.4690 - val_acc: 0.8013\n",
      "Epoch 5/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4606 - acc: 0.8004 - val_loss: 0.4565 - val_acc: 0.8144\n",
      "Epoch 6/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4478 - acc: 0.8041 - val_loss: 0.4524 - val_acc: 0.8079\n",
      "Epoch 7/300\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.4398 - acc: 0.8121 - val_loss: 0.4447 - val_acc: 0.8144\n",
      "Epoch 8/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4352 - acc: 0.8101 - val_loss: 0.4445 - val_acc: 0.8095\n",
      "Epoch 9/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4301 - acc: 0.8165 - val_loss: 0.4392 - val_acc: 0.8112\n",
      "Epoch 10/300\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.4250 - acc: 0.8165 - val_loss: 0.4348 - val_acc: 0.8194\n",
      "Epoch 11/300\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.4202 - acc: 0.8139 - val_loss: 0.4329 - val_acc: 0.8227\n",
      "Epoch 12/300\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.4177 - acc: 0.8192 - val_loss: 0.4352 - val_acc: 0.8112\n",
      "Epoch 13/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4119 - acc: 0.8207 - val_loss: 0.4286 - val_acc: 0.8259\n",
      "Epoch 14/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4094 - acc: 0.8223 - val_loss: 0.4332 - val_acc: 0.8144\n",
      "Epoch 15/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4056 - acc: 0.8239 - val_loss: 0.4270 - val_acc: 0.8210\n",
      "Epoch 16/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4042 - acc: 0.8247 - val_loss: 0.4290 - val_acc: 0.8161\n",
      "Epoch 17/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4013 - acc: 0.8265 - val_loss: 0.4260 - val_acc: 0.8259\n",
      "Epoch 18/300\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3985 - acc: 0.8311 - val_loss: 0.4232 - val_acc: 0.8210\n",
      "Epoch 19/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3955 - acc: 0.8292 - val_loss: 0.4263 - val_acc: 0.8112\n",
      "Epoch 20/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3937 - acc: 0.8296 - val_loss: 0.4343 - val_acc: 0.8079\n",
      "Epoch 21/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3905 - acc: 0.8303 - val_loss: 0.4233 - val_acc: 0.8194\n",
      "Epoch 22/300\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3887 - acc: 0.8298 - val_loss: 0.4313 - val_acc: 0.8095\n",
      "Epoch 23/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3865 - acc: 0.8305 - val_loss: 0.4201 - val_acc: 0.8210\n",
      "Epoch 24/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3841 - acc: 0.8338 - val_loss: 0.4213 - val_acc: 0.8177\n",
      "Epoch 25/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3845 - acc: 0.8345 - val_loss: 0.4238 - val_acc: 0.8144\n",
      "Epoch 26/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3804 - acc: 0.8367 - val_loss: 0.4227 - val_acc: 0.8243\n",
      "Epoch 27/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3789 - acc: 0.8342 - val_loss: 0.4194 - val_acc: 0.8210\n",
      "Epoch 28/300\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3766 - acc: 0.8369 - val_loss: 0.4211 - val_acc: 0.8144\n",
      "Epoch 29/300\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3761 - acc: 0.8360 - val_loss: 0.4368 - val_acc: 0.8112\n",
      "Epoch 30/300\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3753 - acc: 0.8360 - val_loss: 0.4190 - val_acc: 0.8177\n",
      "Epoch 31/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3712 - acc: 0.8385 - val_loss: 0.4210 - val_acc: 0.8128\n",
      "Epoch 32/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3693 - acc: 0.8394 - val_loss: 0.4277 - val_acc: 0.8112\n",
      "Epoch 33/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3674 - acc: 0.8407 - val_loss: 0.4178 - val_acc: 0.8177\n",
      "Epoch 34/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3651 - acc: 0.8402 - val_loss: 0.4178 - val_acc: 0.8144\n",
      "Epoch 35/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3641 - acc: 0.8424 - val_loss: 0.4171 - val_acc: 0.8128\n",
      "Epoch 36/300\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3637 - acc: 0.8415 - val_loss: 0.4197 - val_acc: 0.8144\n",
      "Epoch 37/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3609 - acc: 0.8440 - val_loss: 0.4166 - val_acc: 0.8161\n",
      "Epoch 38/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3571 - acc: 0.8444 - val_loss: 0.4167 - val_acc: 0.8144\n",
      "Epoch 39/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3576 - acc: 0.8451 - val_loss: 0.4194 - val_acc: 0.8161\n",
      "Epoch 40/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3536 - acc: 0.8467 - val_loss: 0.4188 - val_acc: 0.8161\n",
      "Epoch 41/300\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3548 - acc: 0.8446 - val_loss: 0.4161 - val_acc: 0.8210\n",
      "Epoch 42/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3513 - acc: 0.8493 - val_loss: 0.4213 - val_acc: 0.8144\n",
      "Epoch 43/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3511 - acc: 0.8497 - val_loss: 0.4175 - val_acc: 0.8177\n",
      "Epoch 44/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3497 - acc: 0.8484 - val_loss: 0.4188 - val_acc: 0.8144\n",
      "Epoch 45/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3501 - acc: 0.8458 - val_loss: 0.4180 - val_acc: 0.8161\n",
      "Epoch 46/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3481 - acc: 0.8478 - val_loss: 0.4197 - val_acc: 0.8144\n",
      "Training with parameters {'batch_size': 70, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_97\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_102 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_268 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_171 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_269 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_172 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_270 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 1s 5ms/step - loss: 0.6095 - acc: 0.6676 - val_loss: 0.5133 - val_acc: 0.7783\n",
      "Epoch 2/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4912 - acc: 0.7800 - val_loss: 0.4602 - val_acc: 0.7947\n",
      "Epoch 3/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4507 - acc: 0.8055 - val_loss: 0.4465 - val_acc: 0.8128\n",
      "Epoch 4/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4330 - acc: 0.8088 - val_loss: 0.4310 - val_acc: 0.8194\n",
      "Epoch 5/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4224 - acc: 0.8176 - val_loss: 0.4254 - val_acc: 0.8276\n",
      "Epoch 6/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4160 - acc: 0.8203 - val_loss: 0.4315 - val_acc: 0.8079\n",
      "Epoch 7/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4033 - acc: 0.8298 - val_loss: 0.4233 - val_acc: 0.8144\n",
      "Epoch 8/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3970 - acc: 0.8305 - val_loss: 0.4228 - val_acc: 0.8276\n",
      "Epoch 9/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3916 - acc: 0.8320 - val_loss: 0.4196 - val_acc: 0.8177\n",
      "Epoch 10/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3809 - acc: 0.8358 - val_loss: 0.4169 - val_acc: 0.8210\n",
      "Epoch 11/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3737 - acc: 0.8391 - val_loss: 0.4450 - val_acc: 0.8112\n",
      "Epoch 12/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3695 - acc: 0.8404 - val_loss: 0.4157 - val_acc: 0.8227\n",
      "Epoch 13/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3616 - acc: 0.8442 - val_loss: 0.4222 - val_acc: 0.8194\n",
      "Epoch 14/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3560 - acc: 0.8462 - val_loss: 0.4165 - val_acc: 0.8259\n",
      "Epoch 15/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3477 - acc: 0.8524 - val_loss: 0.4213 - val_acc: 0.8227\n",
      "Epoch 16/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3402 - acc: 0.8509 - val_loss: 0.4508 - val_acc: 0.8112\n",
      "Epoch 17/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3323 - acc: 0.8564 - val_loss: 0.4189 - val_acc: 0.8243\n",
      "Training with parameters {'batch_size': 70, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_98\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_103 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_271 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_173 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_272 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_174 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_273 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 0.6477 - acc: 0.6402 - val_loss: 0.5626 - val_acc: 0.7685\n",
      "Epoch 2/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.5370 - acc: 0.7508 - val_loss: 0.4906 - val_acc: 0.7898\n",
      "Epoch 3/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4822 - acc: 0.7845 - val_loss: 0.4605 - val_acc: 0.8030\n",
      "Epoch 4/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4595 - acc: 0.7960 - val_loss: 0.4467 - val_acc: 0.8046\n",
      "Epoch 5/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4458 - acc: 0.7993 - val_loss: 0.4418 - val_acc: 0.8062\n",
      "Epoch 6/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4322 - acc: 0.8099 - val_loss: 0.4443 - val_acc: 0.8062\n",
      "Epoch 7/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4251 - acc: 0.8135 - val_loss: 0.4307 - val_acc: 0.8243\n",
      "Epoch 8/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4168 - acc: 0.8172 - val_loss: 0.4278 - val_acc: 0.8227\n",
      "Epoch 9/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4111 - acc: 0.8185 - val_loss: 0.4446 - val_acc: 0.8095\n",
      "Epoch 10/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4100 - acc: 0.8186 - val_loss: 0.4242 - val_acc: 0.8079\n",
      "Epoch 11/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4033 - acc: 0.8234 - val_loss: 0.4219 - val_acc: 0.8177\n",
      "Epoch 12/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3980 - acc: 0.8281 - val_loss: 0.4266 - val_acc: 0.8062\n",
      "Epoch 13/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3896 - acc: 0.8303 - val_loss: 0.4231 - val_acc: 0.8030\n",
      "Epoch 14/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3898 - acc: 0.8332 - val_loss: 0.4262 - val_acc: 0.8046\n",
      "Epoch 15/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3846 - acc: 0.8316 - val_loss: 0.4260 - val_acc: 0.8095\n",
      "Epoch 16/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3798 - acc: 0.8387 - val_loss: 0.4199 - val_acc: 0.8161\n",
      "Epoch 17/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3808 - acc: 0.8404 - val_loss: 0.4180 - val_acc: 0.8144\n",
      "Epoch 18/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3752 - acc: 0.8362 - val_loss: 0.4181 - val_acc: 0.8144\n",
      "Epoch 19/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3729 - acc: 0.8393 - val_loss: 0.4175 - val_acc: 0.8177\n",
      "Epoch 20/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3645 - acc: 0.8405 - val_loss: 0.4192 - val_acc: 0.8161\n",
      "Epoch 21/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3604 - acc: 0.8424 - val_loss: 0.4181 - val_acc: 0.8210\n",
      "Epoch 22/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3565 - acc: 0.8473 - val_loss: 0.4199 - val_acc: 0.8194\n",
      "Epoch 23/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3523 - acc: 0.8497 - val_loss: 0.4189 - val_acc: 0.8144\n",
      "Epoch 24/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3488 - acc: 0.8491 - val_loss: 0.4182 - val_acc: 0.8227\n",
      "Training with parameters {'batch_size': 70, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_99\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_104 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_274 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_175 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_275 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_176 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_276 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 0.6412 - acc: 0.6344 - val_loss: 0.5723 - val_acc: 0.7553\n",
      "Epoch 2/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.5357 - acc: 0.7475 - val_loss: 0.4927 - val_acc: 0.7947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4839 - acc: 0.7811 - val_loss: 0.4637 - val_acc: 0.8079\n",
      "Epoch 4/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4617 - acc: 0.7907 - val_loss: 0.4497 - val_acc: 0.8062\n",
      "Epoch 5/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4429 - acc: 0.8035 - val_loss: 0.4451 - val_acc: 0.8062\n",
      "Epoch 6/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4342 - acc: 0.8110 - val_loss: 0.4376 - val_acc: 0.8095\n",
      "Epoch 7/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4248 - acc: 0.8130 - val_loss: 0.4331 - val_acc: 0.8112\n",
      "Epoch 8/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4175 - acc: 0.8152 - val_loss: 0.4292 - val_acc: 0.8128\n",
      "Epoch 9/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4132 - acc: 0.8154 - val_loss: 0.4295 - val_acc: 0.8079\n",
      "Epoch 10/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4081 - acc: 0.8234 - val_loss: 0.4270 - val_acc: 0.8112\n",
      "Epoch 11/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4048 - acc: 0.8217 - val_loss: 0.4285 - val_acc: 0.8079\n",
      "Epoch 12/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3970 - acc: 0.8265 - val_loss: 0.4268 - val_acc: 0.8112\n",
      "Epoch 13/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3926 - acc: 0.8300 - val_loss: 0.4236 - val_acc: 0.8161\n",
      "Epoch 14/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3894 - acc: 0.8332 - val_loss: 0.4215 - val_acc: 0.8112\n",
      "Epoch 15/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3870 - acc: 0.8378 - val_loss: 0.4224 - val_acc: 0.8144\n",
      "Epoch 16/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3846 - acc: 0.8331 - val_loss: 0.4190 - val_acc: 0.8194\n",
      "Epoch 17/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3765 - acc: 0.8409 - val_loss: 0.4211 - val_acc: 0.8144\n",
      "Epoch 18/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3727 - acc: 0.8420 - val_loss: 0.4178 - val_acc: 0.8144\n",
      "Epoch 19/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3683 - acc: 0.8407 - val_loss: 0.4169 - val_acc: 0.8177\n",
      "Epoch 20/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3612 - acc: 0.8484 - val_loss: 0.4199 - val_acc: 0.8194\n",
      "Epoch 21/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3569 - acc: 0.8460 - val_loss: 0.4197 - val_acc: 0.8177\n",
      "Epoch 22/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3555 - acc: 0.8446 - val_loss: 0.4190 - val_acc: 0.8112\n",
      "Epoch 23/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3499 - acc: 0.8502 - val_loss: 0.4231 - val_acc: 0.8144\n",
      "Epoch 24/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3461 - acc: 0.8528 - val_loss: 0.4162 - val_acc: 0.8144\n",
      "Epoch 25/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3422 - acc: 0.8529 - val_loss: 0.4221 - val_acc: 0.8062\n",
      "Epoch 26/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3388 - acc: 0.8544 - val_loss: 0.4248 - val_acc: 0.8112\n",
      "Epoch 27/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3333 - acc: 0.8551 - val_loss: 0.4227 - val_acc: 0.8095\n",
      "Epoch 28/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3260 - acc: 0.8630 - val_loss: 0.4243 - val_acc: 0.8095\n",
      "Epoch 29/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3223 - acc: 0.8628 - val_loss: 0.4245 - val_acc: 0.8161\n",
      "Training with parameters {'batch_size': 70, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_100\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_105 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_277 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_177 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_278 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_178 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_279 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 0.6274 - acc: 0.6532 - val_loss: 0.5366 - val_acc: 0.7570\n",
      "Epoch 2/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.5115 - acc: 0.7639 - val_loss: 0.4708 - val_acc: 0.7947\n",
      "Epoch 3/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4639 - acc: 0.7953 - val_loss: 0.4476 - val_acc: 0.8062\n",
      "Epoch 4/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4430 - acc: 0.8004 - val_loss: 0.4413 - val_acc: 0.8095\n",
      "Epoch 5/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4287 - acc: 0.8132 - val_loss: 0.4356 - val_acc: 0.8161\n",
      "Epoch 6/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4209 - acc: 0.8117 - val_loss: 0.4312 - val_acc: 0.8194\n",
      "Epoch 7/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4133 - acc: 0.8186 - val_loss: 0.4266 - val_acc: 0.8161\n",
      "Epoch 8/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4070 - acc: 0.8216 - val_loss: 0.4245 - val_acc: 0.8243\n",
      "Epoch 9/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3996 - acc: 0.8256 - val_loss: 0.4228 - val_acc: 0.8259\n",
      "Epoch 10/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3915 - acc: 0.8301 - val_loss: 0.4242 - val_acc: 0.8144\n",
      "Epoch 11/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3849 - acc: 0.8352 - val_loss: 0.4364 - val_acc: 0.8095\n",
      "Epoch 12/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3847 - acc: 0.8351 - val_loss: 0.4191 - val_acc: 0.8243\n",
      "Epoch 13/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3719 - acc: 0.8398 - val_loss: 0.4212 - val_acc: 0.8194\n",
      "Epoch 14/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3663 - acc: 0.8424 - val_loss: 0.4198 - val_acc: 0.8227\n",
      "Epoch 15/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3623 - acc: 0.8435 - val_loss: 0.4277 - val_acc: 0.8128\n",
      "Epoch 16/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3511 - acc: 0.8467 - val_loss: 0.4308 - val_acc: 0.8128\n",
      "Epoch 17/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3495 - acc: 0.8473 - val_loss: 0.4171 - val_acc: 0.8177\n",
      "Epoch 18/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3372 - acc: 0.8560 - val_loss: 0.4169 - val_acc: 0.8227\n",
      "Epoch 19/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3323 - acc: 0.8579 - val_loss: 0.4280 - val_acc: 0.8095\n",
      "Epoch 20/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3246 - acc: 0.8608 - val_loss: 0.4628 - val_acc: 0.8112\n",
      "Epoch 21/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3190 - acc: 0.8626 - val_loss: 0.4213 - val_acc: 0.8177\n",
      "Epoch 22/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3053 - acc: 0.8714 - val_loss: 0.4340 - val_acc: 0.8161\n",
      "Epoch 23/300\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.3036 - acc: 0.8721 - val_loss: 0.4336 - val_acc: 0.8079\n",
      "Training with parameters {'batch_size': 200, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_101\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_106 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_280 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_179 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_281 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_180 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_282 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.6426 - acc: 0.6192 - val_loss: 0.5728 - val_acc: 0.7521\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5430 - acc: 0.7508 - val_loss: 0.4991 - val_acc: 0.7849\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4876 - acc: 0.7772 - val_loss: 0.4675 - val_acc: 0.7980\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4605 - acc: 0.7935 - val_loss: 0.4589 - val_acc: 0.8112\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4394 - acc: 0.8101 - val_loss: 0.4418 - val_acc: 0.8095\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4276 - acc: 0.8124 - val_loss: 0.4327 - val_acc: 0.8161\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4200 - acc: 0.8157 - val_loss: 0.4292 - val_acc: 0.8194\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4105 - acc: 0.8223 - val_loss: 0.4294 - val_acc: 0.8161\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4050 - acc: 0.8234 - val_loss: 0.4351 - val_acc: 0.8013\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3984 - acc: 0.8287 - val_loss: 0.4265 - val_acc: 0.8128\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3945 - acc: 0.8301 - val_loss: 0.4309 - val_acc: 0.8112\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3899 - acc: 0.8327 - val_loss: 0.4189 - val_acc: 0.8259\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3842 - acc: 0.8356 - val_loss: 0.4186 - val_acc: 0.8227\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3751 - acc: 0.8409 - val_loss: 0.4200 - val_acc: 0.8194\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3702 - acc: 0.8404 - val_loss: 0.4328 - val_acc: 0.8112\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3681 - acc: 0.8440 - val_loss: 0.4202 - val_acc: 0.8227\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3613 - acc: 0.8482 - val_loss: 0.4187 - val_acc: 0.8227\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3547 - acc: 0.8477 - val_loss: 0.4195 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': 200, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_102\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_107 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_283 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_181 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_284 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_182 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_285 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.6922 - acc: 0.5453 - val_loss: 0.6276 - val_acc: 0.6108\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5971 - acc: 0.7032 - val_loss: 0.5565 - val_acc: 0.7521\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5459 - acc: 0.7451 - val_loss: 0.5080 - val_acc: 0.7718\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5047 - acc: 0.7692 - val_loss: 0.4772 - val_acc: 0.8013\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4729 - acc: 0.7873 - val_loss: 0.4597 - val_acc: 0.7947\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4592 - acc: 0.7951 - val_loss: 0.4577 - val_acc: 0.8112\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4463 - acc: 0.8008 - val_loss: 0.4441 - val_acc: 0.8144\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4387 - acc: 0.8026 - val_loss: 0.4411 - val_acc: 0.8079\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4253 - acc: 0.8126 - val_loss: 0.4364 - val_acc: 0.8161\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4222 - acc: 0.8161 - val_loss: 0.4352 - val_acc: 0.8177\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4145 - acc: 0.8159 - val_loss: 0.4328 - val_acc: 0.8194\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4120 - acc: 0.8196 - val_loss: 0.4300 - val_acc: 0.8210\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4077 - acc: 0.8196 - val_loss: 0.4364 - val_acc: 0.8112\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4055 - acc: 0.8205 - val_loss: 0.4269 - val_acc: 0.8259\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3987 - acc: 0.8267 - val_loss: 0.4276 - val_acc: 0.8227\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3933 - acc: 0.8285 - val_loss: 0.4388 - val_acc: 0.8079\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3920 - acc: 0.8303 - val_loss: 0.4253 - val_acc: 0.8243\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3834 - acc: 0.8352 - val_loss: 0.4292 - val_acc: 0.8177\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3798 - acc: 0.8360 - val_loss: 0.4266 - val_acc: 0.8177\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3774 - acc: 0.8378 - val_loss: 0.4236 - val_acc: 0.8227\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3728 - acc: 0.8398 - val_loss: 0.4322 - val_acc: 0.8128\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3688 - acc: 0.8433 - val_loss: 0.4237 - val_acc: 0.8292\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3673 - acc: 0.8435 - val_loss: 0.4478 - val_acc: 0.8128\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3652 - acc: 0.8393 - val_loss: 0.4223 - val_acc: 0.8243\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3546 - acc: 0.8469 - val_loss: 0.4225 - val_acc: 0.8276\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3517 - acc: 0.8511 - val_loss: 0.4294 - val_acc: 0.8227\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3512 - acc: 0.8504 - val_loss: 0.4244 - val_acc: 0.8177\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3402 - acc: 0.8524 - val_loss: 0.4249 - val_acc: 0.8177\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3392 - acc: 0.8540 - val_loss: 0.4333 - val_acc: 0.8046\n",
      "Training with parameters {'batch_size': 200, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_103\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_108 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_286 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_183 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_287 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 5ms/step - loss: 0.6335 - acc: 0.6539 - val_loss: 0.5973 - val_acc: 0.7307\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5771 - acc: 0.7234 - val_loss: 0.5570 - val_acc: 0.7603\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5438 - acc: 0.7524 - val_loss: 0.5242 - val_acc: 0.7833\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5200 - acc: 0.7630 - val_loss: 0.5055 - val_acc: 0.7915\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5015 - acc: 0.7760 - val_loss: 0.4905 - val_acc: 0.7898\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4901 - acc: 0.7805 - val_loss: 0.4800 - val_acc: 0.7964\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4766 - acc: 0.7913 - val_loss: 0.4727 - val_acc: 0.7947\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4701 - acc: 0.7895 - val_loss: 0.4658 - val_acc: 0.8013\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4613 - acc: 0.8004 - val_loss: 0.4606 - val_acc: 0.8079\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4535 - acc: 0.7991 - val_loss: 0.4613 - val_acc: 0.8030\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4502 - acc: 0.8033 - val_loss: 0.4539 - val_acc: 0.8046\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4445 - acc: 0.8046 - val_loss: 0.4514 - val_acc: 0.8079\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4393 - acc: 0.8090 - val_loss: 0.4465 - val_acc: 0.8062\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4404 - acc: 0.8082 - val_loss: 0.4442 - val_acc: 0.8095\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4342 - acc: 0.8084 - val_loss: 0.4444 - val_acc: 0.8062\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4326 - acc: 0.8121 - val_loss: 0.4412 - val_acc: 0.8128\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4282 - acc: 0.8163 - val_loss: 0.4416 - val_acc: 0.8112\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4246 - acc: 0.8192 - val_loss: 0.4383 - val_acc: 0.8177\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4221 - acc: 0.8166 - val_loss: 0.4361 - val_acc: 0.8177\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4227 - acc: 0.8157 - val_loss: 0.4348 - val_acc: 0.8194\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4195 - acc: 0.8165 - val_loss: 0.4341 - val_acc: 0.8177\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4148 - acc: 0.8205 - val_loss: 0.4320 - val_acc: 0.8259\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4120 - acc: 0.8216 - val_loss: 0.4320 - val_acc: 0.8194\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4116 - acc: 0.8223 - val_loss: 0.4302 - val_acc: 0.8194\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4115 - acc: 0.8239 - val_loss: 0.4311 - val_acc: 0.8161\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4077 - acc: 0.8248 - val_loss: 0.4304 - val_acc: 0.8144\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4059 - acc: 0.8269 - val_loss: 0.4280 - val_acc: 0.8276\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4052 - acc: 0.8247 - val_loss: 0.4275 - val_acc: 0.8243\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4040 - acc: 0.8245 - val_loss: 0.4268 - val_acc: 0.8309\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4017 - acc: 0.8274 - val_loss: 0.4283 - val_acc: 0.8144\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4010 - acc: 0.8281 - val_loss: 0.4278 - val_acc: 0.8144\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3983 - acc: 0.8289 - val_loss: 0.4316 - val_acc: 0.8112\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3967 - acc: 0.8289 - val_loss: 0.4259 - val_acc: 0.8177\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3948 - acc: 0.8305 - val_loss: 0.4244 - val_acc: 0.8194\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3927 - acc: 0.8289 - val_loss: 0.4247 - val_acc: 0.8227\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3927 - acc: 0.8290 - val_loss: 0.4246 - val_acc: 0.8177\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3920 - acc: 0.8336 - val_loss: 0.4238 - val_acc: 0.8276\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3912 - acc: 0.8301 - val_loss: 0.4225 - val_acc: 0.8194\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3888 - acc: 0.8323 - val_loss: 0.4256 - val_acc: 0.8112\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3883 - acc: 0.8307 - val_loss: 0.4246 - val_acc: 0.8112\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3872 - acc: 0.8321 - val_loss: 0.4214 - val_acc: 0.8161\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3847 - acc: 0.8376 - val_loss: 0.4217 - val_acc: 0.8144\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3837 - acc: 0.8354 - val_loss: 0.4220 - val_acc: 0.8144\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3816 - acc: 0.8352 - val_loss: 0.4207 - val_acc: 0.8161\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3812 - acc: 0.8334 - val_loss: 0.4276 - val_acc: 0.8128\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3805 - acc: 0.8320 - val_loss: 0.4253 - val_acc: 0.8128\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3791 - acc: 0.8351 - val_loss: 0.4252 - val_acc: 0.8128\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3790 - acc: 0.8371 - val_loss: 0.4215 - val_acc: 0.8112\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3767 - acc: 0.8351 - val_loss: 0.4205 - val_acc: 0.8144\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3759 - acc: 0.8380 - val_loss: 0.4199 - val_acc: 0.8194\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3759 - acc: 0.8363 - val_loss: 0.4194 - val_acc: 0.8128\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3729 - acc: 0.8374 - val_loss: 0.4198 - val_acc: 0.8161\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3721 - acc: 0.8382 - val_loss: 0.4237 - val_acc: 0.8112\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3713 - acc: 0.8367 - val_loss: 0.4184 - val_acc: 0.8161\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3707 - acc: 0.8376 - val_loss: 0.4192 - val_acc: 0.8161\n",
      "Epoch 56/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3707 - acc: 0.8398 - val_loss: 0.4216 - val_acc: 0.8128\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3677 - acc: 0.8413 - val_loss: 0.4199 - val_acc: 0.8144\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3680 - acc: 0.8393 - val_loss: 0.4199 - val_acc: 0.8144\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3661 - acc: 0.8400 - val_loss: 0.4183 - val_acc: 0.8144\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3658 - acc: 0.8433 - val_loss: 0.4183 - val_acc: 0.8161\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3660 - acc: 0.8418 - val_loss: 0.4188 - val_acc: 0.8177\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3630 - acc: 0.8436 - val_loss: 0.4177 - val_acc: 0.8128\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3623 - acc: 0.8446 - val_loss: 0.4200 - val_acc: 0.8161\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3610 - acc: 0.8467 - val_loss: 0.4226 - val_acc: 0.8095\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3607 - acc: 0.8447 - val_loss: 0.4187 - val_acc: 0.8194\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3601 - acc: 0.8438 - val_loss: 0.4206 - val_acc: 0.8095\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3580 - acc: 0.8460 - val_loss: 0.4202 - val_acc: 0.8144\n",
      "Training with parameters {'batch_size': 200, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_104\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_109 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_288 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_184 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_289 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_185 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_290 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.6721 - acc: 0.5742 - val_loss: 0.5923 - val_acc: 0.7356\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5701 - acc: 0.7263 - val_loss: 0.5256 - val_acc: 0.7553\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5136 - acc: 0.7645 - val_loss: 0.4853 - val_acc: 0.7931\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4782 - acc: 0.7856 - val_loss: 0.4642 - val_acc: 0.7964\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4560 - acc: 0.8011 - val_loss: 0.4494 - val_acc: 0.8095\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4399 - acc: 0.8070 - val_loss: 0.4407 - val_acc: 0.8144\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4278 - acc: 0.8134 - val_loss: 0.4358 - val_acc: 0.8144\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4217 - acc: 0.8135 - val_loss: 0.4342 - val_acc: 0.8144\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4163 - acc: 0.8210 - val_loss: 0.4270 - val_acc: 0.8276\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4075 - acc: 0.8254 - val_loss: 0.4284 - val_acc: 0.8227\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4036 - acc: 0.8248 - val_loss: 0.4225 - val_acc: 0.8243\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3970 - acc: 0.8289 - val_loss: 0.4239 - val_acc: 0.8210\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3979 - acc: 0.8292 - val_loss: 0.4266 - val_acc: 0.8079\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3924 - acc: 0.8314 - val_loss: 0.4233 - val_acc: 0.8144\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3859 - acc: 0.8338 - val_loss: 0.4186 - val_acc: 0.8177\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3781 - acc: 0.8354 - val_loss: 0.4165 - val_acc: 0.8276\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3739 - acc: 0.8391 - val_loss: 0.4205 - val_acc: 0.8210\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3723 - acc: 0.8387 - val_loss: 0.4202 - val_acc: 0.8144\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3670 - acc: 0.8433 - val_loss: 0.4164 - val_acc: 0.8210\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3619 - acc: 0.8477 - val_loss: 0.4146 - val_acc: 0.8210\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3585 - acc: 0.8460 - val_loss: 0.4188 - val_acc: 0.8161\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3541 - acc: 0.8455 - val_loss: 0.4166 - val_acc: 0.8194\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3475 - acc: 0.8524 - val_loss: 0.4244 - val_acc: 0.8144\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3445 - acc: 0.8506 - val_loss: 0.4306 - val_acc: 0.8194\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3385 - acc: 0.8542 - val_loss: 0.4175 - val_acc: 0.8243\n",
      "Training with parameters {'batch_size': 200, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_105\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_110 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_291 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_186 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_292 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_187 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_293 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "28/28 [==============================] - 1s 10ms/step - loss: 0.7165 - acc: 0.5362 - val_loss: 0.6418 - val_acc: 0.6601\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6121 - acc: 0.7061 - val_loss: 0.5761 - val_acc: 0.7586\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5681 - acc: 0.7367 - val_loss: 0.5366 - val_acc: 0.7685\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5323 - acc: 0.7601 - val_loss: 0.5058 - val_acc: 0.7849\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5074 - acc: 0.7681 - val_loss: 0.4848 - val_acc: 0.7898\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4818 - acc: 0.7847 - val_loss: 0.4733 - val_acc: 0.8030\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4668 - acc: 0.7854 - val_loss: 0.4604 - val_acc: 0.8095\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4556 - acc: 0.7922 - val_loss: 0.4524 - val_acc: 0.8095\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4481 - acc: 0.8015 - val_loss: 0.4497 - val_acc: 0.8030\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4409 - acc: 0.8092 - val_loss: 0.4408 - val_acc: 0.8062\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4337 - acc: 0.8072 - val_loss: 0.4378 - val_acc: 0.8112\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4283 - acc: 0.8081 - val_loss: 0.4442 - val_acc: 0.8030\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4245 - acc: 0.8161 - val_loss: 0.4349 - val_acc: 0.8046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4180 - acc: 0.8183 - val_loss: 0.4309 - val_acc: 0.8079\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4158 - acc: 0.8205 - val_loss: 0.4275 - val_acc: 0.8210\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4113 - acc: 0.8177 - val_loss: 0.4270 - val_acc: 0.8128\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4079 - acc: 0.8230 - val_loss: 0.4293 - val_acc: 0.8030\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4063 - acc: 0.8201 - val_loss: 0.4235 - val_acc: 0.8259\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4022 - acc: 0.8250 - val_loss: 0.4235 - val_acc: 0.8177\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3986 - acc: 0.8247 - val_loss: 0.4315 - val_acc: 0.8062\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3975 - acc: 0.8283 - val_loss: 0.4211 - val_acc: 0.8194\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3907 - acc: 0.8300 - val_loss: 0.4276 - val_acc: 0.8079\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3879 - acc: 0.8331 - val_loss: 0.4210 - val_acc: 0.8177\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3890 - acc: 0.8307 - val_loss: 0.4207 - val_acc: 0.8194\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3867 - acc: 0.8332 - val_loss: 0.4243 - val_acc: 0.8062\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3801 - acc: 0.8365 - val_loss: 0.4197 - val_acc: 0.8144\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3817 - acc: 0.8354 - val_loss: 0.4194 - val_acc: 0.8144\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3768 - acc: 0.8367 - val_loss: 0.4194 - val_acc: 0.8177\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3752 - acc: 0.8387 - val_loss: 0.4189 - val_acc: 0.8144\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3702 - acc: 0.8358 - val_loss: 0.4207 - val_acc: 0.8144\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3702 - acc: 0.8449 - val_loss: 0.4177 - val_acc: 0.8144\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3668 - acc: 0.8438 - val_loss: 0.4208 - val_acc: 0.8144\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3646 - acc: 0.8475 - val_loss: 0.4180 - val_acc: 0.8210\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3598 - acc: 0.8478 - val_loss: 0.4234 - val_acc: 0.8144\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3598 - acc: 0.8467 - val_loss: 0.4195 - val_acc: 0.8161\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3530 - acc: 0.8478 - val_loss: 0.4255 - val_acc: 0.8161\n",
      "Training with parameters {'batch_size': 200, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_106\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_111 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_294 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_188 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_295 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_189 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_296 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.6848 - acc: 0.5700 - val_loss: 0.6422 - val_acc: 0.5961\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6154 - acc: 0.6882 - val_loss: 0.5851 - val_acc: 0.7537\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5727 - acc: 0.7307 - val_loss: 0.5425 - val_acc: 0.7734\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5315 - acc: 0.7588 - val_loss: 0.5076 - val_acc: 0.7947\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5026 - acc: 0.7732 - val_loss: 0.4831 - val_acc: 0.8046\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4830 - acc: 0.7854 - val_loss: 0.4754 - val_acc: 0.7931\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4686 - acc: 0.7885 - val_loss: 0.4732 - val_acc: 0.7997\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4574 - acc: 0.8009 - val_loss: 0.4572 - val_acc: 0.8030\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4516 - acc: 0.8006 - val_loss: 0.4493 - val_acc: 0.8095\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4439 - acc: 0.8075 - val_loss: 0.4480 - val_acc: 0.8062\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4362 - acc: 0.8095 - val_loss: 0.4413 - val_acc: 0.8144\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4317 - acc: 0.8082 - val_loss: 0.4380 - val_acc: 0.8144\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4247 - acc: 0.8132 - val_loss: 0.4358 - val_acc: 0.8144\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4221 - acc: 0.8148 - val_loss: 0.4344 - val_acc: 0.8161\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4185 - acc: 0.8154 - val_loss: 0.4323 - val_acc: 0.8144\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4123 - acc: 0.8192 - val_loss: 0.4334 - val_acc: 0.8079\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4120 - acc: 0.8239 - val_loss: 0.4320 - val_acc: 0.8079\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4083 - acc: 0.8210 - val_loss: 0.4306 - val_acc: 0.8112\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4055 - acc: 0.8269 - val_loss: 0.4265 - val_acc: 0.8194\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4036 - acc: 0.8238 - val_loss: 0.4269 - val_acc: 0.8128\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4004 - acc: 0.8243 - val_loss: 0.4270 - val_acc: 0.8144\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3924 - acc: 0.8320 - val_loss: 0.4259 - val_acc: 0.8161\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3940 - acc: 0.8280 - val_loss: 0.4257 - val_acc: 0.8128\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3889 - acc: 0.8325 - val_loss: 0.4221 - val_acc: 0.8210\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3899 - acc: 0.8294 - val_loss: 0.4252 - val_acc: 0.8128\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3844 - acc: 0.8325 - val_loss: 0.4226 - val_acc: 0.8161\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3815 - acc: 0.8367 - val_loss: 0.4211 - val_acc: 0.8177\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3787 - acc: 0.8358 - val_loss: 0.4262 - val_acc: 0.8128\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3745 - acc: 0.8384 - val_loss: 0.4208 - val_acc: 0.8128\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3717 - acc: 0.8376 - val_loss: 0.4205 - val_acc: 0.8177\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3729 - acc: 0.8396 - val_loss: 0.4197 - val_acc: 0.8194\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3678 - acc: 0.8396 - val_loss: 0.4216 - val_acc: 0.8128\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3657 - acc: 0.8407 - val_loss: 0.4271 - val_acc: 0.8144\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3613 - acc: 0.8442 - val_loss: 0.4233 - val_acc: 0.8112\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3569 - acc: 0.8444 - val_loss: 0.4299 - val_acc: 0.8161\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3597 - acc: 0.8460 - val_loss: 0.4227 - val_acc: 0.8095\n",
      "Training with parameters {'batch_size': 200, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_107\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_112 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_297 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_190 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_298 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_191 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_299 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.6905 - acc: 0.5494 - val_loss: 0.6285 - val_acc: 0.6026\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5976 - acc: 0.7086 - val_loss: 0.5568 - val_acc: 0.7488\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5438 - acc: 0.7497 - val_loss: 0.5062 - val_acc: 0.7783\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5054 - acc: 0.7685 - val_loss: 0.4823 - val_acc: 0.7964\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4767 - acc: 0.7873 - val_loss: 0.4626 - val_acc: 0.8013\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4581 - acc: 0.7944 - val_loss: 0.4599 - val_acc: 0.8095\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4460 - acc: 0.7977 - val_loss: 0.4444 - val_acc: 0.8095\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4361 - acc: 0.8066 - val_loss: 0.4402 - val_acc: 0.8095\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4288 - acc: 0.8104 - val_loss: 0.4356 - val_acc: 0.8112\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4237 - acc: 0.8099 - val_loss: 0.4360 - val_acc: 0.8161\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4155 - acc: 0.8176 - val_loss: 0.4377 - val_acc: 0.8144\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4111 - acc: 0.8190 - val_loss: 0.4312 - val_acc: 0.8161\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4033 - acc: 0.8228 - val_loss: 0.4315 - val_acc: 0.8144\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4014 - acc: 0.8269 - val_loss: 0.4270 - val_acc: 0.8177\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3987 - acc: 0.8247 - val_loss: 0.4256 - val_acc: 0.8243\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3940 - acc: 0.8283 - val_loss: 0.4261 - val_acc: 0.8161\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3901 - acc: 0.8318 - val_loss: 0.4262 - val_acc: 0.8177\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3837 - acc: 0.8354 - val_loss: 0.4322 - val_acc: 0.8144\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3802 - acc: 0.8365 - val_loss: 0.4224 - val_acc: 0.8259\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3782 - acc: 0.8369 - val_loss: 0.4246 - val_acc: 0.8144\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3733 - acc: 0.8369 - val_loss: 0.4238 - val_acc: 0.8243\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3693 - acc: 0.8402 - val_loss: 0.4275 - val_acc: 0.8128\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3659 - acc: 0.8451 - val_loss: 0.4223 - val_acc: 0.8227\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3626 - acc: 0.8436 - val_loss: 0.4286 - val_acc: 0.8144\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3571 - acc: 0.8429 - val_loss: 0.4303 - val_acc: 0.8144\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3553 - acc: 0.8424 - val_loss: 0.4218 - val_acc: 0.8243\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3491 - acc: 0.8467 - val_loss: 0.4244 - val_acc: 0.8243\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3453 - acc: 0.8509 - val_loss: 0.4227 - val_acc: 0.8243\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3396 - acc: 0.8524 - val_loss: 0.4245 - val_acc: 0.8259\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3315 - acc: 0.8597 - val_loss: 0.4243 - val_acc: 0.8259\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3303 - acc: 0.8546 - val_loss: 0.4366 - val_acc: 0.8177\n",
      "Training with parameters {'batch_size': 350, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_108\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_113 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_300 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_192 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_301 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_193 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_302 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6844 - acc: 0.5380 - val_loss: 0.6318 - val_acc: 0.6092\n",
      "Epoch 2/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6017 - acc: 0.6951 - val_loss: 0.5638 - val_acc: 0.7537\n",
      "Epoch 3/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5502 - acc: 0.7453 - val_loss: 0.5172 - val_acc: 0.7734\n",
      "Epoch 4/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5087 - acc: 0.7718 - val_loss: 0.4851 - val_acc: 0.7898\n",
      "Epoch 5/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4794 - acc: 0.7893 - val_loss: 0.4697 - val_acc: 0.7997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4586 - acc: 0.7971 - val_loss: 0.4568 - val_acc: 0.8095\n",
      "Epoch 7/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4431 - acc: 0.8066 - val_loss: 0.4454 - val_acc: 0.8079\n",
      "Epoch 8/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4352 - acc: 0.8073 - val_loss: 0.4425 - val_acc: 0.8144\n",
      "Epoch 9/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4250 - acc: 0.8155 - val_loss: 0.4398 - val_acc: 0.8128\n",
      "Epoch 10/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4196 - acc: 0.8216 - val_loss: 0.4405 - val_acc: 0.8112\n",
      "Epoch 11/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4170 - acc: 0.8161 - val_loss: 0.4286 - val_acc: 0.8227\n",
      "Epoch 12/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4110 - acc: 0.8241 - val_loss: 0.4276 - val_acc: 0.8177\n",
      "Epoch 13/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4064 - acc: 0.8263 - val_loss: 0.4314 - val_acc: 0.8079\n",
      "Epoch 14/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4008 - acc: 0.8270 - val_loss: 0.4248 - val_acc: 0.8177\n",
      "Epoch 15/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3925 - acc: 0.8311 - val_loss: 0.4220 - val_acc: 0.8227\n",
      "Epoch 16/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3894 - acc: 0.8307 - val_loss: 0.4219 - val_acc: 0.8227\n",
      "Epoch 17/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3856 - acc: 0.8342 - val_loss: 0.4236 - val_acc: 0.8194\n",
      "Epoch 18/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3813 - acc: 0.8378 - val_loss: 0.4278 - val_acc: 0.8144\n",
      "Epoch 19/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3812 - acc: 0.8347 - val_loss: 0.4241 - val_acc: 0.8161\n",
      "Epoch 20/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3740 - acc: 0.8413 - val_loss: 0.4205 - val_acc: 0.8210\n",
      "Epoch 21/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3709 - acc: 0.8409 - val_loss: 0.4239 - val_acc: 0.8177\n",
      "Epoch 22/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3648 - acc: 0.8416 - val_loss: 0.4198 - val_acc: 0.8227\n",
      "Epoch 23/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3642 - acc: 0.8447 - val_loss: 0.4223 - val_acc: 0.8177\n",
      "Epoch 24/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3586 - acc: 0.8446 - val_loss: 0.4302 - val_acc: 0.8177\n",
      "Epoch 25/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3543 - acc: 0.8489 - val_loss: 0.4210 - val_acc: 0.8210\n",
      "Epoch 26/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3495 - acc: 0.8506 - val_loss: 0.4219 - val_acc: 0.8177\n",
      "Epoch 27/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3424 - acc: 0.8539 - val_loss: 0.4251 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': 350, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_109\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_114 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_303 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_194 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_304 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_195 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_305 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.7288 - acc: 0.5010 - val_loss: 0.6620 - val_acc: 0.5550\n",
      "Epoch 2/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6398 - acc: 0.6243 - val_loss: 0.6085 - val_acc: 0.7159\n",
      "Epoch 3/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5928 - acc: 0.7101 - val_loss: 0.5657 - val_acc: 0.7504\n",
      "Epoch 4/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5639 - acc: 0.7338 - val_loss: 0.5328 - val_acc: 0.7619\n",
      "Epoch 5/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5308 - acc: 0.7488 - val_loss: 0.5037 - val_acc: 0.7833\n",
      "Epoch 6/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5081 - acc: 0.7663 - val_loss: 0.4847 - val_acc: 0.7980\n",
      "Epoch 7/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4858 - acc: 0.7783 - val_loss: 0.4719 - val_acc: 0.7997\n",
      "Epoch 8/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4649 - acc: 0.7957 - val_loss: 0.4616 - val_acc: 0.8046\n",
      "Epoch 9/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4547 - acc: 0.7988 - val_loss: 0.4511 - val_acc: 0.8095\n",
      "Epoch 10/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4485 - acc: 0.7984 - val_loss: 0.4455 - val_acc: 0.8112\n",
      "Epoch 11/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4400 - acc: 0.8008 - val_loss: 0.4432 - val_acc: 0.8095\n",
      "Epoch 12/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4310 - acc: 0.8106 - val_loss: 0.4405 - val_acc: 0.8128\n",
      "Epoch 13/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4275 - acc: 0.8121 - val_loss: 0.4364 - val_acc: 0.8177\n",
      "Epoch 14/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4202 - acc: 0.8157 - val_loss: 0.4369 - val_acc: 0.8161\n",
      "Epoch 15/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4163 - acc: 0.8155 - val_loss: 0.4349 - val_acc: 0.8177\n",
      "Epoch 16/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4133 - acc: 0.8192 - val_loss: 0.4351 - val_acc: 0.8161\n",
      "Epoch 17/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4104 - acc: 0.8196 - val_loss: 0.4307 - val_acc: 0.8194\n",
      "Epoch 18/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4062 - acc: 0.8216 - val_loss: 0.4286 - val_acc: 0.8177\n",
      "Epoch 19/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4028 - acc: 0.8245 - val_loss: 0.4321 - val_acc: 0.8128\n",
      "Epoch 20/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3981 - acc: 0.8274 - val_loss: 0.4338 - val_acc: 0.8079\n",
      "Epoch 21/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4002 - acc: 0.8245 - val_loss: 0.4301 - val_acc: 0.8112\n",
      "Epoch 22/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3931 - acc: 0.8289 - val_loss: 0.4238 - val_acc: 0.8259\n",
      "Epoch 23/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3887 - acc: 0.8340 - val_loss: 0.4263 - val_acc: 0.8177\n",
      "Epoch 24/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3862 - acc: 0.8331 - val_loss: 0.4240 - val_acc: 0.8194\n",
      "Epoch 25/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3840 - acc: 0.8342 - val_loss: 0.4269 - val_acc: 0.8128\n",
      "Epoch 26/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3805 - acc: 0.8329 - val_loss: 0.4230 - val_acc: 0.8210\n",
      "Epoch 27/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3770 - acc: 0.8373 - val_loss: 0.4269 - val_acc: 0.8161\n",
      "Epoch 28/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3777 - acc: 0.8352 - val_loss: 0.4253 - val_acc: 0.8210\n",
      "Epoch 29/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3699 - acc: 0.8400 - val_loss: 0.4218 - val_acc: 0.8276\n",
      "Epoch 30/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3685 - acc: 0.8415 - val_loss: 0.4233 - val_acc: 0.8210\n",
      "Epoch 31/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3691 - acc: 0.8396 - val_loss: 0.4225 - val_acc: 0.8243\n",
      "Epoch 32/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3628 - acc: 0.8444 - val_loss: 0.4248 - val_acc: 0.8227\n",
      "Epoch 33/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3586 - acc: 0.8431 - val_loss: 0.4217 - val_acc: 0.8243\n",
      "Epoch 34/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3593 - acc: 0.8442 - val_loss: 0.4216 - val_acc: 0.8227\n",
      "Epoch 35/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3523 - acc: 0.8475 - val_loss: 0.4269 - val_acc: 0.8227\n",
      "Epoch 36/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3487 - acc: 0.8500 - val_loss: 0.4224 - val_acc: 0.8259\n",
      "Epoch 37/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3437 - acc: 0.8528 - val_loss: 0.4223 - val_acc: 0.8243\n",
      "Epoch 38/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3414 - acc: 0.8566 - val_loss: 0.4344 - val_acc: 0.8128\n",
      "Epoch 39/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3395 - acc: 0.8555 - val_loss: 0.4250 - val_acc: 0.8243\n",
      "Training with parameters {'batch_size': 350, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_110\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_115 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_306 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_196 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_307 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6463 - acc: 0.6344 - val_loss: 0.6201 - val_acc: 0.6897\n",
      "Epoch 2/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6030 - acc: 0.7079 - val_loss: 0.5826 - val_acc: 0.7504\n",
      "Epoch 3/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5746 - acc: 0.7278 - val_loss: 0.5578 - val_acc: 0.7570\n",
      "Epoch 4/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5499 - acc: 0.7444 - val_loss: 0.5353 - val_acc: 0.7767\n",
      "Epoch 5/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5322 - acc: 0.7581 - val_loss: 0.5196 - val_acc: 0.7833\n",
      "Epoch 6/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5162 - acc: 0.7723 - val_loss: 0.5070 - val_acc: 0.7865\n",
      "Epoch 7/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5059 - acc: 0.7723 - val_loss: 0.4968 - val_acc: 0.7898\n",
      "Epoch 8/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4940 - acc: 0.7842 - val_loss: 0.4874 - val_acc: 0.7964\n",
      "Epoch 9/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4863 - acc: 0.7867 - val_loss: 0.4811 - val_acc: 0.7882\n",
      "Epoch 10/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4783 - acc: 0.7887 - val_loss: 0.4755 - val_acc: 0.7915\n",
      "Epoch 11/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4713 - acc: 0.7968 - val_loss: 0.4704 - val_acc: 0.7947\n",
      "Epoch 12/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4655 - acc: 0.7955 - val_loss: 0.4660 - val_acc: 0.7931\n",
      "Epoch 13/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4597 - acc: 0.8004 - val_loss: 0.4626 - val_acc: 0.8030\n",
      "Epoch 14/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4552 - acc: 0.8006 - val_loss: 0.4578 - val_acc: 0.8079\n",
      "Epoch 15/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4514 - acc: 0.8048 - val_loss: 0.4566 - val_acc: 0.8079\n",
      "Epoch 16/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4485 - acc: 0.8057 - val_loss: 0.4549 - val_acc: 0.8062\n",
      "Epoch 17/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4460 - acc: 0.8053 - val_loss: 0.4508 - val_acc: 0.8112\n",
      "Epoch 18/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4418 - acc: 0.8072 - val_loss: 0.4496 - val_acc: 0.8128\n",
      "Epoch 19/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4384 - acc: 0.8106 - val_loss: 0.4464 - val_acc: 0.8128\n",
      "Epoch 20/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4360 - acc: 0.8090 - val_loss: 0.4461 - val_acc: 0.8112\n",
      "Epoch 21/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4336 - acc: 0.8115 - val_loss: 0.4437 - val_acc: 0.8161\n",
      "Epoch 22/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4325 - acc: 0.8128 - val_loss: 0.4466 - val_acc: 0.8046\n",
      "Epoch 23/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4287 - acc: 0.8119 - val_loss: 0.4412 - val_acc: 0.8128\n",
      "Epoch 24/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4269 - acc: 0.8168 - val_loss: 0.4387 - val_acc: 0.8161\n",
      "Epoch 25/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4248 - acc: 0.8166 - val_loss: 0.4392 - val_acc: 0.8144\n",
      "Epoch 26/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4222 - acc: 0.8144 - val_loss: 0.4404 - val_acc: 0.8128\n",
      "Epoch 27/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4219 - acc: 0.8143 - val_loss: 0.4379 - val_acc: 0.8161\n",
      "Epoch 28/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4190 - acc: 0.8170 - val_loss: 0.4360 - val_acc: 0.8210\n",
      "Epoch 29/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4172 - acc: 0.8186 - val_loss: 0.4355 - val_acc: 0.8177\n",
      "Epoch 30/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4157 - acc: 0.8201 - val_loss: 0.4357 - val_acc: 0.8144\n",
      "Epoch 31/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4151 - acc: 0.8210 - val_loss: 0.4330 - val_acc: 0.8194\n",
      "Epoch 32/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4133 - acc: 0.8194 - val_loss: 0.4345 - val_acc: 0.8144\n",
      "Epoch 33/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4119 - acc: 0.8205 - val_loss: 0.4319 - val_acc: 0.8144\n",
      "Epoch 34/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4090 - acc: 0.8239 - val_loss: 0.4310 - val_acc: 0.8161\n",
      "Epoch 35/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4094 - acc: 0.8243 - val_loss: 0.4301 - val_acc: 0.8194\n",
      "Epoch 36/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4090 - acc: 0.8247 - val_loss: 0.4297 - val_acc: 0.8194\n",
      "Epoch 37/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4075 - acc: 0.8223 - val_loss: 0.4326 - val_acc: 0.8128\n",
      "Epoch 38/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4056 - acc: 0.8250 - val_loss: 0.4282 - val_acc: 0.8227\n",
      "Epoch 39/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4044 - acc: 0.8259 - val_loss: 0.4276 - val_acc: 0.8243\n",
      "Epoch 40/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4044 - acc: 0.8254 - val_loss: 0.4305 - val_acc: 0.8128\n",
      "Epoch 41/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4009 - acc: 0.8285 - val_loss: 0.4278 - val_acc: 0.8161\n",
      "Epoch 42/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4009 - acc: 0.8261 - val_loss: 0.4267 - val_acc: 0.8194\n",
      "Epoch 43/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3979 - acc: 0.8311 - val_loss: 0.4270 - val_acc: 0.8161\n",
      "Epoch 44/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3990 - acc: 0.8269 - val_loss: 0.4257 - val_acc: 0.8227\n",
      "Epoch 45/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3954 - acc: 0.8303 - val_loss: 0.4261 - val_acc: 0.8177\n",
      "Epoch 46/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3959 - acc: 0.8292 - val_loss: 0.4310 - val_acc: 0.8128\n",
      "Epoch 47/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3952 - acc: 0.8298 - val_loss: 0.4255 - val_acc: 0.8194\n",
      "Epoch 48/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3960 - acc: 0.8312 - val_loss: 0.4254 - val_acc: 0.8128\n",
      "Epoch 49/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3936 - acc: 0.8305 - val_loss: 0.4263 - val_acc: 0.8128\n",
      "Epoch 50/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3913 - acc: 0.8318 - val_loss: 0.4249 - val_acc: 0.8128\n",
      "Epoch 51/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3913 - acc: 0.8325 - val_loss: 0.4237 - val_acc: 0.8194\n",
      "Epoch 52/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3895 - acc: 0.8318 - val_loss: 0.4264 - val_acc: 0.8112\n",
      "Epoch 53/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3900 - acc: 0.8331 - val_loss: 0.4245 - val_acc: 0.8112\n",
      "Epoch 54/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3872 - acc: 0.8343 - val_loss: 0.4230 - val_acc: 0.8161\n",
      "Epoch 55/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3866 - acc: 0.8340 - val_loss: 0.4232 - val_acc: 0.8144\n",
      "Epoch 56/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3865 - acc: 0.8309 - val_loss: 0.4249 - val_acc: 0.8095\n",
      "Epoch 57/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3865 - acc: 0.8311 - val_loss: 0.4297 - val_acc: 0.8144\n",
      "Epoch 58/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3858 - acc: 0.8358 - val_loss: 0.4211 - val_acc: 0.8161\n",
      "Epoch 59/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3833 - acc: 0.8358 - val_loss: 0.4222 - val_acc: 0.8128\n",
      "Epoch 60/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3823 - acc: 0.8360 - val_loss: 0.4228 - val_acc: 0.8095\n",
      "Epoch 61/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3806 - acc: 0.8345 - val_loss: 0.4230 - val_acc: 0.8112\n",
      "Epoch 62/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3803 - acc: 0.8351 - val_loss: 0.4220 - val_acc: 0.8128\n",
      "Epoch 63/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3770 - acc: 0.8391 - val_loss: 0.4215 - val_acc: 0.8144\n",
      "Training with parameters {'batch_size': 350, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_111\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_116 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_308 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_197 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_309 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_198 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_310 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.7178 - acc: 0.5081 - val_loss: 0.6566 - val_acc: 0.5435\n",
      "Epoch 2/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6197 - acc: 0.6583 - val_loss: 0.5804 - val_acc: 0.7356\n",
      "Epoch 3/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5709 - acc: 0.7203 - val_loss: 0.5386 - val_acc: 0.7718\n",
      "Epoch 4/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5296 - acc: 0.7541 - val_loss: 0.5027 - val_acc: 0.7882\n",
      "Epoch 5/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4989 - acc: 0.7767 - val_loss: 0.4810 - val_acc: 0.7964\n",
      "Epoch 6/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4751 - acc: 0.7896 - val_loss: 0.4639 - val_acc: 0.8013\n",
      "Epoch 7/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4584 - acc: 0.7995 - val_loss: 0.4540 - val_acc: 0.8046\n",
      "Epoch 8/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4455 - acc: 0.8048 - val_loss: 0.4511 - val_acc: 0.8112\n",
      "Epoch 9/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4362 - acc: 0.8117 - val_loss: 0.4487 - val_acc: 0.8095\n",
      "Epoch 10/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4281 - acc: 0.8157 - val_loss: 0.4396 - val_acc: 0.8161\n",
      "Epoch 11/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4235 - acc: 0.8123 - val_loss: 0.4408 - val_acc: 0.8095\n",
      "Epoch 12/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4159 - acc: 0.8199 - val_loss: 0.4331 - val_acc: 0.8144\n",
      "Epoch 13/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4103 - acc: 0.8248 - val_loss: 0.4274 - val_acc: 0.8243\n",
      "Epoch 14/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4086 - acc: 0.8278 - val_loss: 0.4250 - val_acc: 0.8194\n",
      "Epoch 15/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4046 - acc: 0.8285 - val_loss: 0.4265 - val_acc: 0.8161\n",
      "Epoch 16/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4002 - acc: 0.8270 - val_loss: 0.4266 - val_acc: 0.8128\n",
      "Epoch 17/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3964 - acc: 0.8340 - val_loss: 0.4276 - val_acc: 0.8144\n",
      "Epoch 18/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3953 - acc: 0.8280 - val_loss: 0.4197 - val_acc: 0.8243\n",
      "Epoch 19/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3902 - acc: 0.8331 - val_loss: 0.4255 - val_acc: 0.8161\n",
      "Epoch 20/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3885 - acc: 0.8343 - val_loss: 0.4278 - val_acc: 0.8095\n",
      "Epoch 21/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3833 - acc: 0.8362 - val_loss: 0.4284 - val_acc: 0.8112\n",
      "Epoch 22/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3799 - acc: 0.8411 - val_loss: 0.4325 - val_acc: 0.8095\n",
      "Epoch 23/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3781 - acc: 0.8351 - val_loss: 0.4202 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': 350, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_112\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_117 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_311 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_199 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_312 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_200 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_313 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.7625 - acc: 0.4623 - val_loss: 0.6707 - val_acc: 0.6552\n",
      "Epoch 2/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6511 - acc: 0.6502 - val_loss: 0.6240 - val_acc: 0.7225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6090 - acc: 0.7123 - val_loss: 0.5848 - val_acc: 0.7586\n",
      "Epoch 4/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5814 - acc: 0.7260 - val_loss: 0.5552 - val_acc: 0.7652\n",
      "Epoch 5/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5550 - acc: 0.7417 - val_loss: 0.5300 - val_acc: 0.7750\n",
      "Epoch 6/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5327 - acc: 0.7583 - val_loss: 0.5110 - val_acc: 0.7882\n",
      "Epoch 7/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5110 - acc: 0.7694 - val_loss: 0.4928 - val_acc: 0.7947\n",
      "Epoch 8/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4973 - acc: 0.7730 - val_loss: 0.4874 - val_acc: 0.7915\n",
      "Epoch 9/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4851 - acc: 0.7871 - val_loss: 0.4698 - val_acc: 0.8046\n",
      "Epoch 10/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4759 - acc: 0.7871 - val_loss: 0.4623 - val_acc: 0.8013\n",
      "Epoch 11/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4633 - acc: 0.7975 - val_loss: 0.4637 - val_acc: 0.7997\n",
      "Epoch 12/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4563 - acc: 0.7971 - val_loss: 0.4526 - val_acc: 0.8079\n",
      "Epoch 13/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4529 - acc: 0.7986 - val_loss: 0.4486 - val_acc: 0.8095\n",
      "Epoch 14/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4479 - acc: 0.7997 - val_loss: 0.4442 - val_acc: 0.8112\n",
      "Epoch 15/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4395 - acc: 0.8053 - val_loss: 0.4417 - val_acc: 0.8079\n",
      "Epoch 16/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4375 - acc: 0.8072 - val_loss: 0.4396 - val_acc: 0.8046\n",
      "Epoch 17/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4327 - acc: 0.8081 - val_loss: 0.4377 - val_acc: 0.8079\n",
      "Epoch 18/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4271 - acc: 0.8117 - val_loss: 0.4346 - val_acc: 0.8144\n",
      "Epoch 19/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4267 - acc: 0.8113 - val_loss: 0.4322 - val_acc: 0.8177\n",
      "Epoch 20/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4230 - acc: 0.8135 - val_loss: 0.4311 - val_acc: 0.8227\n",
      "Epoch 21/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4206 - acc: 0.8177 - val_loss: 0.4298 - val_acc: 0.8177\n",
      "Epoch 22/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4144 - acc: 0.8208 - val_loss: 0.4410 - val_acc: 0.8046\n",
      "Epoch 23/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4137 - acc: 0.8163 - val_loss: 0.4288 - val_acc: 0.8079\n",
      "Epoch 24/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4136 - acc: 0.8212 - val_loss: 0.4267 - val_acc: 0.8194\n",
      "Epoch 25/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4083 - acc: 0.8199 - val_loss: 0.4261 - val_acc: 0.8161\n",
      "Epoch 26/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4035 - acc: 0.8247 - val_loss: 0.4255 - val_acc: 0.8095\n",
      "Epoch 27/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4037 - acc: 0.8247 - val_loss: 0.4294 - val_acc: 0.8046\n",
      "Epoch 28/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3997 - acc: 0.8300 - val_loss: 0.4241 - val_acc: 0.8112\n",
      "Epoch 29/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3992 - acc: 0.8254 - val_loss: 0.4247 - val_acc: 0.8079\n",
      "Epoch 30/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3951 - acc: 0.8250 - val_loss: 0.4213 - val_acc: 0.8194\n",
      "Epoch 31/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3966 - acc: 0.8274 - val_loss: 0.4207 - val_acc: 0.8194\n",
      "Epoch 32/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3929 - acc: 0.8298 - val_loss: 0.4225 - val_acc: 0.8144\n",
      "Epoch 33/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3913 - acc: 0.8292 - val_loss: 0.4221 - val_acc: 0.8161\n",
      "Epoch 34/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3853 - acc: 0.8334 - val_loss: 0.4221 - val_acc: 0.8144\n",
      "Epoch 35/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3853 - acc: 0.8342 - val_loss: 0.4194 - val_acc: 0.8210\n",
      "Epoch 36/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3849 - acc: 0.8334 - val_loss: 0.4195 - val_acc: 0.8177\n",
      "Epoch 37/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3837 - acc: 0.8340 - val_loss: 0.4215 - val_acc: 0.8128\n",
      "Epoch 38/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3804 - acc: 0.8367 - val_loss: 0.4187 - val_acc: 0.8227\n",
      "Epoch 39/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3812 - acc: 0.8354 - val_loss: 0.4197 - val_acc: 0.8227\n",
      "Epoch 40/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3746 - acc: 0.8411 - val_loss: 0.4209 - val_acc: 0.8144\n",
      "Epoch 41/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3741 - acc: 0.8380 - val_loss: 0.4198 - val_acc: 0.8144\n",
      "Epoch 42/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3718 - acc: 0.8391 - val_loss: 0.4189 - val_acc: 0.8194\n",
      "Epoch 43/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3673 - acc: 0.8420 - val_loss: 0.4185 - val_acc: 0.8194\n",
      "Epoch 44/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3659 - acc: 0.8422 - val_loss: 0.4198 - val_acc: 0.8177\n",
      "Epoch 45/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3636 - acc: 0.8453 - val_loss: 0.4195 - val_acc: 0.8194\n",
      "Epoch 46/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3636 - acc: 0.8429 - val_loss: 0.4192 - val_acc: 0.8161\n",
      "Epoch 47/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3600 - acc: 0.8478 - val_loss: 0.4200 - val_acc: 0.8194\n",
      "Epoch 48/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3551 - acc: 0.8469 - val_loss: 0.4232 - val_acc: 0.8177\n",
      "Training with parameters {'batch_size': 350, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_113\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_118 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_314 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_201 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_315 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_202 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_316 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "16/16 [==============================] - 1s 8ms/step - loss: 0.7066 - acc: 0.5163 - val_loss: 0.6614 - val_acc: 0.6125\n",
      "Epoch 2/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6490 - acc: 0.6313 - val_loss: 0.6307 - val_acc: 0.6420\n",
      "Epoch 3/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6160 - acc: 0.7006 - val_loss: 0.5949 - val_acc: 0.7455\n",
      "Epoch 4/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5869 - acc: 0.7174 - val_loss: 0.5677 - val_acc: 0.7619\n",
      "Epoch 5/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5608 - acc: 0.7393 - val_loss: 0.5385 - val_acc: 0.7767\n",
      "Epoch 6/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5350 - acc: 0.7550 - val_loss: 0.5153 - val_acc: 0.7882\n",
      "Epoch 7/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5125 - acc: 0.7685 - val_loss: 0.4955 - val_acc: 0.7931\n",
      "Epoch 8/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4949 - acc: 0.7791 - val_loss: 0.4807 - val_acc: 0.7964\n",
      "Epoch 9/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4813 - acc: 0.7825 - val_loss: 0.4708 - val_acc: 0.8013\n",
      "Epoch 10/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4698 - acc: 0.7904 - val_loss: 0.4651 - val_acc: 0.7997\n",
      "Epoch 11/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4606 - acc: 0.7984 - val_loss: 0.4611 - val_acc: 0.8013\n",
      "Epoch 12/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4551 - acc: 0.7940 - val_loss: 0.4537 - val_acc: 0.8013\n",
      "Epoch 13/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4467 - acc: 0.8030 - val_loss: 0.4508 - val_acc: 0.8046\n",
      "Epoch 14/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4453 - acc: 0.8030 - val_loss: 0.4458 - val_acc: 0.8095\n",
      "Epoch 15/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4389 - acc: 0.8103 - val_loss: 0.4432 - val_acc: 0.8112\n",
      "Epoch 16/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4361 - acc: 0.8079 - val_loss: 0.4435 - val_acc: 0.8046\n",
      "Epoch 17/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4304 - acc: 0.8121 - val_loss: 0.4389 - val_acc: 0.8112\n",
      "Epoch 18/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4280 - acc: 0.8104 - val_loss: 0.4384 - val_acc: 0.8095\n",
      "Epoch 19/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4245 - acc: 0.8119 - val_loss: 0.4360 - val_acc: 0.8128\n",
      "Epoch 20/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4223 - acc: 0.8126 - val_loss: 0.4338 - val_acc: 0.8128\n",
      "Epoch 21/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4194 - acc: 0.8170 - val_loss: 0.4325 - val_acc: 0.8112\n",
      "Epoch 22/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4160 - acc: 0.8210 - val_loss: 0.4320 - val_acc: 0.8095\n",
      "Epoch 23/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4133 - acc: 0.8192 - val_loss: 0.4321 - val_acc: 0.8079\n",
      "Epoch 24/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4072 - acc: 0.8227 - val_loss: 0.4313 - val_acc: 0.8095\n",
      "Epoch 25/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4075 - acc: 0.8214 - val_loss: 0.4282 - val_acc: 0.8144\n",
      "Epoch 26/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4064 - acc: 0.8217 - val_loss: 0.4280 - val_acc: 0.8095\n",
      "Epoch 27/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4031 - acc: 0.8239 - val_loss: 0.4291 - val_acc: 0.8095\n",
      "Epoch 28/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3976 - acc: 0.8247 - val_loss: 0.4319 - val_acc: 0.8079\n",
      "Epoch 29/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3973 - acc: 0.8290 - val_loss: 0.4258 - val_acc: 0.8112\n",
      "Epoch 30/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3978 - acc: 0.8243 - val_loss: 0.4245 - val_acc: 0.8128\n",
      "Epoch 31/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3963 - acc: 0.8320 - val_loss: 0.4240 - val_acc: 0.8112\n",
      "Epoch 32/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3954 - acc: 0.8278 - val_loss: 0.4252 - val_acc: 0.8161\n",
      "Epoch 33/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3918 - acc: 0.8265 - val_loss: 0.4261 - val_acc: 0.8128\n",
      "Epoch 34/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3893 - acc: 0.8303 - val_loss: 0.4243 - val_acc: 0.8161\n",
      "Epoch 35/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3840 - acc: 0.8338 - val_loss: 0.4225 - val_acc: 0.8161\n",
      "Epoch 36/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3852 - acc: 0.8351 - val_loss: 0.4229 - val_acc: 0.8161\n",
      "Epoch 37/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3822 - acc: 0.8342 - val_loss: 0.4254 - val_acc: 0.8128\n",
      "Epoch 38/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3819 - acc: 0.8367 - val_loss: 0.4250 - val_acc: 0.8095\n",
      "Epoch 39/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3768 - acc: 0.8374 - val_loss: 0.4205 - val_acc: 0.8194\n",
      "Epoch 40/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3766 - acc: 0.8363 - val_loss: 0.4218 - val_acc: 0.8144\n",
      "Epoch 41/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3788 - acc: 0.8358 - val_loss: 0.4200 - val_acc: 0.8210\n",
      "Epoch 42/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3725 - acc: 0.8411 - val_loss: 0.4223 - val_acc: 0.8128\n",
      "Epoch 43/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3708 - acc: 0.8415 - val_loss: 0.4257 - val_acc: 0.8161\n",
      "Epoch 44/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3703 - acc: 0.8396 - val_loss: 0.4215 - val_acc: 0.8144\n",
      "Epoch 45/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3666 - acc: 0.8402 - val_loss: 0.4210 - val_acc: 0.8128\n",
      "Epoch 46/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3645 - acc: 0.8429 - val_loss: 0.4209 - val_acc: 0.8112\n",
      "Training with parameters {'batch_size': 350, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_114\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_119 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_317 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_203 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_318 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_204 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_319 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.7267 - acc: 0.5023 - val_loss: 0.6631 - val_acc: 0.5534\n",
      "Epoch 2/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6390 - acc: 0.6214 - val_loss: 0.6075 - val_acc: 0.7159\n",
      "Epoch 3/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5950 - acc: 0.7148 - val_loss: 0.5669 - val_acc: 0.7455\n",
      "Epoch 4/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5661 - acc: 0.7271 - val_loss: 0.5344 - val_acc: 0.7635\n",
      "Epoch 5/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5318 - acc: 0.7544 - val_loss: 0.5048 - val_acc: 0.7865\n",
      "Epoch 6/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5070 - acc: 0.7657 - val_loss: 0.4877 - val_acc: 0.7964\n",
      "Epoch 7/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4843 - acc: 0.7829 - val_loss: 0.4740 - val_acc: 0.7997\n",
      "Epoch 8/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4685 - acc: 0.7907 - val_loss: 0.4590 - val_acc: 0.7997\n",
      "Epoch 9/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4607 - acc: 0.7916 - val_loss: 0.4529 - val_acc: 0.8128\n",
      "Epoch 10/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4482 - acc: 0.7993 - val_loss: 0.4466 - val_acc: 0.8095\n",
      "Epoch 11/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4374 - acc: 0.8055 - val_loss: 0.4439 - val_acc: 0.8128\n",
      "Epoch 12/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4368 - acc: 0.8061 - val_loss: 0.4398 - val_acc: 0.8161\n",
      "Epoch 13/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4290 - acc: 0.8126 - val_loss: 0.4378 - val_acc: 0.8144\n",
      "Epoch 14/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4233 - acc: 0.8121 - val_loss: 0.4364 - val_acc: 0.8161\n",
      "Epoch 15/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4181 - acc: 0.8168 - val_loss: 0.4411 - val_acc: 0.8128\n",
      "Epoch 16/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4157 - acc: 0.8174 - val_loss: 0.4328 - val_acc: 0.8194\n",
      "Epoch 17/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4107 - acc: 0.8212 - val_loss: 0.4292 - val_acc: 0.8161\n",
      "Epoch 18/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4109 - acc: 0.8205 - val_loss: 0.4295 - val_acc: 0.8177\n",
      "Epoch 19/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4017 - acc: 0.8250 - val_loss: 0.4278 - val_acc: 0.8210\n",
      "Epoch 20/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4008 - acc: 0.8267 - val_loss: 0.4284 - val_acc: 0.8128\n",
      "Epoch 21/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3969 - acc: 0.8280 - val_loss: 0.4270 - val_acc: 0.8210\n",
      "Epoch 22/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3951 - acc: 0.8259 - val_loss: 0.4264 - val_acc: 0.8210\n",
      "Epoch 23/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3931 - acc: 0.8278 - val_loss: 0.4242 - val_acc: 0.8243\n",
      "Epoch 24/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3891 - acc: 0.8314 - val_loss: 0.4238 - val_acc: 0.8243\n",
      "Epoch 25/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3852 - acc: 0.8332 - val_loss: 0.4269 - val_acc: 0.8095\n",
      "Epoch 26/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3790 - acc: 0.8347 - val_loss: 0.4244 - val_acc: 0.8227\n",
      "Epoch 27/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3778 - acc: 0.8365 - val_loss: 0.4238 - val_acc: 0.8210\n",
      "Epoch 28/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3735 - acc: 0.8384 - val_loss: 0.4247 - val_acc: 0.8194\n",
      "Epoch 29/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3749 - acc: 0.8373 - val_loss: 0.4227 - val_acc: 0.8276\n",
      "Epoch 30/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3657 - acc: 0.8424 - val_loss: 0.4222 - val_acc: 0.8259\n",
      "Epoch 31/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3632 - acc: 0.8422 - val_loss: 0.4231 - val_acc: 0.8161\n",
      "Epoch 32/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3601 - acc: 0.8447 - val_loss: 0.4282 - val_acc: 0.8194\n",
      "Epoch 33/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3557 - acc: 0.8469 - val_loss: 0.4247 - val_acc: 0.8227\n",
      "Epoch 34/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3532 - acc: 0.8473 - val_loss: 0.4233 - val_acc: 0.8210\n",
      "Epoch 35/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3535 - acc: 0.8482 - val_loss: 0.4248 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': 500, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_115\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_120 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_320 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_205 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_321 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_206 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_322 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.7011 - acc: 0.5209 - val_loss: 0.6654 - val_acc: 0.5468\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6276 - acc: 0.6468 - val_loss: 0.5955 - val_acc: 0.7340\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5851 - acc: 0.7083 - val_loss: 0.5572 - val_acc: 0.7570\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5487 - acc: 0.7475 - val_loss: 0.5217 - val_acc: 0.7668\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5183 - acc: 0.7670 - val_loss: 0.4963 - val_acc: 0.7816\n",
      "Epoch 6/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4930 - acc: 0.7769 - val_loss: 0.4826 - val_acc: 0.7915\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4748 - acc: 0.7878 - val_loss: 0.4675 - val_acc: 0.7964\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4567 - acc: 0.8002 - val_loss: 0.4560 - val_acc: 0.8112\n",
      "Epoch 9/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4475 - acc: 0.8022 - val_loss: 0.4503 - val_acc: 0.8144\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4379 - acc: 0.8101 - val_loss: 0.4433 - val_acc: 0.8128\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4301 - acc: 0.8128 - val_loss: 0.4382 - val_acc: 0.8161\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4218 - acc: 0.8146 - val_loss: 0.4369 - val_acc: 0.8161\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4195 - acc: 0.8150 - val_loss: 0.4325 - val_acc: 0.8161\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4141 - acc: 0.8228 - val_loss: 0.4322 - val_acc: 0.8144\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4109 - acc: 0.8228 - val_loss: 0.4410 - val_acc: 0.8046\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4075 - acc: 0.8201 - val_loss: 0.4350 - val_acc: 0.8112\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4035 - acc: 0.8296 - val_loss: 0.4310 - val_acc: 0.8128\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3975 - acc: 0.8296 - val_loss: 0.4265 - val_acc: 0.8210\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3943 - acc: 0.8281 - val_loss: 0.4230 - val_acc: 0.8177\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3913 - acc: 0.8312 - val_loss: 0.4224 - val_acc: 0.8194\n",
      "Epoch 21/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3900 - acc: 0.8321 - val_loss: 0.4221 - val_acc: 0.8194\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3824 - acc: 0.8342 - val_loss: 0.4271 - val_acc: 0.8144\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3775 - acc: 0.8376 - val_loss: 0.4206 - val_acc: 0.8161\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3754 - acc: 0.8393 - val_loss: 0.4209 - val_acc: 0.8177\n",
      "Epoch 25/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3738 - acc: 0.8387 - val_loss: 0.4200 - val_acc: 0.8194\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3679 - acc: 0.8444 - val_loss: 0.4225 - val_acc: 0.8144\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3676 - acc: 0.8431 - val_loss: 0.4219 - val_acc: 0.8177\n",
      "Epoch 28/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3642 - acc: 0.8427 - val_loss: 0.4201 - val_acc: 0.8161\n",
      "Epoch 29/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3602 - acc: 0.8453 - val_loss: 0.4189 - val_acc: 0.8243\n",
      "Epoch 30/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3545 - acc: 0.8488 - val_loss: 0.4193 - val_acc: 0.8194\n",
      "Epoch 31/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3548 - acc: 0.8488 - val_loss: 0.4218 - val_acc: 0.8177\n",
      "Epoch 32/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3549 - acc: 0.8464 - val_loss: 0.4323 - val_acc: 0.8128\n",
      "Epoch 33/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3503 - acc: 0.8486 - val_loss: 0.4267 - val_acc: 0.8194\n",
      "Epoch 34/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3397 - acc: 0.8524 - val_loss: 0.4209 - val_acc: 0.8161\n",
      "Training with parameters {'batch_size': 500, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_116\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_121 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_323 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_207 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_324 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_208 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_325 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.7546 - acc: 0.4623 - val_loss: 0.6741 - val_acc: 0.6092\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6630 - acc: 0.5968 - val_loss: 0.6446 - val_acc: 0.5534\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6280 - acc: 0.6451 - val_loss: 0.6035 - val_acc: 0.7241\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5981 - acc: 0.7022 - val_loss: 0.5738 - val_acc: 0.7471\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5759 - acc: 0.7218 - val_loss: 0.5518 - val_acc: 0.7603\n",
      "Epoch 6/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5492 - acc: 0.7457 - val_loss: 0.5258 - val_acc: 0.7635\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5281 - acc: 0.7592 - val_loss: 0.5059 - val_acc: 0.7816\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5067 - acc: 0.7696 - val_loss: 0.4884 - val_acc: 0.7947\n",
      "Epoch 9/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4894 - acc: 0.7792 - val_loss: 0.4783 - val_acc: 0.7997\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4800 - acc: 0.7812 - val_loss: 0.4662 - val_acc: 0.7964\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4686 - acc: 0.7882 - val_loss: 0.4599 - val_acc: 0.8046\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4593 - acc: 0.7969 - val_loss: 0.4606 - val_acc: 0.8079\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4537 - acc: 0.7977 - val_loss: 0.4501 - val_acc: 0.8046\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4451 - acc: 0.8009 - val_loss: 0.4568 - val_acc: 0.8144\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4418 - acc: 0.8017 - val_loss: 0.4478 - val_acc: 0.8194\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4327 - acc: 0.8101 - val_loss: 0.4407 - val_acc: 0.8112\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4317 - acc: 0.8082 - val_loss: 0.4402 - val_acc: 0.8128\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4280 - acc: 0.8101 - val_loss: 0.4424 - val_acc: 0.8161\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4237 - acc: 0.8112 - val_loss: 0.4384 - val_acc: 0.8194\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4191 - acc: 0.8141 - val_loss: 0.4335 - val_acc: 0.8210\n",
      "Epoch 21/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4135 - acc: 0.8232 - val_loss: 0.4326 - val_acc: 0.8161\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4089 - acc: 0.8219 - val_loss: 0.4331 - val_acc: 0.8177\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4103 - acc: 0.8217 - val_loss: 0.4296 - val_acc: 0.8161\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4049 - acc: 0.8210 - val_loss: 0.4295 - val_acc: 0.8177\n",
      "Epoch 25/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4013 - acc: 0.8219 - val_loss: 0.4295 - val_acc: 0.8194\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3980 - acc: 0.8245 - val_loss: 0.4273 - val_acc: 0.8177\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3990 - acc: 0.8278 - val_loss: 0.4326 - val_acc: 0.8095\n",
      "Epoch 28/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3974 - acc: 0.8263 - val_loss: 0.4333 - val_acc: 0.8095\n",
      "Epoch 29/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3926 - acc: 0.8278 - val_loss: 0.4255 - val_acc: 0.8227\n",
      "Epoch 30/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3891 - acc: 0.8292 - val_loss: 0.4253 - val_acc: 0.8194\n",
      "Epoch 31/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3870 - acc: 0.8280 - val_loss: 0.4251 - val_acc: 0.8227\n",
      "Epoch 32/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3848 - acc: 0.8325 - val_loss: 0.4242 - val_acc: 0.8210\n",
      "Epoch 33/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3827 - acc: 0.8349 - val_loss: 0.4244 - val_acc: 0.8227\n",
      "Epoch 34/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3768 - acc: 0.8396 - val_loss: 0.4249 - val_acc: 0.8194\n",
      "Epoch 35/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3784 - acc: 0.8352 - val_loss: 0.4235 - val_acc: 0.8243\n",
      "Epoch 36/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3762 - acc: 0.8334 - val_loss: 0.4286 - val_acc: 0.8161\n",
      "Epoch 37/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3717 - acc: 0.8382 - val_loss: 0.4297 - val_acc: 0.8161\n",
      "Epoch 38/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3696 - acc: 0.8409 - val_loss: 0.4216 - val_acc: 0.8276\n",
      "Epoch 39/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3653 - acc: 0.8422 - val_loss: 0.4217 - val_acc: 0.8276\n",
      "Epoch 40/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3665 - acc: 0.8391 - val_loss: 0.4209 - val_acc: 0.8276\n",
      "Epoch 41/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3638 - acc: 0.8433 - val_loss: 0.4254 - val_acc: 0.8161\n",
      "Epoch 42/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3599 - acc: 0.8467 - val_loss: 0.4244 - val_acc: 0.8243\n",
      "Epoch 43/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3570 - acc: 0.8466 - val_loss: 0.4268 - val_acc: 0.8210\n",
      "Epoch 44/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3538 - acc: 0.8491 - val_loss: 0.4208 - val_acc: 0.8259\n",
      "Epoch 45/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3519 - acc: 0.8491 - val_loss: 0.4232 - val_acc: 0.8243\n",
      "Epoch 46/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3489 - acc: 0.8489 - val_loss: 0.4222 - val_acc: 0.8227\n",
      "Epoch 47/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3469 - acc: 0.8471 - val_loss: 0.4212 - val_acc: 0.8259\n",
      "Epoch 48/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3461 - acc: 0.8520 - val_loss: 0.4227 - val_acc: 0.8259\n",
      "Epoch 49/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3366 - acc: 0.8548 - val_loss: 0.4236 - val_acc: 0.8227\n",
      "Training with parameters {'batch_size': 500, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_117\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_122 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_326 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_209 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_327 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.6515 - acc: 0.6212 - val_loss: 0.6342 - val_acc: 0.6305\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6184 - acc: 0.6800 - val_loss: 0.6028 - val_acc: 0.7274\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5936 - acc: 0.7101 - val_loss: 0.5794 - val_acc: 0.7504\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5750 - acc: 0.7251 - val_loss: 0.5610 - val_acc: 0.7537\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5573 - acc: 0.7373 - val_loss: 0.5435 - val_acc: 0.7685\n",
      "Epoch 6/300\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5429 - acc: 0.7531 - val_loss: 0.5303 - val_acc: 0.7816\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5276 - acc: 0.7681 - val_loss: 0.5175 - val_acc: 0.7882\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5192 - acc: 0.7641 - val_loss: 0.5086 - val_acc: 0.7915\n",
      "Epoch 9/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5081 - acc: 0.7761 - val_loss: 0.5006 - val_acc: 0.7882\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4991 - acc: 0.7792 - val_loss: 0.4938 - val_acc: 0.7898\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4911 - acc: 0.7776 - val_loss: 0.4874 - val_acc: 0.7898\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4854 - acc: 0.7884 - val_loss: 0.4828 - val_acc: 0.7882\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4784 - acc: 0.7907 - val_loss: 0.4769 - val_acc: 0.7964\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4733 - acc: 0.7924 - val_loss: 0.4741 - val_acc: 0.7915\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4703 - acc: 0.7947 - val_loss: 0.4695 - val_acc: 0.7947\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4655 - acc: 0.7977 - val_loss: 0.4665 - val_acc: 0.7964\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4628 - acc: 0.7944 - val_loss: 0.4630 - val_acc: 0.7980\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4567 - acc: 0.8011 - val_loss: 0.4616 - val_acc: 0.7997\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4536 - acc: 0.8011 - val_loss: 0.4586 - val_acc: 0.8013\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4509 - acc: 0.8024 - val_loss: 0.4555 - val_acc: 0.8095\n",
      "Epoch 21/300\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4468 - acc: 0.8044 - val_loss: 0.4546 - val_acc: 0.8079\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4446 - acc: 0.8084 - val_loss: 0.4520 - val_acc: 0.8112\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4419 - acc: 0.8077 - val_loss: 0.4502 - val_acc: 0.8128\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4396 - acc: 0.8110 - val_loss: 0.4493 - val_acc: 0.8112\n",
      "Epoch 25/300\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4368 - acc: 0.8135 - val_loss: 0.4467 - val_acc: 0.8079\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4364 - acc: 0.8128 - val_loss: 0.4473 - val_acc: 0.8079\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4329 - acc: 0.8126 - val_loss: 0.4442 - val_acc: 0.8144\n",
      "Epoch 28/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4308 - acc: 0.8170 - val_loss: 0.4437 - val_acc: 0.8128\n",
      "Epoch 29/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4300 - acc: 0.8159 - val_loss: 0.4434 - val_acc: 0.8079\n",
      "Epoch 30/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4293 - acc: 0.8139 - val_loss: 0.4410 - val_acc: 0.8112\n",
      "Epoch 31/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4253 - acc: 0.8159 - val_loss: 0.4413 - val_acc: 0.8128\n",
      "Epoch 32/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4267 - acc: 0.8165 - val_loss: 0.4390 - val_acc: 0.8177\n",
      "Epoch 33/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4230 - acc: 0.8152 - val_loss: 0.4392 - val_acc: 0.8161\n",
      "Epoch 34/300\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4220 - acc: 0.8208 - val_loss: 0.4367 - val_acc: 0.8177\n",
      "Epoch 35/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4198 - acc: 0.8190 - val_loss: 0.4366 - val_acc: 0.8177\n",
      "Epoch 36/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4193 - acc: 0.8185 - val_loss: 0.4372 - val_acc: 0.8161\n",
      "Epoch 37/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4180 - acc: 0.8170 - val_loss: 0.4348 - val_acc: 0.8161\n",
      "Epoch 38/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4160 - acc: 0.8212 - val_loss: 0.4360 - val_acc: 0.8161\n",
      "Epoch 39/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4164 - acc: 0.8196 - val_loss: 0.4334 - val_acc: 0.8177\n",
      "Epoch 40/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4145 - acc: 0.8201 - val_loss: 0.4334 - val_acc: 0.8177\n",
      "Epoch 41/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4118 - acc: 0.8228 - val_loss: 0.4329 - val_acc: 0.8161\n",
      "Epoch 42/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4119 - acc: 0.8236 - val_loss: 0.4320 - val_acc: 0.8210\n",
      "Epoch 43/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4112 - acc: 0.8197 - val_loss: 0.4321 - val_acc: 0.8144\n",
      "Epoch 44/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4085 - acc: 0.8248 - val_loss: 0.4310 - val_acc: 0.8161\n",
      "Epoch 45/300\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4075 - acc: 0.8250 - val_loss: 0.4310 - val_acc: 0.8161\n",
      "Epoch 46/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4062 - acc: 0.8261 - val_loss: 0.4303 - val_acc: 0.8128\n",
      "Epoch 47/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4046 - acc: 0.8250 - val_loss: 0.4295 - val_acc: 0.8161\n",
      "Epoch 48/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4043 - acc: 0.8241 - val_loss: 0.4304 - val_acc: 0.8161\n",
      "Epoch 49/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4034 - acc: 0.8241 - val_loss: 0.4281 - val_acc: 0.8227\n",
      "Epoch 50/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4029 - acc: 0.8259 - val_loss: 0.4293 - val_acc: 0.8144\n",
      "Epoch 51/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4017 - acc: 0.8289 - val_loss: 0.4273 - val_acc: 0.8210\n",
      "Epoch 52/300\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4014 - acc: 0.8280 - val_loss: 0.4280 - val_acc: 0.8144\n",
      "Epoch 53/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3997 - acc: 0.8280 - val_loss: 0.4269 - val_acc: 0.8161\n",
      "Epoch 54/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3991 - acc: 0.8269 - val_loss: 0.4272 - val_acc: 0.8161\n",
      "Epoch 55/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3989 - acc: 0.8272 - val_loss: 0.4268 - val_acc: 0.8161\n",
      "Epoch 56/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3993 - acc: 0.8267 - val_loss: 0.4261 - val_acc: 0.8161\n",
      "Epoch 57/300\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3959 - acc: 0.8258 - val_loss: 0.4258 - val_acc: 0.8177\n",
      "Epoch 58/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3963 - acc: 0.8263 - val_loss: 0.4255 - val_acc: 0.8161\n",
      "Epoch 59/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3949 - acc: 0.8309 - val_loss: 0.4258 - val_acc: 0.8128\n",
      "Epoch 60/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3940 - acc: 0.8301 - val_loss: 0.4255 - val_acc: 0.8144\n",
      "Epoch 61/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3931 - acc: 0.8314 - val_loss: 0.4242 - val_acc: 0.8177\n",
      "Epoch 62/300\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3918 - acc: 0.8289 - val_loss: 0.4243 - val_acc: 0.8210\n",
      "Epoch 63/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3917 - acc: 0.8309 - val_loss: 0.4247 - val_acc: 0.8128\n",
      "Epoch 64/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3901 - acc: 0.8323 - val_loss: 0.4245 - val_acc: 0.8128\n",
      "Epoch 65/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3899 - acc: 0.8305 - val_loss: 0.4238 - val_acc: 0.8144\n",
      "Epoch 66/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3881 - acc: 0.8340 - val_loss: 0.4240 - val_acc: 0.8128\n",
      "Epoch 67/300\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3878 - acc: 0.8334 - val_loss: 0.4238 - val_acc: 0.8112\n",
      "Epoch 68/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3870 - acc: 0.8292 - val_loss: 0.4229 - val_acc: 0.8144\n",
      "Epoch 69/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3867 - acc: 0.8336 - val_loss: 0.4223 - val_acc: 0.8194\n",
      "Epoch 70/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3845 - acc: 0.8331 - val_loss: 0.4235 - val_acc: 0.8112\n",
      "Epoch 71/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3833 - acc: 0.8342 - val_loss: 0.4233 - val_acc: 0.8095\n",
      "Epoch 72/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3845 - acc: 0.8360 - val_loss: 0.4233 - val_acc: 0.8112\n",
      "Epoch 73/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3841 - acc: 0.8351 - val_loss: 0.4218 - val_acc: 0.8161\n",
      "Epoch 74/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3828 - acc: 0.8323 - val_loss: 0.4227 - val_acc: 0.8112\n",
      "Epoch 75/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3822 - acc: 0.8345 - val_loss: 0.4217 - val_acc: 0.8144\n",
      "Epoch 76/300\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3818 - acc: 0.8340 - val_loss: 0.4229 - val_acc: 0.8095\n",
      "Epoch 77/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3802 - acc: 0.8340 - val_loss: 0.4213 - val_acc: 0.8144\n",
      "Epoch 78/300\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.3755 - acc: 0.848 - 0s 4ms/step - loss: 0.3810 - acc: 0.8373 - val_loss: 0.4222 - val_acc: 0.8112\n",
      "Epoch 79/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3815 - acc: 0.8369 - val_loss: 0.4214 - val_acc: 0.8112\n",
      "Epoch 80/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3785 - acc: 0.8371 - val_loss: 0.4209 - val_acc: 0.8144\n",
      "Epoch 81/300\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3777 - acc: 0.8385 - val_loss: 0.4212 - val_acc: 0.8144\n",
      "Epoch 82/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3782 - acc: 0.8362 - val_loss: 0.4209 - val_acc: 0.8144\n",
      "Epoch 83/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3777 - acc: 0.8367 - val_loss: 0.4202 - val_acc: 0.8144\n",
      "Epoch 84/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3777 - acc: 0.8345 - val_loss: 0.4204 - val_acc: 0.8144\n",
      "Epoch 85/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3742 - acc: 0.8369 - val_loss: 0.4214 - val_acc: 0.8112\n",
      "Epoch 86/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3748 - acc: 0.8374 - val_loss: 0.4205 - val_acc: 0.8144\n",
      "Epoch 87/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3747 - acc: 0.8400 - val_loss: 0.4209 - val_acc: 0.8128\n",
      "Epoch 88/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3731 - acc: 0.8384 - val_loss: 0.4201 - val_acc: 0.8128\n",
      "Epoch 89/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3730 - acc: 0.8402 - val_loss: 0.4207 - val_acc: 0.8128\n",
      "Epoch 90/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3716 - acc: 0.8374 - val_loss: 0.4199 - val_acc: 0.8161\n",
      "Epoch 91/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3722 - acc: 0.8404 - val_loss: 0.4197 - val_acc: 0.8144\n",
      "Epoch 92/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3706 - acc: 0.8391 - val_loss: 0.4207 - val_acc: 0.8128\n",
      "Epoch 93/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3690 - acc: 0.8405 - val_loss: 0.4203 - val_acc: 0.8144\n",
      "Epoch 94/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3691 - acc: 0.8427 - val_loss: 0.4198 - val_acc: 0.8144\n",
      "Epoch 95/300\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3685 - acc: 0.8422 - val_loss: 0.4203 - val_acc: 0.8161\n",
      "Epoch 96/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3679 - acc: 0.8400 - val_loss: 0.4205 - val_acc: 0.8128\n",
      "Training with parameters {'batch_size': 500, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_118\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_123 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_328 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_210 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_329 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_211 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_330 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.7402 - acc: 0.4725 - val_loss: 0.6743 - val_acc: 0.5435\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6486 - acc: 0.5964 - val_loss: 0.6157 - val_acc: 0.6897\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6015 - acc: 0.6995 - val_loss: 0.5737 - val_acc: 0.7356\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5682 - acc: 0.7296 - val_loss: 0.5430 - val_acc: 0.7668\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5384 - acc: 0.7548 - val_loss: 0.5141 - val_acc: 0.7767\n",
      "Epoch 6/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5132 - acc: 0.7657 - val_loss: 0.4975 - val_acc: 0.7849\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4939 - acc: 0.7818 - val_loss: 0.4787 - val_acc: 0.7931\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4750 - acc: 0.7862 - val_loss: 0.4683 - val_acc: 0.7980\n",
      "Epoch 9/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4626 - acc: 0.7973 - val_loss: 0.4592 - val_acc: 0.7964\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4513 - acc: 0.8030 - val_loss: 0.4519 - val_acc: 0.8046\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4425 - acc: 0.8061 - val_loss: 0.4470 - val_acc: 0.8095\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4353 - acc: 0.8077 - val_loss: 0.4447 - val_acc: 0.8161\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4306 - acc: 0.8092 - val_loss: 0.4414 - val_acc: 0.8144\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4243 - acc: 0.8159 - val_loss: 0.4373 - val_acc: 0.8177\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4207 - acc: 0.8165 - val_loss: 0.4341 - val_acc: 0.8161\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4167 - acc: 0.8192 - val_loss: 0.4306 - val_acc: 0.8243\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4158 - acc: 0.8214 - val_loss: 0.4284 - val_acc: 0.8276\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4118 - acc: 0.8194 - val_loss: 0.4329 - val_acc: 0.8112\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4068 - acc: 0.8254 - val_loss: 0.4351 - val_acc: 0.8079\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3999 - acc: 0.8238 - val_loss: 0.4237 - val_acc: 0.8259\n",
      "Epoch 21/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4008 - acc: 0.8281 - val_loss: 0.4249 - val_acc: 0.8243\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3977 - acc: 0.8316 - val_loss: 0.4220 - val_acc: 0.8276\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3941 - acc: 0.8307 - val_loss: 0.4243 - val_acc: 0.8177\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3901 - acc: 0.8296 - val_loss: 0.4277 - val_acc: 0.8095\n",
      "Epoch 25/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3880 - acc: 0.8298 - val_loss: 0.4268 - val_acc: 0.8144\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3850 - acc: 0.8352 - val_loss: 0.4229 - val_acc: 0.8128\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3830 - acc: 0.8338 - val_loss: 0.4194 - val_acc: 0.8227\n",
      "Epoch 28/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3774 - acc: 0.8387 - val_loss: 0.4195 - val_acc: 0.8243\n",
      "Epoch 29/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3765 - acc: 0.8349 - val_loss: 0.4189 - val_acc: 0.8259\n",
      "Epoch 30/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3750 - acc: 0.8396 - val_loss: 0.4178 - val_acc: 0.8259\n",
      "Epoch 31/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3691 - acc: 0.8398 - val_loss: 0.4181 - val_acc: 0.8243\n",
      "Epoch 32/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3679 - acc: 0.8436 - val_loss: 0.4191 - val_acc: 0.8161\n",
      "Epoch 33/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3653 - acc: 0.8415 - val_loss: 0.4172 - val_acc: 0.8259\n",
      "Epoch 34/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3627 - acc: 0.8453 - val_loss: 0.4184 - val_acc: 0.8243\n",
      "Epoch 35/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3579 - acc: 0.8466 - val_loss: 0.4205 - val_acc: 0.8128\n",
      "Epoch 36/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3557 - acc: 0.8480 - val_loss: 0.4188 - val_acc: 0.8177\n",
      "Epoch 37/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3529 - acc: 0.8466 - val_loss: 0.4180 - val_acc: 0.8227\n",
      "Epoch 38/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3480 - acc: 0.8508 - val_loss: 0.4178 - val_acc: 0.8259\n",
      "Training with parameters {'batch_size': 500, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_119\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_124 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_331 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_212 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_332 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_213 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_333 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.7795 - acc: 0.4392 - val_loss: 0.6888 - val_acc: 0.5123\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6714 - acc: 0.6003 - val_loss: 0.6509 - val_acc: 0.6355\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6381 - acc: 0.6676 - val_loss: 0.6156 - val_acc: 0.7438\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6080 - acc: 0.7132 - val_loss: 0.5876 - val_acc: 0.7570\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5888 - acc: 0.7218 - val_loss: 0.5665 - val_acc: 0.7603\n",
      "Epoch 6/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5701 - acc: 0.7362 - val_loss: 0.5454 - val_acc: 0.7734\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5493 - acc: 0.7453 - val_loss: 0.5282 - val_acc: 0.7750\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5311 - acc: 0.7559 - val_loss: 0.5138 - val_acc: 0.7882\n",
      "Epoch 9/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5198 - acc: 0.7688 - val_loss: 0.4992 - val_acc: 0.7931\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5064 - acc: 0.7666 - val_loss: 0.4901 - val_acc: 0.7931\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4958 - acc: 0.7729 - val_loss: 0.4789 - val_acc: 0.7980\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4842 - acc: 0.7836 - val_loss: 0.4704 - val_acc: 0.8013\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4727 - acc: 0.7927 - val_loss: 0.4644 - val_acc: 0.8062\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4651 - acc: 0.7957 - val_loss: 0.4589 - val_acc: 0.8079\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4600 - acc: 0.7955 - val_loss: 0.4546 - val_acc: 0.8062\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4495 - acc: 0.8000 - val_loss: 0.4506 - val_acc: 0.8062\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4448 - acc: 0.8026 - val_loss: 0.4498 - val_acc: 0.8062\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4444 - acc: 0.8026 - val_loss: 0.4455 - val_acc: 0.8046\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4392 - acc: 0.7980 - val_loss: 0.4443 - val_acc: 0.8095\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4318 - acc: 0.8081 - val_loss: 0.4407 - val_acc: 0.8079\n",
      "Epoch 21/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4310 - acc: 0.8117 - val_loss: 0.4383 - val_acc: 0.8112\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4245 - acc: 0.8104 - val_loss: 0.4396 - val_acc: 0.8030\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4239 - acc: 0.8152 - val_loss: 0.4352 - val_acc: 0.8095\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4225 - acc: 0.8117 - val_loss: 0.4336 - val_acc: 0.8144\n",
      "Epoch 25/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4212 - acc: 0.8115 - val_loss: 0.4362 - val_acc: 0.8030\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4107 - acc: 0.8214 - val_loss: 0.4306 - val_acc: 0.8177\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4160 - acc: 0.8170 - val_loss: 0.4306 - val_acc: 0.8112\n",
      "Epoch 28/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4126 - acc: 0.8146 - val_loss: 0.4335 - val_acc: 0.8046\n",
      "Epoch 29/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4090 - acc: 0.8155 - val_loss: 0.4284 - val_acc: 0.8128\n",
      "Epoch 30/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4089 - acc: 0.8196 - val_loss: 0.4279 - val_acc: 0.8128\n",
      "Epoch 31/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4067 - acc: 0.8201 - val_loss: 0.4331 - val_acc: 0.8013\n",
      "Epoch 32/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4018 - acc: 0.8248 - val_loss: 0.4254 - val_acc: 0.8227\n",
      "Epoch 33/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4036 - acc: 0.8230 - val_loss: 0.4269 - val_acc: 0.8095\n",
      "Epoch 34/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4021 - acc: 0.8223 - val_loss: 0.4254 - val_acc: 0.8144\n",
      "Epoch 35/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3983 - acc: 0.8298 - val_loss: 0.4271 - val_acc: 0.8095\n",
      "Epoch 36/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3974 - acc: 0.8314 - val_loss: 0.4241 - val_acc: 0.8194\n",
      "Epoch 37/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3959 - acc: 0.8290 - val_loss: 0.4241 - val_acc: 0.8194\n",
      "Epoch 38/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3954 - acc: 0.8290 - val_loss: 0.4239 - val_acc: 0.8161\n",
      "Epoch 39/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3923 - acc: 0.8303 - val_loss: 0.4217 - val_acc: 0.8210\n",
      "Epoch 40/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3895 - acc: 0.8307 - val_loss: 0.4263 - val_acc: 0.8046\n",
      "Epoch 41/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3906 - acc: 0.8314 - val_loss: 0.4227 - val_acc: 0.8194\n",
      "Epoch 42/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3890 - acc: 0.8340 - val_loss: 0.4214 - val_acc: 0.8194\n",
      "Epoch 43/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3894 - acc: 0.8290 - val_loss: 0.4205 - val_acc: 0.8161\n",
      "Epoch 44/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3824 - acc: 0.8318 - val_loss: 0.4255 - val_acc: 0.8062\n",
      "Epoch 45/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3807 - acc: 0.8318 - val_loss: 0.4207 - val_acc: 0.8161\n",
      "Epoch 46/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3811 - acc: 0.8385 - val_loss: 0.4220 - val_acc: 0.8144\n",
      "Epoch 47/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3802 - acc: 0.8371 - val_loss: 0.4205 - val_acc: 0.8161\n",
      "Epoch 48/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3798 - acc: 0.8363 - val_loss: 0.4196 - val_acc: 0.8194\n",
      "Epoch 49/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3758 - acc: 0.8400 - val_loss: 0.4244 - val_acc: 0.8079\n",
      "Epoch 50/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3748 - acc: 0.8380 - val_loss: 0.4215 - val_acc: 0.8128\n",
      "Epoch 51/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3717 - acc: 0.8394 - val_loss: 0.4189 - val_acc: 0.8161\n",
      "Epoch 52/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3715 - acc: 0.8387 - val_loss: 0.4217 - val_acc: 0.8161\n",
      "Epoch 53/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3689 - acc: 0.8405 - val_loss: 0.4264 - val_acc: 0.8112\n",
      "Epoch 54/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3671 - acc: 0.8449 - val_loss: 0.4187 - val_acc: 0.8177\n",
      "Epoch 55/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3638 - acc: 0.8453 - val_loss: 0.4212 - val_acc: 0.8177\n",
      "Epoch 56/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3652 - acc: 0.8471 - val_loss: 0.4220 - val_acc: 0.8144\n",
      "Epoch 57/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3652 - acc: 0.8436 - val_loss: 0.4186 - val_acc: 0.8194\n",
      "Epoch 58/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3648 - acc: 0.8415 - val_loss: 0.4209 - val_acc: 0.8210\n",
      "Epoch 59/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3621 - acc: 0.8475 - val_loss: 0.4218 - val_acc: 0.8227\n",
      "Epoch 60/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3632 - acc: 0.8464 - val_loss: 0.4184 - val_acc: 0.8194\n",
      "Epoch 61/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3568 - acc: 0.8478 - val_loss: 0.4182 - val_acc: 0.8194\n",
      "Epoch 62/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3534 - acc: 0.8533 - val_loss: 0.4211 - val_acc: 0.8210\n",
      "Epoch 63/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3539 - acc: 0.8489 - val_loss: 0.4189 - val_acc: 0.8210\n",
      "Epoch 64/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3515 - acc: 0.8478 - val_loss: 0.4178 - val_acc: 0.8144\n",
      "Epoch 65/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3490 - acc: 0.8517 - val_loss: 0.4201 - val_acc: 0.8194\n",
      "Epoch 66/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3483 - acc: 0.8489 - val_loss: 0.4199 - val_acc: 0.8177\n",
      "Epoch 67/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3481 - acc: 0.8517 - val_loss: 0.4206 - val_acc: 0.8210\n",
      "Epoch 68/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3438 - acc: 0.8544 - val_loss: 0.4230 - val_acc: 0.8210\n",
      "Epoch 69/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3434 - acc: 0.8509 - val_loss: 0.4236 - val_acc: 0.8161\n",
      "Training with parameters {'batch_size': 500, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_120\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_125 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_334 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_214 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_335 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_215 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_336 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.7195 - acc: 0.4786 - val_loss: 0.6724 - val_acc: 0.6667\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6612 - acc: 0.6294 - val_loss: 0.6505 - val_acc: 0.5714\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6387 - acc: 0.6406 - val_loss: 0.6261 - val_acc: 0.6585\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6156 - acc: 0.6931 - val_loss: 0.5986 - val_acc: 0.7389\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5914 - acc: 0.7199 - val_loss: 0.5767 - val_acc: 0.7537\n",
      "Epoch 6/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5725 - acc: 0.7351 - val_loss: 0.5567 - val_acc: 0.7701\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5554 - acc: 0.7427 - val_loss: 0.5356 - val_acc: 0.7734\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5351 - acc: 0.7526 - val_loss: 0.5180 - val_acc: 0.7833\n",
      "Epoch 9/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5173 - acc: 0.7687 - val_loss: 0.5021 - val_acc: 0.7931\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5051 - acc: 0.7694 - val_loss: 0.4900 - val_acc: 0.7947\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4916 - acc: 0.7796 - val_loss: 0.4806 - val_acc: 0.7964\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4802 - acc: 0.7836 - val_loss: 0.4745 - val_acc: 0.7997\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4717 - acc: 0.7898 - val_loss: 0.4684 - val_acc: 0.8013\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4658 - acc: 0.7924 - val_loss: 0.4638 - val_acc: 0.8013\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4598 - acc: 0.7977 - val_loss: 0.4597 - val_acc: 0.8030\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4552 - acc: 0.8000 - val_loss: 0.4540 - val_acc: 0.7997\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.4490 - acc: 0.8011 - val_loss: 0.4536 - val_acc: 0.8030\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4471 - acc: 0.8053 - val_loss: 0.4482 - val_acc: 0.8062\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4404 - acc: 0.8062 - val_loss: 0.4499 - val_acc: 0.8062\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4361 - acc: 0.8062 - val_loss: 0.4449 - val_acc: 0.8112\n",
      "Epoch 21/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4367 - acc: 0.8055 - val_loss: 0.4426 - val_acc: 0.8095\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4315 - acc: 0.8101 - val_loss: 0.4424 - val_acc: 0.8030\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4300 - acc: 0.8123 - val_loss: 0.4383 - val_acc: 0.8112\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4231 - acc: 0.8130 - val_loss: 0.4382 - val_acc: 0.8128\n",
      "Epoch 25/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4212 - acc: 0.8152 - val_loss: 0.4363 - val_acc: 0.8144\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4196 - acc: 0.8172 - val_loss: 0.4337 - val_acc: 0.8128\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4184 - acc: 0.8212 - val_loss: 0.4340 - val_acc: 0.8128\n",
      "Epoch 28/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4184 - acc: 0.8124 - val_loss: 0.4355 - val_acc: 0.8112\n",
      "Epoch 29/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4127 - acc: 0.8207 - val_loss: 0.4343 - val_acc: 0.8144\n",
      "Epoch 30/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4130 - acc: 0.8217 - val_loss: 0.4298 - val_acc: 0.8161\n",
      "Epoch 31/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4078 - acc: 0.8228 - val_loss: 0.4348 - val_acc: 0.8112\n",
      "Epoch 32/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4051 - acc: 0.8232 - val_loss: 0.4291 - val_acc: 0.8128\n",
      "Epoch 33/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4047 - acc: 0.8258 - val_loss: 0.4282 - val_acc: 0.8128\n",
      "Epoch 34/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4052 - acc: 0.8214 - val_loss: 0.4364 - val_acc: 0.8046\n",
      "Epoch 35/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4034 - acc: 0.8258 - val_loss: 0.4267 - val_acc: 0.8128\n",
      "Epoch 36/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4003 - acc: 0.8269 - val_loss: 0.4285 - val_acc: 0.8128\n",
      "Epoch 37/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3988 - acc: 0.8296 - val_loss: 0.4281 - val_acc: 0.8128\n",
      "Epoch 38/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3978 - acc: 0.8283 - val_loss: 0.4248 - val_acc: 0.8128\n",
      "Epoch 39/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3919 - acc: 0.8316 - val_loss: 0.4258 - val_acc: 0.8161\n",
      "Epoch 40/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3903 - acc: 0.8305 - val_loss: 0.4253 - val_acc: 0.8161\n",
      "Epoch 41/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3905 - acc: 0.8316 - val_loss: 0.4249 - val_acc: 0.8144\n",
      "Epoch 42/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3882 - acc: 0.8283 - val_loss: 0.4227 - val_acc: 0.8144\n",
      "Epoch 43/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3868 - acc: 0.8332 - val_loss: 0.4259 - val_acc: 0.8144\n",
      "Epoch 44/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3875 - acc: 0.8325 - val_loss: 0.4240 - val_acc: 0.8194\n",
      "Epoch 45/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3833 - acc: 0.8336 - val_loss: 0.4211 - val_acc: 0.8210\n",
      "Epoch 46/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3844 - acc: 0.8296 - val_loss: 0.4246 - val_acc: 0.8128\n",
      "Epoch 47/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3794 - acc: 0.8378 - val_loss: 0.4234 - val_acc: 0.8177\n",
      "Epoch 48/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3797 - acc: 0.8356 - val_loss: 0.4205 - val_acc: 0.8243\n",
      "Epoch 49/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3796 - acc: 0.8384 - val_loss: 0.4223 - val_acc: 0.8161\n",
      "Epoch 50/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3762 - acc: 0.8343 - val_loss: 0.4218 - val_acc: 0.8144\n",
      "Epoch 51/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3754 - acc: 0.8394 - val_loss: 0.4200 - val_acc: 0.8144\n",
      "Epoch 52/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3745 - acc: 0.8360 - val_loss: 0.4302 - val_acc: 0.8161\n",
      "Epoch 53/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3716 - acc: 0.8367 - val_loss: 0.4189 - val_acc: 0.8177\n",
      "Epoch 54/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3718 - acc: 0.8415 - val_loss: 0.4188 - val_acc: 0.8161\n",
      "Epoch 55/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3687 - acc: 0.8400 - val_loss: 0.4265 - val_acc: 0.8112\n",
      "Epoch 56/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3659 - acc: 0.8393 - val_loss: 0.4200 - val_acc: 0.8095\n",
      "Epoch 57/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3631 - acc: 0.8455 - val_loss: 0.4212 - val_acc: 0.8095\n",
      "Epoch 58/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3632 - acc: 0.8442 - val_loss: 0.4241 - val_acc: 0.8128\n",
      "Epoch 59/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3623 - acc: 0.8411 - val_loss: 0.4194 - val_acc: 0.8128\n",
      "Training with parameters {'batch_size': 500, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_121\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_126 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_337 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_216 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_338 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_217 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_339 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.7515 - acc: 0.4621 - val_loss: 0.6739 - val_acc: 0.6092\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6594 - acc: 0.5977 - val_loss: 0.6455 - val_acc: 0.5534\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6254 - acc: 0.6508 - val_loss: 0.6010 - val_acc: 0.7340\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5959 - acc: 0.7050 - val_loss: 0.5712 - val_acc: 0.7438\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5698 - acc: 0.7225 - val_loss: 0.5482 - val_acc: 0.7586\n",
      "Epoch 6/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5469 - acc: 0.7393 - val_loss: 0.5246 - val_acc: 0.7750\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5240 - acc: 0.7614 - val_loss: 0.5040 - val_acc: 0.7849\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5043 - acc: 0.7719 - val_loss: 0.4879 - val_acc: 0.7947\n",
      "Epoch 9/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4895 - acc: 0.7780 - val_loss: 0.4735 - val_acc: 0.7947\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4740 - acc: 0.7860 - val_loss: 0.4662 - val_acc: 0.8030\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4633 - acc: 0.7902 - val_loss: 0.4604 - val_acc: 0.8095\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4524 - acc: 0.7997 - val_loss: 0.4554 - val_acc: 0.8079\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4446 - acc: 0.8019 - val_loss: 0.4493 - val_acc: 0.8112\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4408 - acc: 0.8057 - val_loss: 0.4495 - val_acc: 0.8128\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4350 - acc: 0.8095 - val_loss: 0.4456 - val_acc: 0.8161\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4288 - acc: 0.8070 - val_loss: 0.4393 - val_acc: 0.8112\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4252 - acc: 0.8123 - val_loss: 0.4387 - val_acc: 0.8161\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4222 - acc: 0.8176 - val_loss: 0.4358 - val_acc: 0.8161\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4169 - acc: 0.8192 - val_loss: 0.4435 - val_acc: 0.8128\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4139 - acc: 0.8186 - val_loss: 0.4369 - val_acc: 0.8161\n",
      "Epoch 21/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4125 - acc: 0.8174 - val_loss: 0.4311 - val_acc: 0.8243\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4099 - acc: 0.8179 - val_loss: 0.4311 - val_acc: 0.8243\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4112 - acc: 0.8143 - val_loss: 0.4330 - val_acc: 0.8161\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4049 - acc: 0.8217 - val_loss: 0.4297 - val_acc: 0.8210\n",
      "Epoch 25/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4006 - acc: 0.8234 - val_loss: 0.4311 - val_acc: 0.8144\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3981 - acc: 0.8254 - val_loss: 0.4276 - val_acc: 0.8210\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3972 - acc: 0.8243 - val_loss: 0.4308 - val_acc: 0.8112\n",
      "Epoch 28/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3915 - acc: 0.8274 - val_loss: 0.4304 - val_acc: 0.8079\n",
      "Epoch 29/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3913 - acc: 0.8270 - val_loss: 0.4355 - val_acc: 0.8095\n",
      "Epoch 30/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3898 - acc: 0.8290 - val_loss: 0.4271 - val_acc: 0.8194\n",
      "Epoch 31/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3894 - acc: 0.8323 - val_loss: 0.4243 - val_acc: 0.8259\n",
      "Epoch 32/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3876 - acc: 0.8312 - val_loss: 0.4237 - val_acc: 0.8243\n",
      "Epoch 33/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3848 - acc: 0.8312 - val_loss: 0.4239 - val_acc: 0.8227\n",
      "Epoch 34/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3846 - acc: 0.8325 - val_loss: 0.4237 - val_acc: 0.8243\n",
      "Epoch 35/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3774 - acc: 0.8363 - val_loss: 0.4267 - val_acc: 0.8210\n",
      "Epoch 36/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3762 - acc: 0.8369 - val_loss: 0.4308 - val_acc: 0.8144\n",
      "Epoch 37/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3740 - acc: 0.8393 - val_loss: 0.4257 - val_acc: 0.8227\n",
      "Epoch 38/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3699 - acc: 0.8391 - val_loss: 0.4291 - val_acc: 0.8194\n",
      "Epoch 39/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3664 - acc: 0.8402 - val_loss: 0.4222 - val_acc: 0.8243\n",
      "Epoch 40/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3648 - acc: 0.8425 - val_loss: 0.4222 - val_acc: 0.8276\n",
      "Epoch 41/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3607 - acc: 0.8431 - val_loss: 0.4229 - val_acc: 0.8309\n",
      "Epoch 42/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3603 - acc: 0.8429 - val_loss: 0.4227 - val_acc: 0.8292\n",
      "Epoch 43/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3561 - acc: 0.8440 - val_loss: 0.4272 - val_acc: 0.8227\n",
      "Epoch 44/300\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3549 - acc: 0.8491 - val_loss: 0.4227 - val_acc: 0.8259\n",
      "Training with parameters {'batch_size': 700, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_122\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_127 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_340 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_218 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_341 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_219 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_342 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.7124 - acc: 0.4952 - val_loss: 0.6754 - val_acc: 0.5468\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6531 - acc: 0.5804 - val_loss: 0.6307 - val_acc: 0.6108\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.6119 - acc: 0.6833 - val_loss: 0.5866 - val_acc: 0.7258\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5830 - acc: 0.7072 - val_loss: 0.5589 - val_acc: 0.7619\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5533 - acc: 0.7477 - val_loss: 0.5333 - val_acc: 0.7734\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5294 - acc: 0.7586 - val_loss: 0.5088 - val_acc: 0.7783\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5081 - acc: 0.7694 - val_loss: 0.4917 - val_acc: 0.7898\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4896 - acc: 0.7778 - val_loss: 0.4777 - val_acc: 0.7915\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4764 - acc: 0.7893 - val_loss: 0.4702 - val_acc: 0.7980\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4619 - acc: 0.7935 - val_loss: 0.4595 - val_acc: 0.8046\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4528 - acc: 0.7966 - val_loss: 0.4516 - val_acc: 0.7964\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4441 - acc: 0.8019 - val_loss: 0.4467 - val_acc: 0.8112\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4368 - acc: 0.8095 - val_loss: 0.4480 - val_acc: 0.8079\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4319 - acc: 0.8132 - val_loss: 0.4386 - val_acc: 0.8161\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4257 - acc: 0.8166 - val_loss: 0.4397 - val_acc: 0.8128\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4219 - acc: 0.8128 - val_loss: 0.4358 - val_acc: 0.8128\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4160 - acc: 0.8199 - val_loss: 0.4320 - val_acc: 0.8177\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4137 - acc: 0.8186 - val_loss: 0.4395 - val_acc: 0.8112\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4109 - acc: 0.8212 - val_loss: 0.4306 - val_acc: 0.8161\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4054 - acc: 0.8248 - val_loss: 0.4272 - val_acc: 0.8177\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4052 - acc: 0.8276 - val_loss: 0.4316 - val_acc: 0.8112\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4013 - acc: 0.8267 - val_loss: 0.4303 - val_acc: 0.8095\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3979 - acc: 0.8270 - val_loss: 0.4242 - val_acc: 0.8243\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3943 - acc: 0.8276 - val_loss: 0.4251 - val_acc: 0.8161\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3905 - acc: 0.8329 - val_loss: 0.4257 - val_acc: 0.8128\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3884 - acc: 0.8327 - val_loss: 0.4266 - val_acc: 0.8144\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3862 - acc: 0.8318 - val_loss: 0.4221 - val_acc: 0.8243\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3841 - acc: 0.8338 - val_loss: 0.4216 - val_acc: 0.8259\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3803 - acc: 0.8340 - val_loss: 0.4248 - val_acc: 0.8177\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3761 - acc: 0.8385 - val_loss: 0.4232 - val_acc: 0.8177\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3755 - acc: 0.8398 - val_loss: 0.4210 - val_acc: 0.8194\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3721 - acc: 0.8402 - val_loss: 0.4230 - val_acc: 0.8194\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3694 - acc: 0.8427 - val_loss: 0.4255 - val_acc: 0.8161\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3679 - acc: 0.8407 - val_loss: 0.4215 - val_acc: 0.8194\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3643 - acc: 0.8451 - val_loss: 0.4214 - val_acc: 0.8194\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3605 - acc: 0.8444 - val_loss: 0.4210 - val_acc: 0.8194\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3569 - acc: 0.8460 - val_loss: 0.4252 - val_acc: 0.8161\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3536 - acc: 0.8473 - val_loss: 0.4203 - val_acc: 0.8161\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3544 - acc: 0.8491 - val_loss: 0.4214 - val_acc: 0.8194\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3483 - acc: 0.8526 - val_loss: 0.4214 - val_acc: 0.8161\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3460 - acc: 0.8535 - val_loss: 0.4222 - val_acc: 0.8128\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3441 - acc: 0.8528 - val_loss: 0.4253 - val_acc: 0.8161\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3385 - acc: 0.8573 - val_loss: 0.4247 - val_acc: 0.8177\n",
      "Training with parameters {'batch_size': 700, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_123\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_128 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_343 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_220 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_344 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_221 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_345 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.7795 - acc: 0.4364 - val_loss: 0.6925 - val_acc: 0.5090\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6817 - acc: 0.5634 - val_loss: 0.6606 - val_acc: 0.5550\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6499 - acc: 0.6003 - val_loss: 0.6365 - val_acc: 0.5681\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6239 - acc: 0.6625 - val_loss: 0.6047 - val_acc: 0.7241\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5993 - acc: 0.7126 - val_loss: 0.5808 - val_acc: 0.7406\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5807 - acc: 0.7225 - val_loss: 0.5613 - val_acc: 0.7438\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5657 - acc: 0.7285 - val_loss: 0.5441 - val_acc: 0.7635\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5492 - acc: 0.7457 - val_loss: 0.5255 - val_acc: 0.7685\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5302 - acc: 0.7528 - val_loss: 0.5087 - val_acc: 0.7783\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5142 - acc: 0.7646 - val_loss: 0.4963 - val_acc: 0.7915\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5001 - acc: 0.7719 - val_loss: 0.4833 - val_acc: 0.7964\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4839 - acc: 0.7823 - val_loss: 0.4764 - val_acc: 0.8013\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4743 - acc: 0.7882 - val_loss: 0.4666 - val_acc: 0.7980\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4678 - acc: 0.7915 - val_loss: 0.4635 - val_acc: 0.8062\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4607 - acc: 0.7951 - val_loss: 0.4572 - val_acc: 0.8062\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4556 - acc: 0.7975 - val_loss: 0.4552 - val_acc: 0.8079\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4464 - acc: 0.8059 - val_loss: 0.4498 - val_acc: 0.8112\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4414 - acc: 0.8024 - val_loss: 0.4475 - val_acc: 0.8112\n",
      "Epoch 19/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4374 - acc: 0.8041 - val_loss: 0.4436 - val_acc: 0.8095\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4352 - acc: 0.8081 - val_loss: 0.4453 - val_acc: 0.8144\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4299 - acc: 0.8095 - val_loss: 0.4391 - val_acc: 0.8144\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4256 - acc: 0.8121 - val_loss: 0.4409 - val_acc: 0.8161\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4231 - acc: 0.8139 - val_loss: 0.4382 - val_acc: 0.8144\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4229 - acc: 0.8130 - val_loss: 0.4362 - val_acc: 0.8177\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4161 - acc: 0.8177 - val_loss: 0.4351 - val_acc: 0.8144\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4133 - acc: 0.8148 - val_loss: 0.4363 - val_acc: 0.8144\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4116 - acc: 0.8214 - val_loss: 0.4322 - val_acc: 0.8161\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4102 - acc: 0.8196 - val_loss: 0.4361 - val_acc: 0.8144\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4086 - acc: 0.8217 - val_loss: 0.4318 - val_acc: 0.8177\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4054 - acc: 0.8223 - val_loss: 0.4293 - val_acc: 0.8243\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4053 - acc: 0.8172 - val_loss: 0.4335 - val_acc: 0.8128\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3996 - acc: 0.8217 - val_loss: 0.4278 - val_acc: 0.8243\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3971 - acc: 0.8294 - val_loss: 0.4324 - val_acc: 0.8128\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3936 - acc: 0.8290 - val_loss: 0.4267 - val_acc: 0.8276\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3943 - acc: 0.8278 - val_loss: 0.4281 - val_acc: 0.8144\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3936 - acc: 0.8292 - val_loss: 0.4332 - val_acc: 0.8095\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3935 - acc: 0.8267 - val_loss: 0.4255 - val_acc: 0.8276\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3889 - acc: 0.8325 - val_loss: 0.4268 - val_acc: 0.8194\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3850 - acc: 0.8334 - val_loss: 0.4275 - val_acc: 0.8177\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3858 - acc: 0.8338 - val_loss: 0.4251 - val_acc: 0.8227\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3823 - acc: 0.8334 - val_loss: 0.4284 - val_acc: 0.8177\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3782 - acc: 0.8356 - val_loss: 0.4237 - val_acc: 0.8243\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3774 - acc: 0.8385 - val_loss: 0.4241 - val_acc: 0.8243\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3756 - acc: 0.8389 - val_loss: 0.4293 - val_acc: 0.8161\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3717 - acc: 0.8409 - val_loss: 0.4222 - val_acc: 0.8243\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3741 - acc: 0.8363 - val_loss: 0.4241 - val_acc: 0.8276\n",
      "Epoch 47/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3713 - acc: 0.8391 - val_loss: 0.4255 - val_acc: 0.8259\n",
      "Epoch 48/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3643 - acc: 0.8420 - val_loss: 0.4246 - val_acc: 0.8259\n",
      "Epoch 49/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3652 - acc: 0.8444 - val_loss: 0.4234 - val_acc: 0.8259\n",
      "Epoch 50/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3625 - acc: 0.8425 - val_loss: 0.4297 - val_acc: 0.8177\n",
      "Training with parameters {'batch_size': 700, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_124\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_129 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_346 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_222 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_347 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.6568 - acc: 0.6130 - val_loss: 0.6402 - val_acc: 0.6338\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6285 - acc: 0.6519 - val_loss: 0.6174 - val_acc: 0.7011\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6085 - acc: 0.6867 - val_loss: 0.5948 - val_acc: 0.7340\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5880 - acc: 0.7110 - val_loss: 0.5779 - val_acc: 0.7504\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5733 - acc: 0.7263 - val_loss: 0.5640 - val_acc: 0.7537\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5616 - acc: 0.7365 - val_loss: 0.5497 - val_acc: 0.7619\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5475 - acc: 0.7458 - val_loss: 0.5381 - val_acc: 0.7767\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5386 - acc: 0.7537 - val_loss: 0.5275 - val_acc: 0.7849\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5285 - acc: 0.7623 - val_loss: 0.5186 - val_acc: 0.7882\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5184 - acc: 0.7643 - val_loss: 0.5103 - val_acc: 0.7898\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5108 - acc: 0.7729 - val_loss: 0.5053 - val_acc: 0.7898\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5030 - acc: 0.7761 - val_loss: 0.4981 - val_acc: 0.7947\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4953 - acc: 0.7800 - val_loss: 0.4937 - val_acc: 0.7898\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4920 - acc: 0.7820 - val_loss: 0.4880 - val_acc: 0.7898\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4862 - acc: 0.7853 - val_loss: 0.4843 - val_acc: 0.7898\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4827 - acc: 0.7871 - val_loss: 0.4802 - val_acc: 0.7931\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4769 - acc: 0.7911 - val_loss: 0.4772 - val_acc: 0.7898\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4717 - acc: 0.7929 - val_loss: 0.4737 - val_acc: 0.7898\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4680 - acc: 0.7977 - val_loss: 0.4711 - val_acc: 0.7947\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4635 - acc: 0.7978 - val_loss: 0.4676 - val_acc: 0.7947\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4641 - acc: 0.7960 - val_loss: 0.4655 - val_acc: 0.7964\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4584 - acc: 0.8006 - val_loss: 0.4633 - val_acc: 0.7997\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4566 - acc: 0.8022 - val_loss: 0.4609 - val_acc: 0.8046\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4552 - acc: 0.8037 - val_loss: 0.4588 - val_acc: 0.8062\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4509 - acc: 0.8017 - val_loss: 0.4569 - val_acc: 0.8079\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4485 - acc: 0.8088 - val_loss: 0.4553 - val_acc: 0.8062\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4465 - acc: 0.8106 - val_loss: 0.4532 - val_acc: 0.8095\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4452 - acc: 0.8051 - val_loss: 0.4521 - val_acc: 0.8079\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4417 - acc: 0.8082 - val_loss: 0.4513 - val_acc: 0.8095\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4399 - acc: 0.8119 - val_loss: 0.4488 - val_acc: 0.8128\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4390 - acc: 0.8101 - val_loss: 0.4492 - val_acc: 0.8079\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4349 - acc: 0.8143 - val_loss: 0.4467 - val_acc: 0.8128\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4342 - acc: 0.8113 - val_loss: 0.4465 - val_acc: 0.8095\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4342 - acc: 0.8119 - val_loss: 0.4441 - val_acc: 0.8112\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4322 - acc: 0.8121 - val_loss: 0.4443 - val_acc: 0.8062\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4281 - acc: 0.8150 - val_loss: 0.4427 - val_acc: 0.8112\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4276 - acc: 0.8168 - val_loss: 0.4418 - val_acc: 0.8128\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4245 - acc: 0.8179 - val_loss: 0.4412 - val_acc: 0.8112\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4257 - acc: 0.8159 - val_loss: 0.4401 - val_acc: 0.8144\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4238 - acc: 0.8163 - val_loss: 0.4398 - val_acc: 0.8161\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4226 - acc: 0.8194 - val_loss: 0.4386 - val_acc: 0.8177\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4224 - acc: 0.8183 - val_loss: 0.4388 - val_acc: 0.8161\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4209 - acc: 0.8183 - val_loss: 0.4374 - val_acc: 0.8161\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4200 - acc: 0.8174 - val_loss: 0.4362 - val_acc: 0.8177\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4182 - acc: 0.8188 - val_loss: 0.4366 - val_acc: 0.8161\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4175 - acc: 0.8186 - val_loss: 0.4349 - val_acc: 0.8177\n",
      "Epoch 47/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4180 - acc: 0.8190 - val_loss: 0.4356 - val_acc: 0.8144\n",
      "Epoch 48/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4152 - acc: 0.8210 - val_loss: 0.4335 - val_acc: 0.8194\n",
      "Epoch 49/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4140 - acc: 0.8181 - val_loss: 0.4344 - val_acc: 0.8128\n",
      "Epoch 50/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4127 - acc: 0.8223 - val_loss: 0.4327 - val_acc: 0.8194\n",
      "Epoch 51/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4129 - acc: 0.8177 - val_loss: 0.4336 - val_acc: 0.8144\n",
      "Epoch 52/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4114 - acc: 0.8225 - val_loss: 0.4320 - val_acc: 0.8194\n",
      "Epoch 53/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4092 - acc: 0.8252 - val_loss: 0.4316 - val_acc: 0.8194\n",
      "Epoch 54/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4094 - acc: 0.8223 - val_loss: 0.4319 - val_acc: 0.8128\n",
      "Epoch 55/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4108 - acc: 0.8227 - val_loss: 0.4304 - val_acc: 0.8194\n",
      "Epoch 56/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4071 - acc: 0.8234 - val_loss: 0.4309 - val_acc: 0.8128\n",
      "Epoch 57/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4057 - acc: 0.8245 - val_loss: 0.4302 - val_acc: 0.8161\n",
      "Epoch 58/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4072 - acc: 0.8208 - val_loss: 0.4300 - val_acc: 0.8161\n",
      "Epoch 59/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4053 - acc: 0.8259 - val_loss: 0.4295 - val_acc: 0.8161\n",
      "Epoch 60/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4049 - acc: 0.8267 - val_loss: 0.4297 - val_acc: 0.8128\n",
      "Epoch 61/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4034 - acc: 0.8254 - val_loss: 0.4287 - val_acc: 0.8128\n",
      "Epoch 62/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4015 - acc: 0.8265 - val_loss: 0.4293 - val_acc: 0.8144\n",
      "Epoch 63/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4015 - acc: 0.8265 - val_loss: 0.4280 - val_acc: 0.8128\n",
      "Epoch 64/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4025 - acc: 0.8252 - val_loss: 0.4275 - val_acc: 0.8161\n",
      "Epoch 65/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3996 - acc: 0.8263 - val_loss: 0.4282 - val_acc: 0.8144\n",
      "Epoch 66/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4003 - acc: 0.8280 - val_loss: 0.4270 - val_acc: 0.8128\n",
      "Epoch 67/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3982 - acc: 0.8274 - val_loss: 0.4270 - val_acc: 0.8144\n",
      "Epoch 68/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3973 - acc: 0.8300 - val_loss: 0.4269 - val_acc: 0.8144\n",
      "Epoch 69/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3976 - acc: 0.8280 - val_loss: 0.4267 - val_acc: 0.8144\n",
      "Epoch 70/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3980 - acc: 0.8269 - val_loss: 0.4257 - val_acc: 0.8144\n",
      "Epoch 71/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3959 - acc: 0.8316 - val_loss: 0.4269 - val_acc: 0.8144\n",
      "Epoch 72/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3945 - acc: 0.8311 - val_loss: 0.4248 - val_acc: 0.8227\n",
      "Epoch 73/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3959 - acc: 0.8283 - val_loss: 0.4268 - val_acc: 0.8144\n",
      "Epoch 74/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3922 - acc: 0.8314 - val_loss: 0.4254 - val_acc: 0.8144\n",
      "Epoch 75/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3923 - acc: 0.8292 - val_loss: 0.4243 - val_acc: 0.8177\n",
      "Epoch 76/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3941 - acc: 0.8290 - val_loss: 0.4256 - val_acc: 0.8112\n",
      "Epoch 77/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3924 - acc: 0.8303 - val_loss: 0.4245 - val_acc: 0.8144\n",
      "Epoch 78/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3916 - acc: 0.8296 - val_loss: 0.4251 - val_acc: 0.8112\n",
      "Epoch 79/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3899 - acc: 0.8314 - val_loss: 0.4233 - val_acc: 0.8194\n",
      "Epoch 80/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3898 - acc: 0.8331 - val_loss: 0.4247 - val_acc: 0.8128\n",
      "Epoch 81/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3895 - acc: 0.8294 - val_loss: 0.4243 - val_acc: 0.8128\n",
      "Epoch 82/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3875 - acc: 0.8331 - val_loss: 0.4242 - val_acc: 0.8128\n",
      "Epoch 83/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3882 - acc: 0.8331 - val_loss: 0.4233 - val_acc: 0.8144\n",
      "Epoch 84/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3875 - acc: 0.8323 - val_loss: 0.4240 - val_acc: 0.8112\n",
      "Epoch 85/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3867 - acc: 0.8340 - val_loss: 0.4241 - val_acc: 0.8112\n",
      "Epoch 86/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3881 - acc: 0.8312 - val_loss: 0.4234 - val_acc: 0.8112\n",
      "Epoch 87/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3846 - acc: 0.8321 - val_loss: 0.4230 - val_acc: 0.8112\n",
      "Epoch 88/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3865 - acc: 0.8365 - val_loss: 0.4231 - val_acc: 0.8095\n",
      "Epoch 89/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3846 - acc: 0.8340 - val_loss: 0.4226 - val_acc: 0.8112\n",
      "Epoch 90/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3831 - acc: 0.8338 - val_loss: 0.4230 - val_acc: 0.8112\n",
      "Epoch 91/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3836 - acc: 0.8316 - val_loss: 0.4220 - val_acc: 0.8144\n",
      "Epoch 92/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3825 - acc: 0.8347 - val_loss: 0.4226 - val_acc: 0.8112\n",
      "Epoch 93/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3821 - acc: 0.8349 - val_loss: 0.4228 - val_acc: 0.8112\n",
      "Epoch 94/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3808 - acc: 0.8343 - val_loss: 0.4221 - val_acc: 0.8144\n",
      "Epoch 95/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3801 - acc: 0.8349 - val_loss: 0.4228 - val_acc: 0.8112\n",
      "Epoch 96/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3801 - acc: 0.8323 - val_loss: 0.4215 - val_acc: 0.8144\n",
      "Epoch 97/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3792 - acc: 0.8358 - val_loss: 0.4223 - val_acc: 0.8112\n",
      "Epoch 98/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3785 - acc: 0.8371 - val_loss: 0.4217 - val_acc: 0.8128\n",
      "Epoch 99/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3791 - acc: 0.8347 - val_loss: 0.4223 - val_acc: 0.8112\n",
      "Epoch 100/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3779 - acc: 0.8360 - val_loss: 0.4215 - val_acc: 0.8144\n",
      "Epoch 101/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3780 - acc: 0.8394 - val_loss: 0.4218 - val_acc: 0.8128\n",
      "Training with parameters {'batch_size': 700, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_125\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_130 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_348 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_223 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_349 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_224 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_350 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.7598 - acc: 0.4627 - val_loss: 0.6812 - val_acc: 0.5452\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6665 - acc: 0.5771 - val_loss: 0.6517 - val_acc: 0.5435\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6284 - acc: 0.6316 - val_loss: 0.6020 - val_acc: 0.7373\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5969 - acc: 0.7002 - val_loss: 0.5732 - val_acc: 0.7356\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5709 - acc: 0.7187 - val_loss: 0.5530 - val_acc: 0.7685\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5512 - acc: 0.7477 - val_loss: 0.5284 - val_acc: 0.7619\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5300 - acc: 0.7521 - val_loss: 0.5117 - val_acc: 0.7865\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5130 - acc: 0.7665 - val_loss: 0.4947 - val_acc: 0.7882\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4979 - acc: 0.7769 - val_loss: 0.4836 - val_acc: 0.7931\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4807 - acc: 0.7891 - val_loss: 0.4728 - val_acc: 0.7997\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4696 - acc: 0.7960 - val_loss: 0.4657 - val_acc: 0.7964\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4610 - acc: 0.8006 - val_loss: 0.4584 - val_acc: 0.7997\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4536 - acc: 0.8020 - val_loss: 0.4533 - val_acc: 0.8079\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4466 - acc: 0.8020 - val_loss: 0.4480 - val_acc: 0.8079\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4403 - acc: 0.8108 - val_loss: 0.4470 - val_acc: 0.8095\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4346 - acc: 0.8121 - val_loss: 0.4429 - val_acc: 0.8144\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4311 - acc: 0.8128 - val_loss: 0.4405 - val_acc: 0.8144\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4264 - acc: 0.8137 - val_loss: 0.4365 - val_acc: 0.8177\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4227 - acc: 0.8176 - val_loss: 0.4349 - val_acc: 0.8144\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4187 - acc: 0.8181 - val_loss: 0.4370 - val_acc: 0.8128\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4150 - acc: 0.8239 - val_loss: 0.4318 - val_acc: 0.8144\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4108 - acc: 0.8241 - val_loss: 0.4286 - val_acc: 0.8243\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4089 - acc: 0.8252 - val_loss: 0.4276 - val_acc: 0.8243\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4062 - acc: 0.8252 - val_loss: 0.4340 - val_acc: 0.8079\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4044 - acc: 0.8245 - val_loss: 0.4255 - val_acc: 0.8243\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4022 - acc: 0.8252 - val_loss: 0.4241 - val_acc: 0.8227\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3975 - acc: 0.8281 - val_loss: 0.4282 - val_acc: 0.8128\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3950 - acc: 0.8311 - val_loss: 0.4263 - val_acc: 0.8144\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3948 - acc: 0.8312 - val_loss: 0.4215 - val_acc: 0.8259\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3906 - acc: 0.8312 - val_loss: 0.4257 - val_acc: 0.8177\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3875 - acc: 0.8336 - val_loss: 0.4214 - val_acc: 0.8177\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3852 - acc: 0.8354 - val_loss: 0.4240 - val_acc: 0.8161\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3823 - acc: 0.8332 - val_loss: 0.4213 - val_acc: 0.8161\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3795 - acc: 0.8373 - val_loss: 0.4218 - val_acc: 0.8177\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3774 - acc: 0.8380 - val_loss: 0.4219 - val_acc: 0.8161\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3751 - acc: 0.8389 - val_loss: 0.4250 - val_acc: 0.8095\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3740 - acc: 0.8393 - val_loss: 0.4214 - val_acc: 0.8161\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3725 - acc: 0.8404 - val_loss: 0.4189 - val_acc: 0.8210\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3670 - acc: 0.8433 - val_loss: 0.4223 - val_acc: 0.8161\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3672 - acc: 0.8407 - val_loss: 0.4265 - val_acc: 0.8112\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3647 - acc: 0.8415 - val_loss: 0.4182 - val_acc: 0.8210\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3664 - acc: 0.8407 - val_loss: 0.4195 - val_acc: 0.8194\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3592 - acc: 0.8451 - val_loss: 0.4218 - val_acc: 0.8128\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3569 - acc: 0.8449 - val_loss: 0.4199 - val_acc: 0.8177\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3529 - acc: 0.8473 - val_loss: 0.4187 - val_acc: 0.8227\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3508 - acc: 0.8484 - val_loss: 0.4245 - val_acc: 0.8144\n",
      "Training with parameters {'batch_size': 700, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_126\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_131 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_351 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_225 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_352 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_226 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_353 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.8080 - acc: 0.4309 - val_loss: 0.7125 - val_acc: 0.4614\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6974 - acc: 0.5109 - val_loss: 0.6689 - val_acc: 0.6322\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6586 - acc: 0.6325 - val_loss: 0.6468 - val_acc: 0.6289\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6361 - acc: 0.6710 - val_loss: 0.6196 - val_acc: 0.7323\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6153 - acc: 0.7147 - val_loss: 0.5966 - val_acc: 0.7570\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5973 - acc: 0.7179 - val_loss: 0.5788 - val_acc: 0.7553\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5817 - acc: 0.7152 - val_loss: 0.5627 - val_acc: 0.7619\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5659 - acc: 0.7369 - val_loss: 0.5471 - val_acc: 0.7718\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5503 - acc: 0.7479 - val_loss: 0.5326 - val_acc: 0.7701\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5373 - acc: 0.7522 - val_loss: 0.5190 - val_acc: 0.7816\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5231 - acc: 0.7597 - val_loss: 0.5072 - val_acc: 0.7882\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5162 - acc: 0.7621 - val_loss: 0.4978 - val_acc: 0.7947\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5030 - acc: 0.7694 - val_loss: 0.4906 - val_acc: 0.7915\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4955 - acc: 0.7767 - val_loss: 0.4820 - val_acc: 0.7964\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4860 - acc: 0.7816 - val_loss: 0.4764 - val_acc: 0.8013\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4772 - acc: 0.7887 - val_loss: 0.4705 - val_acc: 0.8030\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4740 - acc: 0.7867 - val_loss: 0.4668 - val_acc: 0.8030\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4672 - acc: 0.7935 - val_loss: 0.4623 - val_acc: 0.8079\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4609 - acc: 0.7971 - val_loss: 0.4581 - val_acc: 0.8079\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4573 - acc: 0.7949 - val_loss: 0.4554 - val_acc: 0.8095\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4552 - acc: 0.7991 - val_loss: 0.4523 - val_acc: 0.8095\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4495 - acc: 0.7989 - val_loss: 0.4505 - val_acc: 0.8079\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4471 - acc: 0.8002 - val_loss: 0.4473 - val_acc: 0.8079\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4434 - acc: 0.8006 - val_loss: 0.4459 - val_acc: 0.8062\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4412 - acc: 0.8051 - val_loss: 0.4442 - val_acc: 0.8046\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4401 - acc: 0.8053 - val_loss: 0.4420 - val_acc: 0.8079\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4375 - acc: 0.8095 - val_loss: 0.4407 - val_acc: 0.8062\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4325 - acc: 0.8077 - val_loss: 0.4390 - val_acc: 0.8095\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4259 - acc: 0.8154 - val_loss: 0.4374 - val_acc: 0.8079\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4268 - acc: 0.8139 - val_loss: 0.4367 - val_acc: 0.8079\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4238 - acc: 0.8143 - val_loss: 0.4348 - val_acc: 0.8128\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4190 - acc: 0.8143 - val_loss: 0.4352 - val_acc: 0.8112\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4234 - acc: 0.8119 - val_loss: 0.4329 - val_acc: 0.8128\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4187 - acc: 0.8174 - val_loss: 0.4328 - val_acc: 0.8112\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4171 - acc: 0.8205 - val_loss: 0.4331 - val_acc: 0.8079\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4148 - acc: 0.8177 - val_loss: 0.4301 - val_acc: 0.8128\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4116 - acc: 0.8225 - val_loss: 0.4301 - val_acc: 0.8079\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4139 - acc: 0.8197 - val_loss: 0.4289 - val_acc: 0.8112\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4127 - acc: 0.8188 - val_loss: 0.4295 - val_acc: 0.8095\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4082 - acc: 0.8203 - val_loss: 0.4267 - val_acc: 0.8128\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4079 - acc: 0.8183 - val_loss: 0.4299 - val_acc: 0.8062\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4061 - acc: 0.8252 - val_loss: 0.4266 - val_acc: 0.8112\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4008 - acc: 0.8254 - val_loss: 0.4269 - val_acc: 0.8112\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3995 - acc: 0.8263 - val_loss: 0.4257 - val_acc: 0.8128\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3993 - acc: 0.8278 - val_loss: 0.4257 - val_acc: 0.8161\n",
      "Epoch 46/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3994 - acc: 0.8296 - val_loss: 0.4254 - val_acc: 0.8161\n",
      "Epoch 47/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3953 - acc: 0.8311 - val_loss: 0.4250 - val_acc: 0.8144\n",
      "Epoch 48/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3967 - acc: 0.8280 - val_loss: 0.4245 - val_acc: 0.8128\n",
      "Epoch 49/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3950 - acc: 0.8281 - val_loss: 0.4235 - val_acc: 0.8177\n",
      "Epoch 50/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3909 - acc: 0.8309 - val_loss: 0.4239 - val_acc: 0.8177\n",
      "Epoch 51/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3888 - acc: 0.8309 - val_loss: 0.4227 - val_acc: 0.8194\n",
      "Epoch 52/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3907 - acc: 0.8287 - val_loss: 0.4231 - val_acc: 0.8194\n",
      "Epoch 53/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3884 - acc: 0.8352 - val_loss: 0.4225 - val_acc: 0.8194\n",
      "Epoch 54/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3879 - acc: 0.8373 - val_loss: 0.4219 - val_acc: 0.8210\n",
      "Epoch 55/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3841 - acc: 0.8342 - val_loss: 0.4214 - val_acc: 0.8177\n",
      "Epoch 56/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3842 - acc: 0.8360 - val_loss: 0.4233 - val_acc: 0.8161\n",
      "Epoch 57/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3840 - acc: 0.8331 - val_loss: 0.4209 - val_acc: 0.8194\n",
      "Epoch 58/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3825 - acc: 0.8378 - val_loss: 0.4208 - val_acc: 0.8194\n",
      "Epoch 59/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3791 - acc: 0.8338 - val_loss: 0.4210 - val_acc: 0.8194\n",
      "Epoch 60/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3779 - acc: 0.8385 - val_loss: 0.4208 - val_acc: 0.8194\n",
      "Epoch 61/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3767 - acc: 0.8365 - val_loss: 0.4219 - val_acc: 0.8161\n",
      "Epoch 62/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3755 - acc: 0.8396 - val_loss: 0.4190 - val_acc: 0.8177\n",
      "Epoch 63/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3734 - acc: 0.8389 - val_loss: 0.4207 - val_acc: 0.8177\n",
      "Epoch 64/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3707 - acc: 0.8402 - val_loss: 0.4188 - val_acc: 0.8194\n",
      "Epoch 65/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3721 - acc: 0.8405 - val_loss: 0.4218 - val_acc: 0.8161\n",
      "Epoch 66/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3683 - acc: 0.8427 - val_loss: 0.4200 - val_acc: 0.8194\n",
      "Epoch 67/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3659 - acc: 0.8462 - val_loss: 0.4190 - val_acc: 0.8194\n",
      "Epoch 68/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3644 - acc: 0.8447 - val_loss: 0.4202 - val_acc: 0.8144\n",
      "Epoch 69/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3649 - acc: 0.8422 - val_loss: 0.4205 - val_acc: 0.8161\n",
      "Training with parameters {'batch_size': 700, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_127\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_132 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_354 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_227 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_355 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_228 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_356 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.7343 - acc: 0.4424 - val_loss: 0.6844 - val_acc: 0.5517\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6759 - acc: 0.5917 - val_loss: 0.6604 - val_acc: 0.5977\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6533 - acc: 0.6271 - val_loss: 0.6464 - val_acc: 0.5747\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6367 - acc: 0.6453 - val_loss: 0.6267 - val_acc: 0.6601\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6198 - acc: 0.6926 - val_loss: 0.6059 - val_acc: 0.7406\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6021 - acc: 0.7165 - val_loss: 0.5886 - val_acc: 0.7471\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5862 - acc: 0.7176 - val_loss: 0.5732 - val_acc: 0.7570\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5722 - acc: 0.7302 - val_loss: 0.5586 - val_acc: 0.7652\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5575 - acc: 0.7435 - val_loss: 0.5429 - val_acc: 0.7734\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5428 - acc: 0.7495 - val_loss: 0.5270 - val_acc: 0.7800\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5296 - acc: 0.7539 - val_loss: 0.5142 - val_acc: 0.7849\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5173 - acc: 0.7588 - val_loss: 0.5028 - val_acc: 0.7915\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5041 - acc: 0.7741 - val_loss: 0.4932 - val_acc: 0.7947\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4929 - acc: 0.7845 - val_loss: 0.4858 - val_acc: 0.7980\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4853 - acc: 0.7860 - val_loss: 0.4781 - val_acc: 0.7980\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4782 - acc: 0.7905 - val_loss: 0.4735 - val_acc: 0.8013\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4747 - acc: 0.7885 - val_loss: 0.4676 - val_acc: 0.8046\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4666 - acc: 0.7938 - val_loss: 0.4638 - val_acc: 0.8046\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4609 - acc: 0.7933 - val_loss: 0.4615 - val_acc: 0.8013\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4574 - acc: 0.8019 - val_loss: 0.4570 - val_acc: 0.8030\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4513 - acc: 0.7986 - val_loss: 0.4545 - val_acc: 0.8046\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4506 - acc: 0.8031 - val_loss: 0.4516 - val_acc: 0.8079\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4441 - acc: 0.8073 - val_loss: 0.4498 - val_acc: 0.8062\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4387 - acc: 0.8037 - val_loss: 0.4473 - val_acc: 0.8095\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4421 - acc: 0.8075 - val_loss: 0.4474 - val_acc: 0.8062\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4336 - acc: 0.8104 - val_loss: 0.4430 - val_acc: 0.8128\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4326 - acc: 0.8108 - val_loss: 0.4429 - val_acc: 0.8095\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4288 - acc: 0.8106 - val_loss: 0.4401 - val_acc: 0.8128\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4258 - acc: 0.8146 - val_loss: 0.4404 - val_acc: 0.8079\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4242 - acc: 0.8139 - val_loss: 0.4384 - val_acc: 0.8144\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4241 - acc: 0.8143 - val_loss: 0.4375 - val_acc: 0.8062\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4193 - acc: 0.8124 - val_loss: 0.4372 - val_acc: 0.8079\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4197 - acc: 0.8208 - val_loss: 0.4346 - val_acc: 0.8112\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4208 - acc: 0.8196 - val_loss: 0.4355 - val_acc: 0.8112\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4134 - acc: 0.8256 - val_loss: 0.4339 - val_acc: 0.8095\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4128 - acc: 0.8163 - val_loss: 0.4324 - val_acc: 0.8112\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4092 - acc: 0.8230 - val_loss: 0.4345 - val_acc: 0.8095\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4117 - acc: 0.8201 - val_loss: 0.4301 - val_acc: 0.8161\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4056 - acc: 0.8248 - val_loss: 0.4328 - val_acc: 0.8112\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4017 - acc: 0.8283 - val_loss: 0.4296 - val_acc: 0.8112\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4039 - acc: 0.8227 - val_loss: 0.4308 - val_acc: 0.8079\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4033 - acc: 0.8259 - val_loss: 0.4296 - val_acc: 0.8095\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4016 - acc: 0.8263 - val_loss: 0.4307 - val_acc: 0.8128\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4017 - acc: 0.8221 - val_loss: 0.4268 - val_acc: 0.8112\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3984 - acc: 0.8245 - val_loss: 0.4305 - val_acc: 0.8128\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3950 - acc: 0.8298 - val_loss: 0.4261 - val_acc: 0.8128\n",
      "Epoch 47/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3952 - acc: 0.8285 - val_loss: 0.4276 - val_acc: 0.8144\n",
      "Epoch 48/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3943 - acc: 0.8303 - val_loss: 0.4264 - val_acc: 0.8128\n",
      "Epoch 49/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3933 - acc: 0.8283 - val_loss: 0.4265 - val_acc: 0.8144\n",
      "Epoch 50/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3906 - acc: 0.8252 - val_loss: 0.4256 - val_acc: 0.8144\n",
      "Epoch 51/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3883 - acc: 0.8321 - val_loss: 0.4264 - val_acc: 0.8144\n",
      "Epoch 52/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3866 - acc: 0.8325 - val_loss: 0.4252 - val_acc: 0.8161\n",
      "Epoch 53/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3865 - acc: 0.8327 - val_loss: 0.4239 - val_acc: 0.8144\n",
      "Epoch 54/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3865 - acc: 0.8327 - val_loss: 0.4250 - val_acc: 0.8177\n",
      "Epoch 55/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3834 - acc: 0.8332 - val_loss: 0.4233 - val_acc: 0.8128\n",
      "Epoch 56/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3841 - acc: 0.8327 - val_loss: 0.4266 - val_acc: 0.8144\n",
      "Epoch 57/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3792 - acc: 0.8398 - val_loss: 0.4247 - val_acc: 0.8161\n",
      "Epoch 58/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3796 - acc: 0.8400 - val_loss: 0.4235 - val_acc: 0.8177\n",
      "Epoch 59/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3793 - acc: 0.8373 - val_loss: 0.4257 - val_acc: 0.8128\n",
      "Epoch 60/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3724 - acc: 0.8380 - val_loss: 0.4237 - val_acc: 0.8144\n",
      "Training with parameters {'batch_size': 700, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_128\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_133 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_357 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_229 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_358 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_230 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_359 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "8/8 [==============================] - 1s 35ms/step - loss: 0.7783 - acc: 0.4408 - val_loss: 0.6924 - val_acc: 0.5090\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6798 - acc: 0.5658 - val_loss: 0.6603 - val_acc: 0.5567\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6496 - acc: 0.6021 - val_loss: 0.6375 - val_acc: 0.5665\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6260 - acc: 0.6568 - val_loss: 0.6047 - val_acc: 0.7274\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6006 - acc: 0.7174 - val_loss: 0.5812 - val_acc: 0.7373\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5812 - acc: 0.7185 - val_loss: 0.5621 - val_acc: 0.7422\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5622 - acc: 0.7314 - val_loss: 0.5446 - val_acc: 0.7603\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5461 - acc: 0.7429 - val_loss: 0.5253 - val_acc: 0.7652\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5294 - acc: 0.7515 - val_loss: 0.5091 - val_acc: 0.7800\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5143 - acc: 0.7688 - val_loss: 0.4945 - val_acc: 0.7947\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5008 - acc: 0.7696 - val_loss: 0.4832 - val_acc: 0.7964\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4857 - acc: 0.7814 - val_loss: 0.4748 - val_acc: 0.8030\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4760 - acc: 0.7862 - val_loss: 0.4672 - val_acc: 0.8062\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4643 - acc: 0.7896 - val_loss: 0.4611 - val_acc: 0.8013\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4564 - acc: 0.7969 - val_loss: 0.4560 - val_acc: 0.8079\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4506 - acc: 0.7993 - val_loss: 0.4537 - val_acc: 0.8128\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4441 - acc: 0.8026 - val_loss: 0.4481 - val_acc: 0.8095\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4426 - acc: 0.8048 - val_loss: 0.4476 - val_acc: 0.8112\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4375 - acc: 0.8077 - val_loss: 0.4462 - val_acc: 0.8161\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4342 - acc: 0.8075 - val_loss: 0.4403 - val_acc: 0.8128\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4287 - acc: 0.8101 - val_loss: 0.4407 - val_acc: 0.8128\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4252 - acc: 0.8137 - val_loss: 0.4392 - val_acc: 0.8177\n",
      "Epoch 23/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4205 - acc: 0.8157 - val_loss: 0.4353 - val_acc: 0.8177\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4178 - acc: 0.8188 - val_loss: 0.4374 - val_acc: 0.8177\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4170 - acc: 0.8144 - val_loss: 0.4333 - val_acc: 0.8161\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4142 - acc: 0.8132 - val_loss: 0.4320 - val_acc: 0.8161\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4118 - acc: 0.8174 - val_loss: 0.4334 - val_acc: 0.8177\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4078 - acc: 0.8216 - val_loss: 0.4297 - val_acc: 0.8210\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4047 - acc: 0.8203 - val_loss: 0.4312 - val_acc: 0.8177\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4025 - acc: 0.8216 - val_loss: 0.4314 - val_acc: 0.8144\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4009 - acc: 0.8258 - val_loss: 0.4278 - val_acc: 0.8259\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3995 - acc: 0.8225 - val_loss: 0.4299 - val_acc: 0.8144\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3922 - acc: 0.8272 - val_loss: 0.4279 - val_acc: 0.8194\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3928 - acc: 0.8321 - val_loss: 0.4259 - val_acc: 0.8276\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3948 - acc: 0.8292 - val_loss: 0.4265 - val_acc: 0.8194\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3906 - acc: 0.8318 - val_loss: 0.4259 - val_acc: 0.8210\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3886 - acc: 0.8301 - val_loss: 0.4253 - val_acc: 0.8243\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3875 - acc: 0.8314 - val_loss: 0.4253 - val_acc: 0.8210\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3831 - acc: 0.8338 - val_loss: 0.4248 - val_acc: 0.8210\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3825 - acc: 0.8343 - val_loss: 0.4236 - val_acc: 0.8243\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3798 - acc: 0.8360 - val_loss: 0.4246 - val_acc: 0.8259\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3757 - acc: 0.8373 - val_loss: 0.4279 - val_acc: 0.8194\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3769 - acc: 0.8342 - val_loss: 0.4225 - val_acc: 0.8259\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3732 - acc: 0.8371 - val_loss: 0.4232 - val_acc: 0.8292\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3709 - acc: 0.8411 - val_loss: 0.4220 - val_acc: 0.8292\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3693 - acc: 0.8415 - val_loss: 0.4256 - val_acc: 0.8227\n",
      "Epoch 47/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3664 - acc: 0.8404 - val_loss: 0.4266 - val_acc: 0.8194\n",
      "Epoch 48/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3626 - acc: 0.8424 - val_loss: 0.4231 - val_acc: 0.8276\n",
      "Epoch 49/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3616 - acc: 0.8422 - val_loss: 0.4220 - val_acc: 0.8292\n",
      "Epoch 50/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3590 - acc: 0.8446 - val_loss: 0.4246 - val_acc: 0.8259\n"
     ]
    }
   ],
   "source": [
    "mlp_perf_metrics2 = make_MLP_exp(X_train_vect, y_train.values, params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.857325</td>\n",
       "      <td>0.806240</td>\n",
       "      <td>0.339870</td>\n",
       "      <td>0.431074</td>\n",
       "      <td>11</td>\n",
       "      <td>0.814839</td>\n",
       "      <td>0.808874</td>\n",
       "      <td>0.736025</td>\n",
       "      <td>0.872582</td>\n",
       "      <td>0.770732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.851487</td>\n",
       "      <td>0.819376</td>\n",
       "      <td>0.346702</td>\n",
       "      <td>0.423231</td>\n",
       "      <td>13</td>\n",
       "      <td>0.814183</td>\n",
       "      <td>0.804384</td>\n",
       "      <td>0.740683</td>\n",
       "      <td>0.868032</td>\n",
       "      <td>0.771221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.849115</td>\n",
       "      <td>0.811166</td>\n",
       "      <td>0.348551</td>\n",
       "      <td>0.424856</td>\n",
       "      <td>32</td>\n",
       "      <td>0.812213</td>\n",
       "      <td>0.796358</td>\n",
       "      <td>0.746894</td>\n",
       "      <td>0.860068</td>\n",
       "      <td>0.770833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.866083</td>\n",
       "      <td>0.807882</td>\n",
       "      <td>0.317675</td>\n",
       "      <td>0.425181</td>\n",
       "      <td>14</td>\n",
       "      <td>0.814183</td>\n",
       "      <td>0.799337</td>\n",
       "      <td>0.748447</td>\n",
       "      <td>0.862344</td>\n",
       "      <td>0.773055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.857508</td>\n",
       "      <td>0.821018</td>\n",
       "      <td>0.324476</td>\n",
       "      <td>0.429557</td>\n",
       "      <td>22</td>\n",
       "      <td>0.816152</td>\n",
       "      <td>0.802326</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.864619</td>\n",
       "      <td>0.775281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>700.0</td>\n",
       "      <td>0.839445</td>\n",
       "      <td>0.812808</td>\n",
       "      <td>0.378041</td>\n",
       "      <td>0.421845</td>\n",
       "      <td>101</td>\n",
       "      <td>0.811556</td>\n",
       "      <td>0.803056</td>\n",
       "      <td>0.734472</td>\n",
       "      <td>0.868032</td>\n",
       "      <td>0.767234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>700.0</td>\n",
       "      <td>0.848385</td>\n",
       "      <td>0.814450</td>\n",
       "      <td>0.350844</td>\n",
       "      <td>0.424528</td>\n",
       "      <td>46</td>\n",
       "      <td>0.806960</td>\n",
       "      <td>0.783172</td>\n",
       "      <td>0.751553</td>\n",
       "      <td>0.847554</td>\n",
       "      <td>0.767036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>700.0</td>\n",
       "      <td>0.842182</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.364904</td>\n",
       "      <td>0.420518</td>\n",
       "      <td>69</td>\n",
       "      <td>0.813526</td>\n",
       "      <td>0.804054</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.868032</td>\n",
       "      <td>0.770227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>700.0</td>\n",
       "      <td>0.837986</td>\n",
       "      <td>0.814450</td>\n",
       "      <td>0.372405</td>\n",
       "      <td>0.423731</td>\n",
       "      <td>60</td>\n",
       "      <td>0.814183</td>\n",
       "      <td>0.805415</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.869170</td>\n",
       "      <td>0.770850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>700.0</td>\n",
       "      <td>0.844554</td>\n",
       "      <td>0.825944</td>\n",
       "      <td>0.359003</td>\n",
       "      <td>0.424638</td>\n",
       "      <td>50</td>\n",
       "      <td>0.816152</td>\n",
       "      <td>0.803333</td>\n",
       "      <td>0.748447</td>\n",
       "      <td>0.865757</td>\n",
       "      <td>0.774920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_layers  layer_conf      lr  dropout  batch_size  accuracy_train  \\\n",
       "0          2  [512, 256]  0.0001      0.1         NaN        0.857325   \n",
       "1          2  [256, 256]  0.0001      0.1         NaN        0.851487   \n",
       "2          1       [128]  0.0001      0.1         NaN        0.849115   \n",
       "3          2  [512, 128]  0.0001      0.1         NaN        0.866083   \n",
       "4          2   [256, 32]  0.0001      0.1         NaN        0.857508   \n",
       "..       ...         ...     ...      ...         ...             ...   \n",
       "65         1       [128]  0.0001      0.1       700.0        0.839445   \n",
       "66         2  [512, 128]  0.0001      0.1       700.0        0.848385   \n",
       "67         2   [256, 32]  0.0001      0.1       700.0        0.842182   \n",
       "68         2  [128, 128]  0.0001      0.1       700.0        0.837986   \n",
       "69         2  [256, 256]  0.0001      0.1       700.0        0.844554   \n",
       "\n",
       "    accuracy_val  loss_train  loss_val  epochs  accuracy  precision    recall  \\\n",
       "0       0.806240    0.339870  0.431074      11  0.814839   0.808874  0.736025   \n",
       "1       0.819376    0.346702  0.423231      13  0.814183   0.804384  0.740683   \n",
       "2       0.811166    0.348551  0.424856      32  0.812213   0.796358  0.746894   \n",
       "3       0.807882    0.317675  0.425181      14  0.814183   0.799337  0.748447   \n",
       "4       0.821018    0.324476  0.429557      22  0.816152   0.802326  0.750000   \n",
       "..           ...         ...       ...     ...       ...        ...       ...   \n",
       "65      0.812808    0.378041  0.421845     101  0.811556   0.803056  0.734472   \n",
       "66      0.814450    0.350844  0.424528      46  0.806960   0.783172  0.751553   \n",
       "67      0.816092    0.364904  0.420518      69  0.813526   0.804054  0.739130   \n",
       "68      0.814450    0.372405  0.423731      60  0.814183   0.805415  0.739130   \n",
       "69      0.825944    0.359003  0.424638      50  0.816152   0.803333  0.748447   \n",
       "\n",
       "    specificity  f1_score  \n",
       "0      0.872582  0.770732  \n",
       "1      0.868032  0.771221  \n",
       "2      0.860068  0.770833  \n",
       "3      0.862344  0.773055  \n",
       "4      0.864619  0.775281  \n",
       "..          ...       ...  \n",
       "65     0.868032  0.767234  \n",
       "66     0.847554  0.767036  \n",
       "67     0.868032  0.770227  \n",
       "68     0.869170  0.770850  \n",
       "69     0.865757  0.774920  \n",
       "\n",
       "[70 rows x 15 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_perf_metrics2.to_csv('mlp_perf_metrics2.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.860427</td>\n",
       "      <td>0.812808</td>\n",
       "      <td>0.323961</td>\n",
       "      <td>0.438386</td>\n",
       "      <td>10</td>\n",
       "      <td>0.824032</td>\n",
       "      <td>0.848148</td>\n",
       "      <td>0.711180</td>\n",
       "      <td>0.906712</td>\n",
       "      <td>0.773649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.862251</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.330685</td>\n",
       "      <td>0.425597</td>\n",
       "      <td>18</td>\n",
       "      <td>0.820749</td>\n",
       "      <td>0.820380</td>\n",
       "      <td>0.737578</td>\n",
       "      <td>0.881684</td>\n",
       "      <td>0.776778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.879584</td>\n",
       "      <td>0.784893</td>\n",
       "      <td>0.284675</td>\n",
       "      <td>0.509556</td>\n",
       "      <td>12</td>\n",
       "      <td>0.820092</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.897611</td>\n",
       "      <td>0.770519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.874111</td>\n",
       "      <td>0.812808</td>\n",
       "      <td>0.305726</td>\n",
       "      <td>0.429264</td>\n",
       "      <td>13</td>\n",
       "      <td>0.817466</td>\n",
       "      <td>0.815517</td>\n",
       "      <td>0.734472</td>\n",
       "      <td>0.878271</td>\n",
       "      <td>0.772876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.857508</td>\n",
       "      <td>0.821018</td>\n",
       "      <td>0.324476</td>\n",
       "      <td>0.429557</td>\n",
       "      <td>22</td>\n",
       "      <td>0.816152</td>\n",
       "      <td>0.802326</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.864619</td>\n",
       "      <td>0.775281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_layers  layer_conf      lr  dropout  batch_size  accuracy_train  \\\n",
       "11         2   [256, 32]  0.0001      0.1         1.0        0.860427   \n",
       "25         2   [256, 32]  0.0001      0.1        20.0        0.862251   \n",
       "12         2  [128, 128]  0.0001      0.1         1.0        0.879584   \n",
       "24         2  [512, 128]  0.0001      0.1        20.0        0.874111   \n",
       "4          2   [256, 32]  0.0001      0.1         NaN        0.857508   \n",
       "\n",
       "    accuracy_val  loss_train  loss_val  epochs  accuracy  precision    recall  \\\n",
       "11      0.812808    0.323961  0.438386      10  0.824032   0.848148  0.711180   \n",
       "25      0.816092    0.330685  0.425597      18  0.820749   0.820380  0.737578   \n",
       "12      0.784893    0.284675  0.509556      12  0.820092   0.836364  0.714286   \n",
       "24      0.812808    0.305726  0.429264      13  0.817466   0.815517  0.734472   \n",
       "4       0.821018    0.324476  0.429557      22  0.816152   0.802326  0.750000   \n",
       "\n",
       "    specificity  f1_score  \n",
       "11     0.906712  0.773649  \n",
       "25     0.881684  0.776778  \n",
       "12     0.897611  0.770519  \n",
       "24     0.878271  0.772876  \n",
       "4      0.864619  0.775281  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics2.nlargest(5, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.819376</td>\n",
       "      <td>0.325917</td>\n",
       "      <td>0.422478</td>\n",
       "      <td>18</td>\n",
       "      <td>0.812213</td>\n",
       "      <td>0.780564</td>\n",
       "      <td>0.773292</td>\n",
       "      <td>0.840728</td>\n",
       "      <td>0.776911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.862251</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.330685</td>\n",
       "      <td>0.425597</td>\n",
       "      <td>18</td>\n",
       "      <td>0.820749</td>\n",
       "      <td>0.820380</td>\n",
       "      <td>0.737578</td>\n",
       "      <td>0.881684</td>\n",
       "      <td>0.776778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.871921</td>\n",
       "      <td>0.806240</td>\n",
       "      <td>0.299735</td>\n",
       "      <td>0.430372</td>\n",
       "      <td>22</td>\n",
       "      <td>0.815496</td>\n",
       "      <td>0.796085</td>\n",
       "      <td>0.757764</td>\n",
       "      <td>0.857793</td>\n",
       "      <td>0.776452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.857508</td>\n",
       "      <td>0.821018</td>\n",
       "      <td>0.324476</td>\n",
       "      <td>0.429557</td>\n",
       "      <td>22</td>\n",
       "      <td>0.816152</td>\n",
       "      <td>0.802326</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.864619</td>\n",
       "      <td>0.775281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.873746</td>\n",
       "      <td>0.829228</td>\n",
       "      <td>0.300918</td>\n",
       "      <td>0.435925</td>\n",
       "      <td>16</td>\n",
       "      <td>0.813526</td>\n",
       "      <td>0.791262</td>\n",
       "      <td>0.759317</td>\n",
       "      <td>0.853242</td>\n",
       "      <td>0.774960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_layers  layer_conf      lr  dropout  batch_size  accuracy_train  \\\n",
       "34         2  [256, 256]  0.0001      0.1        50.0        0.857143   \n",
       "25         2   [256, 32]  0.0001      0.1        20.0        0.862251   \n",
       "26         2  [128, 128]  0.0001      0.1        20.0        0.871921   \n",
       "4          2   [256, 32]  0.0001      0.1         NaN        0.857508   \n",
       "28         2  [512, 256]  0.0001      0.1        50.0        0.873746   \n",
       "\n",
       "    accuracy_val  loss_train  loss_val  epochs  accuracy  precision    recall  \\\n",
       "34      0.819376    0.325917  0.422478      18  0.812213   0.780564  0.773292   \n",
       "25      0.816092    0.330685  0.425597      18  0.820749   0.820380  0.737578   \n",
       "26      0.806240    0.299735  0.430372      22  0.815496   0.796085  0.757764   \n",
       "4       0.821018    0.324476  0.429557      22  0.816152   0.802326  0.750000   \n",
       "28      0.829228    0.300918  0.435925      16  0.813526   0.791262  0.759317   \n",
       "\n",
       "    specificity  f1_score  \n",
       "34     0.840728  0.776911  \n",
       "25     0.881684  0.776778  \n",
       "26     0.857793  0.776452  \n",
       "4      0.864619  0.775281  \n",
       "28     0.853242  0.774960  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics2.nlargest(5, 'f1_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`batch_size=20` seems to provide higuer results for most of the top results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grid higer `batch_size` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params3 = {\n",
    "        'lay_conf': best_arch1,\n",
    "        'lr': [1e-4],\n",
    "        'dropout': [0.1],\n",
    "        'max_epochs': [300],\n",
    "        'batch_size': [1000, 1250, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000],\n",
    "        'seed': [123456]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters {'batch_size': 1000, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 40ms/step - loss: 0.7299 - acc: 0.4583 - val_loss: 0.6604 - val_acc: 0.6125\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.6666 - acc: 0.5731 - val_loss: 0.6273 - val_acc: 0.6108\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.6339 - acc: 0.6232 - val_loss: 0.6055 - val_acc: 0.7176\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.6025 - acc: 0.6997 - val_loss: 0.5963 - val_acc: 0.6568\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.5822 - acc: 0.7026 - val_loss: 0.5710 - val_acc: 0.6929\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.5612 - acc: 0.7318 - val_loss: 0.5475 - val_acc: 0.7340\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.5427 - acc: 0.7466 - val_loss: 0.5351 - val_acc: 0.7241\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.5254 - acc: 0.7537 - val_loss: 0.5127 - val_acc: 0.7603\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.5080 - acc: 0.7694 - val_loss: 0.4970 - val_acc: 0.7734\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4937 - acc: 0.7811 - val_loss: 0.4872 - val_acc: 0.7833\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4811 - acc: 0.7853 - val_loss: 0.4746 - val_acc: 0.7849\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4703 - acc: 0.7931 - val_loss: 0.4668 - val_acc: 0.7865\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4616 - acc: 0.7993 - val_loss: 0.4580 - val_acc: 0.7849\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4543 - acc: 0.8009 - val_loss: 0.4522 - val_acc: 0.7931\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4453 - acc: 0.8064 - val_loss: 0.4422 - val_acc: 0.8030\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4394 - acc: 0.8070 - val_loss: 0.4382 - val_acc: 0.7997\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4363 - acc: 0.8073 - val_loss: 0.4333 - val_acc: 0.8062\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4340 - acc: 0.8126 - val_loss: 0.4368 - val_acc: 0.8030\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4285 - acc: 0.8113 - val_loss: 0.4244 - val_acc: 0.8128\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.4254 - acc: 0.8139 - val_loss: 0.4281 - val_acc: 0.8079\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4214 - acc: 0.8148 - val_loss: 0.4234 - val_acc: 0.8112\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4158 - acc: 0.8186 - val_loss: 0.4224 - val_acc: 0.8128\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4130 - acc: 0.8239 - val_loss: 0.4194 - val_acc: 0.8144\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4100 - acc: 0.8203 - val_loss: 0.4203 - val_acc: 0.8161\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4082 - acc: 0.8252 - val_loss: 0.4158 - val_acc: 0.8177\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4037 - acc: 0.8280 - val_loss: 0.4140 - val_acc: 0.8177\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4004 - acc: 0.8281 - val_loss: 0.4161 - val_acc: 0.8194\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3987 - acc: 0.8294 - val_loss: 0.4115 - val_acc: 0.8194\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3955 - acc: 0.8290 - val_loss: 0.4158 - val_acc: 0.8210\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.3952 - acc: 0.8296 - val_loss: 0.4131 - val_acc: 0.8227\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.3908 - acc: 0.8311 - val_loss: 0.4133 - val_acc: 0.8210\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.3897 - acc: 0.8349 - val_loss: 0.4123 - val_acc: 0.8243\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.3859 - acc: 0.8342 - val_loss: 0.4066 - val_acc: 0.8177\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3841 - acc: 0.8380 - val_loss: 0.4097 - val_acc: 0.8227\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.3795 - acc: 0.8420 - val_loss: 0.4147 - val_acc: 0.8227\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.3788 - acc: 0.8363 - val_loss: 0.4041 - val_acc: 0.8177\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3811 - acc: 0.8376 - val_loss: 0.4162 - val_acc: 0.8210\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.3740 - acc: 0.8402 - val_loss: 0.4092 - val_acc: 0.8227\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.3722 - acc: 0.8398 - val_loss: 0.4049 - val_acc: 0.8210\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.3706 - acc: 0.8436 - val_loss: 0.4193 - val_acc: 0.8177\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.3683 - acc: 0.8420 - val_loss: 0.4046 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': 1000, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.7948 - acc: 0.4426 - val_loss: 0.7286 - val_acc: 0.3924\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.7002 - acc: 0.5070 - val_loss: 0.6609 - val_acc: 0.6404\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.6655 - acc: 0.5975 - val_loss: 0.6332 - val_acc: 0.6207\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.6425 - acc: 0.6136 - val_loss: 0.6152 - val_acc: 0.6420\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.6232 - acc: 0.6685 - val_loss: 0.6018 - val_acc: 0.7274\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.6042 - acc: 0.7154 - val_loss: 0.5920 - val_acc: 0.7011\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.5908 - acc: 0.7032 - val_loss: 0.5797 - val_acc: 0.7011\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.5735 - acc: 0.7163 - val_loss: 0.5650 - val_acc: 0.7143\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.5627 - acc: 0.7325 - val_loss: 0.5496 - val_acc: 0.7225\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.5498 - acc: 0.7391 - val_loss: 0.5378 - val_acc: 0.7291\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.5370 - acc: 0.7446 - val_loss: 0.5243 - val_acc: 0.7373\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.5229 - acc: 0.7577 - val_loss: 0.5076 - val_acc: 0.7422\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.5078 - acc: 0.7672 - val_loss: 0.4989 - val_acc: 0.7488\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.5009 - acc: 0.7763 - val_loss: 0.4845 - val_acc: 0.7652\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4904 - acc: 0.7803 - val_loss: 0.4750 - val_acc: 0.7718\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4791 - acc: 0.7847 - val_loss: 0.4671 - val_acc: 0.7816\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4758 - acc: 0.7865 - val_loss: 0.4609 - val_acc: 0.7833\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4681 - acc: 0.7865 - val_loss: 0.4562 - val_acc: 0.7849\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4617 - acc: 0.7960 - val_loss: 0.4501 - val_acc: 0.7898\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4571 - acc: 0.7964 - val_loss: 0.4450 - val_acc: 0.7980\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4527 - acc: 0.8017 - val_loss: 0.4410 - val_acc: 0.8013\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4466 - acc: 0.7999 - val_loss: 0.4393 - val_acc: 0.8013\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4426 - acc: 0.8048 - val_loss: 0.4342 - val_acc: 0.8062\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4426 - acc: 0.8082 - val_loss: 0.4322 - val_acc: 0.8062\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.4381 - acc: 0.8088 - val_loss: 0.4288 - val_acc: 0.8112\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4330 - acc: 0.8101 - val_loss: 0.4255 - val_acc: 0.8079\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4315 - acc: 0.8104 - val_loss: 0.4289 - val_acc: 0.8079\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.4311 - acc: 0.8095 - val_loss: 0.4229 - val_acc: 0.8079\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4269 - acc: 0.8117 - val_loss: 0.4216 - val_acc: 0.8062\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.4248 - acc: 0.8176 - val_loss: 0.4222 - val_acc: 0.8095\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4236 - acc: 0.8185 - val_loss: 0.4176 - val_acc: 0.8128\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4196 - acc: 0.8157 - val_loss: 0.4199 - val_acc: 0.8128\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4165 - acc: 0.8168 - val_loss: 0.4155 - val_acc: 0.8095\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4140 - acc: 0.8238 - val_loss: 0.4176 - val_acc: 0.8144\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4126 - acc: 0.8192 - val_loss: 0.4149 - val_acc: 0.8144\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4114 - acc: 0.8228 - val_loss: 0.4140 - val_acc: 0.8161\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4090 - acc: 0.8269 - val_loss: 0.4142 - val_acc: 0.8128\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4072 - acc: 0.8303 - val_loss: 0.4155 - val_acc: 0.8128\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4061 - acc: 0.8261 - val_loss: 0.4127 - val_acc: 0.8144\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4042 - acc: 0.8214 - val_loss: 0.4132 - val_acc: 0.8112\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4014 - acc: 0.8280 - val_loss: 0.4100 - val_acc: 0.8177\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4003 - acc: 0.8270 - val_loss: 0.4134 - val_acc: 0.8112\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3975 - acc: 0.8248 - val_loss: 0.4103 - val_acc: 0.8194\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.3961 - acc: 0.8254 - val_loss: 0.4118 - val_acc: 0.8161\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.3952 - acc: 0.8301 - val_loss: 0.4103 - val_acc: 0.8177\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.3908 - acc: 0.8343 - val_loss: 0.4133 - val_acc: 0.8177\n",
      "Training with parameters {'batch_size': 1000, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.6608 - acc: 0.6253 - val_loss: 0.6401 - val_acc: 0.6568\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6355 - acc: 0.6532 - val_loss: 0.6208 - val_acc: 0.6847\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6184 - acc: 0.6856 - val_loss: 0.6084 - val_acc: 0.6831\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6013 - acc: 0.7021 - val_loss: 0.5991 - val_acc: 0.6782\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5907 - acc: 0.7079 - val_loss: 0.5892 - val_acc: 0.6831\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5783 - acc: 0.7132 - val_loss: 0.5761 - val_acc: 0.7044\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5665 - acc: 0.7325 - val_loss: 0.5644 - val_acc: 0.7159\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5566 - acc: 0.7407 - val_loss: 0.5555 - val_acc: 0.7225\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5454 - acc: 0.7475 - val_loss: 0.5486 - val_acc: 0.7258\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5389 - acc: 0.7521 - val_loss: 0.5390 - val_acc: 0.7373\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5301 - acc: 0.7619 - val_loss: 0.5290 - val_acc: 0.7521\n",
      "Epoch 12/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5240 - acc: 0.7654 - val_loss: 0.5249 - val_acc: 0.7455\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5167 - acc: 0.7707 - val_loss: 0.5174 - val_acc: 0.7521\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5112 - acc: 0.7741 - val_loss: 0.5109 - val_acc: 0.7668\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5061 - acc: 0.7738 - val_loss: 0.5034 - val_acc: 0.7668\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4993 - acc: 0.7798 - val_loss: 0.5013 - val_acc: 0.7668\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4952 - acc: 0.7798 - val_loss: 0.5002 - val_acc: 0.7586\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4915 - acc: 0.7814 - val_loss: 0.4908 - val_acc: 0.7734\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4886 - acc: 0.7834 - val_loss: 0.4886 - val_acc: 0.7701\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4825 - acc: 0.7913 - val_loss: 0.4858 - val_acc: 0.7750\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4798 - acc: 0.7876 - val_loss: 0.4796 - val_acc: 0.7750\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4752 - acc: 0.7916 - val_loss: 0.4764 - val_acc: 0.7833\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4721 - acc: 0.7889 - val_loss: 0.4745 - val_acc: 0.7849\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4712 - acc: 0.7951 - val_loss: 0.4704 - val_acc: 0.7865\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4678 - acc: 0.7969 - val_loss: 0.4671 - val_acc: 0.7898\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4650 - acc: 0.7984 - val_loss: 0.4669 - val_acc: 0.7947\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4636 - acc: 0.7986 - val_loss: 0.4641 - val_acc: 0.7947\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4611 - acc: 0.8009 - val_loss: 0.4603 - val_acc: 0.7898\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4588 - acc: 0.8024 - val_loss: 0.4610 - val_acc: 0.7947\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4563 - acc: 0.8022 - val_loss: 0.4571 - val_acc: 0.7947\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4537 - acc: 0.8053 - val_loss: 0.4573 - val_acc: 0.7964\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4513 - acc: 0.8079 - val_loss: 0.4534 - val_acc: 0.7964\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4489 - acc: 0.8057 - val_loss: 0.4521 - val_acc: 0.8013\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4477 - acc: 0.8092 - val_loss: 0.4531 - val_acc: 0.7980\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4456 - acc: 0.8077 - val_loss: 0.4508 - val_acc: 0.7997\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4455 - acc: 0.8068 - val_loss: 0.4487 - val_acc: 0.8030\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4440 - acc: 0.8115 - val_loss: 0.4467 - val_acc: 0.8046\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4421 - acc: 0.8084 - val_loss: 0.4460 - val_acc: 0.8062\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4405 - acc: 0.8108 - val_loss: 0.4438 - val_acc: 0.8062\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4386 - acc: 0.8088 - val_loss: 0.4432 - val_acc: 0.8079\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4375 - acc: 0.8112 - val_loss: 0.4402 - val_acc: 0.8062\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4364 - acc: 0.8113 - val_loss: 0.4398 - val_acc: 0.8095\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4368 - acc: 0.8104 - val_loss: 0.4408 - val_acc: 0.8095\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4329 - acc: 0.8165 - val_loss: 0.4361 - val_acc: 0.8095\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4330 - acc: 0.8148 - val_loss: 0.4359 - val_acc: 0.8112\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4307 - acc: 0.8155 - val_loss: 0.4371 - val_acc: 0.8128\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4297 - acc: 0.8163 - val_loss: 0.4356 - val_acc: 0.8112\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4282 - acc: 0.8143 - val_loss: 0.4332 - val_acc: 0.8112\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4273 - acc: 0.8170 - val_loss: 0.4324 - val_acc: 0.8112\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4259 - acc: 0.8152 - val_loss: 0.4311 - val_acc: 0.8144\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4264 - acc: 0.8163 - val_loss: 0.4313 - val_acc: 0.8144\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4254 - acc: 0.8172 - val_loss: 0.4333 - val_acc: 0.8128\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4245 - acc: 0.8170 - val_loss: 0.4305 - val_acc: 0.8144\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4232 - acc: 0.8210 - val_loss: 0.4261 - val_acc: 0.8161\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4210 - acc: 0.8172 - val_loss: 0.4351 - val_acc: 0.8079\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4209 - acc: 0.8161 - val_loss: 0.4273 - val_acc: 0.8161\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4201 - acc: 0.8174 - val_loss: 0.4259 - val_acc: 0.8177\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4169 - acc: 0.8221 - val_loss: 0.4290 - val_acc: 0.8144\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4194 - acc: 0.8179 - val_loss: 0.4290 - val_acc: 0.8177\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4180 - acc: 0.8183 - val_loss: 0.4231 - val_acc: 0.8161\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4165 - acc: 0.8223 - val_loss: 0.4273 - val_acc: 0.8194\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4145 - acc: 0.8196 - val_loss: 0.4246 - val_acc: 0.8161\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4154 - acc: 0.8207 - val_loss: 0.4243 - val_acc: 0.8161\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4137 - acc: 0.8207 - val_loss: 0.4257 - val_acc: 0.8177\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4133 - acc: 0.8207 - val_loss: 0.4223 - val_acc: 0.8177\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4121 - acc: 0.8236 - val_loss: 0.4237 - val_acc: 0.8161\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4118 - acc: 0.8250 - val_loss: 0.4220 - val_acc: 0.8177\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4126 - acc: 0.8194 - val_loss: 0.4213 - val_acc: 0.8177\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4108 - acc: 0.8234 - val_loss: 0.4265 - val_acc: 0.8144\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4098 - acc: 0.8254 - val_loss: 0.4202 - val_acc: 0.8177\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4081 - acc: 0.8234 - val_loss: 0.4215 - val_acc: 0.8161\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4074 - acc: 0.8278 - val_loss: 0.4232 - val_acc: 0.8177\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.4082 - acc: 0.8232 - val_loss: 0.4200 - val_acc: 0.8177\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4062 - acc: 0.8247 - val_loss: 0.4220 - val_acc: 0.8161\n",
      "Epoch 75/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4071 - acc: 0.8227 - val_loss: 0.4184 - val_acc: 0.8177\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4048 - acc: 0.8225 - val_loss: 0.4210 - val_acc: 0.8177\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4048 - acc: 0.8250 - val_loss: 0.4201 - val_acc: 0.8177\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4045 - acc: 0.8254 - val_loss: 0.4186 - val_acc: 0.8210\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4021 - acc: 0.8252 - val_loss: 0.4188 - val_acc: 0.8194\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4032 - acc: 0.8285 - val_loss: 0.4179 - val_acc: 0.8194\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4025 - acc: 0.8274 - val_loss: 0.4188 - val_acc: 0.8194\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4011 - acc: 0.8276 - val_loss: 0.4187 - val_acc: 0.8194\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4001 - acc: 0.8298 - val_loss: 0.4191 - val_acc: 0.8177\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4000 - acc: 0.8283 - val_loss: 0.4173 - val_acc: 0.8210\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4014 - acc: 0.8256 - val_loss: 0.4189 - val_acc: 0.8194\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4002 - acc: 0.8272 - val_loss: 0.4174 - val_acc: 0.8194\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3981 - acc: 0.8280 - val_loss: 0.4159 - val_acc: 0.8194\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3987 - acc: 0.8285 - val_loss: 0.4184 - val_acc: 0.8177\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3975 - acc: 0.8294 - val_loss: 0.4157 - val_acc: 0.8194\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3968 - acc: 0.8287 - val_loss: 0.4174 - val_acc: 0.8210\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3962 - acc: 0.8298 - val_loss: 0.4172 - val_acc: 0.8177\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3956 - acc: 0.8285 - val_loss: 0.4149 - val_acc: 0.8194\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3954 - acc: 0.8325 - val_loss: 0.4167 - val_acc: 0.8194\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3946 - acc: 0.8294 - val_loss: 0.4176 - val_acc: 0.8194\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3933 - acc: 0.8312 - val_loss: 0.4140 - val_acc: 0.8177\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3934 - acc: 0.8331 - val_loss: 0.4177 - val_acc: 0.8194\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.3931 - acc: 0.8312 - val_loss: 0.4153 - val_acc: 0.8227\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3909 - acc: 0.8332 - val_loss: 0.4158 - val_acc: 0.8210\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.3916 - acc: 0.8336 - val_loss: 0.4144 - val_acc: 0.8227\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3915 - acc: 0.8334 - val_loss: 0.4164 - val_acc: 0.8194\n",
      "Training with parameters {'batch_size': 1000, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 37ms/step - loss: 0.7834 - acc: 0.4432 - val_loss: 0.6928 - val_acc: 0.5238\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.6819 - acc: 0.5506 - val_loss: 0.6428 - val_acc: 0.6141\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.6540 - acc: 0.5773 - val_loss: 0.6171 - val_acc: 0.6437\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.6197 - acc: 0.6621 - val_loss: 0.6031 - val_acc: 0.6864\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.5987 - acc: 0.7008 - val_loss: 0.5975 - val_acc: 0.6634\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.5813 - acc: 0.7028 - val_loss: 0.5736 - val_acc: 0.6847\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.5626 - acc: 0.7313 - val_loss: 0.5511 - val_acc: 0.7176\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.5436 - acc: 0.7453 - val_loss: 0.5381 - val_acc: 0.7126\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.5299 - acc: 0.7522 - val_loss: 0.5207 - val_acc: 0.7340\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.5126 - acc: 0.7687 - val_loss: 0.5023 - val_acc: 0.7635\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.5006 - acc: 0.7765 - val_loss: 0.4930 - val_acc: 0.7652\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.4887 - acc: 0.7827 - val_loss: 0.4794 - val_acc: 0.7734\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4790 - acc: 0.7858 - val_loss: 0.4718 - val_acc: 0.7767\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4692 - acc: 0.7960 - val_loss: 0.4632 - val_acc: 0.7882\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4620 - acc: 0.7993 - val_loss: 0.4532 - val_acc: 0.8013\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4554 - acc: 0.8008 - val_loss: 0.4499 - val_acc: 0.7997\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4501 - acc: 0.8062 - val_loss: 0.4438 - val_acc: 0.7964\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4450 - acc: 0.8057 - val_loss: 0.4416 - val_acc: 0.7980\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4396 - acc: 0.8112 - val_loss: 0.4357 - val_acc: 0.8030\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4342 - acc: 0.8095 - val_loss: 0.4321 - val_acc: 0.8030\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.4295 - acc: 0.8170 - val_loss: 0.4313 - val_acc: 0.8046\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4289 - acc: 0.8139 - val_loss: 0.4249 - val_acc: 0.8079\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4255 - acc: 0.8166 - val_loss: 0.4264 - val_acc: 0.8046\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.4195 - acc: 0.8207 - val_loss: 0.4225 - val_acc: 0.8079\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.4196 - acc: 0.8221 - val_loss: 0.4187 - val_acc: 0.8112\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4168 - acc: 0.8223 - val_loss: 0.4177 - val_acc: 0.8112\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4126 - acc: 0.8228 - val_loss: 0.4200 - val_acc: 0.8144\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4086 - acc: 0.8261 - val_loss: 0.4151 - val_acc: 0.8128\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4091 - acc: 0.8248 - val_loss: 0.4159 - val_acc: 0.8112\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4121 - acc: 0.8239 - val_loss: 0.4177 - val_acc: 0.8161\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4053 - acc: 0.8278 - val_loss: 0.4095 - val_acc: 0.8161\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4012 - acc: 0.8296 - val_loss: 0.4197 - val_acc: 0.8194\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4010 - acc: 0.8272 - val_loss: 0.4075 - val_acc: 0.8194\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.3972 - acc: 0.8292 - val_loss: 0.4150 - val_acc: 0.8210\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.3943 - acc: 0.8321 - val_loss: 0.4101 - val_acc: 0.8194\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.3936 - acc: 0.8347 - val_loss: 0.4081 - val_acc: 0.8194\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.3922 - acc: 0.8336 - val_loss: 0.4153 - val_acc: 0.8243\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.3887 - acc: 0.8292 - val_loss: 0.4058 - val_acc: 0.8177\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.3876 - acc: 0.8360 - val_loss: 0.4150 - val_acc: 0.8243\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3869 - acc: 0.8374 - val_loss: 0.4060 - val_acc: 0.8161\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.3842 - acc: 0.8336 - val_loss: 0.4098 - val_acc: 0.8194\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.3838 - acc: 0.8332 - val_loss: 0.4129 - val_acc: 0.8210\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.3803 - acc: 0.8378 - val_loss: 0.4081 - val_acc: 0.8194\n",
      "Training with parameters {'batch_size': 1000, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.8305 - acc: 0.4355 - val_loss: 0.7633 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.7247 - acc: 0.4589 - val_loss: 0.6887 - val_acc: 0.5287\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.6766 - acc: 0.5877 - val_loss: 0.6540 - val_acc: 0.6831\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.6564 - acc: 0.6333 - val_loss: 0.6337 - val_acc: 0.6929\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.6386 - acc: 0.6623 - val_loss: 0.6184 - val_acc: 0.7455\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.6207 - acc: 0.7019 - val_loss: 0.6086 - val_acc: 0.7061\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.6055 - acc: 0.7185 - val_loss: 0.5994 - val_acc: 0.6995\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.5934 - acc: 0.7141 - val_loss: 0.5872 - val_acc: 0.7011\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.5816 - acc: 0.7252 - val_loss: 0.5753 - val_acc: 0.7094\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.5709 - acc: 0.7291 - val_loss: 0.5640 - val_acc: 0.7061\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.5616 - acc: 0.7369 - val_loss: 0.5544 - val_acc: 0.7094\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.5472 - acc: 0.7438 - val_loss: 0.5442 - val_acc: 0.7159\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.5393 - acc: 0.7541 - val_loss: 0.5324 - val_acc: 0.7274\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.5297 - acc: 0.7588 - val_loss: 0.5215 - val_acc: 0.7373\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.5186 - acc: 0.7650 - val_loss: 0.5134 - val_acc: 0.7438\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.5135 - acc: 0.7645 - val_loss: 0.5038 - val_acc: 0.7521\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.5063 - acc: 0.7703 - val_loss: 0.4953 - val_acc: 0.7553\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4981 - acc: 0.7769 - val_loss: 0.4893 - val_acc: 0.7553\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4914 - acc: 0.7754 - val_loss: 0.4837 - val_acc: 0.7586\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.4879 - acc: 0.7781 - val_loss: 0.4743 - val_acc: 0.7718\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.4810 - acc: 0.7882 - val_loss: 0.4717 - val_acc: 0.7734\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4757 - acc: 0.7916 - val_loss: 0.4661 - val_acc: 0.7800\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4699 - acc: 0.7944 - val_loss: 0.4616 - val_acc: 0.7800\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4671 - acc: 0.7924 - val_loss: 0.4584 - val_acc: 0.7833\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.4615 - acc: 0.7958 - val_loss: 0.4528 - val_acc: 0.7865\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4584 - acc: 0.7997 - val_loss: 0.4506 - val_acc: 0.7931\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4555 - acc: 0.7971 - val_loss: 0.4481 - val_acc: 0.7931\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.4520 - acc: 0.7973 - val_loss: 0.4451 - val_acc: 0.7964\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4493 - acc: 0.8062 - val_loss: 0.4420 - val_acc: 0.7964\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.4457 - acc: 0.8068 - val_loss: 0.4413 - val_acc: 0.8030\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4449 - acc: 0.8037 - val_loss: 0.4385 - val_acc: 0.8013\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4404 - acc: 0.8070 - val_loss: 0.4357 - val_acc: 0.8046\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4414 - acc: 0.8051 - val_loss: 0.4354 - val_acc: 0.8046\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4350 - acc: 0.8113 - val_loss: 0.4323 - val_acc: 0.8062\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4341 - acc: 0.8119 - val_loss: 0.4322 - val_acc: 0.8062\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4372 - acc: 0.8062 - val_loss: 0.4282 - val_acc: 0.8095\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4288 - acc: 0.8112 - val_loss: 0.4275 - val_acc: 0.8079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.4246 - acc: 0.8168 - val_loss: 0.4286 - val_acc: 0.8030\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.4230 - acc: 0.8157 - val_loss: 0.4255 - val_acc: 0.8095\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4241 - acc: 0.8154 - val_loss: 0.4261 - val_acc: 0.8062\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4222 - acc: 0.8159 - val_loss: 0.4235 - val_acc: 0.8128\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4176 - acc: 0.8144 - val_loss: 0.4228 - val_acc: 0.8128\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4201 - acc: 0.8165 - val_loss: 0.4211 - val_acc: 0.8144\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4194 - acc: 0.8210 - val_loss: 0.4213 - val_acc: 0.8161\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.4158 - acc: 0.8190 - val_loss: 0.4159 - val_acc: 0.8161\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4138 - acc: 0.8223 - val_loss: 0.4234 - val_acc: 0.8128\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4117 - acc: 0.8232 - val_loss: 0.4156 - val_acc: 0.8227\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4129 - acc: 0.8214 - val_loss: 0.4187 - val_acc: 0.8144\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4072 - acc: 0.8241 - val_loss: 0.4199 - val_acc: 0.8128\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.4085 - acc: 0.8259 - val_loss: 0.4131 - val_acc: 0.8210\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4061 - acc: 0.8243 - val_loss: 0.4207 - val_acc: 0.8128\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4069 - acc: 0.8197 - val_loss: 0.4132 - val_acc: 0.8227\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4018 - acc: 0.8259 - val_loss: 0.4169 - val_acc: 0.8144\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4018 - acc: 0.8265 - val_loss: 0.4143 - val_acc: 0.8243\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4014 - acc: 0.8248 - val_loss: 0.4148 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': 1000, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.7408 - acc: 0.4421 - val_loss: 0.7149 - val_acc: 0.4302\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.6908 - acc: 0.5348 - val_loss: 0.6694 - val_acc: 0.6568\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.6638 - acc: 0.6373 - val_loss: 0.6464 - val_acc: 0.6568\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.6507 - acc: 0.6263 - val_loss: 0.6318 - val_acc: 0.6568\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.6334 - acc: 0.6596 - val_loss: 0.6199 - val_acc: 0.6946\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.6218 - acc: 0.6855 - val_loss: 0.6109 - val_acc: 0.7061\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.6080 - acc: 0.7097 - val_loss: 0.6031 - val_acc: 0.6962\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5954 - acc: 0.7152 - val_loss: 0.5935 - val_acc: 0.6946\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5841 - acc: 0.7174 - val_loss: 0.5826 - val_acc: 0.7028\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.5739 - acc: 0.7280 - val_loss: 0.5706 - val_acc: 0.7143\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5602 - acc: 0.7409 - val_loss: 0.5596 - val_acc: 0.7192\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5494 - acc: 0.7448 - val_loss: 0.5500 - val_acc: 0.7209\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.5380 - acc: 0.7557 - val_loss: 0.5387 - val_acc: 0.7340\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5273 - acc: 0.7603 - val_loss: 0.5286 - val_acc: 0.7406\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5191 - acc: 0.7625 - val_loss: 0.5162 - val_acc: 0.7504\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5099 - acc: 0.7736 - val_loss: 0.5069 - val_acc: 0.7553\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.5033 - acc: 0.7723 - val_loss: 0.4998 - val_acc: 0.7603\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4960 - acc: 0.7760 - val_loss: 0.4934 - val_acc: 0.7685\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4920 - acc: 0.7822 - val_loss: 0.4863 - val_acc: 0.7783\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4835 - acc: 0.7867 - val_loss: 0.4793 - val_acc: 0.7800\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4787 - acc: 0.7838 - val_loss: 0.4741 - val_acc: 0.7800\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4750 - acc: 0.7876 - val_loss: 0.4736 - val_acc: 0.7816\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4706 - acc: 0.7924 - val_loss: 0.4639 - val_acc: 0.7931\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4652 - acc: 0.7971 - val_loss: 0.4642 - val_acc: 0.7964\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4622 - acc: 0.7968 - val_loss: 0.4562 - val_acc: 0.7931\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4572 - acc: 0.7988 - val_loss: 0.4549 - val_acc: 0.7947\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4543 - acc: 0.7995 - val_loss: 0.4527 - val_acc: 0.7947\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4530 - acc: 0.8015 - val_loss: 0.4506 - val_acc: 0.7980\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4508 - acc: 0.8008 - val_loss: 0.4476 - val_acc: 0.8013\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4475 - acc: 0.8031 - val_loss: 0.4457 - val_acc: 0.8030\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4429 - acc: 0.8041 - val_loss: 0.4447 - val_acc: 0.8030\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.4408 - acc: 0.8126 - val_loss: 0.4413 - val_acc: 0.8046\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4378 - acc: 0.8112 - val_loss: 0.4389 - val_acc: 0.8079\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4370 - acc: 0.8113 - val_loss: 0.4363 - val_acc: 0.8095\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4352 - acc: 0.8104 - val_loss: 0.4364 - val_acc: 0.8062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4326 - acc: 0.8088 - val_loss: 0.4402 - val_acc: 0.8030\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4309 - acc: 0.8155 - val_loss: 0.4320 - val_acc: 0.8112\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4283 - acc: 0.8179 - val_loss: 0.4332 - val_acc: 0.8046\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.4265 - acc: 0.8124 - val_loss: 0.4308 - val_acc: 0.8095\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4240 - acc: 0.8132 - val_loss: 0.4310 - val_acc: 0.8095\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4242 - acc: 0.8159 - val_loss: 0.4289 - val_acc: 0.8095\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4221 - acc: 0.8179 - val_loss: 0.4299 - val_acc: 0.8062\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4187 - acc: 0.8165 - val_loss: 0.4252 - val_acc: 0.8112\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4233 - acc: 0.8132 - val_loss: 0.4307 - val_acc: 0.8030\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4177 - acc: 0.8225 - val_loss: 0.4225 - val_acc: 0.8144\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4143 - acc: 0.8223 - val_loss: 0.4269 - val_acc: 0.8095\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4116 - acc: 0.8210 - val_loss: 0.4252 - val_acc: 0.8112\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4122 - acc: 0.8241 - val_loss: 0.4238 - val_acc: 0.8128\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4103 - acc: 0.8259 - val_loss: 0.4241 - val_acc: 0.8128\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4089 - acc: 0.8234 - val_loss: 0.4238 - val_acc: 0.8144\n",
      "Training with parameters {'batch_size': 1000, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 34ms/step - loss: 0.7974 - acc: 0.4402 - val_loss: 0.7287 - val_acc: 0.3924\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.7022 - acc: 0.5032 - val_loss: 0.6609 - val_acc: 0.6420\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.6643 - acc: 0.6046 - val_loss: 0.6335 - val_acc: 0.6190\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.6468 - acc: 0.6121 - val_loss: 0.6155 - val_acc: 0.6404\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6243 - acc: 0.6637 - val_loss: 0.6021 - val_acc: 0.7291\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.6053 - acc: 0.7074 - val_loss: 0.5927 - val_acc: 0.6979\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.5895 - acc: 0.7121 - val_loss: 0.5810 - val_acc: 0.6995\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.5761 - acc: 0.7165 - val_loss: 0.5647 - val_acc: 0.7176\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.5625 - acc: 0.7314 - val_loss: 0.5503 - val_acc: 0.7192\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.5525 - acc: 0.7364 - val_loss: 0.5397 - val_acc: 0.7241\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.5351 - acc: 0.7519 - val_loss: 0.5257 - val_acc: 0.7356\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.5274 - acc: 0.7550 - val_loss: 0.5110 - val_acc: 0.7422\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.5129 - acc: 0.7641 - val_loss: 0.4991 - val_acc: 0.7504\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.5037 - acc: 0.7729 - val_loss: 0.4870 - val_acc: 0.7603\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4913 - acc: 0.7747 - val_loss: 0.4782 - val_acc: 0.7701\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4866 - acc: 0.7783 - val_loss: 0.4689 - val_acc: 0.7767\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4772 - acc: 0.7874 - val_loss: 0.4616 - val_acc: 0.7816\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4698 - acc: 0.7889 - val_loss: 0.4592 - val_acc: 0.7833\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4643 - acc: 0.7953 - val_loss: 0.4491 - val_acc: 0.8013\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4596 - acc: 0.7933 - val_loss: 0.4504 - val_acc: 0.7915\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4565 - acc: 0.7958 - val_loss: 0.4391 - val_acc: 0.8013\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4484 - acc: 0.8042 - val_loss: 0.4398 - val_acc: 0.8013\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4475 - acc: 0.8002 - val_loss: 0.4337 - val_acc: 0.8128\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4435 - acc: 0.8048 - val_loss: 0.4342 - val_acc: 0.8062\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4405 - acc: 0.8084 - val_loss: 0.4290 - val_acc: 0.8112\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4371 - acc: 0.8086 - val_loss: 0.4268 - val_acc: 0.8095\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.4322 - acc: 0.8130 - val_loss: 0.4244 - val_acc: 0.8079\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.4300 - acc: 0.8093 - val_loss: 0.4252 - val_acc: 0.8079\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.4266 - acc: 0.8155 - val_loss: 0.4205 - val_acc: 0.8128\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4254 - acc: 0.8161 - val_loss: 0.4198 - val_acc: 0.8062\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.4229 - acc: 0.8163 - val_loss: 0.4181 - val_acc: 0.8095\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4224 - acc: 0.8203 - val_loss: 0.4195 - val_acc: 0.8128\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4165 - acc: 0.8170 - val_loss: 0.4171 - val_acc: 0.8144\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.4154 - acc: 0.8216 - val_loss: 0.4145 - val_acc: 0.8144\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4124 - acc: 0.8210 - val_loss: 0.4148 - val_acc: 0.8144\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4093 - acc: 0.8241 - val_loss: 0.4147 - val_acc: 0.8112\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.4071 - acc: 0.8247 - val_loss: 0.4123 - val_acc: 0.8144\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4092 - acc: 0.8258 - val_loss: 0.4154 - val_acc: 0.8144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4055 - acc: 0.8238 - val_loss: 0.4135 - val_acc: 0.8128\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4036 - acc: 0.8247 - val_loss: 0.4101 - val_acc: 0.8177\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4018 - acc: 0.8265 - val_loss: 0.4109 - val_acc: 0.8161\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4010 - acc: 0.8267 - val_loss: 0.4131 - val_acc: 0.8144\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.3970 - acc: 0.8287 - val_loss: 0.4115 - val_acc: 0.8161\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.3953 - acc: 0.8285 - val_loss: 0.4101 - val_acc: 0.8177\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.3962 - acc: 0.8254 - val_loss: 0.4107 - val_acc: 0.8161\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.3914 - acc: 0.8287 - val_loss: 0.4063 - val_acc: 0.8210\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.3902 - acc: 0.8323 - val_loss: 0.4103 - val_acc: 0.8177\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.3914 - acc: 0.8323 - val_loss: 0.4065 - val_acc: 0.8194\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.3862 - acc: 0.8300 - val_loss: 0.4150 - val_acc: 0.8227\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.3852 - acc: 0.8318 - val_loss: 0.4043 - val_acc: 0.8210\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.3828 - acc: 0.8342 - val_loss: 0.4109 - val_acc: 0.8227\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.3836 - acc: 0.8351 - val_loss: 0.4091 - val_acc: 0.8210\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.3815 - acc: 0.8325 - val_loss: 0.4065 - val_acc: 0.8177\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.3769 - acc: 0.8367 - val_loss: 0.4098 - val_acc: 0.8227\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.3744 - acc: 0.8363 - val_loss: 0.4072 - val_acc: 0.8194\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 1s 44ms/step - loss: 0.7390 - acc: 0.4470 - val_loss: 0.6710 - val_acc: 0.5961\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.6728 - acc: 0.5672 - val_loss: 0.6369 - val_acc: 0.6108\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.6472 - acc: 0.5837 - val_loss: 0.6145 - val_acc: 0.6716\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6203 - acc: 0.6756 - val_loss: 0.6045 - val_acc: 0.6765\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5997 - acc: 0.6979 - val_loss: 0.5931 - val_acc: 0.6568\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5810 - acc: 0.7105 - val_loss: 0.5695 - val_acc: 0.6929\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5629 - acc: 0.7340 - val_loss: 0.5512 - val_acc: 0.7241\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5466 - acc: 0.7440 - val_loss: 0.5365 - val_acc: 0.7340\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5302 - acc: 0.7517 - val_loss: 0.5258 - val_acc: 0.7258\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5144 - acc: 0.7603 - val_loss: 0.5049 - val_acc: 0.7685\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5006 - acc: 0.7776 - val_loss: 0.4939 - val_acc: 0.7750\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4919 - acc: 0.7778 - val_loss: 0.4867 - val_acc: 0.7800\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4776 - acc: 0.7916 - val_loss: 0.4721 - val_acc: 0.7849\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4720 - acc: 0.7931 - val_loss: 0.4721 - val_acc: 0.7800\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4661 - acc: 0.7873 - val_loss: 0.4581 - val_acc: 0.7947\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4583 - acc: 0.8008 - val_loss: 0.4523 - val_acc: 0.7931\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4517 - acc: 0.7986 - val_loss: 0.4537 - val_acc: 0.7931\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4467 - acc: 0.8042 - val_loss: 0.4415 - val_acc: 0.8013\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4399 - acc: 0.8082 - val_loss: 0.4417 - val_acc: 0.7947\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4388 - acc: 0.8101 - val_loss: 0.4359 - val_acc: 0.8079\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4324 - acc: 0.8119 - val_loss: 0.4357 - val_acc: 0.8030\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4267 - acc: 0.8150 - val_loss: 0.4285 - val_acc: 0.8095\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4250 - acc: 0.8132 - val_loss: 0.4297 - val_acc: 0.8079\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4231 - acc: 0.8157 - val_loss: 0.4250 - val_acc: 0.8095\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4207 - acc: 0.8196 - val_loss: 0.4234 - val_acc: 0.8095\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4174 - acc: 0.8208 - val_loss: 0.4238 - val_acc: 0.8112\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4126 - acc: 0.8214 - val_loss: 0.4191 - val_acc: 0.8144\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4124 - acc: 0.8197 - val_loss: 0.4205 - val_acc: 0.8128\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4087 - acc: 0.8214 - val_loss: 0.4147 - val_acc: 0.8161\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4084 - acc: 0.8254 - val_loss: 0.4177 - val_acc: 0.8161\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4057 - acc: 0.8245 - val_loss: 0.4138 - val_acc: 0.8177\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3993 - acc: 0.8301 - val_loss: 0.4148 - val_acc: 0.8194\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3993 - acc: 0.8259 - val_loss: 0.4121 - val_acc: 0.8194\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3995 - acc: 0.8259 - val_loss: 0.4184 - val_acc: 0.8210\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4004 - acc: 0.8278 - val_loss: 0.4111 - val_acc: 0.8194\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3938 - acc: 0.8318 - val_loss: 0.4126 - val_acc: 0.8227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3936 - acc: 0.8283 - val_loss: 0.4118 - val_acc: 0.8227\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3881 - acc: 0.8380 - val_loss: 0.4115 - val_acc: 0.8210\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3890 - acc: 0.8314 - val_loss: 0.4138 - val_acc: 0.8227\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3876 - acc: 0.8343 - val_loss: 0.4088 - val_acc: 0.8210\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3843 - acc: 0.8318 - val_loss: 0.4132 - val_acc: 0.8227\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3817 - acc: 0.8349 - val_loss: 0.4077 - val_acc: 0.8177\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3792 - acc: 0.8349 - val_loss: 0.4160 - val_acc: 0.8227\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3809 - acc: 0.8336 - val_loss: 0.4066 - val_acc: 0.8210\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3788 - acc: 0.8393 - val_loss: 0.4109 - val_acc: 0.8227\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3735 - acc: 0.8394 - val_loss: 0.4075 - val_acc: 0.8210\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3710 - acc: 0.8440 - val_loss: 0.4099 - val_acc: 0.8210\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3674 - acc: 0.8425 - val_loss: 0.4071 - val_acc: 0.8210\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3666 - acc: 0.8422 - val_loss: 0.4154 - val_acc: 0.8227\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.8074 - acc: 0.4377 - val_loss: 0.7465 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.7173 - acc: 0.4777 - val_loss: 0.6761 - val_acc: 0.6223\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6775 - acc: 0.5827 - val_loss: 0.6445 - val_acc: 0.6256\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6555 - acc: 0.5982 - val_loss: 0.6272 - val_acc: 0.6207\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6421 - acc: 0.6108 - val_loss: 0.6130 - val_acc: 0.6502\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6226 - acc: 0.6667 - val_loss: 0.6021 - val_acc: 0.7274\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6060 - acc: 0.7043 - val_loss: 0.5947 - val_acc: 0.6979\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5926 - acc: 0.7044 - val_loss: 0.5855 - val_acc: 0.6995\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5818 - acc: 0.7097 - val_loss: 0.5721 - val_acc: 0.7077\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5703 - acc: 0.7223 - val_loss: 0.5589 - val_acc: 0.7159\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5589 - acc: 0.7331 - val_loss: 0.5472 - val_acc: 0.7209\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5509 - acc: 0.7360 - val_loss: 0.5384 - val_acc: 0.7258\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5390 - acc: 0.7444 - val_loss: 0.5273 - val_acc: 0.7323\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5272 - acc: 0.7539 - val_loss: 0.5122 - val_acc: 0.7455\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5163 - acc: 0.7643 - val_loss: 0.5013 - val_acc: 0.7521\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5038 - acc: 0.7730 - val_loss: 0.4944 - val_acc: 0.7521\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4955 - acc: 0.7778 - val_loss: 0.4819 - val_acc: 0.7668\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4923 - acc: 0.7831 - val_loss: 0.4748 - val_acc: 0.7734\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4830 - acc: 0.7882 - val_loss: 0.4708 - val_acc: 0.7750\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4762 - acc: 0.7895 - val_loss: 0.4632 - val_acc: 0.7800\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4729 - acc: 0.7891 - val_loss: 0.4550 - val_acc: 0.7849\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4653 - acc: 0.7891 - val_loss: 0.4572 - val_acc: 0.7865\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4575 - acc: 0.7969 - val_loss: 0.4471 - val_acc: 0.7980\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4547 - acc: 0.7968 - val_loss: 0.4424 - val_acc: 0.8013\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4501 - acc: 0.8000 - val_loss: 0.4434 - val_acc: 0.7997\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4492 - acc: 0.8037 - val_loss: 0.4343 - val_acc: 0.8079\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4442 - acc: 0.8019 - val_loss: 0.4372 - val_acc: 0.7980\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4422 - acc: 0.8024 - val_loss: 0.4329 - val_acc: 0.8079\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4374 - acc: 0.8051 - val_loss: 0.4311 - val_acc: 0.8062\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4357 - acc: 0.8079 - val_loss: 0.4253 - val_acc: 0.8144\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4332 - acc: 0.8106 - val_loss: 0.4272 - val_acc: 0.8062\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4305 - acc: 0.8152 - val_loss: 0.4226 - val_acc: 0.8128\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4286 - acc: 0.8154 - val_loss: 0.4246 - val_acc: 0.8046\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4251 - acc: 0.8137 - val_loss: 0.4225 - val_acc: 0.8079\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4221 - acc: 0.8132 - val_loss: 0.4186 - val_acc: 0.8128\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4239 - acc: 0.8141 - val_loss: 0.4220 - val_acc: 0.8079\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4211 - acc: 0.8166 - val_loss: 0.4158 - val_acc: 0.8128\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4187 - acc: 0.8207 - val_loss: 0.4161 - val_acc: 0.8095\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4130 - acc: 0.8203 - val_loss: 0.4151 - val_acc: 0.8095\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4158 - acc: 0.8219 - val_loss: 0.4150 - val_acc: 0.8112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4097 - acc: 0.8192 - val_loss: 0.4196 - val_acc: 0.8128\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4112 - acc: 0.8190 - val_loss: 0.4135 - val_acc: 0.8161\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4078 - acc: 0.8261 - val_loss: 0.4160 - val_acc: 0.8128\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4088 - acc: 0.8230 - val_loss: 0.4092 - val_acc: 0.8194\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4071 - acc: 0.8258 - val_loss: 0.4154 - val_acc: 0.8144\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4060 - acc: 0.8225 - val_loss: 0.4102 - val_acc: 0.8210\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4031 - acc: 0.8221 - val_loss: 0.4148 - val_acc: 0.8144\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4004 - acc: 0.8217 - val_loss: 0.4109 - val_acc: 0.8177\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4003 - acc: 0.8294 - val_loss: 0.4119 - val_acc: 0.8161\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.6628 - acc: 0.6231 - val_loss: 0.6449 - val_acc: 0.6453\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6404 - acc: 0.6499 - val_loss: 0.6257 - val_acc: 0.6683\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6272 - acc: 0.6603 - val_loss: 0.6134 - val_acc: 0.6782\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6104 - acc: 0.6955 - val_loss: 0.6052 - val_acc: 0.6814\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6004 - acc: 0.7039 - val_loss: 0.5975 - val_acc: 0.6847\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5900 - acc: 0.7063 - val_loss: 0.5874 - val_acc: 0.6880\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5781 - acc: 0.7207 - val_loss: 0.5766 - val_acc: 0.7077\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5678 - acc: 0.7331 - val_loss: 0.5677 - val_acc: 0.7126\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5583 - acc: 0.7429 - val_loss: 0.5610 - val_acc: 0.7143\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5517 - acc: 0.7424 - val_loss: 0.5526 - val_acc: 0.7291\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5423 - acc: 0.7515 - val_loss: 0.5438 - val_acc: 0.7340\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5373 - acc: 0.7533 - val_loss: 0.5375 - val_acc: 0.7373\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5297 - acc: 0.7612 - val_loss: 0.5312 - val_acc: 0.7438\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5237 - acc: 0.7677 - val_loss: 0.5252 - val_acc: 0.7521\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5188 - acc: 0.7672 - val_loss: 0.5195 - val_acc: 0.7537\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5141 - acc: 0.7677 - val_loss: 0.5139 - val_acc: 0.7586\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5089 - acc: 0.7727 - val_loss: 0.5095 - val_acc: 0.7668\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5034 - acc: 0.7783 - val_loss: 0.5061 - val_acc: 0.7652\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4979 - acc: 0.7807 - val_loss: 0.5015 - val_acc: 0.7652\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4953 - acc: 0.7827 - val_loss: 0.4963 - val_acc: 0.7668\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4907 - acc: 0.7823 - val_loss: 0.4930 - val_acc: 0.7734\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4888 - acc: 0.7865 - val_loss: 0.4895 - val_acc: 0.7767\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4858 - acc: 0.7873 - val_loss: 0.4848 - val_acc: 0.7750\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4822 - acc: 0.7885 - val_loss: 0.4820 - val_acc: 0.7767\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4769 - acc: 0.7913 - val_loss: 0.4812 - val_acc: 0.7800\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4754 - acc: 0.7920 - val_loss: 0.4780 - val_acc: 0.7800\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4739 - acc: 0.7918 - val_loss: 0.4745 - val_acc: 0.7849\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4689 - acc: 0.7935 - val_loss: 0.4716 - val_acc: 0.7898\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4682 - acc: 0.7940 - val_loss: 0.4688 - val_acc: 0.7898\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4646 - acc: 0.7966 - val_loss: 0.4660 - val_acc: 0.7931\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4617 - acc: 0.8066 - val_loss: 0.4637 - val_acc: 0.7882\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4621 - acc: 0.8000 - val_loss: 0.4617 - val_acc: 0.7915\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4601 - acc: 0.7984 - val_loss: 0.4628 - val_acc: 0.7931\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4577 - acc: 0.8006 - val_loss: 0.4614 - val_acc: 0.7947\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4547 - acc: 0.8050 - val_loss: 0.4570 - val_acc: 0.7931\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4536 - acc: 0.8035 - val_loss: 0.4559 - val_acc: 0.7947\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4519 - acc: 0.8039 - val_loss: 0.4568 - val_acc: 0.7980\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4505 - acc: 0.8033 - val_loss: 0.4544 - val_acc: 0.7980\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4471 - acc: 0.8064 - val_loss: 0.4503 - val_acc: 0.7997\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4482 - acc: 0.8068 - val_loss: 0.4504 - val_acc: 0.8013\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4459 - acc: 0.8055 - val_loss: 0.4511 - val_acc: 0.7997\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4443 - acc: 0.8061 - val_loss: 0.4480 - val_acc: 0.8062\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4417 - acc: 0.8117 - val_loss: 0.4457 - val_acc: 0.8062\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4427 - acc: 0.8077 - val_loss: 0.4450 - val_acc: 0.8062\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4403 - acc: 0.8092 - val_loss: 0.4446 - val_acc: 0.8062\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4391 - acc: 0.8137 - val_loss: 0.4430 - val_acc: 0.8062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4383 - acc: 0.8137 - val_loss: 0.4393 - val_acc: 0.8095\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4375 - acc: 0.8092 - val_loss: 0.4402 - val_acc: 0.8079\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4366 - acc: 0.8117 - val_loss: 0.4426 - val_acc: 0.8095\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4346 - acc: 0.8115 - val_loss: 0.4380 - val_acc: 0.8112\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4324 - acc: 0.8135 - val_loss: 0.4379 - val_acc: 0.8112\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4320 - acc: 0.8093 - val_loss: 0.4369 - val_acc: 0.8128\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4316 - acc: 0.8128 - val_loss: 0.4359 - val_acc: 0.8128\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4302 - acc: 0.8194 - val_loss: 0.4360 - val_acc: 0.8112\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4292 - acc: 0.8135 - val_loss: 0.4345 - val_acc: 0.8128\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4279 - acc: 0.8177 - val_loss: 0.4351 - val_acc: 0.8112\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4282 - acc: 0.8176 - val_loss: 0.4334 - val_acc: 0.8128\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4270 - acc: 0.8183 - val_loss: 0.4323 - val_acc: 0.8144\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4242 - acc: 0.8176 - val_loss: 0.4323 - val_acc: 0.8128\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4238 - acc: 0.8190 - val_loss: 0.4315 - val_acc: 0.8128\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4238 - acc: 0.8176 - val_loss: 0.4301 - val_acc: 0.8144\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4229 - acc: 0.8163 - val_loss: 0.4286 - val_acc: 0.8161\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4214 - acc: 0.8194 - val_loss: 0.4299 - val_acc: 0.8112\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4207 - acc: 0.8214 - val_loss: 0.4277 - val_acc: 0.8194\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4213 - acc: 0.8181 - val_loss: 0.4279 - val_acc: 0.8177\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4195 - acc: 0.8210 - val_loss: 0.4271 - val_acc: 0.8194\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4196 - acc: 0.8201 - val_loss: 0.4263 - val_acc: 0.8177\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4174 - acc: 0.8190 - val_loss: 0.4292 - val_acc: 0.8144\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4167 - acc: 0.8225 - val_loss: 0.4255 - val_acc: 0.8177\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4180 - acc: 0.8177 - val_loss: 0.4239 - val_acc: 0.8177\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4165 - acc: 0.8192 - val_loss: 0.4273 - val_acc: 0.8177\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4138 - acc: 0.8236 - val_loss: 0.4258 - val_acc: 0.8144\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4145 - acc: 0.8219 - val_loss: 0.4238 - val_acc: 0.8177\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4131 - acc: 0.8217 - val_loss: 0.4220 - val_acc: 0.8161\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4129 - acc: 0.8238 - val_loss: 0.4258 - val_acc: 0.8161\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4122 - acc: 0.8216 - val_loss: 0.4262 - val_acc: 0.8161\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4112 - acc: 0.8223 - val_loss: 0.4217 - val_acc: 0.8177\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4105 - acc: 0.8236 - val_loss: 0.4225 - val_acc: 0.8177\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4087 - acc: 0.8243 - val_loss: 0.4253 - val_acc: 0.8144\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4093 - acc: 0.8239 - val_loss: 0.4223 - val_acc: 0.8177\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4087 - acc: 0.8250 - val_loss: 0.4208 - val_acc: 0.8161\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4084 - acc: 0.8241 - val_loss: 0.4221 - val_acc: 0.8161\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4078 - acc: 0.8239 - val_loss: 0.4232 - val_acc: 0.8161\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4073 - acc: 0.8258 - val_loss: 0.4205 - val_acc: 0.8177\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4065 - acc: 0.8261 - val_loss: 0.4196 - val_acc: 0.8161\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4063 - acc: 0.8259 - val_loss: 0.4227 - val_acc: 0.8161\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4041 - acc: 0.8243 - val_loss: 0.4207 - val_acc: 0.8177\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4048 - acc: 0.8254 - val_loss: 0.4179 - val_acc: 0.8177\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4046 - acc: 0.8274 - val_loss: 0.4203 - val_acc: 0.8177\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4043 - acc: 0.8292 - val_loss: 0.4211 - val_acc: 0.8177\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4023 - acc: 0.8252 - val_loss: 0.4190 - val_acc: 0.8210\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4030 - acc: 0.8274 - val_loss: 0.4195 - val_acc: 0.8194\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4033 - acc: 0.8263 - val_loss: 0.4203 - val_acc: 0.8177\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 1s 43ms/step - loss: 0.8007 - acc: 0.4361 - val_loss: 0.7123 - val_acc: 0.4417\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.6931 - acc: 0.5227 - val_loss: 0.6528 - val_acc: 0.6125\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6653 - acc: 0.5718 - val_loss: 0.6293 - val_acc: 0.6190\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.6393 - acc: 0.5990 - val_loss: 0.6097 - val_acc: 0.6831\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6127 - acc: 0.6864 - val_loss: 0.6023 - val_acc: 0.6732\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5935 - acc: 0.6980 - val_loss: 0.5955 - val_acc: 0.6683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.5800 - acc: 0.7008 - val_loss: 0.5761 - val_acc: 0.6864\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.5652 - acc: 0.7272 - val_loss: 0.5561 - val_acc: 0.7126\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.5511 - acc: 0.7386 - val_loss: 0.5436 - val_acc: 0.7110\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.5369 - acc: 0.7438 - val_loss: 0.5293 - val_acc: 0.7258\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.5199 - acc: 0.7628 - val_loss: 0.5117 - val_acc: 0.7586\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5096 - acc: 0.7732 - val_loss: 0.4983 - val_acc: 0.7668\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4980 - acc: 0.7763 - val_loss: 0.4922 - val_acc: 0.7652\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4902 - acc: 0.7805 - val_loss: 0.4805 - val_acc: 0.7783\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4794 - acc: 0.7869 - val_loss: 0.4702 - val_acc: 0.7816\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4738 - acc: 0.7911 - val_loss: 0.4680 - val_acc: 0.7882\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4688 - acc: 0.7937 - val_loss: 0.4594 - val_acc: 0.7915\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4591 - acc: 0.7977 - val_loss: 0.4513 - val_acc: 0.8062\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4572 - acc: 0.8031 - val_loss: 0.4526 - val_acc: 0.7980\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4499 - acc: 0.8028 - val_loss: 0.4414 - val_acc: 0.8062\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4465 - acc: 0.8062 - val_loss: 0.4391 - val_acc: 0.8030\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4424 - acc: 0.8103 - val_loss: 0.4364 - val_acc: 0.8046\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4363 - acc: 0.8135 - val_loss: 0.4319 - val_acc: 0.8079\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4304 - acc: 0.8163 - val_loss: 0.4350 - val_acc: 0.8046\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4311 - acc: 0.8137 - val_loss: 0.4280 - val_acc: 0.8079\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4261 - acc: 0.8166 - val_loss: 0.4263 - val_acc: 0.8095\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4241 - acc: 0.8212 - val_loss: 0.4255 - val_acc: 0.8079\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4215 - acc: 0.8174 - val_loss: 0.4235 - val_acc: 0.8095\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4171 - acc: 0.8227 - val_loss: 0.4217 - val_acc: 0.8062\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4160 - acc: 0.8212 - val_loss: 0.4220 - val_acc: 0.8079\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4173 - acc: 0.8194 - val_loss: 0.4173 - val_acc: 0.8161\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4127 - acc: 0.8261 - val_loss: 0.4216 - val_acc: 0.8177\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4103 - acc: 0.8207 - val_loss: 0.4139 - val_acc: 0.8177\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4083 - acc: 0.8261 - val_loss: 0.4185 - val_acc: 0.8177\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4061 - acc: 0.8261 - val_loss: 0.4128 - val_acc: 0.8161\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4040 - acc: 0.8267 - val_loss: 0.4155 - val_acc: 0.8177\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4017 - acc: 0.8283 - val_loss: 0.4135 - val_acc: 0.8161\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4008 - acc: 0.8269 - val_loss: 0.4110 - val_acc: 0.8161\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3976 - acc: 0.8292 - val_loss: 0.4134 - val_acc: 0.8194\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3951 - acc: 0.8283 - val_loss: 0.4113 - val_acc: 0.8177\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3939 - acc: 0.8327 - val_loss: 0.4119 - val_acc: 0.8177\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3946 - acc: 0.8331 - val_loss: 0.4129 - val_acc: 0.8194\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3902 - acc: 0.8336 - val_loss: 0.4091 - val_acc: 0.8210\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3890 - acc: 0.8349 - val_loss: 0.4128 - val_acc: 0.8227\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3860 - acc: 0.8347 - val_loss: 0.4093 - val_acc: 0.8194\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3858 - acc: 0.8334 - val_loss: 0.4104 - val_acc: 0.8210\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3804 - acc: 0.8402 - val_loss: 0.4109 - val_acc: 0.8177\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3810 - acc: 0.8420 - val_loss: 0.4148 - val_acc: 0.8243\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.8419 - acc: 0.4348 - val_loss: 0.7813 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.7407 - acc: 0.4474 - val_loss: 0.7060 - val_acc: 0.4351\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6911 - acc: 0.5286 - val_loss: 0.6676 - val_acc: 0.6502\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6655 - acc: 0.6170 - val_loss: 0.6455 - val_acc: 0.6831\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6505 - acc: 0.6420 - val_loss: 0.6298 - val_acc: 0.7192\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6348 - acc: 0.6793 - val_loss: 0.6178 - val_acc: 0.7389\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6194 - acc: 0.7068 - val_loss: 0.6088 - val_acc: 0.7094\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6075 - acc: 0.7055 - val_loss: 0.6005 - val_acc: 0.7011\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5946 - acc: 0.7134 - val_loss: 0.5909 - val_acc: 0.7011\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5865 - acc: 0.7179 - val_loss: 0.5815 - val_acc: 0.7028\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5753 - acc: 0.7322 - val_loss: 0.5707 - val_acc: 0.7011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5649 - acc: 0.7369 - val_loss: 0.5603 - val_acc: 0.7126\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5595 - acc: 0.7387 - val_loss: 0.5520 - val_acc: 0.7143\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5505 - acc: 0.7455 - val_loss: 0.5434 - val_acc: 0.7077\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5409 - acc: 0.7519 - val_loss: 0.5362 - val_acc: 0.7258\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5347 - acc: 0.7559 - val_loss: 0.5271 - val_acc: 0.7389\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5268 - acc: 0.7646 - val_loss: 0.5180 - val_acc: 0.7488\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5227 - acc: 0.7639 - val_loss: 0.5118 - val_acc: 0.7488\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5142 - acc: 0.7648 - val_loss: 0.5066 - val_acc: 0.7471\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5068 - acc: 0.7752 - val_loss: 0.4978 - val_acc: 0.7586\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5029 - acc: 0.7741 - val_loss: 0.4921 - val_acc: 0.7586\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4979 - acc: 0.7745 - val_loss: 0.4877 - val_acc: 0.7553\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4945 - acc: 0.7732 - val_loss: 0.4826 - val_acc: 0.7619\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4867 - acc: 0.7823 - val_loss: 0.4766 - val_acc: 0.7685\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4772 - acc: 0.7918 - val_loss: 0.4714 - val_acc: 0.7734\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4780 - acc: 0.7865 - val_loss: 0.4678 - val_acc: 0.7783\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4735 - acc: 0.7924 - val_loss: 0.4655 - val_acc: 0.7750\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4683 - acc: 0.7871 - val_loss: 0.4609 - val_acc: 0.7767\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4661 - acc: 0.7900 - val_loss: 0.4566 - val_acc: 0.7833\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4611 - acc: 0.7968 - val_loss: 0.4530 - val_acc: 0.7898\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4573 - acc: 0.7995 - val_loss: 0.4492 - val_acc: 0.7947\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4537 - acc: 0.8015 - val_loss: 0.4471 - val_acc: 0.7947\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4509 - acc: 0.8002 - val_loss: 0.4460 - val_acc: 0.7980\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4500 - acc: 0.8015 - val_loss: 0.4392 - val_acc: 0.8030\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4477 - acc: 0.8042 - val_loss: 0.4402 - val_acc: 0.8030\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4409 - acc: 0.8062 - val_loss: 0.4360 - val_acc: 0.8030\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4400 - acc: 0.8070 - val_loss: 0.4342 - val_acc: 0.8046\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4375 - acc: 0.8082 - val_loss: 0.4396 - val_acc: 0.7997\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4355 - acc: 0.8115 - val_loss: 0.4319 - val_acc: 0.8062\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4339 - acc: 0.8112 - val_loss: 0.4289 - val_acc: 0.8079\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4330 - acc: 0.8123 - val_loss: 0.4310 - val_acc: 0.8046\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4304 - acc: 0.8110 - val_loss: 0.4277 - val_acc: 0.8062\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4277 - acc: 0.8132 - val_loss: 0.4254 - val_acc: 0.8095\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4262 - acc: 0.8150 - val_loss: 0.4278 - val_acc: 0.8062\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4269 - acc: 0.8141 - val_loss: 0.4247 - val_acc: 0.8079\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4252 - acc: 0.8146 - val_loss: 0.4240 - val_acc: 0.8095\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4255 - acc: 0.8128 - val_loss: 0.4282 - val_acc: 0.8062\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4219 - acc: 0.8148 - val_loss: 0.4199 - val_acc: 0.8161\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4190 - acc: 0.8196 - val_loss: 0.4254 - val_acc: 0.8095\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4172 - acc: 0.8203 - val_loss: 0.4236 - val_acc: 0.8128\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4172 - acc: 0.8221 - val_loss: 0.4200 - val_acc: 0.8128\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4144 - acc: 0.8188 - val_loss: 0.4201 - val_acc: 0.8128\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4150 - acc: 0.8208 - val_loss: 0.4202 - val_acc: 0.8161\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.7482 - acc: 0.4402 - val_loss: 0.7266 - val_acc: 0.4089\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.7023 - acc: 0.4839 - val_loss: 0.6820 - val_acc: 0.5616\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6734 - acc: 0.6121 - val_loss: 0.6565 - val_acc: 0.6864\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6539 - acc: 0.6508 - val_loss: 0.6407 - val_acc: 0.6502\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6457 - acc: 0.6311 - val_loss: 0.6289 - val_acc: 0.6552\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6347 - acc: 0.6477 - val_loss: 0.6191 - val_acc: 0.6995\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6231 - acc: 0.6802 - val_loss: 0.6111 - val_acc: 0.7110\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6117 - acc: 0.7064 - val_loss: 0.6046 - val_acc: 0.6962\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6000 - acc: 0.7134 - val_loss: 0.5971 - val_acc: 0.6979\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5887 - acc: 0.7143 - val_loss: 0.5881 - val_acc: 0.6995\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5822 - acc: 0.7229 - val_loss: 0.5793 - val_acc: 0.7044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5713 - acc: 0.7289 - val_loss: 0.5706 - val_acc: 0.7143\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5610 - acc: 0.7449 - val_loss: 0.5611 - val_acc: 0.7209\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5531 - acc: 0.7460 - val_loss: 0.5521 - val_acc: 0.7307\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5415 - acc: 0.7535 - val_loss: 0.5426 - val_acc: 0.7340\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5366 - acc: 0.7544 - val_loss: 0.5329 - val_acc: 0.7455\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5275 - acc: 0.7568 - val_loss: 0.5251 - val_acc: 0.7438\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5192 - acc: 0.7661 - val_loss: 0.5157 - val_acc: 0.7504\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5114 - acc: 0.7677 - val_loss: 0.5082 - val_acc: 0.7553\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5035 - acc: 0.7683 - val_loss: 0.5009 - val_acc: 0.7570\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4994 - acc: 0.7783 - val_loss: 0.4952 - val_acc: 0.7652\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4933 - acc: 0.7781 - val_loss: 0.4915 - val_acc: 0.7718\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4852 - acc: 0.7842 - val_loss: 0.4861 - val_acc: 0.7734\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4818 - acc: 0.7865 - val_loss: 0.4789 - val_acc: 0.7800\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4779 - acc: 0.7873 - val_loss: 0.4751 - val_acc: 0.7816\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4718 - acc: 0.7895 - val_loss: 0.4729 - val_acc: 0.7833\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4691 - acc: 0.7889 - val_loss: 0.4660 - val_acc: 0.7898\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4655 - acc: 0.7940 - val_loss: 0.4634 - val_acc: 0.7964\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4595 - acc: 0.7969 - val_loss: 0.4585 - val_acc: 0.7931\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4607 - acc: 0.8017 - val_loss: 0.4555 - val_acc: 0.7915\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4565 - acc: 0.7957 - val_loss: 0.4567 - val_acc: 0.7931\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4527 - acc: 0.7978 - val_loss: 0.4501 - val_acc: 0.7947\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4486 - acc: 0.8020 - val_loss: 0.4491 - val_acc: 0.7980\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4452 - acc: 0.8024 - val_loss: 0.4477 - val_acc: 0.7997\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4434 - acc: 0.8081 - val_loss: 0.4450 - val_acc: 0.8030\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4430 - acc: 0.8039 - val_loss: 0.4446 - val_acc: 0.8030\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4390 - acc: 0.8086 - val_loss: 0.4424 - val_acc: 0.8046\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4391 - acc: 0.8072 - val_loss: 0.4408 - val_acc: 0.8062\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4377 - acc: 0.8093 - val_loss: 0.4361 - val_acc: 0.8079\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4357 - acc: 0.8119 - val_loss: 0.4396 - val_acc: 0.8046\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4332 - acc: 0.8097 - val_loss: 0.4373 - val_acc: 0.8030\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4331 - acc: 0.8070 - val_loss: 0.4326 - val_acc: 0.8079\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4321 - acc: 0.8130 - val_loss: 0.4368 - val_acc: 0.8030\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4256 - acc: 0.8168 - val_loss: 0.4295 - val_acc: 0.8095\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4245 - acc: 0.8148 - val_loss: 0.4307 - val_acc: 0.8079\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4210 - acc: 0.8238 - val_loss: 0.4304 - val_acc: 0.8095\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4246 - acc: 0.8155 - val_loss: 0.4289 - val_acc: 0.8112\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4224 - acc: 0.8132 - val_loss: 0.4297 - val_acc: 0.8079\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4195 - acc: 0.8181 - val_loss: 0.4274 - val_acc: 0.8128\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4164 - acc: 0.8234 - val_loss: 0.4270 - val_acc: 0.8144\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4143 - acc: 0.8216 - val_loss: 0.4266 - val_acc: 0.8144\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4170 - acc: 0.8181 - val_loss: 0.4261 - val_acc: 0.8144\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4145 - acc: 0.8199 - val_loss: 0.4254 - val_acc: 0.8144\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4130 - acc: 0.8223 - val_loss: 0.4270 - val_acc: 0.8144\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4127 - acc: 0.8239 - val_loss: 0.4211 - val_acc: 0.8161\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4133 - acc: 0.8203 - val_loss: 0.4301 - val_acc: 0.8144\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4110 - acc: 0.8243 - val_loss: 0.4198 - val_acc: 0.8194\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4076 - acc: 0.8252 - val_loss: 0.4248 - val_acc: 0.8161\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4097 - acc: 0.8216 - val_loss: 0.4230 - val_acc: 0.8161\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4063 - acc: 0.8254 - val_loss: 0.4209 - val_acc: 0.8194\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4043 - acc: 0.8274 - val_loss: 0.4221 - val_acc: 0.8177\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4022 - acc: 0.8287 - val_loss: 0.4223 - val_acc: 0.8194\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.8075 - acc: 0.4395 - val_loss: 0.7465 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.7175 - acc: 0.4632 - val_loss: 0.6763 - val_acc: 0.6240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6745 - acc: 0.5957 - val_loss: 0.6447 - val_acc: 0.6256\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6564 - acc: 0.6032 - val_loss: 0.6268 - val_acc: 0.6223\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6395 - acc: 0.6180 - val_loss: 0.6124 - val_acc: 0.6552\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.6198 - acc: 0.6652 - val_loss: 0.6015 - val_acc: 0.7225\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.6055 - acc: 0.7123 - val_loss: 0.5942 - val_acc: 0.6979\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5925 - acc: 0.7039 - val_loss: 0.5856 - val_acc: 0.6979\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5796 - acc: 0.7108 - val_loss: 0.5727 - val_acc: 0.7028\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.5705 - acc: 0.7227 - val_loss: 0.5590 - val_acc: 0.7176\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.5602 - acc: 0.7302 - val_loss: 0.5469 - val_acc: 0.7225\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5501 - acc: 0.7375 - val_loss: 0.5369 - val_acc: 0.7225\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5353 - acc: 0.7535 - val_loss: 0.5276 - val_acc: 0.7274\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.5293 - acc: 0.7526 - val_loss: 0.5177 - val_acc: 0.7307\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.5153 - acc: 0.7610 - val_loss: 0.5018 - val_acc: 0.7553\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5088 - acc: 0.7718 - val_loss: 0.4917 - val_acc: 0.7619\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4969 - acc: 0.7761 - val_loss: 0.4867 - val_acc: 0.7586\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4898 - acc: 0.7820 - val_loss: 0.4771 - val_acc: 0.7718\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4843 - acc: 0.7831 - val_loss: 0.4688 - val_acc: 0.7750\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4760 - acc: 0.7874 - val_loss: 0.4642 - val_acc: 0.7800\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4681 - acc: 0.7922 - val_loss: 0.4569 - val_acc: 0.7849\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4661 - acc: 0.7880 - val_loss: 0.4507 - val_acc: 0.7964\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4625 - acc: 0.7937 - val_loss: 0.4493 - val_acc: 0.7947\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4546 - acc: 0.7982 - val_loss: 0.4436 - val_acc: 0.8030\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4536 - acc: 0.8022 - val_loss: 0.4377 - val_acc: 0.8030\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4487 - acc: 0.7997 - val_loss: 0.4413 - val_acc: 0.7964\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4477 - acc: 0.8050 - val_loss: 0.4340 - val_acc: 0.8095\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4413 - acc: 0.8053 - val_loss: 0.4317 - val_acc: 0.8112\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4384 - acc: 0.8075 - val_loss: 0.4299 - val_acc: 0.8095\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4365 - acc: 0.8093 - val_loss: 0.4263 - val_acc: 0.8144\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4329 - acc: 0.8103 - val_loss: 0.4260 - val_acc: 0.8079\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4316 - acc: 0.8104 - val_loss: 0.4226 - val_acc: 0.8144\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4271 - acc: 0.8124 - val_loss: 0.4261 - val_acc: 0.8112\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4275 - acc: 0.8123 - val_loss: 0.4191 - val_acc: 0.8161\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4223 - acc: 0.8194 - val_loss: 0.4234 - val_acc: 0.8112\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4230 - acc: 0.8152 - val_loss: 0.4178 - val_acc: 0.8144\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4215 - acc: 0.8172 - val_loss: 0.4200 - val_acc: 0.8112\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4179 - acc: 0.8148 - val_loss: 0.4165 - val_acc: 0.8128\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4135 - acc: 0.8207 - val_loss: 0.4154 - val_acc: 0.8112\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4134 - acc: 0.8201 - val_loss: 0.4175 - val_acc: 0.8128\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4126 - acc: 0.8181 - val_loss: 0.4174 - val_acc: 0.8128\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4088 - acc: 0.8210 - val_loss: 0.4114 - val_acc: 0.8210\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4089 - acc: 0.8250 - val_loss: 0.4163 - val_acc: 0.8128\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4071 - acc: 0.8228 - val_loss: 0.4115 - val_acc: 0.8194\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4039 - acc: 0.8252 - val_loss: 0.4158 - val_acc: 0.8128\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4028 - acc: 0.8265 - val_loss: 0.4121 - val_acc: 0.8210\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4015 - acc: 0.8254 - val_loss: 0.4118 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': 1500, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "4/4 [==============================] - 1s 56ms/step - loss: 0.7453 - acc: 0.4454 - val_loss: 0.6849 - val_acc: 0.5961\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.6811 - acc: 0.5504 - val_loss: 0.6465 - val_acc: 0.6125\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.6612 - acc: 0.5729 - val_loss: 0.6271 - val_acc: 0.6108\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.6386 - acc: 0.5986 - val_loss: 0.6097 - val_acc: 0.7176\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.6153 - acc: 0.6895 - val_loss: 0.6022 - val_acc: 0.6765\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5994 - acc: 0.6988 - val_loss: 0.5952 - val_acc: 0.6585\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5841 - acc: 0.6982 - val_loss: 0.5773 - val_acc: 0.6798\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5696 - acc: 0.7223 - val_loss: 0.5585 - val_acc: 0.7209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5554 - acc: 0.7411 - val_loss: 0.5448 - val_acc: 0.7340\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5406 - acc: 0.7528 - val_loss: 0.5354 - val_acc: 0.7274\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5270 - acc: 0.7568 - val_loss: 0.5231 - val_acc: 0.7438\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5144 - acc: 0.7623 - val_loss: 0.5088 - val_acc: 0.7603\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5046 - acc: 0.7716 - val_loss: 0.4967 - val_acc: 0.7734\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4936 - acc: 0.7833 - val_loss: 0.4897 - val_acc: 0.7816\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4847 - acc: 0.7811 - val_loss: 0.4821 - val_acc: 0.7833\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4752 - acc: 0.7907 - val_loss: 0.4704 - val_acc: 0.7849\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4687 - acc: 0.7895 - val_loss: 0.4689 - val_acc: 0.7816\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4628 - acc: 0.7971 - val_loss: 0.4590 - val_acc: 0.7882\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4532 - acc: 0.8026 - val_loss: 0.4560 - val_acc: 0.7882\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4520 - acc: 0.8026 - val_loss: 0.4517 - val_acc: 0.7915\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4471 - acc: 0.8037 - val_loss: 0.4442 - val_acc: 0.7980\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4422 - acc: 0.8072 - val_loss: 0.4432 - val_acc: 0.7964\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4360 - acc: 0.8113 - val_loss: 0.4344 - val_acc: 0.8095\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4338 - acc: 0.8121 - val_loss: 0.4357 - val_acc: 0.8030\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4326 - acc: 0.8093 - val_loss: 0.4318 - val_acc: 0.8062\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4318 - acc: 0.8101 - val_loss: 0.4254 - val_acc: 0.8128\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4252 - acc: 0.8130 - val_loss: 0.4320 - val_acc: 0.8062\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4213 - acc: 0.8174 - val_loss: 0.4214 - val_acc: 0.8128\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4197 - acc: 0.8157 - val_loss: 0.4243 - val_acc: 0.8128\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4171 - acc: 0.8192 - val_loss: 0.4207 - val_acc: 0.8112\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4132 - acc: 0.8221 - val_loss: 0.4192 - val_acc: 0.8144\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4120 - acc: 0.8197 - val_loss: 0.4202 - val_acc: 0.8144\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4089 - acc: 0.8250 - val_loss: 0.4160 - val_acc: 0.8177\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4075 - acc: 0.8232 - val_loss: 0.4177 - val_acc: 0.8161\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4052 - acc: 0.8270 - val_loss: 0.4131 - val_acc: 0.8194\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4024 - acc: 0.8274 - val_loss: 0.4168 - val_acc: 0.8210\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4025 - acc: 0.8258 - val_loss: 0.4133 - val_acc: 0.8210\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4003 - acc: 0.8280 - val_loss: 0.4148 - val_acc: 0.8210\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3942 - acc: 0.8345 - val_loss: 0.4127 - val_acc: 0.8227\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3935 - acc: 0.8303 - val_loss: 0.4107 - val_acc: 0.8227\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3932 - acc: 0.8314 - val_loss: 0.4144 - val_acc: 0.8227\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3903 - acc: 0.8345 - val_loss: 0.4088 - val_acc: 0.8227\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3874 - acc: 0.8352 - val_loss: 0.4117 - val_acc: 0.8227\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3853 - acc: 0.8362 - val_loss: 0.4118 - val_acc: 0.8227\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3837 - acc: 0.8374 - val_loss: 0.4102 - val_acc: 0.8227\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3828 - acc: 0.8385 - val_loss: 0.4105 - val_acc: 0.8210\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3785 - acc: 0.8405 - val_loss: 0.4094 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': 1500, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "4/4 [==============================] - 1s 52ms/step - loss: 0.8150 - acc: 0.4371 - val_loss: 0.7677 - val_acc: 0.3892\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.7341 - acc: 0.4536 - val_loss: 0.6980 - val_acc: 0.4811\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.6883 - acc: 0.5488 - val_loss: 0.6601 - val_acc: 0.6388\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.6668 - acc: 0.6086 - val_loss: 0.6399 - val_acc: 0.6223\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6539 - acc: 0.6006 - val_loss: 0.6263 - val_acc: 0.6223\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.6413 - acc: 0.6156 - val_loss: 0.6146 - val_acc: 0.6388\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.6305 - acc: 0.6482 - val_loss: 0.6051 - val_acc: 0.7291\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.6135 - acc: 0.6962 - val_loss: 0.5983 - val_acc: 0.7159\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.6013 - acc: 0.7032 - val_loss: 0.5918 - val_acc: 0.6995\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.5911 - acc: 0.7043 - val_loss: 0.5836 - val_acc: 0.7028\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.5795 - acc: 0.7154 - val_loss: 0.5737 - val_acc: 0.7061\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5707 - acc: 0.7210 - val_loss: 0.5627 - val_acc: 0.7159\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5631 - acc: 0.7289 - val_loss: 0.5522 - val_acc: 0.7241\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.5543 - acc: 0.7369 - val_loss: 0.5441 - val_acc: 0.7258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.5440 - acc: 0.7451 - val_loss: 0.5361 - val_acc: 0.7258\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.5358 - acc: 0.7458 - val_loss: 0.5259 - val_acc: 0.7373\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.5257 - acc: 0.7575 - val_loss: 0.5155 - val_acc: 0.7438\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.5184 - acc: 0.7614 - val_loss: 0.5062 - val_acc: 0.7422\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5081 - acc: 0.7685 - val_loss: 0.4984 - val_acc: 0.7521\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4998 - acc: 0.7772 - val_loss: 0.4889 - val_acc: 0.7619\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4930 - acc: 0.7787 - val_loss: 0.4822 - val_acc: 0.7685\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4891 - acc: 0.7761 - val_loss: 0.4743 - val_acc: 0.7734\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4798 - acc: 0.7869 - val_loss: 0.4709 - val_acc: 0.7767\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4741 - acc: 0.7853 - val_loss: 0.4629 - val_acc: 0.7816\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4695 - acc: 0.7905 - val_loss: 0.4591 - val_acc: 0.7882\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4661 - acc: 0.7933 - val_loss: 0.4539 - val_acc: 0.7947\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.4621 - acc: 0.7955 - val_loss: 0.4510 - val_acc: 0.7980\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4571 - acc: 0.7982 - val_loss: 0.4467 - val_acc: 0.8030\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4558 - acc: 0.7982 - val_loss: 0.4427 - val_acc: 0.8062\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4498 - acc: 0.8037 - val_loss: 0.4396 - val_acc: 0.8079\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4476 - acc: 0.8039 - val_loss: 0.4404 - val_acc: 0.8013\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4441 - acc: 0.8031 - val_loss: 0.4334 - val_acc: 0.8128\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4432 - acc: 0.8068 - val_loss: 0.4308 - val_acc: 0.8112\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4376 - acc: 0.8090 - val_loss: 0.4310 - val_acc: 0.8079\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4362 - acc: 0.8068 - val_loss: 0.4288 - val_acc: 0.8095\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4329 - acc: 0.8130 - val_loss: 0.4238 - val_acc: 0.8128\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4319 - acc: 0.8124 - val_loss: 0.4288 - val_acc: 0.8062\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4303 - acc: 0.8099 - val_loss: 0.4239 - val_acc: 0.8112\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4240 - acc: 0.8110 - val_loss: 0.4217 - val_acc: 0.8112\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4259 - acc: 0.8135 - val_loss: 0.4227 - val_acc: 0.8079\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.4266 - acc: 0.8141 - val_loss: 0.4199 - val_acc: 0.8112\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4206 - acc: 0.8176 - val_loss: 0.4189 - val_acc: 0.8161\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.4244 - acc: 0.8119 - val_loss: 0.4205 - val_acc: 0.8112\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4193 - acc: 0.8146 - val_loss: 0.4179 - val_acc: 0.8128\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4153 - acc: 0.8185 - val_loss: 0.4142 - val_acc: 0.8177\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4159 - acc: 0.8155 - val_loss: 0.4176 - val_acc: 0.8161\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4133 - acc: 0.8196 - val_loss: 0.4145 - val_acc: 0.8112\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4091 - acc: 0.8217 - val_loss: 0.4138 - val_acc: 0.8144\n",
      "Epoch 49/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4101 - acc: 0.8207 - val_loss: 0.4140 - val_acc: 0.8161\n",
      "Epoch 50/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4117 - acc: 0.8230 - val_loss: 0.4115 - val_acc: 0.8177\n",
      "Epoch 51/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4077 - acc: 0.8239 - val_loss: 0.4164 - val_acc: 0.8144\n",
      "Epoch 52/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4065 - acc: 0.8219 - val_loss: 0.4107 - val_acc: 0.8194\n",
      "Epoch 53/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4033 - acc: 0.8232 - val_loss: 0.4121 - val_acc: 0.8161\n",
      "Epoch 54/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4018 - acc: 0.8230 - val_loss: 0.4095 - val_acc: 0.8210\n",
      "Epoch 55/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.4012 - acc: 0.8259 - val_loss: 0.4126 - val_acc: 0.8161\n",
      "Epoch 56/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.3998 - acc: 0.8245 - val_loss: 0.4104 - val_acc: 0.8194\n",
      "Epoch 57/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.3997 - acc: 0.8276 - val_loss: 0.4083 - val_acc: 0.8227\n",
      "Epoch 58/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.3984 - acc: 0.8272 - val_loss: 0.4165 - val_acc: 0.8177\n",
      "Epoch 59/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.3954 - acc: 0.8280 - val_loss: 0.4079 - val_acc: 0.8276\n",
      "Epoch 60/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.3920 - acc: 0.8323 - val_loss: 0.4098 - val_acc: 0.8194\n",
      "Epoch 61/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.3929 - acc: 0.8305 - val_loss: 0.4118 - val_acc: 0.8161\n",
      "Epoch 62/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.3919 - acc: 0.8323 - val_loss: 0.4107 - val_acc: 0.8177\n",
      "Epoch 63/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.3901 - acc: 0.8298 - val_loss: 0.4104 - val_acc: 0.8227\n",
      "Epoch 64/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.3901 - acc: 0.8345 - val_loss: 0.4063 - val_acc: 0.8276\n",
      "Epoch 65/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.3883 - acc: 0.8338 - val_loss: 0.4132 - val_acc: 0.8210\n",
      "Epoch 66/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.3877 - acc: 0.8323 - val_loss: 0.4090 - val_acc: 0.8210\n",
      "Epoch 67/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.3842 - acc: 0.8352 - val_loss: 0.4098 - val_acc: 0.8210\n",
      "Epoch 68/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.3849 - acc: 0.8311 - val_loss: 0.4110 - val_acc: 0.8243\n",
      "Epoch 69/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.3804 - acc: 0.8354 - val_loss: 0.4074 - val_acc: 0.8259\n",
      "Training with parameters {'batch_size': 1500, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 42ms/step - loss: 0.6652 - acc: 0.6141 - val_loss: 0.6508 - val_acc: 0.6289\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.6469 - acc: 0.6409 - val_loss: 0.6319 - val_acc: 0.6601\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.6328 - acc: 0.6532 - val_loss: 0.6203 - val_acc: 0.6798\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.6223 - acc: 0.6714 - val_loss: 0.6116 - val_acc: 0.6700\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6095 - acc: 0.6960 - val_loss: 0.6043 - val_acc: 0.6814\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5985 - acc: 0.7099 - val_loss: 0.5983 - val_acc: 0.6782\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5907 - acc: 0.7099 - val_loss: 0.5911 - val_acc: 0.6847\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5818 - acc: 0.7183 - val_loss: 0.5834 - val_acc: 0.6929\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5733 - acc: 0.7247 - val_loss: 0.5750 - val_acc: 0.7028\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5664 - acc: 0.7322 - val_loss: 0.5679 - val_acc: 0.7126\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5598 - acc: 0.7318 - val_loss: 0.5608 - val_acc: 0.7126\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5511 - acc: 0.7473 - val_loss: 0.5538 - val_acc: 0.7225\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5459 - acc: 0.7442 - val_loss: 0.5473 - val_acc: 0.7323\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5397 - acc: 0.7550 - val_loss: 0.5425 - val_acc: 0.7356\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5345 - acc: 0.7575 - val_loss: 0.5367 - val_acc: 0.7406\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5305 - acc: 0.7561 - val_loss: 0.5311 - val_acc: 0.7438\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5237 - acc: 0.7628 - val_loss: 0.5250 - val_acc: 0.7553\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5198 - acc: 0.7721 - val_loss: 0.5202 - val_acc: 0.7537\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5149 - acc: 0.7712 - val_loss: 0.5159 - val_acc: 0.7553\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5108 - acc: 0.7712 - val_loss: 0.5122 - val_acc: 0.7652\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5055 - acc: 0.7741 - val_loss: 0.5092 - val_acc: 0.7635\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5025 - acc: 0.7776 - val_loss: 0.5039 - val_acc: 0.7652\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4987 - acc: 0.7778 - val_loss: 0.4996 - val_acc: 0.7652\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4950 - acc: 0.7820 - val_loss: 0.4966 - val_acc: 0.7685\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4909 - acc: 0.7847 - val_loss: 0.4932 - val_acc: 0.7701\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4899 - acc: 0.7854 - val_loss: 0.4896 - val_acc: 0.7767\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4854 - acc: 0.7853 - val_loss: 0.4873 - val_acc: 0.7734\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4826 - acc: 0.7871 - val_loss: 0.4847 - val_acc: 0.7734\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4802 - acc: 0.7907 - val_loss: 0.4812 - val_acc: 0.7767\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4793 - acc: 0.7891 - val_loss: 0.4786 - val_acc: 0.7800\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4741 - acc: 0.7924 - val_loss: 0.4770 - val_acc: 0.7833\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4724 - acc: 0.7951 - val_loss: 0.4760 - val_acc: 0.7816\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4693 - acc: 0.7962 - val_loss: 0.4717 - val_acc: 0.7849\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4674 - acc: 0.7975 - val_loss: 0.4691 - val_acc: 0.7865\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4669 - acc: 0.7977 - val_loss: 0.4702 - val_acc: 0.7915\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4651 - acc: 0.7951 - val_loss: 0.4676 - val_acc: 0.7964\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4615 - acc: 0.7973 - val_loss: 0.4650 - val_acc: 0.7964\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4610 - acc: 0.8002 - val_loss: 0.4619 - val_acc: 0.7931\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4594 - acc: 0.8019 - val_loss: 0.4612 - val_acc: 0.7980\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4591 - acc: 0.8019 - val_loss: 0.4595 - val_acc: 0.7980\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4559 - acc: 0.8019 - val_loss: 0.4587 - val_acc: 0.7980\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4551 - acc: 0.8030 - val_loss: 0.4578 - val_acc: 0.7980\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4532 - acc: 0.8041 - val_loss: 0.4548 - val_acc: 0.7947\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4516 - acc: 0.8031 - val_loss: 0.4549 - val_acc: 0.7980\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4514 - acc: 0.8055 - val_loss: 0.4529 - val_acc: 0.7980\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4487 - acc: 0.8075 - val_loss: 0.4510 - val_acc: 0.7997\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4477 - acc: 0.8070 - val_loss: 0.4504 - val_acc: 0.8013\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4458 - acc: 0.8086 - val_loss: 0.4490 - val_acc: 0.8013\n",
      "Epoch 49/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4433 - acc: 0.8113 - val_loss: 0.4475 - val_acc: 0.8030\n",
      "Epoch 50/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4437 - acc: 0.8093 - val_loss: 0.4457 - val_acc: 0.8030\n",
      "Epoch 51/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4434 - acc: 0.8095 - val_loss: 0.4448 - val_acc: 0.8046\n",
      "Epoch 52/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4412 - acc: 0.8084 - val_loss: 0.4441 - val_acc: 0.8046\n",
      "Epoch 53/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4397 - acc: 0.8104 - val_loss: 0.4432 - val_acc: 0.8062\n",
      "Epoch 54/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4377 - acc: 0.8126 - val_loss: 0.4426 - val_acc: 0.8095\n",
      "Epoch 55/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4383 - acc: 0.8119 - val_loss: 0.4415 - val_acc: 0.8095\n",
      "Epoch 56/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4369 - acc: 0.8146 - val_loss: 0.4406 - val_acc: 0.8095\n",
      "Epoch 57/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4344 - acc: 0.8106 - val_loss: 0.4392 - val_acc: 0.8095\n",
      "Epoch 58/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4343 - acc: 0.8141 - val_loss: 0.4407 - val_acc: 0.8095\n",
      "Epoch 59/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4336 - acc: 0.8137 - val_loss: 0.4395 - val_acc: 0.8095\n",
      "Epoch 60/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4328 - acc: 0.8150 - val_loss: 0.4372 - val_acc: 0.8112\n",
      "Epoch 61/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4311 - acc: 0.8196 - val_loss: 0.4353 - val_acc: 0.8112\n",
      "Epoch 62/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.4323 - acc: 0.8155 - val_loss: 0.4379 - val_acc: 0.8112\n",
      "Epoch 63/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.4304 - acc: 0.8112 - val_loss: 0.4367 - val_acc: 0.8112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4299 - acc: 0.8144 - val_loss: 0.4345 - val_acc: 0.8128\n",
      "Epoch 65/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4284 - acc: 0.8179 - val_loss: 0.4342 - val_acc: 0.8112\n",
      "Epoch 66/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4269 - acc: 0.8159 - val_loss: 0.4342 - val_acc: 0.8112\n",
      "Epoch 67/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4261 - acc: 0.8144 - val_loss: 0.4326 - val_acc: 0.8144\n",
      "Epoch 68/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4261 - acc: 0.8170 - val_loss: 0.4315 - val_acc: 0.8161\n",
      "Epoch 69/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4236 - acc: 0.8179 - val_loss: 0.4337 - val_acc: 0.8112\n",
      "Epoch 70/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4238 - acc: 0.8165 - val_loss: 0.4334 - val_acc: 0.8112\n",
      "Epoch 71/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4221 - acc: 0.8205 - val_loss: 0.4296 - val_acc: 0.8177\n",
      "Epoch 72/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4218 - acc: 0.8212 - val_loss: 0.4288 - val_acc: 0.8177\n",
      "Epoch 73/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4219 - acc: 0.8188 - val_loss: 0.4307 - val_acc: 0.8112\n",
      "Epoch 74/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.4216 - acc: 0.8194 - val_loss: 0.4311 - val_acc: 0.8128\n",
      "Epoch 75/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4195 - acc: 0.8197 - val_loss: 0.4270 - val_acc: 0.8177\n",
      "Epoch 76/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4213 - acc: 0.8192 - val_loss: 0.4282 - val_acc: 0.8161\n",
      "Epoch 77/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4191 - acc: 0.8201 - val_loss: 0.4289 - val_acc: 0.8161\n",
      "Epoch 78/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4179 - acc: 0.8241 - val_loss: 0.4290 - val_acc: 0.8161\n",
      "Epoch 79/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4171 - acc: 0.8212 - val_loss: 0.4271 - val_acc: 0.8161\n",
      "Epoch 80/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4172 - acc: 0.8207 - val_loss: 0.4262 - val_acc: 0.8177\n",
      "Epoch 81/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4162 - acc: 0.8225 - val_loss: 0.4264 - val_acc: 0.8161\n",
      "Epoch 82/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4156 - acc: 0.8236 - val_loss: 0.4258 - val_acc: 0.8177\n",
      "Epoch 83/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4152 - acc: 0.8247 - val_loss: 0.4245 - val_acc: 0.8177\n",
      "Epoch 84/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4155 - acc: 0.8227 - val_loss: 0.4256 - val_acc: 0.8161\n",
      "Epoch 85/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4148 - acc: 0.8212 - val_loss: 0.4248 - val_acc: 0.8177\n",
      "Epoch 86/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4130 - acc: 0.8212 - val_loss: 0.4244 - val_acc: 0.8177\n",
      "Epoch 87/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4130 - acc: 0.8221 - val_loss: 0.4246 - val_acc: 0.8161\n",
      "Epoch 88/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4117 - acc: 0.8232 - val_loss: 0.4244 - val_acc: 0.8177\n",
      "Epoch 89/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4112 - acc: 0.8216 - val_loss: 0.4220 - val_acc: 0.8177\n",
      "Epoch 90/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4103 - acc: 0.8208 - val_loss: 0.4227 - val_acc: 0.8177\n",
      "Epoch 91/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4089 - acc: 0.8250 - val_loss: 0.4228 - val_acc: 0.8177\n",
      "Epoch 92/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4109 - acc: 0.8221 - val_loss: 0.4220 - val_acc: 0.8177\n",
      "Epoch 93/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4084 - acc: 0.8258 - val_loss: 0.4202 - val_acc: 0.8177\n",
      "Epoch 94/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4083 - acc: 0.8247 - val_loss: 0.4205 - val_acc: 0.8177\n",
      "Epoch 95/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4087 - acc: 0.8227 - val_loss: 0.4222 - val_acc: 0.8194\n",
      "Epoch 96/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4072 - acc: 0.8256 - val_loss: 0.4219 - val_acc: 0.8194\n",
      "Epoch 97/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4057 - acc: 0.8250 - val_loss: 0.4200 - val_acc: 0.8177\n",
      "Epoch 98/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.4061 - acc: 0.8259 - val_loss: 0.4201 - val_acc: 0.8177\n",
      "Epoch 99/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4055 - acc: 0.8261 - val_loss: 0.4211 - val_acc: 0.8194\n",
      "Epoch 100/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4035 - acc: 0.8267 - val_loss: 0.4211 - val_acc: 0.8177\n",
      "Epoch 101/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4044 - acc: 0.8232 - val_loss: 0.4193 - val_acc: 0.8177\n",
      "Epoch 102/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4050 - acc: 0.8267 - val_loss: 0.4205 - val_acc: 0.8177\n",
      "Epoch 103/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4044 - acc: 0.8276 - val_loss: 0.4199 - val_acc: 0.8194\n",
      "Epoch 104/300\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4033 - acc: 0.8267 - val_loss: 0.4190 - val_acc: 0.8210\n",
      "Epoch 105/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4034 - acc: 0.8241 - val_loss: 0.4188 - val_acc: 0.8210\n",
      "Epoch 106/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4013 - acc: 0.8287 - val_loss: 0.4192 - val_acc: 0.8227\n",
      "Epoch 107/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4031 - acc: 0.8258 - val_loss: 0.4193 - val_acc: 0.8210\n",
      "Epoch 108/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4012 - acc: 0.8267 - val_loss: 0.4190 - val_acc: 0.8210\n",
      "Epoch 109/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4003 - acc: 0.8292 - val_loss: 0.4172 - val_acc: 0.8194\n",
      "Epoch 110/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3997 - acc: 0.8269 - val_loss: 0.4182 - val_acc: 0.8194\n",
      "Epoch 111/300\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3974 - acc: 0.8312 - val_loss: 0.4180 - val_acc: 0.8194\n",
      "Epoch 112/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3981 - acc: 0.8303 - val_loss: 0.4163 - val_acc: 0.8177\n",
      "Epoch 113/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4009 - acc: 0.8280 - val_loss: 0.4179 - val_acc: 0.8194\n",
      "Epoch 114/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3992 - acc: 0.8272 - val_loss: 0.4190 - val_acc: 0.8177\n",
      "Epoch 115/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3971 - acc: 0.8294 - val_loss: 0.4168 - val_acc: 0.8210\n",
      "Epoch 116/300\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3974 - acc: 0.8316 - val_loss: 0.4159 - val_acc: 0.8210\n",
      "Epoch 117/300\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.3967 - acc: 0.8289 - val_loss: 0.4183 - val_acc: 0.8177\n",
      "Epoch 118/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3970 - acc: 0.8301 - val_loss: 0.4183 - val_acc: 0.8177\n",
      "Epoch 119/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3961 - acc: 0.8323 - val_loss: 0.4154 - val_acc: 0.8210\n",
      "Epoch 120/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3948 - acc: 0.8316 - val_loss: 0.4155 - val_acc: 0.8210\n",
      "Epoch 121/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3956 - acc: 0.8298 - val_loss: 0.4177 - val_acc: 0.8177\n",
      "Epoch 122/300\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3939 - acc: 0.8292 - val_loss: 0.4178 - val_acc: 0.8177\n",
      "Epoch 123/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3946 - acc: 0.8298 - val_loss: 0.4166 - val_acc: 0.8177\n",
      "Epoch 124/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3942 - acc: 0.8325 - val_loss: 0.4154 - val_acc: 0.8210\n",
      "Epoch 125/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3934 - acc: 0.8320 - val_loss: 0.4147 - val_acc: 0.8210\n",
      "Epoch 126/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3931 - acc: 0.8331 - val_loss: 0.4177 - val_acc: 0.8177\n",
      "Epoch 127/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3935 - acc: 0.8289 - val_loss: 0.4162 - val_acc: 0.8177\n",
      "Epoch 128/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3911 - acc: 0.8323 - val_loss: 0.4150 - val_acc: 0.8194\n",
      "Epoch 129/300\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3919 - acc: 0.8321 - val_loss: 0.4146 - val_acc: 0.8194\n",
      "Epoch 130/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3920 - acc: 0.8292 - val_loss: 0.4152 - val_acc: 0.8194\n",
      "Epoch 131/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3917 - acc: 0.8332 - val_loss: 0.4145 - val_acc: 0.8194\n",
      "Epoch 132/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3902 - acc: 0.8338 - val_loss: 0.4147 - val_acc: 0.8177\n",
      "Epoch 133/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3903 - acc: 0.8312 - val_loss: 0.4175 - val_acc: 0.8161\n",
      "Epoch 134/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3893 - acc: 0.8340 - val_loss: 0.4168 - val_acc: 0.8194\n",
      "Epoch 135/300\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3878 - acc: 0.8331 - val_loss: 0.4143 - val_acc: 0.8161\n",
      "Epoch 136/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3886 - acc: 0.8325 - val_loss: 0.4149 - val_acc: 0.8177\n",
      "Epoch 137/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3883 - acc: 0.8332 - val_loss: 0.4147 - val_acc: 0.8177\n",
      "Epoch 138/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3864 - acc: 0.8318 - val_loss: 0.4159 - val_acc: 0.8177\n",
      "Epoch 139/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3880 - acc: 0.8343 - val_loss: 0.4149 - val_acc: 0.8177\n",
      "Epoch 140/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3863 - acc: 0.8363 - val_loss: 0.4135 - val_acc: 0.8177\n",
      "Epoch 141/300\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3867 - acc: 0.8352 - val_loss: 0.4141 - val_acc: 0.8177\n",
      "Epoch 142/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3872 - acc: 0.8342 - val_loss: 0.4147 - val_acc: 0.8177\n",
      "Epoch 143/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3861 - acc: 0.8358 - val_loss: 0.4136 - val_acc: 0.8161\n",
      "Epoch 144/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3852 - acc: 0.8354 - val_loss: 0.4131 - val_acc: 0.8177\n",
      "Epoch 145/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3860 - acc: 0.8351 - val_loss: 0.4142 - val_acc: 0.8177\n",
      "Epoch 146/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3865 - acc: 0.8334 - val_loss: 0.4154 - val_acc: 0.8161\n",
      "Epoch 147/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3840 - acc: 0.8351 - val_loss: 0.4123 - val_acc: 0.8194\n",
      "Epoch 148/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3837 - acc: 0.8365 - val_loss: 0.4136 - val_acc: 0.8177\n",
      "Epoch 149/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3838 - acc: 0.8371 - val_loss: 0.4139 - val_acc: 0.8177\n",
      "Epoch 150/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3828 - acc: 0.8321 - val_loss: 0.4138 - val_acc: 0.8177\n",
      "Epoch 151/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3839 - acc: 0.8351 - val_loss: 0.4142 - val_acc: 0.8177\n",
      "Epoch 152/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3836 - acc: 0.8345 - val_loss: 0.4140 - val_acc: 0.8177\n",
      "Training with parameters {'batch_size': 1500, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "4/4 [==============================] - 1s 59ms/step - loss: 0.8078 - acc: 0.4362 - val_loss: 0.7342 - val_acc: 0.3892\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.7084 - acc: 0.4815 - val_loss: 0.6663 - val_acc: 0.6108\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.6747 - acc: 0.5639 - val_loss: 0.6417 - val_acc: 0.6158\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.6574 - acc: 0.5731 - val_loss: 0.6241 - val_acc: 0.6207\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.6354 - acc: 0.6085 - val_loss: 0.6101 - val_acc: 0.6979\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.6140 - acc: 0.6867 - val_loss: 0.6045 - val_acc: 0.6749\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.5982 - acc: 0.7039 - val_loss: 0.5994 - val_acc: 0.6667\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.5880 - acc: 0.6986 - val_loss: 0.5864 - val_acc: 0.6700\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5752 - acc: 0.7110 - val_loss: 0.5700 - val_acc: 0.6897\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5617 - acc: 0.7313 - val_loss: 0.5555 - val_acc: 0.7126\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5486 - acc: 0.7451 - val_loss: 0.5439 - val_acc: 0.7159\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5377 - acc: 0.7462 - val_loss: 0.5353 - val_acc: 0.7159\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5256 - acc: 0.7517 - val_loss: 0.5226 - val_acc: 0.7323\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5143 - acc: 0.7676 - val_loss: 0.5079 - val_acc: 0.7603\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5079 - acc: 0.7668 - val_loss: 0.4978 - val_acc: 0.7652\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4980 - acc: 0.7763 - val_loss: 0.4917 - val_acc: 0.7668\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4928 - acc: 0.7831 - val_loss: 0.4833 - val_acc: 0.7783\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4833 - acc: 0.7829 - val_loss: 0.4728 - val_acc: 0.7767\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4736 - acc: 0.7887 - val_loss: 0.4714 - val_acc: 0.7800\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4708 - acc: 0.7911 - val_loss: 0.4637 - val_acc: 0.7849\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4665 - acc: 0.7942 - val_loss: 0.4579 - val_acc: 0.7947\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4564 - acc: 0.7975 - val_loss: 0.4523 - val_acc: 0.8013\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4544 - acc: 0.8019 - val_loss: 0.4485 - val_acc: 0.8013\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4461 - acc: 0.8026 - val_loss: 0.4467 - val_acc: 0.8013\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4451 - acc: 0.8103 - val_loss: 0.4395 - val_acc: 0.8062\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4394 - acc: 0.8104 - val_loss: 0.4389 - val_acc: 0.7997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4388 - acc: 0.8117 - val_loss: 0.4346 - val_acc: 0.8013\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4335 - acc: 0.8146 - val_loss: 0.4336 - val_acc: 0.8030\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4331 - acc: 0.8119 - val_loss: 0.4305 - val_acc: 0.8046\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4281 - acc: 0.8128 - val_loss: 0.4318 - val_acc: 0.8046\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4289 - acc: 0.8168 - val_loss: 0.4252 - val_acc: 0.8079\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4272 - acc: 0.8144 - val_loss: 0.4313 - val_acc: 0.8095\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4246 - acc: 0.8163 - val_loss: 0.4203 - val_acc: 0.8112\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4217 - acc: 0.8172 - val_loss: 0.4192 - val_acc: 0.8161\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4211 - acc: 0.8186 - val_loss: 0.4271 - val_acc: 0.8112\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4175 - acc: 0.8212 - val_loss: 0.4170 - val_acc: 0.8161\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4142 - acc: 0.8234 - val_loss: 0.4210 - val_acc: 0.8112\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4129 - acc: 0.8217 - val_loss: 0.4205 - val_acc: 0.8128\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4125 - acc: 0.8239 - val_loss: 0.4145 - val_acc: 0.8144\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4089 - acc: 0.8238 - val_loss: 0.4177 - val_acc: 0.8144\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4075 - acc: 0.8254 - val_loss: 0.4168 - val_acc: 0.8177\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4068 - acc: 0.8263 - val_loss: 0.4131 - val_acc: 0.8112\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4058 - acc: 0.8228 - val_loss: 0.4164 - val_acc: 0.8144\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4022 - acc: 0.8294 - val_loss: 0.4123 - val_acc: 0.8112\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3987 - acc: 0.8270 - val_loss: 0.4179 - val_acc: 0.8161\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.3985 - acc: 0.8305 - val_loss: 0.4111 - val_acc: 0.8144\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3973 - acc: 0.8312 - val_loss: 0.4118 - val_acc: 0.8194\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3956 - acc: 0.8290 - val_loss: 0.4145 - val_acc: 0.8194\n",
      "Epoch 49/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3929 - acc: 0.8311 - val_loss: 0.4094 - val_acc: 0.8177\n",
      "Epoch 50/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3914 - acc: 0.8340 - val_loss: 0.4138 - val_acc: 0.8210\n",
      "Epoch 51/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3896 - acc: 0.8332 - val_loss: 0.4093 - val_acc: 0.8161\n",
      "Epoch 52/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3895 - acc: 0.8334 - val_loss: 0.4104 - val_acc: 0.8177\n",
      "Epoch 53/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3880 - acc: 0.8325 - val_loss: 0.4139 - val_acc: 0.8210\n",
      "Epoch 54/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.3860 - acc: 0.8321 - val_loss: 0.4076 - val_acc: 0.8144\n",
      "Epoch 55/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.3846 - acc: 0.8391 - val_loss: 0.4141 - val_acc: 0.8210\n",
      "Epoch 56/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3812 - acc: 0.8389 - val_loss: 0.4116 - val_acc: 0.8194\n",
      "Epoch 57/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3778 - acc: 0.8407 - val_loss: 0.4085 - val_acc: 0.8227\n",
      "Epoch 58/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3781 - acc: 0.8382 - val_loss: 0.4123 - val_acc: 0.8243\n",
      "Epoch 59/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3762 - acc: 0.8398 - val_loss: 0.4098 - val_acc: 0.8227\n",
      "Training with parameters {'batch_size': 1500, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "4/4 [==============================] - 1s 106ms/step - loss: 0.8469 - acc: 0.4353 - val_loss: 0.8028 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.7614 - acc: 0.4388 - val_loss: 0.7302 - val_acc: 0.3924\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.7094 - acc: 0.4777 - val_loss: 0.6872 - val_acc: 0.5435\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.6814 - acc: 0.5696 - val_loss: 0.6619 - val_acc: 0.6814\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.6651 - acc: 0.6180 - val_loss: 0.6449 - val_acc: 0.6814\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.6538 - acc: 0.6426 - val_loss: 0.6317 - val_acc: 0.7094\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.6402 - acc: 0.6645 - val_loss: 0.6209 - val_acc: 0.7422\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.6269 - acc: 0.7001 - val_loss: 0.6127 - val_acc: 0.7126\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.6163 - acc: 0.7066 - val_loss: 0.6065 - val_acc: 0.6962\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.6043 - acc: 0.7145 - val_loss: 0.6006 - val_acc: 0.6962\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.5959 - acc: 0.7099 - val_loss: 0.5940 - val_acc: 0.6929\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.5885 - acc: 0.7170 - val_loss: 0.5848 - val_acc: 0.6995\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.5799 - acc: 0.7263 - val_loss: 0.5757 - val_acc: 0.7061\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.5711 - acc: 0.7351 - val_loss: 0.5669 - val_acc: 0.7143\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.5655 - acc: 0.7373 - val_loss: 0.5592 - val_acc: 0.7159\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.5592 - acc: 0.7449 - val_loss: 0.5530 - val_acc: 0.7061\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.5486 - acc: 0.7451 - val_loss: 0.5470 - val_acc: 0.7110\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5427 - acc: 0.7497 - val_loss: 0.5392 - val_acc: 0.7176\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.5344 - acc: 0.7552 - val_loss: 0.5311 - val_acc: 0.7291\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.5299 - acc: 0.7568 - val_loss: 0.5228 - val_acc: 0.7389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.5228 - acc: 0.7615 - val_loss: 0.5162 - val_acc: 0.7406\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.5156 - acc: 0.7661 - val_loss: 0.5108 - val_acc: 0.7488\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.5095 - acc: 0.7701 - val_loss: 0.5050 - val_acc: 0.7504\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.5050 - acc: 0.7736 - val_loss: 0.4978 - val_acc: 0.7553\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.5022 - acc: 0.7750 - val_loss: 0.4926 - val_acc: 0.7553\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4975 - acc: 0.7738 - val_loss: 0.4886 - val_acc: 0.7570\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4924 - acc: 0.7807 - val_loss: 0.4832 - val_acc: 0.7685\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.4885 - acc: 0.7794 - val_loss: 0.4782 - val_acc: 0.7734\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4851 - acc: 0.7798 - val_loss: 0.4740 - val_acc: 0.7734\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4768 - acc: 0.7904 - val_loss: 0.4727 - val_acc: 0.7701\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4751 - acc: 0.7909 - val_loss: 0.4686 - val_acc: 0.7750\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.4725 - acc: 0.7884 - val_loss: 0.4653 - val_acc: 0.7800\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4692 - acc: 0.7947 - val_loss: 0.4628 - val_acc: 0.7816\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4681 - acc: 0.7944 - val_loss: 0.4589 - val_acc: 0.7865\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4649 - acc: 0.7995 - val_loss: 0.4560 - val_acc: 0.7882\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4586 - acc: 0.7984 - val_loss: 0.4539 - val_acc: 0.7898\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.4601 - acc: 0.8009 - val_loss: 0.4527 - val_acc: 0.7915\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.4508 - acc: 0.7980 - val_loss: 0.4497 - val_acc: 0.7915\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4500 - acc: 0.8035 - val_loss: 0.4471 - val_acc: 0.7947\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.4485 - acc: 0.8031 - val_loss: 0.4457 - val_acc: 0.7980\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.4498 - acc: 0.8048 - val_loss: 0.4438 - val_acc: 0.7997\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.4432 - acc: 0.8068 - val_loss: 0.4403 - val_acc: 0.7997\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.4432 - acc: 0.8042 - val_loss: 0.4385 - val_acc: 0.8013\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4437 - acc: 0.8039 - val_loss: 0.4412 - val_acc: 0.7997\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4405 - acc: 0.8081 - val_loss: 0.4357 - val_acc: 0.8046\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4390 - acc: 0.8077 - val_loss: 0.4322 - val_acc: 0.8095\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4348 - acc: 0.8146 - val_loss: 0.4376 - val_acc: 0.8030\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4368 - acc: 0.8110 - val_loss: 0.4359 - val_acc: 0.8013\n",
      "Epoch 49/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.4297 - acc: 0.8124 - val_loss: 0.4289 - val_acc: 0.8095\n",
      "Epoch 50/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4328 - acc: 0.8110 - val_loss: 0.4285 - val_acc: 0.8112\n",
      "Epoch 51/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.4316 - acc: 0.8101 - val_loss: 0.4370 - val_acc: 0.8046\n",
      "Epoch 52/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.4296 - acc: 0.8106 - val_loss: 0.4284 - val_acc: 0.8095\n",
      "Epoch 53/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4269 - acc: 0.8137 - val_loss: 0.4255 - val_acc: 0.8112\n",
      "Epoch 54/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.4259 - acc: 0.8144 - val_loss: 0.4291 - val_acc: 0.8046\n",
      "Epoch 55/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.4210 - acc: 0.8168 - val_loss: 0.4273 - val_acc: 0.8079\n",
      "Epoch 56/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.4219 - acc: 0.8181 - val_loss: 0.4233 - val_acc: 0.8128\n",
      "Epoch 57/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4182 - acc: 0.8177 - val_loss: 0.4239 - val_acc: 0.8112\n",
      "Epoch 58/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.4219 - acc: 0.8183 - val_loss: 0.4288 - val_acc: 0.8079\n",
      "Epoch 59/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4203 - acc: 0.8141 - val_loss: 0.4241 - val_acc: 0.8144\n",
      "Epoch 60/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4145 - acc: 0.8208 - val_loss: 0.4199 - val_acc: 0.8144\n",
      "Epoch 61/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.4188 - acc: 0.8161 - val_loss: 0.4218 - val_acc: 0.8177\n",
      "Epoch 62/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4140 - acc: 0.8197 - val_loss: 0.4232 - val_acc: 0.8079\n",
      "Epoch 63/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.4159 - acc: 0.8183 - val_loss: 0.4191 - val_acc: 0.8177\n",
      "Epoch 64/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.4135 - acc: 0.8210 - val_loss: 0.4187 - val_acc: 0.8177\n",
      "Epoch 65/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.4134 - acc: 0.8207 - val_loss: 0.4221 - val_acc: 0.8128\n",
      "Epoch 66/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4093 - acc: 0.8254 - val_loss: 0.4175 - val_acc: 0.8194\n",
      "Epoch 67/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4104 - acc: 0.8217 - val_loss: 0.4172 - val_acc: 0.8210\n",
      "Epoch 68/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.4093 - acc: 0.8219 - val_loss: 0.4185 - val_acc: 0.8177\n",
      "Epoch 69/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4095 - acc: 0.8212 - val_loss: 0.4189 - val_acc: 0.8177\n",
      "Epoch 70/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4050 - acc: 0.8243 - val_loss: 0.4148 - val_acc: 0.8177\n",
      "Epoch 71/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4061 - acc: 0.8272 - val_loss: 0.4154 - val_acc: 0.8210\n",
      "Epoch 72/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4048 - acc: 0.8241 - val_loss: 0.4201 - val_acc: 0.8144\n",
      "Epoch 73/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4035 - acc: 0.8267 - val_loss: 0.4155 - val_acc: 0.8210\n",
      "Epoch 74/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.4008 - acc: 0.8254 - val_loss: 0.4135 - val_acc: 0.8177\n",
      "Epoch 75/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4015 - acc: 0.8292 - val_loss: 0.4161 - val_acc: 0.8194\n",
      "Epoch 76/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.3997 - acc: 0.8305 - val_loss: 0.4181 - val_acc: 0.8177\n",
      "Epoch 77/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.3954 - acc: 0.8307 - val_loss: 0.4136 - val_acc: 0.8210\n",
      "Epoch 78/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.3978 - acc: 0.8296 - val_loss: 0.4146 - val_acc: 0.8194\n",
      "Epoch 79/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.3954 - acc: 0.8303 - val_loss: 0.4160 - val_acc: 0.8161\n",
      "Training with parameters {'batch_size': 1500, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 46ms/step - loss: 0.7511 - acc: 0.4361 - val_loss: 0.7378 - val_acc: 0.3990\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.7109 - acc: 0.4658 - val_loss: 0.6959 - val_acc: 0.4795\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.6817 - acc: 0.5630 - val_loss: 0.6691 - val_acc: 0.6601\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.6661 - acc: 0.6340 - val_loss: 0.6520 - val_acc: 0.6749\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.6563 - acc: 0.6353 - val_loss: 0.6402 - val_acc: 0.6552\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6456 - acc: 0.6351 - val_loss: 0.6307 - val_acc: 0.6617\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6359 - acc: 0.6517 - val_loss: 0.6227 - val_acc: 0.6946\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.6285 - acc: 0.6751 - val_loss: 0.6155 - val_acc: 0.7077\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.6173 - acc: 0.6922 - val_loss: 0.6092 - val_acc: 0.7110\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.6092 - acc: 0.7057 - val_loss: 0.6036 - val_acc: 0.6962\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.5991 - acc: 0.7130 - val_loss: 0.5975 - val_acc: 0.6946\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.5931 - acc: 0.7159 - val_loss: 0.5906 - val_acc: 0.6979\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.5841 - acc: 0.7161 - val_loss: 0.5836 - val_acc: 0.7028\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.5742 - acc: 0.7318 - val_loss: 0.5755 - val_acc: 0.7126\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5689 - acc: 0.7327 - val_loss: 0.5677 - val_acc: 0.7176\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5617 - acc: 0.7395 - val_loss: 0.5610 - val_acc: 0.7209\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.5513 - acc: 0.7449 - val_loss: 0.5537 - val_acc: 0.7192\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.5436 - acc: 0.7486 - val_loss: 0.5456 - val_acc: 0.7274\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.5368 - acc: 0.7508 - val_loss: 0.5387 - val_acc: 0.7307\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.5305 - acc: 0.7561 - val_loss: 0.5303 - val_acc: 0.7422\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.5230 - acc: 0.7623 - val_loss: 0.5216 - val_acc: 0.7471\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.5169 - acc: 0.7657 - val_loss: 0.5163 - val_acc: 0.7504\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.5112 - acc: 0.7679 - val_loss: 0.5108 - val_acc: 0.7537\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.5071 - acc: 0.7716 - val_loss: 0.5040 - val_acc: 0.7570\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4977 - acc: 0.7785 - val_loss: 0.4971 - val_acc: 0.7652\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4941 - acc: 0.7787 - val_loss: 0.4937 - val_acc: 0.7685\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4875 - acc: 0.7825 - val_loss: 0.4883 - val_acc: 0.7734\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4855 - acc: 0.7814 - val_loss: 0.4837 - val_acc: 0.7767\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4806 - acc: 0.7864 - val_loss: 0.4809 - val_acc: 0.7783\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4747 - acc: 0.7891 - val_loss: 0.4769 - val_acc: 0.7833\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.4741 - acc: 0.7931 - val_loss: 0.4720 - val_acc: 0.7915\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4718 - acc: 0.7864 - val_loss: 0.4683 - val_acc: 0.7931\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.4663 - acc: 0.7927 - val_loss: 0.4663 - val_acc: 0.7931\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4628 - acc: 0.7964 - val_loss: 0.4639 - val_acc: 0.7947\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4624 - acc: 0.7916 - val_loss: 0.4610 - val_acc: 0.7964\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4572 - acc: 0.7991 - val_loss: 0.4577 - val_acc: 0.7947\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4538 - acc: 0.7966 - val_loss: 0.4550 - val_acc: 0.7931\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4525 - acc: 0.7993 - val_loss: 0.4533 - val_acc: 0.7947\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4530 - acc: 0.8020 - val_loss: 0.4493 - val_acc: 0.7997\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4503 - acc: 0.8057 - val_loss: 0.4502 - val_acc: 0.8013\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4456 - acc: 0.8050 - val_loss: 0.4457 - val_acc: 0.8030\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4435 - acc: 0.8066 - val_loss: 0.4442 - val_acc: 0.8030\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4411 - acc: 0.8082 - val_loss: 0.4478 - val_acc: 0.8030\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4404 - acc: 0.8048 - val_loss: 0.4432 - val_acc: 0.8062\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4394 - acc: 0.8070 - val_loss: 0.4401 - val_acc: 0.8079\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4344 - acc: 0.8097 - val_loss: 0.4396 - val_acc: 0.8062\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4339 - acc: 0.8128 - val_loss: 0.4414 - val_acc: 0.8030\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4323 - acc: 0.8101 - val_loss: 0.4372 - val_acc: 0.8079\n",
      "Epoch 49/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4305 - acc: 0.8130 - val_loss: 0.4361 - val_acc: 0.8079\n",
      "Epoch 50/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4275 - acc: 0.8132 - val_loss: 0.4364 - val_acc: 0.8013\n",
      "Epoch 51/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4301 - acc: 0.8152 - val_loss: 0.4340 - val_acc: 0.8095\n",
      "Epoch 52/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4267 - acc: 0.8130 - val_loss: 0.4327 - val_acc: 0.8095\n",
      "Epoch 53/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4251 - acc: 0.8143 - val_loss: 0.4323 - val_acc: 0.8095\n",
      "Epoch 54/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4245 - acc: 0.8132 - val_loss: 0.4304 - val_acc: 0.8079\n",
      "Epoch 55/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4215 - acc: 0.8174 - val_loss: 0.4296 - val_acc: 0.8095\n",
      "Epoch 56/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4219 - acc: 0.8174 - val_loss: 0.4307 - val_acc: 0.8079\n",
      "Epoch 57/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4202 - acc: 0.8208 - val_loss: 0.4274 - val_acc: 0.8144\n",
      "Epoch 58/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4186 - acc: 0.8165 - val_loss: 0.4288 - val_acc: 0.8112\n",
      "Epoch 59/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4191 - acc: 0.8205 - val_loss: 0.4276 - val_acc: 0.8144\n",
      "Epoch 60/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4176 - acc: 0.8212 - val_loss: 0.4276 - val_acc: 0.8144\n",
      "Epoch 61/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4125 - acc: 0.8225 - val_loss: 0.4269 - val_acc: 0.8144\n",
      "Epoch 62/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4131 - acc: 0.8221 - val_loss: 0.4249 - val_acc: 0.8161\n",
      "Epoch 63/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4128 - acc: 0.8214 - val_loss: 0.4248 - val_acc: 0.8161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4140 - acc: 0.8197 - val_loss: 0.4266 - val_acc: 0.8144\n",
      "Epoch 65/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4130 - acc: 0.8201 - val_loss: 0.4232 - val_acc: 0.8177\n",
      "Epoch 66/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4090 - acc: 0.8217 - val_loss: 0.4256 - val_acc: 0.8161\n",
      "Epoch 67/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4094 - acc: 0.8254 - val_loss: 0.4232 - val_acc: 0.8161\n",
      "Epoch 68/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4080 - acc: 0.8252 - val_loss: 0.4234 - val_acc: 0.8161\n",
      "Epoch 69/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4061 - acc: 0.8254 - val_loss: 0.4232 - val_acc: 0.8177\n",
      "Epoch 70/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4065 - acc: 0.8250 - val_loss: 0.4217 - val_acc: 0.8177\n",
      "Epoch 71/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4054 - acc: 0.8252 - val_loss: 0.4223 - val_acc: 0.8194\n",
      "Epoch 72/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4060 - acc: 0.8272 - val_loss: 0.4231 - val_acc: 0.8194\n",
      "Epoch 73/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4056 - acc: 0.8247 - val_loss: 0.4193 - val_acc: 0.8194\n",
      "Epoch 74/300\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4021 - acc: 0.8259 - val_loss: 0.4231 - val_acc: 0.8194\n",
      "Epoch 75/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4000 - acc: 0.8265 - val_loss: 0.4202 - val_acc: 0.8194\n",
      "Epoch 76/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3991 - acc: 0.8301 - val_loss: 0.4189 - val_acc: 0.8177\n",
      "Epoch 77/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3991 - acc: 0.8301 - val_loss: 0.4211 - val_acc: 0.8194\n",
      "Epoch 78/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3977 - acc: 0.8278 - val_loss: 0.4201 - val_acc: 0.8194\n",
      "Epoch 79/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3973 - acc: 0.8298 - val_loss: 0.4189 - val_acc: 0.8194\n",
      "Epoch 80/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3943 - acc: 0.8298 - val_loss: 0.4175 - val_acc: 0.8194\n",
      "Epoch 81/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3949 - acc: 0.8305 - val_loss: 0.4191 - val_acc: 0.8194\n",
      "Epoch 82/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3940 - acc: 0.8329 - val_loss: 0.4169 - val_acc: 0.8194\n",
      "Epoch 83/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.3938 - acc: 0.8325 - val_loss: 0.4184 - val_acc: 0.8194\n",
      "Epoch 84/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3921 - acc: 0.8340 - val_loss: 0.4177 - val_acc: 0.8194\n",
      "Epoch 85/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3932 - acc: 0.8307 - val_loss: 0.4172 - val_acc: 0.8194\n",
      "Epoch 86/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3910 - acc: 0.8312 - val_loss: 0.4174 - val_acc: 0.8210\n",
      "Epoch 87/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3896 - acc: 0.8352 - val_loss: 0.4166 - val_acc: 0.8210\n",
      "Epoch 88/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3902 - acc: 0.8327 - val_loss: 0.4172 - val_acc: 0.8210\n",
      "Epoch 89/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3866 - acc: 0.8323 - val_loss: 0.4178 - val_acc: 0.8227\n",
      "Epoch 90/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3887 - acc: 0.8325 - val_loss: 0.4157 - val_acc: 0.8177\n",
      "Epoch 91/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3882 - acc: 0.8354 - val_loss: 0.4183 - val_acc: 0.8243\n",
      "Epoch 92/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3839 - acc: 0.8336 - val_loss: 0.4144 - val_acc: 0.8194\n",
      "Epoch 93/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3835 - acc: 0.8369 - val_loss: 0.4185 - val_acc: 0.8227\n",
      "Epoch 94/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3829 - acc: 0.8327 - val_loss: 0.4159 - val_acc: 0.8210\n",
      "Epoch 95/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3793 - acc: 0.8373 - val_loss: 0.4155 - val_acc: 0.8194\n",
      "Epoch 96/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3778 - acc: 0.8407 - val_loss: 0.4172 - val_acc: 0.8227\n",
      "Epoch 97/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.3807 - acc: 0.8365 - val_loss: 0.4188 - val_acc: 0.8227\n",
      "Training with parameters {'batch_size': 1500, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "4/4 [==============================] - 1s 61ms/step - loss: 0.8122 - acc: 0.4351 - val_loss: 0.7679 - val_acc: 0.3892\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.7376 - acc: 0.4465 - val_loss: 0.6982 - val_acc: 0.4811\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.6924 - acc: 0.5324 - val_loss: 0.6603 - val_acc: 0.6388\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.6642 - acc: 0.6156 - val_loss: 0.6402 - val_acc: 0.6223\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.6566 - acc: 0.5962 - val_loss: 0.6264 - val_acc: 0.6223\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.6408 - acc: 0.6139 - val_loss: 0.6146 - val_acc: 0.6420\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.6264 - acc: 0.6555 - val_loss: 0.6048 - val_acc: 0.7274\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.6140 - acc: 0.6984 - val_loss: 0.5976 - val_acc: 0.7159\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6011 - acc: 0.7066 - val_loss: 0.5914 - val_acc: 0.6979\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.5899 - acc: 0.7032 - val_loss: 0.5836 - val_acc: 0.7011\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.5795 - acc: 0.7143 - val_loss: 0.5735 - val_acc: 0.7028\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.5695 - acc: 0.7214 - val_loss: 0.5624 - val_acc: 0.7159\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.5620 - acc: 0.7325 - val_loss: 0.5520 - val_acc: 0.7176\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.5527 - acc: 0.7376 - val_loss: 0.5428 - val_acc: 0.7258\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5471 - acc: 0.7448 - val_loss: 0.5358 - val_acc: 0.7274\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.5342 - acc: 0.7499 - val_loss: 0.5260 - val_acc: 0.7373\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.5276 - acc: 0.7511 - val_loss: 0.5156 - val_acc: 0.7406\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.5180 - acc: 0.7643 - val_loss: 0.5051 - val_acc: 0.7471\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.5102 - acc: 0.7648 - val_loss: 0.4973 - val_acc: 0.7553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5020 - acc: 0.7729 - val_loss: 0.4895 - val_acc: 0.7570\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.4930 - acc: 0.7776 - val_loss: 0.4828 - val_acc: 0.7635\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4861 - acc: 0.7801 - val_loss: 0.4736 - val_acc: 0.7767\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4794 - acc: 0.7862 - val_loss: 0.4699 - val_acc: 0.7783\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4779 - acc: 0.7827 - val_loss: 0.4654 - val_acc: 0.7833\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4715 - acc: 0.7885 - val_loss: 0.4599 - val_acc: 0.7865\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.4680 - acc: 0.7889 - val_loss: 0.4547 - val_acc: 0.7947\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4602 - acc: 0.7989 - val_loss: 0.4496 - val_acc: 0.7997\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.4586 - acc: 0.7949 - val_loss: 0.4484 - val_acc: 0.7964\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4517 - acc: 0.7968 - val_loss: 0.4429 - val_acc: 0.8030\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.4504 - acc: 0.8066 - val_loss: 0.4398 - val_acc: 0.8079\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4458 - acc: 0.8039 - val_loss: 0.4397 - val_acc: 0.8013\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4453 - acc: 0.8053 - val_loss: 0.4379 - val_acc: 0.8046\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4411 - acc: 0.8075 - val_loss: 0.4293 - val_acc: 0.8112\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4397 - acc: 0.8068 - val_loss: 0.4320 - val_acc: 0.8095\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.4386 - acc: 0.8013 - val_loss: 0.4304 - val_acc: 0.8062\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4327 - acc: 0.8090 - val_loss: 0.4253 - val_acc: 0.8161\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.4305 - acc: 0.8132 - val_loss: 0.4240 - val_acc: 0.8144\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.4290 - acc: 0.8121 - val_loss: 0.4250 - val_acc: 0.8062\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.4298 - acc: 0.8150 - val_loss: 0.4234 - val_acc: 0.8046\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.4242 - acc: 0.8146 - val_loss: 0.4201 - val_acc: 0.8144\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.4244 - acc: 0.8117 - val_loss: 0.4231 - val_acc: 0.8062\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.4212 - acc: 0.8159 - val_loss: 0.4185 - val_acc: 0.8095\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4196 - acc: 0.8183 - val_loss: 0.4184 - val_acc: 0.8112\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4187 - acc: 0.8146 - val_loss: 0.4180 - val_acc: 0.8095\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.4170 - acc: 0.8192 - val_loss: 0.4152 - val_acc: 0.8128\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4162 - acc: 0.8174 - val_loss: 0.4155 - val_acc: 0.8128\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.4113 - acc: 0.8219 - val_loss: 0.4149 - val_acc: 0.8144\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.4142 - acc: 0.8205 - val_loss: 0.4156 - val_acc: 0.8128\n",
      "Epoch 49/300\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4109 - acc: 0.8210 - val_loss: 0.4148 - val_acc: 0.8112\n",
      "Epoch 50/300\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4084 - acc: 0.8241 - val_loss: 0.4116 - val_acc: 0.8177\n",
      "Epoch 51/300\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4063 - acc: 0.8238 - val_loss: 0.4157 - val_acc: 0.8144\n",
      "Epoch 52/300\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.4042 - acc: 0.8236 - val_loss: 0.4102 - val_acc: 0.8210\n",
      "Epoch 53/300\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4031 - acc: 0.8270 - val_loss: 0.4101 - val_acc: 0.8194\n",
      "Epoch 54/300\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.4011 - acc: 0.8272 - val_loss: 0.4144 - val_acc: 0.8144\n",
      "Epoch 55/300\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.4037 - acc: 0.8236 - val_loss: 0.4110 - val_acc: 0.8194\n",
      "Epoch 56/300\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.4014 - acc: 0.8276 - val_loss: 0.4110 - val_acc: 0.8177\n",
      "Epoch 57/300\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3965 - acc: 0.8269 - val_loss: 0.4128 - val_acc: 0.8161\n",
      "Epoch 58/300\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3976 - acc: 0.8276 - val_loss: 0.4079 - val_acc: 0.8259\n",
      "Epoch 59/300\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3987 - acc: 0.8274 - val_loss: 0.4132 - val_acc: 0.8161\n",
      "Epoch 60/300\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3968 - acc: 0.8270 - val_loss: 0.4075 - val_acc: 0.8243\n",
      "Epoch 61/300\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3914 - acc: 0.8331 - val_loss: 0.4096 - val_acc: 0.8210\n",
      "Epoch 62/300\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3885 - acc: 0.8331 - val_loss: 0.4104 - val_acc: 0.8177\n",
      "Epoch 63/300\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3900 - acc: 0.8311 - val_loss: 0.4077 - val_acc: 0.8194\n",
      "Epoch 64/300\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3877 - acc: 0.8340 - val_loss: 0.4120 - val_acc: 0.8210\n",
      "Epoch 65/300\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3868 - acc: 0.8349 - val_loss: 0.4071 - val_acc: 0.8227\n",
      "Epoch 66/300\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3886 - acc: 0.8323 - val_loss: 0.4126 - val_acc: 0.8210\n",
      "Epoch 67/300\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3854 - acc: 0.8298 - val_loss: 0.4100 - val_acc: 0.8210\n",
      "Epoch 68/300\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3834 - acc: 0.8404 - val_loss: 0.4082 - val_acc: 0.8194\n",
      "Epoch 69/300\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3820 - acc: 0.8354 - val_loss: 0.4121 - val_acc: 0.8243\n",
      "Epoch 70/300\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.3782 - acc: 0.8402 - val_loss: 0.4068 - val_acc: 0.8194\n",
      "Epoch 71/300\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3761 - acc: 0.8387 - val_loss: 0.4094 - val_acc: 0.8243\n",
      "Epoch 72/300\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3783 - acc: 0.8387 - val_loss: 0.4098 - val_acc: 0.8243\n",
      "Epoch 73/300\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3759 - acc: 0.8400 - val_loss: 0.4092 - val_acc: 0.8259\n",
      "Epoch 74/300\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.3725 - acc: 0.8367 - val_loss: 0.4084 - val_acc: 0.8243\n",
      "Epoch 75/300\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.3733 - acc: 0.8382 - val_loss: 0.4107 - val_acc: 0.8227\n",
      "Training with parameters {'batch_size': 2000, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_22 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.7606 - acc: 0.4344 - val_loss: 0.7053 - val_acc: 0.4368\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.6938 - acc: 0.5121 - val_loss: 0.6599 - val_acc: 0.6125\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6704 - acc: 0.5765 - val_loss: 0.6408 - val_acc: 0.6108\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6572 - acc: 0.5738 - val_loss: 0.6262 - val_acc: 0.6108\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6400 - acc: 0.5889 - val_loss: 0.6125 - val_acc: 0.6700\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6232 - acc: 0.6643 - val_loss: 0.6045 - val_acc: 0.7094\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6076 - acc: 0.7022 - val_loss: 0.6005 - val_acc: 0.6765\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5952 - acc: 0.6970 - val_loss: 0.5938 - val_acc: 0.6601\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5841 - acc: 0.7030 - val_loss: 0.5807 - val_acc: 0.6798\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5728 - acc: 0.7152 - val_loss: 0.5659 - val_acc: 0.7028\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5630 - acc: 0.7356 - val_loss: 0.5538 - val_acc: 0.7274\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5513 - acc: 0.7453 - val_loss: 0.5440 - val_acc: 0.7389\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5405 - acc: 0.7504 - val_loss: 0.5364 - val_acc: 0.7323\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5315 - acc: 0.7528 - val_loss: 0.5285 - val_acc: 0.7389\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5223 - acc: 0.7592 - val_loss: 0.5182 - val_acc: 0.7553\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5096 - acc: 0.7723 - val_loss: 0.5057 - val_acc: 0.7668\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5045 - acc: 0.7747 - val_loss: 0.4981 - val_acc: 0.7734\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4946 - acc: 0.7812 - val_loss: 0.4911 - val_acc: 0.7783\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4892 - acc: 0.7796 - val_loss: 0.4846 - val_acc: 0.7800\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4809 - acc: 0.7865 - val_loss: 0.4798 - val_acc: 0.7783\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4760 - acc: 0.7865 - val_loss: 0.4720 - val_acc: 0.7865\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4704 - acc: 0.7916 - val_loss: 0.4673 - val_acc: 0.7882\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4639 - acc: 0.7953 - val_loss: 0.4655 - val_acc: 0.7833\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4610 - acc: 0.7935 - val_loss: 0.4585 - val_acc: 0.7882\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4544 - acc: 0.8011 - val_loss: 0.4528 - val_acc: 0.7947\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4487 - acc: 0.8051 - val_loss: 0.4505 - val_acc: 0.7964\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4465 - acc: 0.8051 - val_loss: 0.4464 - val_acc: 0.7980\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4420 - acc: 0.8075 - val_loss: 0.4412 - val_acc: 0.7997\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4410 - acc: 0.8084 - val_loss: 0.4400 - val_acc: 0.8013\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4348 - acc: 0.8103 - val_loss: 0.4367 - val_acc: 0.8030\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4310 - acc: 0.8144 - val_loss: 0.4328 - val_acc: 0.8079\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4306 - acc: 0.8099 - val_loss: 0.4336 - val_acc: 0.8062\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4263 - acc: 0.8128 - val_loss: 0.4274 - val_acc: 0.8095\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4228 - acc: 0.8166 - val_loss: 0.4268 - val_acc: 0.8095\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4218 - acc: 0.8185 - val_loss: 0.4258 - val_acc: 0.8079\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4180 - acc: 0.8179 - val_loss: 0.4259 - val_acc: 0.8112\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4185 - acc: 0.8197 - val_loss: 0.4220 - val_acc: 0.8161\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4166 - acc: 0.8205 - val_loss: 0.4245 - val_acc: 0.8177\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4140 - acc: 0.8199 - val_loss: 0.4210 - val_acc: 0.8144\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4131 - acc: 0.8177 - val_loss: 0.4150 - val_acc: 0.8210\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4092 - acc: 0.8236 - val_loss: 0.4199 - val_acc: 0.8161\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4070 - acc: 0.8190 - val_loss: 0.4184 - val_acc: 0.8161\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4052 - acc: 0.8254 - val_loss: 0.4133 - val_acc: 0.8194\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4027 - acc: 0.8285 - val_loss: 0.4176 - val_acc: 0.8177\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4002 - acc: 0.8301 - val_loss: 0.4127 - val_acc: 0.8177\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3977 - acc: 0.8320 - val_loss: 0.4134 - val_acc: 0.8210\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3999 - acc: 0.8272 - val_loss: 0.4135 - val_acc: 0.8227\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3976 - acc: 0.8321 - val_loss: 0.4104 - val_acc: 0.8194\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3946 - acc: 0.8309 - val_loss: 0.4140 - val_acc: 0.8210\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3935 - acc: 0.8314 - val_loss: 0.4099 - val_acc: 0.8227\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3908 - acc: 0.8289 - val_loss: 0.4136 - val_acc: 0.8243\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3917 - acc: 0.8309 - val_loss: 0.4111 - val_acc: 0.8243\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3869 - acc: 0.8373 - val_loss: 0.4093 - val_acc: 0.8227\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3877 - acc: 0.8360 - val_loss: 0.4141 - val_acc: 0.8243\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3845 - acc: 0.8342 - val_loss: 0.4078 - val_acc: 0.8227\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3832 - acc: 0.8404 - val_loss: 0.4090 - val_acc: 0.8243\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3850 - acc: 0.8356 - val_loss: 0.4115 - val_acc: 0.8194\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3799 - acc: 0.8416 - val_loss: 0.4083 - val_acc: 0.8227\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3777 - acc: 0.8393 - val_loss: 0.4118 - val_acc: 0.8194\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3751 - acc: 0.8416 - val_loss: 0.4073 - val_acc: 0.8210\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3725 - acc: 0.8394 - val_loss: 0.4106 - val_acc: 0.8210\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3738 - acc: 0.8391 - val_loss: 0.4083 - val_acc: 0.8210\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3717 - acc: 0.8398 - val_loss: 0.4086 - val_acc: 0.8194\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3710 - acc: 0.8415 - val_loss: 0.4096 - val_acc: 0.8194\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.3677 - acc: 0.8431 - val_loss: 0.4132 - val_acc: 0.8194\n",
      "Training with parameters {'batch_size': 2000, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_23 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.8251 - acc: 0.4377 - val_loss: 0.7911 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.7613 - acc: 0.4433 - val_loss: 0.7278 - val_acc: 0.3924\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.7150 - acc: 0.4662 - val_loss: 0.6858 - val_acc: 0.5780\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6840 - acc: 0.5645 - val_loss: 0.6601 - val_acc: 0.6388\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6700 - acc: 0.6034 - val_loss: 0.6439 - val_acc: 0.6240\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6584 - acc: 0.6021 - val_loss: 0.6326 - val_acc: 0.6190\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6523 - acc: 0.5992 - val_loss: 0.6230 - val_acc: 0.6273\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6386 - acc: 0.6214 - val_loss: 0.6143 - val_acc: 0.6420\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6262 - acc: 0.6477 - val_loss: 0.6069 - val_acc: 0.7143\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6164 - acc: 0.6875 - val_loss: 0.6013 - val_acc: 0.7274\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6079 - acc: 0.7021 - val_loss: 0.5966 - val_acc: 0.7061\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6007 - acc: 0.7072 - val_loss: 0.5916 - val_acc: 0.6995\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5930 - acc: 0.7006 - val_loss: 0.5855 - val_acc: 0.7028\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.5839 - acc: 0.7130 - val_loss: 0.5780 - val_acc: 0.7011\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5777 - acc: 0.7185 - val_loss: 0.5696 - val_acc: 0.7126\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5689 - acc: 0.7260 - val_loss: 0.5613 - val_acc: 0.7176\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5628 - acc: 0.7305 - val_loss: 0.5538 - val_acc: 0.7176\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5571 - acc: 0.7345 - val_loss: 0.5470 - val_acc: 0.7241\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.5471 - acc: 0.7415 - val_loss: 0.5402 - val_acc: 0.7258\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5416 - acc: 0.7482 - val_loss: 0.5332 - val_acc: 0.7291\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.5364 - acc: 0.7473 - val_loss: 0.5259 - val_acc: 0.7373\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.5273 - acc: 0.7522 - val_loss: 0.5187 - val_acc: 0.7406\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.5223 - acc: 0.7606 - val_loss: 0.5105 - val_acc: 0.7389\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.5129 - acc: 0.7683 - val_loss: 0.5021 - val_acc: 0.7488\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.5070 - acc: 0.7679 - val_loss: 0.4951 - val_acc: 0.7570\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.5002 - acc: 0.7732 - val_loss: 0.4893 - val_acc: 0.7619\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.4948 - acc: 0.7765 - val_loss: 0.4841 - val_acc: 0.7668\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4905 - acc: 0.7807 - val_loss: 0.4791 - val_acc: 0.7701\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4831 - acc: 0.7847 - val_loss: 0.4733 - val_acc: 0.7734\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4785 - acc: 0.7871 - val_loss: 0.4683 - val_acc: 0.7800\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.4742 - acc: 0.7878 - val_loss: 0.4656 - val_acc: 0.7833\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4695 - acc: 0.7891 - val_loss: 0.4622 - val_acc: 0.7849\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4656 - acc: 0.7911 - val_loss: 0.4559 - val_acc: 0.7882\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4638 - acc: 0.7909 - val_loss: 0.4520 - val_acc: 0.7964\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4625 - acc: 0.7958 - val_loss: 0.4509 - val_acc: 0.7964\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.4580 - acc: 0.7942 - val_loss: 0.4491 - val_acc: 0.7964\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4558 - acc: 0.7964 - val_loss: 0.4435 - val_acc: 0.8046\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.4515 - acc: 0.7995 - val_loss: 0.4406 - val_acc: 0.8079\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4455 - acc: 0.8039 - val_loss: 0.4412 - val_acc: 0.8013\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4436 - acc: 0.8009 - val_loss: 0.4394 - val_acc: 0.8030\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.4445 - acc: 0.8066 - val_loss: 0.4335 - val_acc: 0.8095\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4427 - acc: 0.8053 - val_loss: 0.4312 - val_acc: 0.8095\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4397 - acc: 0.8022 - val_loss: 0.4356 - val_acc: 0.8013\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4376 - acc: 0.8093 - val_loss: 0.4314 - val_acc: 0.8095\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4336 - acc: 0.8108 - val_loss: 0.4272 - val_acc: 0.8161\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4360 - acc: 0.8095 - val_loss: 0.4274 - val_acc: 0.8095\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.4324 - acc: 0.8119 - val_loss: 0.4265 - val_acc: 0.8079\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4312 - acc: 0.8110 - val_loss: 0.4239 - val_acc: 0.8112\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4298 - acc: 0.8130 - val_loss: 0.4221 - val_acc: 0.8128\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.4272 - acc: 0.8117 - val_loss: 0.4247 - val_acc: 0.8095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.4255 - acc: 0.8163 - val_loss: 0.4212 - val_acc: 0.8128\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4277 - acc: 0.8124 - val_loss: 0.4198 - val_acc: 0.8128\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4212 - acc: 0.8146 - val_loss: 0.4211 - val_acc: 0.8095\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4234 - acc: 0.8163 - val_loss: 0.4181 - val_acc: 0.8128\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.4216 - acc: 0.8186 - val_loss: 0.4168 - val_acc: 0.8112\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4217 - acc: 0.8170 - val_loss: 0.4178 - val_acc: 0.8128\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.4153 - acc: 0.8197 - val_loss: 0.4158 - val_acc: 0.8095\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4141 - acc: 0.8185 - val_loss: 0.4181 - val_acc: 0.8128\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4119 - acc: 0.8201 - val_loss: 0.4149 - val_acc: 0.8112\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4123 - acc: 0.8269 - val_loss: 0.4158 - val_acc: 0.8128\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4099 - acc: 0.8205 - val_loss: 0.4167 - val_acc: 0.8144\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4102 - acc: 0.8190 - val_loss: 0.4137 - val_acc: 0.8144\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.4086 - acc: 0.8219 - val_loss: 0.4128 - val_acc: 0.8161\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.4087 - acc: 0.8230 - val_loss: 0.4136 - val_acc: 0.8144\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4059 - acc: 0.8221 - val_loss: 0.4113 - val_acc: 0.8194\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.4028 - acc: 0.8254 - val_loss: 0.4125 - val_acc: 0.8144\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4046 - acc: 0.8241 - val_loss: 0.4127 - val_acc: 0.8128\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4016 - acc: 0.8259 - val_loss: 0.4122 - val_acc: 0.8144\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.4002 - acc: 0.8243 - val_loss: 0.4114 - val_acc: 0.8161\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4011 - acc: 0.8254 - val_loss: 0.4103 - val_acc: 0.8177\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.3969 - acc: 0.8311 - val_loss: 0.4119 - val_acc: 0.8161\n",
      "Epoch 72/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.3965 - acc: 0.8283 - val_loss: 0.4100 - val_acc: 0.8194\n",
      "Epoch 73/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.3952 - acc: 0.8316 - val_loss: 0.4096 - val_acc: 0.8177\n",
      "Epoch 74/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.3949 - acc: 0.8287 - val_loss: 0.4107 - val_acc: 0.8177\n",
      "Epoch 75/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.3939 - acc: 0.8312 - val_loss: 0.4102 - val_acc: 0.8177\n",
      "Epoch 76/300\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.3939 - acc: 0.8263 - val_loss: 0.4089 - val_acc: 0.8194\n",
      "Epoch 77/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.3936 - acc: 0.8309 - val_loss: 0.4096 - val_acc: 0.8194\n",
      "Epoch 78/300\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.3900 - acc: 0.8320 - val_loss: 0.4101 - val_acc: 0.8210\n",
      "Epoch 79/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.3910 - acc: 0.8287 - val_loss: 0.4082 - val_acc: 0.8194\n",
      "Epoch 80/300\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.3887 - acc: 0.8352 - val_loss: 0.4105 - val_acc: 0.8227\n",
      "Epoch 81/300\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.3880 - acc: 0.8336 - val_loss: 0.4079 - val_acc: 0.8210\n",
      "Epoch 82/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.3853 - acc: 0.8354 - val_loss: 0.4101 - val_acc: 0.8227\n",
      "Epoch 83/300\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.3848 - acc: 0.8342 - val_loss: 0.4100 - val_acc: 0.8227\n",
      "Epoch 84/300\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.3857 - acc: 0.8345 - val_loss: 0.4081 - val_acc: 0.8227\n",
      "Epoch 85/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.3847 - acc: 0.8336 - val_loss: 0.4097 - val_acc: 0.8227\n",
      "Epoch 86/300\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.3837 - acc: 0.8331 - val_loss: 0.4134 - val_acc: 0.8194\n",
      "Training with parameters {'batch_size': 2000, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_24 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6696 - acc: 0.6041 - val_loss: 0.6576 - val_acc: 0.6108\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6527 - acc: 0.6404 - val_loss: 0.6409 - val_acc: 0.6502\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6411 - acc: 0.6470 - val_loss: 0.6292 - val_acc: 0.6700\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6320 - acc: 0.6575 - val_loss: 0.6206 - val_acc: 0.6847\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6223 - acc: 0.6787 - val_loss: 0.6137 - val_acc: 0.6683\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6124 - acc: 0.6959 - val_loss: 0.6081 - val_acc: 0.6831\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6053 - acc: 0.7037 - val_loss: 0.6035 - val_acc: 0.6782\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5981 - acc: 0.7039 - val_loss: 0.5988 - val_acc: 0.6831\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5919 - acc: 0.7081 - val_loss: 0.5931 - val_acc: 0.6847\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.5850 - acc: 0.7112 - val_loss: 0.5866 - val_acc: 0.6897\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5775 - acc: 0.7234 - val_loss: 0.5800 - val_acc: 0.6962\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5722 - acc: 0.7267 - val_loss: 0.5738 - val_acc: 0.7126\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5671 - acc: 0.7314 - val_loss: 0.5681 - val_acc: 0.7143\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5620 - acc: 0.7360 - val_loss: 0.5632 - val_acc: 0.7126\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5558 - acc: 0.7437 - val_loss: 0.5586 - val_acc: 0.7176\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5511 - acc: 0.7469 - val_loss: 0.5536 - val_acc: 0.7241\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5473 - acc: 0.7462 - val_loss: 0.5487 - val_acc: 0.7291\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5412 - acc: 0.7537 - val_loss: 0.5440 - val_acc: 0.7340\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5354 - acc: 0.7586 - val_loss: 0.5391 - val_acc: 0.7373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5321 - acc: 0.7619 - val_loss: 0.5343 - val_acc: 0.7422\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5295 - acc: 0.7610 - val_loss: 0.5300 - val_acc: 0.7455\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.5236 - acc: 0.7637 - val_loss: 0.5259 - val_acc: 0.7553\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5207 - acc: 0.7703 - val_loss: 0.5224 - val_acc: 0.7586\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5169 - acc: 0.7656 - val_loss: 0.5197 - val_acc: 0.7488\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5135 - acc: 0.7699 - val_loss: 0.5163 - val_acc: 0.7504\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.5110 - acc: 0.7725 - val_loss: 0.5124 - val_acc: 0.7619\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.5062 - acc: 0.7756 - val_loss: 0.5081 - val_acc: 0.7619\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.5034 - acc: 0.7761 - val_loss: 0.5050 - val_acc: 0.7635\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5003 - acc: 0.7776 - val_loss: 0.5027 - val_acc: 0.7635\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4966 - acc: 0.7818 - val_loss: 0.5003 - val_acc: 0.7652\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4962 - acc: 0.7783 - val_loss: 0.4974 - val_acc: 0.7652\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4921 - acc: 0.7807 - val_loss: 0.4936 - val_acc: 0.7701\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4903 - acc: 0.7864 - val_loss: 0.4907 - val_acc: 0.7734\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4870 - acc: 0.7864 - val_loss: 0.4895 - val_acc: 0.7701\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4837 - acc: 0.7862 - val_loss: 0.4876 - val_acc: 0.7718\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4819 - acc: 0.7873 - val_loss: 0.4850 - val_acc: 0.7750\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4810 - acc: 0.7876 - val_loss: 0.4815 - val_acc: 0.7783\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4770 - acc: 0.7916 - val_loss: 0.4799 - val_acc: 0.7767\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4774 - acc: 0.7931 - val_loss: 0.4787 - val_acc: 0.7816\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4736 - acc: 0.7905 - val_loss: 0.4769 - val_acc: 0.7849\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4721 - acc: 0.7926 - val_loss: 0.4760 - val_acc: 0.7882\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.4710 - acc: 0.7927 - val_loss: 0.4733 - val_acc: 0.7849\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4695 - acc: 0.7964 - val_loss: 0.4712 - val_acc: 0.7882\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4685 - acc: 0.7971 - val_loss: 0.4700 - val_acc: 0.7931\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4652 - acc: 0.7969 - val_loss: 0.4694 - val_acc: 0.7947\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4636 - acc: 0.7973 - val_loss: 0.4676 - val_acc: 0.7947\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4632 - acc: 0.7977 - val_loss: 0.4655 - val_acc: 0.7980\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4626 - acc: 0.7999 - val_loss: 0.4634 - val_acc: 0.7964\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4586 - acc: 0.8000 - val_loss: 0.4619 - val_acc: 0.7964\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4578 - acc: 0.8062 - val_loss: 0.4617 - val_acc: 0.7980\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4573 - acc: 0.7997 - val_loss: 0.4608 - val_acc: 0.7980\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4570 - acc: 0.8048 - val_loss: 0.4595 - val_acc: 0.7980\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4554 - acc: 0.8042 - val_loss: 0.4581 - val_acc: 0.7947\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4547 - acc: 0.8046 - val_loss: 0.4571 - val_acc: 0.7947\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4527 - acc: 0.8028 - val_loss: 0.4556 - val_acc: 0.7947\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4498 - acc: 0.8075 - val_loss: 0.4547 - val_acc: 0.7964\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4507 - acc: 0.8059 - val_loss: 0.4537 - val_acc: 0.7980\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4494 - acc: 0.8050 - val_loss: 0.4518 - val_acc: 0.7997\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4495 - acc: 0.8088 - val_loss: 0.4508 - val_acc: 0.7997\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4457 - acc: 0.8073 - val_loss: 0.4500 - val_acc: 0.7997\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4463 - acc: 0.8106 - val_loss: 0.4490 - val_acc: 0.7997\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4447 - acc: 0.8050 - val_loss: 0.4479 - val_acc: 0.7997\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4426 - acc: 0.8070 - val_loss: 0.4473 - val_acc: 0.8013\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4423 - acc: 0.8082 - val_loss: 0.4465 - val_acc: 0.8030\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4428 - acc: 0.8061 - val_loss: 0.4448 - val_acc: 0.8046\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4401 - acc: 0.8135 - val_loss: 0.4447 - val_acc: 0.8046\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4409 - acc: 0.8099 - val_loss: 0.4441 - val_acc: 0.8046\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4399 - acc: 0.8086 - val_loss: 0.4425 - val_acc: 0.8046\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4378 - acc: 0.8126 - val_loss: 0.4409 - val_acc: 0.8062\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4373 - acc: 0.8124 - val_loss: 0.4412 - val_acc: 0.8046\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4370 - acc: 0.8115 - val_loss: 0.4414 - val_acc: 0.8079\n",
      "Epoch 72/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4344 - acc: 0.8159 - val_loss: 0.4400 - val_acc: 0.8079\n",
      "Epoch 73/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4335 - acc: 0.8124 - val_loss: 0.4386 - val_acc: 0.8079\n",
      "Epoch 74/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4347 - acc: 0.8150 - val_loss: 0.4378 - val_acc: 0.8079\n",
      "Epoch 75/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4336 - acc: 0.8126 - val_loss: 0.4376 - val_acc: 0.8079\n",
      "Epoch 76/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4313 - acc: 0.8165 - val_loss: 0.4377 - val_acc: 0.8095\n",
      "Epoch 77/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4317 - acc: 0.8132 - val_loss: 0.4366 - val_acc: 0.8095\n",
      "Epoch 78/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4306 - acc: 0.8150 - val_loss: 0.4355 - val_acc: 0.8095\n",
      "Epoch 79/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4300 - acc: 0.8150 - val_loss: 0.4351 - val_acc: 0.8095\n",
      "Epoch 80/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4283 - acc: 0.8168 - val_loss: 0.4350 - val_acc: 0.8112\n",
      "Epoch 81/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4283 - acc: 0.8157 - val_loss: 0.4348 - val_acc: 0.8112\n",
      "Epoch 82/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4283 - acc: 0.8154 - val_loss: 0.4345 - val_acc: 0.8095\n",
      "Epoch 83/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4288 - acc: 0.8141 - val_loss: 0.4332 - val_acc: 0.8095\n",
      "Epoch 84/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4286 - acc: 0.8170 - val_loss: 0.4318 - val_acc: 0.8112\n",
      "Epoch 85/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4244 - acc: 0.8179 - val_loss: 0.4325 - val_acc: 0.8112\n",
      "Epoch 86/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4249 - acc: 0.8185 - val_loss: 0.4331 - val_acc: 0.8112\n",
      "Epoch 87/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4239 - acc: 0.8155 - val_loss: 0.4325 - val_acc: 0.8112\n",
      "Epoch 88/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4227 - acc: 0.8197 - val_loss: 0.4306 - val_acc: 0.8112\n",
      "Epoch 89/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4240 - acc: 0.8199 - val_loss: 0.4296 - val_acc: 0.8161\n",
      "Epoch 90/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4221 - acc: 0.8165 - val_loss: 0.4299 - val_acc: 0.8128\n",
      "Epoch 91/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4223 - acc: 0.8190 - val_loss: 0.4300 - val_acc: 0.8128\n",
      "Epoch 92/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4208 - acc: 0.8194 - val_loss: 0.4303 - val_acc: 0.8112\n",
      "Epoch 93/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4207 - acc: 0.8196 - val_loss: 0.4289 - val_acc: 0.8161\n",
      "Epoch 94/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4173 - acc: 0.8176 - val_loss: 0.4278 - val_acc: 0.8161\n",
      "Epoch 95/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4204 - acc: 0.8207 - val_loss: 0.4286 - val_acc: 0.8161\n",
      "Epoch 96/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4181 - acc: 0.8177 - val_loss: 0.4273 - val_acc: 0.8177\n",
      "Epoch 97/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4178 - acc: 0.8188 - val_loss: 0.4269 - val_acc: 0.8177\n",
      "Epoch 98/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4172 - acc: 0.8228 - val_loss: 0.4269 - val_acc: 0.8161\n",
      "Epoch 99/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4175 - acc: 0.8225 - val_loss: 0.4256 - val_acc: 0.8177\n",
      "Epoch 100/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4166 - acc: 0.8234 - val_loss: 0.4254 - val_acc: 0.8177\n",
      "Epoch 101/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4169 - acc: 0.8210 - val_loss: 0.4260 - val_acc: 0.8161\n",
      "Epoch 102/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4157 - acc: 0.8232 - val_loss: 0.4267 - val_acc: 0.8161\n",
      "Epoch 103/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4156 - acc: 0.8210 - val_loss: 0.4261 - val_acc: 0.8161\n",
      "Epoch 104/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4145 - acc: 0.8203 - val_loss: 0.4240 - val_acc: 0.8177\n",
      "Epoch 105/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4139 - acc: 0.8217 - val_loss: 0.4238 - val_acc: 0.8177\n",
      "Epoch 106/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4119 - acc: 0.8250 - val_loss: 0.4244 - val_acc: 0.8161\n",
      "Epoch 107/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4120 - acc: 0.8234 - val_loss: 0.4246 - val_acc: 0.8161\n",
      "Epoch 108/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4121 - acc: 0.8190 - val_loss: 0.4238 - val_acc: 0.8161\n",
      "Epoch 109/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4121 - acc: 0.8223 - val_loss: 0.4229 - val_acc: 0.8177\n",
      "Epoch 110/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4096 - acc: 0.8236 - val_loss: 0.4226 - val_acc: 0.8177\n",
      "Epoch 111/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4102 - acc: 0.8232 - val_loss: 0.4234 - val_acc: 0.8161\n",
      "Epoch 112/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4109 - acc: 0.8236 - val_loss: 0.4231 - val_acc: 0.8161\n",
      "Epoch 113/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4096 - acc: 0.8247 - val_loss: 0.4220 - val_acc: 0.8177\n",
      "Epoch 114/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4091 - acc: 0.8238 - val_loss: 0.4227 - val_acc: 0.8161\n",
      "Epoch 115/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4092 - acc: 0.8245 - val_loss: 0.4212 - val_acc: 0.8177\n",
      "Epoch 116/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4085 - acc: 0.8261 - val_loss: 0.4209 - val_acc: 0.8177\n",
      "Epoch 117/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4068 - acc: 0.8245 - val_loss: 0.4212 - val_acc: 0.8177\n",
      "Epoch 118/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4080 - acc: 0.8258 - val_loss: 0.4221 - val_acc: 0.8161\n",
      "Epoch 119/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4061 - acc: 0.8298 - val_loss: 0.4221 - val_acc: 0.8161\n",
      "Epoch 120/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4070 - acc: 0.8248 - val_loss: 0.4196 - val_acc: 0.8177\n",
      "Epoch 121/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4063 - acc: 0.8265 - val_loss: 0.4195 - val_acc: 0.8177\n",
      "Epoch 122/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4050 - acc: 0.8280 - val_loss: 0.4210 - val_acc: 0.8161\n",
      "Epoch 123/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4044 - acc: 0.8252 - val_loss: 0.4204 - val_acc: 0.8161\n",
      "Epoch 124/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4039 - acc: 0.8261 - val_loss: 0.4204 - val_acc: 0.8144\n",
      "Epoch 125/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4030 - acc: 0.8287 - val_loss: 0.4198 - val_acc: 0.8161\n",
      "Epoch 126/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4031 - acc: 0.8276 - val_loss: 0.4190 - val_acc: 0.8194\n",
      "Epoch 127/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4033 - acc: 0.8248 - val_loss: 0.4187 - val_acc: 0.8177\n",
      "Epoch 128/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4037 - acc: 0.8256 - val_loss: 0.4197 - val_acc: 0.8161\n",
      "Epoch 129/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4025 - acc: 0.8258 - val_loss: 0.4190 - val_acc: 0.8177\n",
      "Epoch 130/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4025 - acc: 0.8247 - val_loss: 0.4194 - val_acc: 0.8194\n",
      "Epoch 131/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4018 - acc: 0.8283 - val_loss: 0.4190 - val_acc: 0.8194\n",
      "Epoch 132/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4009 - acc: 0.8303 - val_loss: 0.4186 - val_acc: 0.8194\n",
      "Epoch 133/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3997 - acc: 0.8278 - val_loss: 0.4184 - val_acc: 0.8194\n",
      "Epoch 134/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4008 - acc: 0.8289 - val_loss: 0.4182 - val_acc: 0.8177\n",
      "Epoch 135/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4002 - acc: 0.8281 - val_loss: 0.4182 - val_acc: 0.8194\n",
      "Epoch 136/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4002 - acc: 0.8247 - val_loss: 0.4191 - val_acc: 0.8194\n",
      "Epoch 137/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3998 - acc: 0.8290 - val_loss: 0.4188 - val_acc: 0.8194\n",
      "Epoch 138/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3984 - acc: 0.8287 - val_loss: 0.4168 - val_acc: 0.8194\n",
      "Epoch 139/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3991 - acc: 0.8289 - val_loss: 0.4162 - val_acc: 0.8194\n",
      "Epoch 140/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3988 - acc: 0.8301 - val_loss: 0.4171 - val_acc: 0.8210\n",
      "Epoch 141/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3974 - acc: 0.8283 - val_loss: 0.4180 - val_acc: 0.8194\n",
      "Epoch 142/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3972 - acc: 0.8280 - val_loss: 0.4177 - val_acc: 0.8194\n",
      "Epoch 143/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3965 - acc: 0.8294 - val_loss: 0.4169 - val_acc: 0.8210\n",
      "Epoch 144/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3964 - acc: 0.8327 - val_loss: 0.4171 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': 2000, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.8262 - acc: 0.4353 - val_loss: 0.7647 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.7362 - acc: 0.4485 - val_loss: 0.6917 - val_acc: 0.5255\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6896 - acc: 0.5335 - val_loss: 0.6585 - val_acc: 0.6026\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.6692 - acc: 0.5756 - val_loss: 0.6419 - val_acc: 0.6158\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6591 - acc: 0.5771 - val_loss: 0.6284 - val_acc: 0.6174\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6470 - acc: 0.5873 - val_loss: 0.6158 - val_acc: 0.6585\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6255 - acc: 0.6371 - val_loss: 0.6066 - val_acc: 0.7094\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.6115 - acc: 0.6838 - val_loss: 0.6023 - val_acc: 0.6814\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5994 - acc: 0.6988 - val_loss: 0.5990 - val_acc: 0.6650\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5894 - acc: 0.7017 - val_loss: 0.5923 - val_acc: 0.6683\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5826 - acc: 0.7043 - val_loss: 0.5807 - val_acc: 0.6782\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5709 - acc: 0.7148 - val_loss: 0.5670 - val_acc: 0.6897\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5602 - acc: 0.7358 - val_loss: 0.5544 - val_acc: 0.7159\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5511 - acc: 0.7451 - val_loss: 0.5448 - val_acc: 0.7192\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5416 - acc: 0.7480 - val_loss: 0.5365 - val_acc: 0.7225\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5320 - acc: 0.7548 - val_loss: 0.5289 - val_acc: 0.7258\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5222 - acc: 0.7594 - val_loss: 0.5189 - val_acc: 0.7389\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5152 - acc: 0.7661 - val_loss: 0.5095 - val_acc: 0.7537\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5059 - acc: 0.7718 - val_loss: 0.5012 - val_acc: 0.7619\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.5014 - acc: 0.7761 - val_loss: 0.4949 - val_acc: 0.7668\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4932 - acc: 0.7825 - val_loss: 0.4871 - val_acc: 0.7734\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4874 - acc: 0.7803 - val_loss: 0.4814 - val_acc: 0.7718\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4821 - acc: 0.7854 - val_loss: 0.4776 - val_acc: 0.7750\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4766 - acc: 0.7873 - val_loss: 0.4705 - val_acc: 0.7849\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4723 - acc: 0.7953 - val_loss: 0.4664 - val_acc: 0.7833\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4684 - acc: 0.7944 - val_loss: 0.4638 - val_acc: 0.7915\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4643 - acc: 0.7895 - val_loss: 0.4587 - val_acc: 0.7947\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4584 - acc: 0.7993 - val_loss: 0.4536 - val_acc: 0.7997\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4553 - acc: 0.8008 - val_loss: 0.4493 - val_acc: 0.8030\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4515 - acc: 0.8042 - val_loss: 0.4464 - val_acc: 0.8013\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4482 - acc: 0.8053 - val_loss: 0.4454 - val_acc: 0.7997\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4443 - acc: 0.8081 - val_loss: 0.4406 - val_acc: 0.8046\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4420 - acc: 0.8097 - val_loss: 0.4374 - val_acc: 0.8062\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4364 - acc: 0.8132 - val_loss: 0.4371 - val_acc: 0.8046\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4351 - acc: 0.8123 - val_loss: 0.4353 - val_acc: 0.8030\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4306 - acc: 0.8143 - val_loss: 0.4304 - val_acc: 0.8062\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4305 - acc: 0.8163 - val_loss: 0.4299 - val_acc: 0.8079\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4311 - acc: 0.8163 - val_loss: 0.4292 - val_acc: 0.8079\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4252 - acc: 0.8174 - val_loss: 0.4275 - val_acc: 0.8079\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4217 - acc: 0.8208 - val_loss: 0.4231 - val_acc: 0.8112\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4231 - acc: 0.8165 - val_loss: 0.4252 - val_acc: 0.8062\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4196 - acc: 0.8217 - val_loss: 0.4255 - val_acc: 0.8079\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4189 - acc: 0.8194 - val_loss: 0.4206 - val_acc: 0.8128\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4180 - acc: 0.8199 - val_loss: 0.4194 - val_acc: 0.8144\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4150 - acc: 0.8207 - val_loss: 0.4210 - val_acc: 0.8128\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4155 - acc: 0.8201 - val_loss: 0.4204 - val_acc: 0.8144\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4111 - acc: 0.8256 - val_loss: 0.4154 - val_acc: 0.8177\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4101 - acc: 0.8259 - val_loss: 0.4191 - val_acc: 0.8128\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4076 - acc: 0.8230 - val_loss: 0.4195 - val_acc: 0.8144\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4066 - acc: 0.8258 - val_loss: 0.4128 - val_acc: 0.8177\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4061 - acc: 0.8287 - val_loss: 0.4147 - val_acc: 0.8161\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4031 - acc: 0.8252 - val_loss: 0.4215 - val_acc: 0.8194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4028 - acc: 0.8272 - val_loss: 0.4107 - val_acc: 0.8144\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4016 - acc: 0.8252 - val_loss: 0.4129 - val_acc: 0.8177\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3996 - acc: 0.8289 - val_loss: 0.4182 - val_acc: 0.8194\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3983 - acc: 0.8309 - val_loss: 0.4109 - val_acc: 0.8161\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3961 - acc: 0.8329 - val_loss: 0.4105 - val_acc: 0.8161\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3934 - acc: 0.8303 - val_loss: 0.4167 - val_acc: 0.8194\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3929 - acc: 0.8300 - val_loss: 0.4121 - val_acc: 0.8227\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.3913 - acc: 0.8336 - val_loss: 0.4086 - val_acc: 0.8144\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3897 - acc: 0.8351 - val_loss: 0.4137 - val_acc: 0.8227\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3899 - acc: 0.8334 - val_loss: 0.4114 - val_acc: 0.8227\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3890 - acc: 0.8360 - val_loss: 0.4088 - val_acc: 0.8210\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3853 - acc: 0.8356 - val_loss: 0.4137 - val_acc: 0.8243\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.3877 - acc: 0.8305 - val_loss: 0.4095 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': 2000, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.8581 - acc: 0.4350 - val_loss: 0.8278 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.7867 - acc: 0.4362 - val_loss: 0.7616 - val_acc: 0.3875\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.7388 - acc: 0.4412 - val_loss: 0.7168 - val_acc: 0.4023\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.7044 - acc: 0.4766 - val_loss: 0.6865 - val_acc: 0.5517\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6820 - acc: 0.5670 - val_loss: 0.6662 - val_acc: 0.6568\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6687 - acc: 0.6161 - val_loss: 0.6514 - val_acc: 0.6814\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6595 - acc: 0.6300 - val_loss: 0.6398 - val_acc: 0.6847\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6490 - acc: 0.6492 - val_loss: 0.6298 - val_acc: 0.7159\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6391 - acc: 0.6690 - val_loss: 0.6217 - val_acc: 0.7389\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6316 - acc: 0.6891 - val_loss: 0.6153 - val_acc: 0.7274\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.6195 - acc: 0.7057 - val_loss: 0.6103 - val_acc: 0.7126\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6125 - acc: 0.7075 - val_loss: 0.6060 - val_acc: 0.6979\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6078 - acc: 0.7035 - val_loss: 0.6012 - val_acc: 0.6979\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5990 - acc: 0.7157 - val_loss: 0.5956 - val_acc: 0.6995\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5937 - acc: 0.7097 - val_loss: 0.5895 - val_acc: 0.6995\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5878 - acc: 0.7123 - val_loss: 0.5828 - val_acc: 0.7011\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.5806 - acc: 0.7265 - val_loss: 0.5759 - val_acc: 0.7044\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5740 - acc: 0.7318 - val_loss: 0.5693 - val_acc: 0.7126\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5690 - acc: 0.7322 - val_loss: 0.5635 - val_acc: 0.7077\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5612 - acc: 0.7426 - val_loss: 0.5583 - val_acc: 0.7110\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5570 - acc: 0.7358 - val_loss: 0.5528 - val_acc: 0.7077\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.5519 - acc: 0.7451 - val_loss: 0.5470 - val_acc: 0.7110\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.5433 - acc: 0.7475 - val_loss: 0.5415 - val_acc: 0.7159\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.5386 - acc: 0.7495 - val_loss: 0.5359 - val_acc: 0.7192\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.5351 - acc: 0.7508 - val_loss: 0.5294 - val_acc: 0.7307\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5296 - acc: 0.7537 - val_loss: 0.5234 - val_acc: 0.7356\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.5235 - acc: 0.7606 - val_loss: 0.5178 - val_acc: 0.7422\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5198 - acc: 0.7646 - val_loss: 0.5129 - val_acc: 0.7455\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5138 - acc: 0.7666 - val_loss: 0.5083 - val_acc: 0.7471\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.5127 - acc: 0.7696 - val_loss: 0.5031 - val_acc: 0.7537\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.5090 - acc: 0.7729 - val_loss: 0.4983 - val_acc: 0.7537\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5019 - acc: 0.7760 - val_loss: 0.4951 - val_acc: 0.7521\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4984 - acc: 0.7785 - val_loss: 0.4915 - val_acc: 0.7504\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4941 - acc: 0.7776 - val_loss: 0.4872 - val_acc: 0.7635\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4931 - acc: 0.7756 - val_loss: 0.4835 - val_acc: 0.7619\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4852 - acc: 0.7889 - val_loss: 0.4803 - val_acc: 0.7635\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4827 - acc: 0.7836 - val_loss: 0.4771 - val_acc: 0.7685\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4811 - acc: 0.7842 - val_loss: 0.4734 - val_acc: 0.7734\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4805 - acc: 0.7882 - val_loss: 0.4706 - val_acc: 0.7750\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4733 - acc: 0.7937 - val_loss: 0.4689 - val_acc: 0.7767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4703 - acc: 0.7942 - val_loss: 0.4665 - val_acc: 0.7800\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4698 - acc: 0.7911 - val_loss: 0.4635 - val_acc: 0.7816\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4682 - acc: 0.7933 - val_loss: 0.4608 - val_acc: 0.7865\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4661 - acc: 0.7937 - val_loss: 0.4588 - val_acc: 0.7865\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4630 - acc: 0.7926 - val_loss: 0.4577 - val_acc: 0.7865\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4582 - acc: 0.7968 - val_loss: 0.4548 - val_acc: 0.7898\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4567 - acc: 0.8053 - val_loss: 0.4529 - val_acc: 0.7915\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.4563 - acc: 0.8019 - val_loss: 0.4520 - val_acc: 0.7898\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4559 - acc: 0.8031 - val_loss: 0.4507 - val_acc: 0.7915\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4507 - acc: 0.8062 - val_loss: 0.4470 - val_acc: 0.7964\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4487 - acc: 0.8022 - val_loss: 0.4449 - val_acc: 0.7997\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4468 - acc: 0.8046 - val_loss: 0.4433 - val_acc: 0.7997\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4493 - acc: 0.8061 - val_loss: 0.4417 - val_acc: 0.8013\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4470 - acc: 0.8042 - val_loss: 0.4406 - val_acc: 0.8013\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4455 - acc: 0.8075 - val_loss: 0.4396 - val_acc: 0.7997\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4440 - acc: 0.8061 - val_loss: 0.4390 - val_acc: 0.8030\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4416 - acc: 0.8042 - val_loss: 0.4355 - val_acc: 0.8030\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4383 - acc: 0.8097 - val_loss: 0.4348 - val_acc: 0.8046\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4373 - acc: 0.8106 - val_loss: 0.4344 - val_acc: 0.8046\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4322 - acc: 0.8079 - val_loss: 0.4340 - val_acc: 0.8046\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4313 - acc: 0.8174 - val_loss: 0.4319 - val_acc: 0.8046\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4293 - acc: 0.8110 - val_loss: 0.4303 - val_acc: 0.8062\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4338 - acc: 0.8077 - val_loss: 0.4292 - val_acc: 0.8062\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4269 - acc: 0.8186 - val_loss: 0.4292 - val_acc: 0.8062\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4273 - acc: 0.8165 - val_loss: 0.4283 - val_acc: 0.8062\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4247 - acc: 0.8161 - val_loss: 0.4275 - val_acc: 0.8062\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4233 - acc: 0.8134 - val_loss: 0.4254 - val_acc: 0.8128\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4261 - acc: 0.8124 - val_loss: 0.4239 - val_acc: 0.8128\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4224 - acc: 0.8146 - val_loss: 0.4262 - val_acc: 0.8046\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4210 - acc: 0.8165 - val_loss: 0.4238 - val_acc: 0.8128\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4214 - acc: 0.8139 - val_loss: 0.4225 - val_acc: 0.8144\n",
      "Epoch 72/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4192 - acc: 0.8172 - val_loss: 0.4220 - val_acc: 0.8128\n",
      "Epoch 73/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4177 - acc: 0.8176 - val_loss: 0.4243 - val_acc: 0.8046\n",
      "Epoch 74/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4177 - acc: 0.8214 - val_loss: 0.4204 - val_acc: 0.8177\n",
      "Epoch 75/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4160 - acc: 0.8192 - val_loss: 0.4199 - val_acc: 0.8194\n",
      "Epoch 76/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4157 - acc: 0.8170 - val_loss: 0.4205 - val_acc: 0.8128\n",
      "Epoch 77/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4143 - acc: 0.8210 - val_loss: 0.4215 - val_acc: 0.8062\n",
      "Epoch 78/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4143 - acc: 0.8197 - val_loss: 0.4199 - val_acc: 0.8095\n",
      "Epoch 79/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4154 - acc: 0.8197 - val_loss: 0.4172 - val_acc: 0.8227\n",
      "Epoch 80/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4111 - acc: 0.8205 - val_loss: 0.4188 - val_acc: 0.8095\n",
      "Epoch 81/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4118 - acc: 0.8219 - val_loss: 0.4204 - val_acc: 0.8095\n",
      "Epoch 82/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4095 - acc: 0.8247 - val_loss: 0.4182 - val_acc: 0.8095\n",
      "Epoch 83/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4044 - acc: 0.8287 - val_loss: 0.4165 - val_acc: 0.8210\n",
      "Epoch 84/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4083 - acc: 0.8228 - val_loss: 0.4165 - val_acc: 0.8177\n",
      "Epoch 85/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4109 - acc: 0.8219 - val_loss: 0.4178 - val_acc: 0.8112\n",
      "Epoch 86/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4053 - acc: 0.8250 - val_loss: 0.4171 - val_acc: 0.8112\n",
      "Epoch 87/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4028 - acc: 0.8280 - val_loss: 0.4156 - val_acc: 0.8161\n",
      "Epoch 88/300\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.4033 - acc: 0.8232 - val_loss: 0.4143 - val_acc: 0.8227\n",
      "Epoch 89/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4035 - acc: 0.8283 - val_loss: 0.4151 - val_acc: 0.8161\n",
      "Epoch 90/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4007 - acc: 0.8280 - val_loss: 0.4163 - val_acc: 0.8144\n",
      "Epoch 91/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.3998 - acc: 0.8287 - val_loss: 0.4156 - val_acc: 0.8144\n",
      "Epoch 92/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4021 - acc: 0.8263 - val_loss: 0.4131 - val_acc: 0.8194\n",
      "Epoch 93/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4005 - acc: 0.8296 - val_loss: 0.4136 - val_acc: 0.8210\n",
      "Epoch 94/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.3972 - acc: 0.8327 - val_loss: 0.4171 - val_acc: 0.8095\n",
      "Epoch 95/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3981 - acc: 0.8272 - val_loss: 0.4150 - val_acc: 0.8144\n",
      "Epoch 96/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3956 - acc: 0.8316 - val_loss: 0.4117 - val_acc: 0.8194\n",
      "Epoch 97/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3949 - acc: 0.8321 - val_loss: 0.4127 - val_acc: 0.8144\n",
      "Epoch 98/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3937 - acc: 0.8316 - val_loss: 0.4161 - val_acc: 0.8144\n",
      "Epoch 99/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.3939 - acc: 0.8290 - val_loss: 0.4150 - val_acc: 0.8144\n",
      "Epoch 100/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.3972 - acc: 0.8280 - val_loss: 0.4109 - val_acc: 0.8210\n",
      "Epoch 101/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3905 - acc: 0.8327 - val_loss: 0.4147 - val_acc: 0.8128\n",
      "Epoch 102/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.3908 - acc: 0.8272 - val_loss: 0.4145 - val_acc: 0.8144\n",
      "Epoch 103/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3897 - acc: 0.8301 - val_loss: 0.4135 - val_acc: 0.8144\n",
      "Epoch 104/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3882 - acc: 0.8327 - val_loss: 0.4117 - val_acc: 0.8144\n",
      "Epoch 105/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3893 - acc: 0.8338 - val_loss: 0.4140 - val_acc: 0.8144\n",
      "Training with parameters {'batch_size': 2000, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_27 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 1s 53ms/step - loss: 0.7585 - acc: 0.4359 - val_loss: 0.7510 - val_acc: 0.3892\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.7228 - acc: 0.4486 - val_loss: 0.7149 - val_acc: 0.4302\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6970 - acc: 0.4977 - val_loss: 0.6880 - val_acc: 0.5238\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6796 - acc: 0.5767 - val_loss: 0.6690 - val_acc: 0.6601\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6674 - acc: 0.6262 - val_loss: 0.6554 - val_acc: 0.6765\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6575 - acc: 0.6393 - val_loss: 0.6454 - val_acc: 0.6552\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6503 - acc: 0.6322 - val_loss: 0.6373 - val_acc: 0.6437\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6446 - acc: 0.6232 - val_loss: 0.6303 - val_acc: 0.6535\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6384 - acc: 0.6389 - val_loss: 0.6240 - val_acc: 0.6798\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6316 - acc: 0.6566 - val_loss: 0.6182 - val_acc: 0.6979\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6243 - acc: 0.6849 - val_loss: 0.6132 - val_acc: 0.7028\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6160 - acc: 0.7011 - val_loss: 0.6088 - val_acc: 0.7094\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6089 - acc: 0.7130 - val_loss: 0.6047 - val_acc: 0.6979\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6019 - acc: 0.7132 - val_loss: 0.6004 - val_acc: 0.6946\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.5957 - acc: 0.7119 - val_loss: 0.5956 - val_acc: 0.6929\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5906 - acc: 0.7134 - val_loss: 0.5899 - val_acc: 0.6979\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5829 - acc: 0.7216 - val_loss: 0.5839 - val_acc: 0.7044\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5763 - acc: 0.7323 - val_loss: 0.5779 - val_acc: 0.7094\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5735 - acc: 0.7276 - val_loss: 0.5719 - val_acc: 0.7176\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.5647 - acc: 0.7360 - val_loss: 0.5661 - val_acc: 0.7209\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.5616 - acc: 0.7382 - val_loss: 0.5606 - val_acc: 0.7241\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.5547 - acc: 0.7418 - val_loss: 0.5554 - val_acc: 0.7225\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.5468 - acc: 0.7457 - val_loss: 0.5494 - val_acc: 0.7274\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.5411 - acc: 0.7491 - val_loss: 0.5433 - val_acc: 0.7274\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.5365 - acc: 0.7550 - val_loss: 0.5366 - val_acc: 0.7422\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.5309 - acc: 0.7592 - val_loss: 0.5300 - val_acc: 0.7471\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5246 - acc: 0.7619 - val_loss: 0.5247 - val_acc: 0.7471\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5203 - acc: 0.7634 - val_loss: 0.5195 - val_acc: 0.7521\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5138 - acc: 0.7646 - val_loss: 0.5138 - val_acc: 0.7504\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.5094 - acc: 0.7657 - val_loss: 0.5083 - val_acc: 0.7537\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5075 - acc: 0.7688 - val_loss: 0.5030 - val_acc: 0.7586\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.5014 - acc: 0.7739 - val_loss: 0.4983 - val_acc: 0.7586\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.4992 - acc: 0.7729 - val_loss: 0.4944 - val_acc: 0.7668\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4892 - acc: 0.7847 - val_loss: 0.4905 - val_acc: 0.7668\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4871 - acc: 0.7871 - val_loss: 0.4863 - val_acc: 0.7734\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4842 - acc: 0.7862 - val_loss: 0.4844 - val_acc: 0.7718\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4823 - acc: 0.7836 - val_loss: 0.4802 - val_acc: 0.7816\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4776 - acc: 0.7902 - val_loss: 0.4767 - val_acc: 0.7865\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4733 - acc: 0.7893 - val_loss: 0.4737 - val_acc: 0.7882\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.4715 - acc: 0.7947 - val_loss: 0.4722 - val_acc: 0.7915\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4691 - acc: 0.7940 - val_loss: 0.4688 - val_acc: 0.7931\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4660 - acc: 0.7955 - val_loss: 0.4667 - val_acc: 0.7931\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4645 - acc: 0.7933 - val_loss: 0.4638 - val_acc: 0.7931\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4618 - acc: 0.7942 - val_loss: 0.4614 - val_acc: 0.7931\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4615 - acc: 0.7982 - val_loss: 0.4595 - val_acc: 0.7931\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4610 - acc: 0.7933 - val_loss: 0.4576 - val_acc: 0.7931\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4539 - acc: 0.7955 - val_loss: 0.4558 - val_acc: 0.7931\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4550 - acc: 0.7973 - val_loss: 0.4531 - val_acc: 0.7915\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4512 - acc: 0.8015 - val_loss: 0.4504 - val_acc: 0.7964\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.4498 - acc: 0.8035 - val_loss: 0.4503 - val_acc: 0.7980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4476 - acc: 0.8039 - val_loss: 0.4507 - val_acc: 0.7980\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4458 - acc: 0.8026 - val_loss: 0.4473 - val_acc: 0.8013\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4448 - acc: 0.8028 - val_loss: 0.4450 - val_acc: 0.8013\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4422 - acc: 0.8030 - val_loss: 0.4432 - val_acc: 0.8046\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4417 - acc: 0.8084 - val_loss: 0.4423 - val_acc: 0.8046\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4409 - acc: 0.8079 - val_loss: 0.4413 - val_acc: 0.8062\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4367 - acc: 0.8103 - val_loss: 0.4408 - val_acc: 0.8062\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4352 - acc: 0.8088 - val_loss: 0.4394 - val_acc: 0.8062\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4346 - acc: 0.8088 - val_loss: 0.4370 - val_acc: 0.8062\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4356 - acc: 0.8126 - val_loss: 0.4373 - val_acc: 0.8062\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4288 - acc: 0.8152 - val_loss: 0.4364 - val_acc: 0.8062\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4304 - acc: 0.8123 - val_loss: 0.4351 - val_acc: 0.8062\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4296 - acc: 0.8157 - val_loss: 0.4341 - val_acc: 0.8062\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4288 - acc: 0.8177 - val_loss: 0.4331 - val_acc: 0.8079\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4272 - acc: 0.8124 - val_loss: 0.4332 - val_acc: 0.8079\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4269 - acc: 0.8110 - val_loss: 0.4318 - val_acc: 0.8079\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4259 - acc: 0.8141 - val_loss: 0.4304 - val_acc: 0.8079\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.4249 - acc: 0.8130 - val_loss: 0.4302 - val_acc: 0.8079\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4212 - acc: 0.8192 - val_loss: 0.4289 - val_acc: 0.8112\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4216 - acc: 0.8144 - val_loss: 0.4275 - val_acc: 0.8128\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4242 - acc: 0.8174 - val_loss: 0.4292 - val_acc: 0.8062\n",
      "Epoch 72/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4201 - acc: 0.8152 - val_loss: 0.4293 - val_acc: 0.8046\n",
      "Epoch 73/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4149 - acc: 0.8212 - val_loss: 0.4277 - val_acc: 0.8095\n",
      "Epoch 74/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4163 - acc: 0.8190 - val_loss: 0.4245 - val_acc: 0.8144\n",
      "Epoch 75/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4185 - acc: 0.8183 - val_loss: 0.4259 - val_acc: 0.8144\n",
      "Epoch 76/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4150 - acc: 0.8207 - val_loss: 0.4262 - val_acc: 0.8112\n",
      "Epoch 77/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4155 - acc: 0.8212 - val_loss: 0.4257 - val_acc: 0.8112\n",
      "Epoch 78/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4159 - acc: 0.8179 - val_loss: 0.4231 - val_acc: 0.8161\n",
      "Epoch 79/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4126 - acc: 0.8199 - val_loss: 0.4245 - val_acc: 0.8128\n",
      "Epoch 80/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4117 - acc: 0.8243 - val_loss: 0.4249 - val_acc: 0.8128\n",
      "Epoch 81/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4082 - acc: 0.8238 - val_loss: 0.4224 - val_acc: 0.8161\n",
      "Epoch 82/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4090 - acc: 0.8217 - val_loss: 0.4200 - val_acc: 0.8144\n",
      "Epoch 83/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4093 - acc: 0.8241 - val_loss: 0.4227 - val_acc: 0.8144\n",
      "Epoch 84/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4062 - acc: 0.8241 - val_loss: 0.4232 - val_acc: 0.8144\n",
      "Epoch 85/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4048 - acc: 0.8259 - val_loss: 0.4196 - val_acc: 0.8161\n",
      "Epoch 86/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4065 - acc: 0.8272 - val_loss: 0.4183 - val_acc: 0.8161\n",
      "Epoch 87/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4048 - acc: 0.8232 - val_loss: 0.4225 - val_acc: 0.8144\n",
      "Epoch 88/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4043 - acc: 0.8238 - val_loss: 0.4239 - val_acc: 0.8161\n",
      "Epoch 89/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4029 - acc: 0.8281 - val_loss: 0.4178 - val_acc: 0.8177\n",
      "Epoch 90/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4034 - acc: 0.8296 - val_loss: 0.4166 - val_acc: 0.8161\n",
      "Epoch 91/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4034 - acc: 0.8287 - val_loss: 0.4224 - val_acc: 0.8161\n",
      "Epoch 92/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4020 - acc: 0.8281 - val_loss: 0.4215 - val_acc: 0.8161\n",
      "Epoch 93/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4021 - acc: 0.8265 - val_loss: 0.4170 - val_acc: 0.8177\n",
      "Epoch 94/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3994 - acc: 0.8272 - val_loss: 0.4169 - val_acc: 0.8177\n",
      "Epoch 95/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.3951 - acc: 0.8321 - val_loss: 0.4206 - val_acc: 0.8161\n",
      "Training with parameters {'batch_size': 2000, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_28 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.8243 - acc: 0.4388 - val_loss: 0.7915 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.7580 - acc: 0.4444 - val_loss: 0.7283 - val_acc: 0.3924\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.7126 - acc: 0.4782 - val_loss: 0.6860 - val_acc: 0.5780\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6862 - acc: 0.5583 - val_loss: 0.6599 - val_acc: 0.6388\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6683 - acc: 0.6099 - val_loss: 0.6438 - val_acc: 0.6256\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6591 - acc: 0.6008 - val_loss: 0.6324 - val_acc: 0.6190\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6500 - acc: 0.5988 - val_loss: 0.6228 - val_acc: 0.6273\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.6391 - acc: 0.6138 - val_loss: 0.6141 - val_acc: 0.6420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.6292 - acc: 0.6473 - val_loss: 0.6067 - val_acc: 0.7126\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.6177 - acc: 0.6915 - val_loss: 0.6010 - val_acc: 0.7258\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.6046 - acc: 0.7114 - val_loss: 0.5962 - val_acc: 0.7061\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6012 - acc: 0.7063 - val_loss: 0.5912 - val_acc: 0.6962\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.5923 - acc: 0.7050 - val_loss: 0.5854 - val_acc: 0.7044\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.5830 - acc: 0.7050 - val_loss: 0.5781 - val_acc: 0.7011\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.5755 - acc: 0.7148 - val_loss: 0.5697 - val_acc: 0.7077\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.5678 - acc: 0.7247 - val_loss: 0.5614 - val_acc: 0.7159\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.5614 - acc: 0.7307 - val_loss: 0.5534 - val_acc: 0.7192\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.5548 - acc: 0.7362 - val_loss: 0.5462 - val_acc: 0.7225\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5492 - acc: 0.7391 - val_loss: 0.5397 - val_acc: 0.7274\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.5415 - acc: 0.7466 - val_loss: 0.5335 - val_acc: 0.7241\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.5339 - acc: 0.7493 - val_loss: 0.5264 - val_acc: 0.7323\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.5286 - acc: 0.7524 - val_loss: 0.5178 - val_acc: 0.7422\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.5194 - acc: 0.7588 - val_loss: 0.5097 - val_acc: 0.7422\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.5108 - acc: 0.7683 - val_loss: 0.5030 - val_acc: 0.7422\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5078 - acc: 0.7670 - val_loss: 0.4974 - val_acc: 0.7504\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.5005 - acc: 0.7758 - val_loss: 0.4913 - val_acc: 0.7553\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4943 - acc: 0.7765 - val_loss: 0.4843 - val_acc: 0.7652\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4921 - acc: 0.7781 - val_loss: 0.4793 - val_acc: 0.7701\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.4848 - acc: 0.7862 - val_loss: 0.4759 - val_acc: 0.7718\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4811 - acc: 0.7864 - val_loss: 0.4713 - val_acc: 0.7750\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.4777 - acc: 0.7842 - val_loss: 0.4649 - val_acc: 0.7800\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4724 - acc: 0.7876 - val_loss: 0.4601 - val_acc: 0.7833\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4699 - acc: 0.7869 - val_loss: 0.4572 - val_acc: 0.7833\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4656 - acc: 0.7938 - val_loss: 0.4551 - val_acc: 0.7915\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4603 - acc: 0.7986 - val_loss: 0.4503 - val_acc: 0.7931\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.4595 - acc: 0.7935 - val_loss: 0.4479 - val_acc: 0.7980\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4538 - acc: 0.8009 - val_loss: 0.4477 - val_acc: 0.7997\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.4522 - acc: 0.7995 - val_loss: 0.4405 - val_acc: 0.8095\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4495 - acc: 0.8006 - val_loss: 0.4369 - val_acc: 0.8079\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4457 - acc: 0.8030 - val_loss: 0.4392 - val_acc: 0.8030\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4458 - acc: 0.8028 - val_loss: 0.4381 - val_acc: 0.8013\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.4425 - acc: 0.8053 - val_loss: 0.4333 - val_acc: 0.8128\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4370 - acc: 0.8068 - val_loss: 0.4306 - val_acc: 0.8128\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.4377 - acc: 0.8066 - val_loss: 0.4313 - val_acc: 0.8112\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4353 - acc: 0.8106 - val_loss: 0.4306 - val_acc: 0.8062\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4333 - acc: 0.8072 - val_loss: 0.4259 - val_acc: 0.8128\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4303 - acc: 0.8134 - val_loss: 0.4263 - val_acc: 0.8079\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4273 - acc: 0.8108 - val_loss: 0.4260 - val_acc: 0.8062\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4308 - acc: 0.8092 - val_loss: 0.4254 - val_acc: 0.8079\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4263 - acc: 0.8092 - val_loss: 0.4232 - val_acc: 0.8079\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.4265 - acc: 0.8128 - val_loss: 0.4218 - val_acc: 0.8144\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4207 - acc: 0.8144 - val_loss: 0.4205 - val_acc: 0.8128\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.4206 - acc: 0.8185 - val_loss: 0.4214 - val_acc: 0.8095\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4218 - acc: 0.8146 - val_loss: 0.4205 - val_acc: 0.8095\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4210 - acc: 0.8172 - val_loss: 0.4179 - val_acc: 0.8128\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4169 - acc: 0.8192 - val_loss: 0.4169 - val_acc: 0.8144\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.4158 - acc: 0.8207 - val_loss: 0.4197 - val_acc: 0.8144\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.4130 - acc: 0.8212 - val_loss: 0.4151 - val_acc: 0.8177\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4120 - acc: 0.8217 - val_loss: 0.4143 - val_acc: 0.8177\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4107 - acc: 0.8217 - val_loss: 0.4172 - val_acc: 0.8161\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4090 - acc: 0.8234 - val_loss: 0.4140 - val_acc: 0.8161\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4097 - acc: 0.8181 - val_loss: 0.4127 - val_acc: 0.8177\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4063 - acc: 0.8259 - val_loss: 0.4137 - val_acc: 0.8144\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4075 - acc: 0.8236 - val_loss: 0.4119 - val_acc: 0.8177\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.4083 - acc: 0.8254 - val_loss: 0.4126 - val_acc: 0.8144\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4027 - acc: 0.8228 - val_loss: 0.4122 - val_acc: 0.8128\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4020 - acc: 0.8269 - val_loss: 0.4087 - val_acc: 0.8259\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4009 - acc: 0.8283 - val_loss: 0.4128 - val_acc: 0.8161\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.3999 - acc: 0.8239 - val_loss: 0.4136 - val_acc: 0.8177\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.3999 - acc: 0.8263 - val_loss: 0.4083 - val_acc: 0.8259\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.3986 - acc: 0.8312 - val_loss: 0.4118 - val_acc: 0.8177\n",
      "Epoch 72/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.3981 - acc: 0.8272 - val_loss: 0.4095 - val_acc: 0.8177\n",
      "Epoch 73/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.3960 - acc: 0.8307 - val_loss: 0.4087 - val_acc: 0.8210\n",
      "Epoch 74/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.3943 - acc: 0.8303 - val_loss: 0.4107 - val_acc: 0.8177\n",
      "Epoch 75/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.3920 - acc: 0.8345 - val_loss: 0.4113 - val_acc: 0.8194\n",
      "Training with parameters {'batch_size': 2500, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.7659 - acc: 0.4373 - val_loss: 0.7046 - val_acc: 0.4384\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.6977 - acc: 0.4937 - val_loss: 0.6598 - val_acc: 0.6092\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.6723 - acc: 0.5658 - val_loss: 0.6416 - val_acc: 0.6108\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.6602 - acc: 0.5700 - val_loss: 0.6273 - val_acc: 0.6108\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.6416 - acc: 0.5862 - val_loss: 0.6139 - val_acc: 0.6634\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6250 - acc: 0.6544 - val_loss: 0.6056 - val_acc: 0.7126\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.6093 - acc: 0.7004 - val_loss: 0.6014 - val_acc: 0.6765\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5999 - acc: 0.6960 - val_loss: 0.5949 - val_acc: 0.6601\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5864 - acc: 0.6973 - val_loss: 0.5830 - val_acc: 0.6765\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5748 - acc: 0.7114 - val_loss: 0.5684 - val_acc: 0.6929\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5656 - acc: 0.7283 - val_loss: 0.5558 - val_acc: 0.7258\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5549 - acc: 0.7437 - val_loss: 0.5453 - val_acc: 0.7373\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5464 - acc: 0.7510 - val_loss: 0.5373 - val_acc: 0.7389\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5338 - acc: 0.7559 - val_loss: 0.5307 - val_acc: 0.7406\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.5260 - acc: 0.7617 - val_loss: 0.5223 - val_acc: 0.7488\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5170 - acc: 0.7670 - val_loss: 0.5122 - val_acc: 0.7570\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5067 - acc: 0.7712 - val_loss: 0.5015 - val_acc: 0.7685\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4980 - acc: 0.7785 - val_loss: 0.4925 - val_acc: 0.7750\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4933 - acc: 0.7794 - val_loss: 0.4848 - val_acc: 0.7783\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4857 - acc: 0.7869 - val_loss: 0.4808 - val_acc: 0.7816\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4814 - acc: 0.7874 - val_loss: 0.4753 - val_acc: 0.7816\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4754 - acc: 0.7880 - val_loss: 0.4663 - val_acc: 0.7915\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4724 - acc: 0.7907 - val_loss: 0.4644 - val_acc: 0.7898\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4640 - acc: 0.7942 - val_loss: 0.4652 - val_acc: 0.7833\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4616 - acc: 0.7973 - val_loss: 0.4581 - val_acc: 0.7898\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4554 - acc: 0.8015 - val_loss: 0.4520 - val_acc: 0.7980\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4539 - acc: 0.8006 - val_loss: 0.4486 - val_acc: 0.7980\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4484 - acc: 0.8031 - val_loss: 0.4474 - val_acc: 0.7964\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4471 - acc: 0.8011 - val_loss: 0.4465 - val_acc: 0.7947\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4430 - acc: 0.8059 - val_loss: 0.4391 - val_acc: 0.8046\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4366 - acc: 0.8113 - val_loss: 0.4421 - val_acc: 0.7964\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4368 - acc: 0.8128 - val_loss: 0.4403 - val_acc: 0.7964\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4336 - acc: 0.8093 - val_loss: 0.4329 - val_acc: 0.8095\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4329 - acc: 0.8112 - val_loss: 0.4310 - val_acc: 0.8079\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4316 - acc: 0.8121 - val_loss: 0.4348 - val_acc: 0.8030\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4285 - acc: 0.8086 - val_loss: 0.4330 - val_acc: 0.8062\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4233 - acc: 0.8137 - val_loss: 0.4257 - val_acc: 0.8128\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.4251 - acc: 0.8154 - val_loss: 0.4251 - val_acc: 0.8112\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4201 - acc: 0.8143 - val_loss: 0.4295 - val_acc: 0.8079\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4196 - acc: 0.8166 - val_loss: 0.4255 - val_acc: 0.8128\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4165 - acc: 0.8230 - val_loss: 0.4232 - val_acc: 0.8112\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4141 - acc: 0.8216 - val_loss: 0.4234 - val_acc: 0.8112\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4116 - acc: 0.8258 - val_loss: 0.4202 - val_acc: 0.8128\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4129 - acc: 0.8223 - val_loss: 0.4214 - val_acc: 0.8128\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4118 - acc: 0.8205 - val_loss: 0.4211 - val_acc: 0.8128\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4090 - acc: 0.8250 - val_loss: 0.4139 - val_acc: 0.8177\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.4074 - acc: 0.8259 - val_loss: 0.4179 - val_acc: 0.8161\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4058 - acc: 0.8248 - val_loss: 0.4213 - val_acc: 0.8161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4054 - acc: 0.8265 - val_loss: 0.4160 - val_acc: 0.8177\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4036 - acc: 0.8307 - val_loss: 0.4152 - val_acc: 0.8194\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4029 - acc: 0.8254 - val_loss: 0.4150 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': 2500, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_30 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.8367 - acc: 0.4357 - val_loss: 0.7917 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.7631 - acc: 0.4448 - val_loss: 0.7287 - val_acc: 0.3924\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.7169 - acc: 0.4649 - val_loss: 0.6862 - val_acc: 0.5764\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6851 - acc: 0.5670 - val_loss: 0.6597 - val_acc: 0.6404\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6699 - acc: 0.6108 - val_loss: 0.6435 - val_acc: 0.6256\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6572 - acc: 0.5968 - val_loss: 0.6324 - val_acc: 0.6174\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6502 - acc: 0.5966 - val_loss: 0.6229 - val_acc: 0.6223\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.6391 - acc: 0.6088 - val_loss: 0.6141 - val_acc: 0.6355\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.6258 - acc: 0.6442 - val_loss: 0.6065 - val_acc: 0.6979\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.6191 - acc: 0.6844 - val_loss: 0.6009 - val_acc: 0.7258\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.6076 - acc: 0.7088 - val_loss: 0.5966 - val_acc: 0.7077\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.6018 - acc: 0.7033 - val_loss: 0.5923 - val_acc: 0.6979\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.5917 - acc: 0.7061 - val_loss: 0.5869 - val_acc: 0.6979\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.5855 - acc: 0.7099 - val_loss: 0.5799 - val_acc: 0.6995\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.5777 - acc: 0.7125 - val_loss: 0.5717 - val_acc: 0.7044\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.5754 - acc: 0.7145 - val_loss: 0.5635 - val_acc: 0.7159\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.5646 - acc: 0.7307 - val_loss: 0.5558 - val_acc: 0.7176\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.5572 - acc: 0.7338 - val_loss: 0.5486 - val_acc: 0.7209\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.5516 - acc: 0.7431 - val_loss: 0.5426 - val_acc: 0.7241\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.5432 - acc: 0.7451 - val_loss: 0.5375 - val_acc: 0.7241\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5362 - acc: 0.7486 - val_loss: 0.5313 - val_acc: 0.7258\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.5326 - acc: 0.7466 - val_loss: 0.5237 - val_acc: 0.7373\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.5233 - acc: 0.7583 - val_loss: 0.5140 - val_acc: 0.7455\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.5192 - acc: 0.7595 - val_loss: 0.5057 - val_acc: 0.7537\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5137 - acc: 0.7692 - val_loss: 0.5002 - val_acc: 0.7570\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.5063 - acc: 0.7708 - val_loss: 0.4993 - val_acc: 0.7504\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.5018 - acc: 0.7705 - val_loss: 0.4951 - val_acc: 0.7504\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4953 - acc: 0.7727 - val_loss: 0.4868 - val_acc: 0.7553\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.4908 - acc: 0.7818 - val_loss: 0.4785 - val_acc: 0.7718\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.4884 - acc: 0.7812 - val_loss: 0.4738 - val_acc: 0.7734\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4830 - acc: 0.7831 - val_loss: 0.4724 - val_acc: 0.7783\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4795 - acc: 0.7851 - val_loss: 0.4710 - val_acc: 0.7783\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4744 - acc: 0.7873 - val_loss: 0.4658 - val_acc: 0.7816\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4730 - acc: 0.7878 - val_loss: 0.4597 - val_acc: 0.7865\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4659 - acc: 0.7958 - val_loss: 0.4555 - val_acc: 0.7882\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4667 - acc: 0.7931 - val_loss: 0.4537 - val_acc: 0.7931\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4640 - acc: 0.7968 - val_loss: 0.4523 - val_acc: 0.7964\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4574 - acc: 0.7957 - val_loss: 0.4501 - val_acc: 0.7915\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4540 - acc: 0.8000 - val_loss: 0.4504 - val_acc: 0.7947\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4558 - acc: 0.7973 - val_loss: 0.4459 - val_acc: 0.7980\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4513 - acc: 0.8002 - val_loss: 0.4400 - val_acc: 0.8079\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.4479 - acc: 0.8033 - val_loss: 0.4366 - val_acc: 0.8062\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.4460 - acc: 0.8051 - val_loss: 0.4383 - val_acc: 0.8046\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4474 - acc: 0.8024 - val_loss: 0.4376 - val_acc: 0.8046\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4434 - acc: 0.8082 - val_loss: 0.4335 - val_acc: 0.8095\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4424 - acc: 0.8039 - val_loss: 0.4336 - val_acc: 0.8079\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4417 - acc: 0.8082 - val_loss: 0.4304 - val_acc: 0.8095\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4356 - acc: 0.8119 - val_loss: 0.4262 - val_acc: 0.8112\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4388 - acc: 0.8066 - val_loss: 0.4274 - val_acc: 0.8112\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.4356 - acc: 0.8095 - val_loss: 0.4301 - val_acc: 0.8046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4322 - acc: 0.8086 - val_loss: 0.4260 - val_acc: 0.8079\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4319 - acc: 0.8106 - val_loss: 0.4252 - val_acc: 0.8079\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4290 - acc: 0.8090 - val_loss: 0.4266 - val_acc: 0.8079\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4281 - acc: 0.8070 - val_loss: 0.4236 - val_acc: 0.8079\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4245 - acc: 0.8144 - val_loss: 0.4229 - val_acc: 0.8079\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.4266 - acc: 0.8134 - val_loss: 0.4226 - val_acc: 0.8079\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4237 - acc: 0.8128 - val_loss: 0.4206 - val_acc: 0.8095\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.4204 - acc: 0.8185 - val_loss: 0.4201 - val_acc: 0.8112\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.4219 - acc: 0.8161 - val_loss: 0.4221 - val_acc: 0.8112\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4214 - acc: 0.8144 - val_loss: 0.4225 - val_acc: 0.8112\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4194 - acc: 0.8177 - val_loss: 0.4199 - val_acc: 0.8112\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.4179 - acc: 0.8208 - val_loss: 0.4165 - val_acc: 0.8161\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.4175 - acc: 0.8188 - val_loss: 0.4173 - val_acc: 0.8144\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4150 - acc: 0.8197 - val_loss: 0.4214 - val_acc: 0.8112\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.4159 - acc: 0.8197 - val_loss: 0.4155 - val_acc: 0.8144\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4140 - acc: 0.8203 - val_loss: 0.4122 - val_acc: 0.8194\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4110 - acc: 0.8203 - val_loss: 0.4202 - val_acc: 0.8112\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4102 - acc: 0.8201 - val_loss: 0.4176 - val_acc: 0.8144\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.4061 - acc: 0.8256 - val_loss: 0.4124 - val_acc: 0.8194\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4077 - acc: 0.8247 - val_loss: 0.4142 - val_acc: 0.8128\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4059 - acc: 0.8183 - val_loss: 0.4167 - val_acc: 0.8144\n",
      "Training with parameters {'batch_size': 2500, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_31 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6707 - acc: 0.5964 - val_loss: 0.6579 - val_acc: 0.6092\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6528 - acc: 0.6422 - val_loss: 0.6415 - val_acc: 0.6502\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6415 - acc: 0.6475 - val_loss: 0.6298 - val_acc: 0.6749\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6313 - acc: 0.6632 - val_loss: 0.6206 - val_acc: 0.6847\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6238 - acc: 0.6749 - val_loss: 0.6132 - val_acc: 0.6814\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6150 - acc: 0.6904 - val_loss: 0.6067 - val_acc: 0.6814\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6055 - acc: 0.7024 - val_loss: 0.6018 - val_acc: 0.6864\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5986 - acc: 0.7090 - val_loss: 0.5978 - val_acc: 0.6782\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5910 - acc: 0.7172 - val_loss: 0.5940 - val_acc: 0.6831\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.5858 - acc: 0.7137 - val_loss: 0.5885 - val_acc: 0.6880\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5796 - acc: 0.7137 - val_loss: 0.5816 - val_acc: 0.6929\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5742 - acc: 0.7240 - val_loss: 0.5742 - val_acc: 0.7110\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5667 - acc: 0.7345 - val_loss: 0.5682 - val_acc: 0.7143\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5633 - acc: 0.7407 - val_loss: 0.5635 - val_acc: 0.7159\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5584 - acc: 0.7375 - val_loss: 0.5595 - val_acc: 0.7159\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5531 - acc: 0.7429 - val_loss: 0.5556 - val_acc: 0.7192\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5490 - acc: 0.7417 - val_loss: 0.5509 - val_acc: 0.7258\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5439 - acc: 0.7462 - val_loss: 0.5457 - val_acc: 0.7307\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5397 - acc: 0.7579 - val_loss: 0.5414 - val_acc: 0.7356\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.5342 - acc: 0.7581 - val_loss: 0.5376 - val_acc: 0.7406\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5327 - acc: 0.7561 - val_loss: 0.5337 - val_acc: 0.7438\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5277 - acc: 0.7612 - val_loss: 0.5300 - val_acc: 0.7455\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5238 - acc: 0.7654 - val_loss: 0.5259 - val_acc: 0.7521\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5196 - acc: 0.7687 - val_loss: 0.5216 - val_acc: 0.7521\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5174 - acc: 0.7703 - val_loss: 0.5176 - val_acc: 0.7570\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5149 - acc: 0.7705 - val_loss: 0.5140 - val_acc: 0.7586\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5111 - acc: 0.7736 - val_loss: 0.5115 - val_acc: 0.7586\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5089 - acc: 0.7756 - val_loss: 0.5108 - val_acc: 0.7619\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5067 - acc: 0.7730 - val_loss: 0.5088 - val_acc: 0.7635\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5031 - acc: 0.7761 - val_loss: 0.5060 - val_acc: 0.7652\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5008 - acc: 0.7754 - val_loss: 0.5023 - val_acc: 0.7685\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4975 - acc: 0.7800 - val_loss: 0.4988 - val_acc: 0.7685\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4935 - acc: 0.7851 - val_loss: 0.4954 - val_acc: 0.7750\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4927 - acc: 0.7829 - val_loss: 0.4931 - val_acc: 0.7734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4917 - acc: 0.7807 - val_loss: 0.4932 - val_acc: 0.7668\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4873 - acc: 0.7843 - val_loss: 0.4950 - val_acc: 0.7668\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4859 - acc: 0.7823 - val_loss: 0.4936 - val_acc: 0.7685\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4858 - acc: 0.7842 - val_loss: 0.4887 - val_acc: 0.7685\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4816 - acc: 0.7876 - val_loss: 0.4823 - val_acc: 0.7750\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4795 - acc: 0.7907 - val_loss: 0.4786 - val_acc: 0.7783\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4789 - acc: 0.7918 - val_loss: 0.4782 - val_acc: 0.7783\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4762 - acc: 0.7960 - val_loss: 0.4805 - val_acc: 0.7783\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4742 - acc: 0.7916 - val_loss: 0.4803 - val_acc: 0.7783\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.4725 - acc: 0.7885 - val_loss: 0.4773 - val_acc: 0.7800\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.4719 - acc: 0.7955 - val_loss: 0.4745 - val_acc: 0.7849\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4687 - acc: 0.7982 - val_loss: 0.4718 - val_acc: 0.7849\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4680 - acc: 0.7971 - val_loss: 0.4700 - val_acc: 0.7882\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.4662 - acc: 0.7968 - val_loss: 0.4681 - val_acc: 0.7882\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4658 - acc: 0.7980 - val_loss: 0.4668 - val_acc: 0.7898\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4644 - acc: 0.7964 - val_loss: 0.4661 - val_acc: 0.7947\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4603 - acc: 0.7999 - val_loss: 0.4654 - val_acc: 0.7947\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4607 - acc: 0.8002 - val_loss: 0.4645 - val_acc: 0.7964\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.4612 - acc: 0.7997 - val_loss: 0.4631 - val_acc: 0.7964\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.4599 - acc: 0.8008 - val_loss: 0.4613 - val_acc: 0.7947\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4595 - acc: 0.8009 - val_loss: 0.4612 - val_acc: 0.7931\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4569 - acc: 0.7977 - val_loss: 0.4596 - val_acc: 0.7931\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4559 - acc: 0.8030 - val_loss: 0.4583 - val_acc: 0.7947\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4558 - acc: 0.8022 - val_loss: 0.4568 - val_acc: 0.7947\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.4533 - acc: 0.8057 - val_loss: 0.4550 - val_acc: 0.7947\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.4523 - acc: 0.8044 - val_loss: 0.4542 - val_acc: 0.7947\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4521 - acc: 0.8041 - val_loss: 0.4543 - val_acc: 0.7964\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4521 - acc: 0.8008 - val_loss: 0.4543 - val_acc: 0.7980\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4524 - acc: 0.8064 - val_loss: 0.4521 - val_acc: 0.7980\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4482 - acc: 0.8070 - val_loss: 0.4508 - val_acc: 0.7997\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.4484 - acc: 0.8028 - val_loss: 0.4502 - val_acc: 0.7997\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.4463 - acc: 0.8077 - val_loss: 0.4513 - val_acc: 0.7997\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4467 - acc: 0.8081 - val_loss: 0.4526 - val_acc: 0.7980\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4463 - acc: 0.8070 - val_loss: 0.4501 - val_acc: 0.7980\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4445 - acc: 0.8075 - val_loss: 0.4472 - val_acc: 0.8030\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4437 - acc: 0.8079 - val_loss: 0.4459 - val_acc: 0.8030\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4432 - acc: 0.8075 - val_loss: 0.4465 - val_acc: 0.8030\n",
      "Epoch 72/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4429 - acc: 0.8084 - val_loss: 0.4459 - val_acc: 0.8030\n",
      "Epoch 73/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4427 - acc: 0.8101 - val_loss: 0.4438 - val_acc: 0.8030\n",
      "Epoch 74/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4397 - acc: 0.8130 - val_loss: 0.4433 - val_acc: 0.8030\n",
      "Epoch 75/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4422 - acc: 0.8104 - val_loss: 0.4438 - val_acc: 0.8062\n",
      "Epoch 76/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4396 - acc: 0.8103 - val_loss: 0.4439 - val_acc: 0.8079\n",
      "Epoch 77/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4386 - acc: 0.8121 - val_loss: 0.4427 - val_acc: 0.8079\n",
      "Epoch 78/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4364 - acc: 0.8121 - val_loss: 0.4410 - val_acc: 0.8062\n",
      "Epoch 79/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4359 - acc: 0.8121 - val_loss: 0.4403 - val_acc: 0.8062\n",
      "Epoch 80/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4355 - acc: 0.8124 - val_loss: 0.4396 - val_acc: 0.8079\n",
      "Epoch 81/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4354 - acc: 0.8110 - val_loss: 0.4395 - val_acc: 0.8095\n",
      "Epoch 82/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4347 - acc: 0.8124 - val_loss: 0.4409 - val_acc: 0.8079\n",
      "Epoch 83/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4331 - acc: 0.8148 - val_loss: 0.4410 - val_acc: 0.8079\n",
      "Epoch 84/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4320 - acc: 0.8135 - val_loss: 0.4400 - val_acc: 0.8095\n",
      "Epoch 85/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4312 - acc: 0.8126 - val_loss: 0.4380 - val_acc: 0.8112\n",
      "Epoch 86/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4316 - acc: 0.8139 - val_loss: 0.4360 - val_acc: 0.8095\n",
      "Epoch 87/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4320 - acc: 0.8165 - val_loss: 0.4357 - val_acc: 0.8112\n",
      "Epoch 88/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4298 - acc: 0.8137 - val_loss: 0.4359 - val_acc: 0.8112\n",
      "Epoch 89/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4302 - acc: 0.8161 - val_loss: 0.4362 - val_acc: 0.8112\n",
      "Epoch 90/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4292 - acc: 0.8172 - val_loss: 0.4358 - val_acc: 0.8112\n",
      "Epoch 91/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.4285 - acc: 0.8174 - val_loss: 0.4341 - val_acc: 0.8128\n",
      "Epoch 92/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4272 - acc: 0.8146 - val_loss: 0.4329 - val_acc: 0.8112\n",
      "Epoch 93/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4282 - acc: 0.8163 - val_loss: 0.4330 - val_acc: 0.8112\n",
      "Epoch 94/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4269 - acc: 0.8154 - val_loss: 0.4337 - val_acc: 0.8112\n",
      "Epoch 95/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4268 - acc: 0.8166 - val_loss: 0.4336 - val_acc: 0.8128\n",
      "Epoch 96/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4255 - acc: 0.8183 - val_loss: 0.4332 - val_acc: 0.8128\n",
      "Epoch 97/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4263 - acc: 0.8148 - val_loss: 0.4318 - val_acc: 0.8128\n",
      "Epoch 98/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4262 - acc: 0.8159 - val_loss: 0.4309 - val_acc: 0.8144\n",
      "Epoch 99/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4225 - acc: 0.8174 - val_loss: 0.4303 - val_acc: 0.8128\n",
      "Epoch 100/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4229 - acc: 0.8144 - val_loss: 0.4311 - val_acc: 0.8128\n",
      "Epoch 101/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4241 - acc: 0.8188 - val_loss: 0.4330 - val_acc: 0.8128\n",
      "Epoch 102/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4209 - acc: 0.8205 - val_loss: 0.4331 - val_acc: 0.8112\n",
      "Epoch 103/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4233 - acc: 0.8165 - val_loss: 0.4316 - val_acc: 0.8128\n",
      "Epoch 104/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4209 - acc: 0.8185 - val_loss: 0.4287 - val_acc: 0.8177\n",
      "Epoch 105/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4211 - acc: 0.8199 - val_loss: 0.4271 - val_acc: 0.8161\n",
      "Epoch 106/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4209 - acc: 0.8188 - val_loss: 0.4273 - val_acc: 0.8177\n",
      "Epoch 107/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4204 - acc: 0.8203 - val_loss: 0.4285 - val_acc: 0.8194\n",
      "Epoch 108/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4181 - acc: 0.8212 - val_loss: 0.4297 - val_acc: 0.8161\n",
      "Epoch 109/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4188 - acc: 0.8207 - val_loss: 0.4284 - val_acc: 0.8194\n",
      "Epoch 110/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4160 - acc: 0.8219 - val_loss: 0.4274 - val_acc: 0.8177\n",
      "Training with parameters {'batch_size': 2500, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_32 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.8357 - acc: 0.4355 - val_loss: 0.7647 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.7389 - acc: 0.4408 - val_loss: 0.6918 - val_acc: 0.5238\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6918 - acc: 0.5262 - val_loss: 0.6593 - val_acc: 0.6026\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6755 - acc: 0.5645 - val_loss: 0.6435 - val_acc: 0.6141\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6658 - acc: 0.5731 - val_loss: 0.6305 - val_acc: 0.6141\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6493 - acc: 0.5826 - val_loss: 0.6177 - val_acc: 0.6355\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.6288 - acc: 0.6196 - val_loss: 0.6090 - val_acc: 0.7061\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6164 - acc: 0.6824 - val_loss: 0.6059 - val_acc: 0.6798\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.6055 - acc: 0.7024 - val_loss: 0.6045 - val_acc: 0.6650\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.5958 - acc: 0.6988 - val_loss: 0.5995 - val_acc: 0.6617\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5864 - acc: 0.6964 - val_loss: 0.5881 - val_acc: 0.6667\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.5756 - acc: 0.7134 - val_loss: 0.5739 - val_acc: 0.6864\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5664 - acc: 0.7280 - val_loss: 0.5606 - val_acc: 0.7126\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5566 - acc: 0.7409 - val_loss: 0.5499 - val_acc: 0.7241\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5482 - acc: 0.7477 - val_loss: 0.5423 - val_acc: 0.7209\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5393 - acc: 0.7522 - val_loss: 0.5378 - val_acc: 0.7176\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5316 - acc: 0.7475 - val_loss: 0.5331 - val_acc: 0.7176\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5266 - acc: 0.7541 - val_loss: 0.5224 - val_acc: 0.7307\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.5147 - acc: 0.7617 - val_loss: 0.5087 - val_acc: 0.7586\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5094 - acc: 0.7714 - val_loss: 0.4990 - val_acc: 0.7685\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5033 - acc: 0.7765 - val_loss: 0.4936 - val_acc: 0.7668\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4961 - acc: 0.7754 - val_loss: 0.4921 - val_acc: 0.7685\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4910 - acc: 0.7801 - val_loss: 0.4872 - val_acc: 0.7685\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4850 - acc: 0.7820 - val_loss: 0.4791 - val_acc: 0.7734\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4797 - acc: 0.7885 - val_loss: 0.4737 - val_acc: 0.7783\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4739 - acc: 0.7922 - val_loss: 0.4716 - val_acc: 0.7767\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4731 - acc: 0.7915 - val_loss: 0.4679 - val_acc: 0.7833\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4681 - acc: 0.7944 - val_loss: 0.4634 - val_acc: 0.7915\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4639 - acc: 0.7966 - val_loss: 0.4590 - val_acc: 0.7915\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4588 - acc: 0.7978 - val_loss: 0.4554 - val_acc: 0.7964\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4558 - acc: 0.7984 - val_loss: 0.4498 - val_acc: 0.8013\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4549 - acc: 0.8053 - val_loss: 0.4463 - val_acc: 0.8013\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4511 - acc: 0.8061 - val_loss: 0.4468 - val_acc: 0.7980\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4470 - acc: 0.8090 - val_loss: 0.4423 - val_acc: 0.8030\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4430 - acc: 0.8086 - val_loss: 0.4378 - val_acc: 0.8062\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4410 - acc: 0.8092 - val_loss: 0.4377 - val_acc: 0.8062\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4400 - acc: 0.8112 - val_loss: 0.4379 - val_acc: 0.8013\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4370 - acc: 0.8143 - val_loss: 0.4338 - val_acc: 0.8030\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.4332 - acc: 0.8128 - val_loss: 0.4313 - val_acc: 0.8079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.4286 - acc: 0.8119 - val_loss: 0.4284 - val_acc: 0.8079\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.4275 - acc: 0.8170 - val_loss: 0.4256 - val_acc: 0.8095\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.4309 - acc: 0.8152 - val_loss: 0.4279 - val_acc: 0.8095\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.4261 - acc: 0.8223 - val_loss: 0.4258 - val_acc: 0.8079\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.4235 - acc: 0.8165 - val_loss: 0.4229 - val_acc: 0.8112\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.4211 - acc: 0.8205 - val_loss: 0.4260 - val_acc: 0.8046\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.4202 - acc: 0.8163 - val_loss: 0.4245 - val_acc: 0.8079\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.4189 - acc: 0.8219 - val_loss: 0.4200 - val_acc: 0.8128\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.4174 - acc: 0.8183 - val_loss: 0.4188 - val_acc: 0.8144\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.4133 - acc: 0.8248 - val_loss: 0.4210 - val_acc: 0.8079\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.4170 - acc: 0.8166 - val_loss: 0.4216 - val_acc: 0.8128\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.4128 - acc: 0.8216 - val_loss: 0.4187 - val_acc: 0.8079\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.4126 - acc: 0.8217 - val_loss: 0.4185 - val_acc: 0.8079\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.4097 - acc: 0.8216 - val_loss: 0.4194 - val_acc: 0.8112\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.4098 - acc: 0.8223 - val_loss: 0.4132 - val_acc: 0.8177\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.4102 - acc: 0.8258 - val_loss: 0.4119 - val_acc: 0.8210\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.4082 - acc: 0.8245 - val_loss: 0.4156 - val_acc: 0.8161\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.4018 - acc: 0.8307 - val_loss: 0.4155 - val_acc: 0.8144\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.4037 - acc: 0.8281 - val_loss: 0.4116 - val_acc: 0.8144\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.4034 - acc: 0.8301 - val_loss: 0.4151 - val_acc: 0.8144\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.3996 - acc: 0.8270 - val_loss: 0.4184 - val_acc: 0.8161\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.3994 - acc: 0.8248 - val_loss: 0.4100 - val_acc: 0.8144\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.4025 - acc: 0.8265 - val_loss: 0.4087 - val_acc: 0.8177\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.3950 - acc: 0.8292 - val_loss: 0.4176 - val_acc: 0.8177\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.3960 - acc: 0.8321 - val_loss: 0.4125 - val_acc: 0.8177\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.3950 - acc: 0.8316 - val_loss: 0.4099 - val_acc: 0.8177\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.3925 - acc: 0.8323 - val_loss: 0.4174 - val_acc: 0.8161\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.3926 - acc: 0.8343 - val_loss: 0.4097 - val_acc: 0.8194\n",
      "Training with parameters {'batch_size': 2500, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_33 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.8663 - acc: 0.4350 - val_loss: 0.8278 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.7916 - acc: 0.4373 - val_loss: 0.7616 - val_acc: 0.3875\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.7391 - acc: 0.4424 - val_loss: 0.7166 - val_acc: 0.4023\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.7062 - acc: 0.4742 - val_loss: 0.6863 - val_acc: 0.5517\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6838 - acc: 0.5672 - val_loss: 0.6663 - val_acc: 0.6601\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6710 - acc: 0.6083 - val_loss: 0.6519 - val_acc: 0.6765\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6608 - acc: 0.6280 - val_loss: 0.6409 - val_acc: 0.6847\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6518 - acc: 0.6417 - val_loss: 0.6316 - val_acc: 0.7159\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6403 - acc: 0.6689 - val_loss: 0.6237 - val_acc: 0.7389\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6317 - acc: 0.6820 - val_loss: 0.6172 - val_acc: 0.7323\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6215 - acc: 0.7074 - val_loss: 0.6117 - val_acc: 0.7110\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6158 - acc: 0.7147 - val_loss: 0.6071 - val_acc: 0.6962\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.6075 - acc: 0.7110 - val_loss: 0.6023 - val_acc: 0.6962\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5997 - acc: 0.7097 - val_loss: 0.5969 - val_acc: 0.6995\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.5959 - acc: 0.7110 - val_loss: 0.5910 - val_acc: 0.7011\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5891 - acc: 0.7119 - val_loss: 0.5847 - val_acc: 0.7028\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5826 - acc: 0.7230 - val_loss: 0.5782 - val_acc: 0.7077\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5779 - acc: 0.7223 - val_loss: 0.5722 - val_acc: 0.7110\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.5681 - acc: 0.7435 - val_loss: 0.5672 - val_acc: 0.7094\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.5651 - acc: 0.7345 - val_loss: 0.5624 - val_acc: 0.7061\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.5592 - acc: 0.7458 - val_loss: 0.5577 - val_acc: 0.7077\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5547 - acc: 0.7375 - val_loss: 0.5519 - val_acc: 0.7126\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5460 - acc: 0.7458 - val_loss: 0.5450 - val_acc: 0.7110\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5439 - acc: 0.7488 - val_loss: 0.5389 - val_acc: 0.7126\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.5402 - acc: 0.7510 - val_loss: 0.5333 - val_acc: 0.7225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.5321 - acc: 0.7594 - val_loss: 0.5278 - val_acc: 0.7307\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5297 - acc: 0.7597 - val_loss: 0.5221 - val_acc: 0.7323\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.5239 - acc: 0.7594 - val_loss: 0.5177 - val_acc: 0.7422\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5191 - acc: 0.7683 - val_loss: 0.5143 - val_acc: 0.7422\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5162 - acc: 0.7634 - val_loss: 0.5120 - val_acc: 0.7422\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5133 - acc: 0.7707 - val_loss: 0.5084 - val_acc: 0.7406\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5071 - acc: 0.7674 - val_loss: 0.5028 - val_acc: 0.7504\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.5080 - acc: 0.7705 - val_loss: 0.4981 - val_acc: 0.7521\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4982 - acc: 0.7809 - val_loss: 0.4944 - val_acc: 0.7504\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4994 - acc: 0.7800 - val_loss: 0.4911 - val_acc: 0.7537\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4950 - acc: 0.7743 - val_loss: 0.4871 - val_acc: 0.7537\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4919 - acc: 0.7794 - val_loss: 0.4836 - val_acc: 0.7652\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4872 - acc: 0.7809 - val_loss: 0.4810 - val_acc: 0.7685\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4827 - acc: 0.7851 - val_loss: 0.4772 - val_acc: 0.7734\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4796 - acc: 0.7833 - val_loss: 0.4732 - val_acc: 0.7767\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4782 - acc: 0.7885 - val_loss: 0.4707 - val_acc: 0.7767\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4749 - acc: 0.7865 - val_loss: 0.4693 - val_acc: 0.7767\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4773 - acc: 0.7843 - val_loss: 0.4670 - val_acc: 0.7783\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4715 - acc: 0.7889 - val_loss: 0.4643 - val_acc: 0.7816\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4673 - acc: 0.7911 - val_loss: 0.4602 - val_acc: 0.7800\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4672 - acc: 0.7947 - val_loss: 0.4585 - val_acc: 0.7800\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4642 - acc: 0.7977 - val_loss: 0.4593 - val_acc: 0.7849\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4604 - acc: 0.8011 - val_loss: 0.4576 - val_acc: 0.7915\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4603 - acc: 0.8000 - val_loss: 0.4535 - val_acc: 0.7931\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4604 - acc: 0.7984 - val_loss: 0.4520 - val_acc: 0.7931\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4581 - acc: 0.7988 - val_loss: 0.4505 - val_acc: 0.7915\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4551 - acc: 0.8033 - val_loss: 0.4501 - val_acc: 0.7947\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4533 - acc: 0.8039 - val_loss: 0.4496 - val_acc: 0.7931\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4536 - acc: 0.8044 - val_loss: 0.4452 - val_acc: 0.7964\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4495 - acc: 0.8053 - val_loss: 0.4432 - val_acc: 0.7964\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4491 - acc: 0.8039 - val_loss: 0.4446 - val_acc: 0.7980\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4449 - acc: 0.8030 - val_loss: 0.4437 - val_acc: 0.7997\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4462 - acc: 0.8026 - val_loss: 0.4408 - val_acc: 0.7980\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4422 - acc: 0.8051 - val_loss: 0.4385 - val_acc: 0.7980\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4373 - acc: 0.8086 - val_loss: 0.4371 - val_acc: 0.7980\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4416 - acc: 0.8035 - val_loss: 0.4374 - val_acc: 0.8030\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4363 - acc: 0.8093 - val_loss: 0.4364 - val_acc: 0.8046\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4320 - acc: 0.8130 - val_loss: 0.4350 - val_acc: 0.8062\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4316 - acc: 0.8115 - val_loss: 0.4327 - val_acc: 0.8046\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.4370 - acc: 0.8130 - val_loss: 0.4325 - val_acc: 0.8062\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4323 - acc: 0.8117 - val_loss: 0.4328 - val_acc: 0.8062\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4341 - acc: 0.8152 - val_loss: 0.4303 - val_acc: 0.8079\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4337 - acc: 0.8099 - val_loss: 0.4305 - val_acc: 0.8079\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4295 - acc: 0.8148 - val_loss: 0.4302 - val_acc: 0.8079\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4277 - acc: 0.8148 - val_loss: 0.4272 - val_acc: 0.8095\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4304 - acc: 0.8132 - val_loss: 0.4264 - val_acc: 0.8112\n",
      "Epoch 72/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4271 - acc: 0.8150 - val_loss: 0.4301 - val_acc: 0.8062\n",
      "Epoch 73/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4259 - acc: 0.8134 - val_loss: 0.4296 - val_acc: 0.8062\n",
      "Epoch 74/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4250 - acc: 0.8157 - val_loss: 0.4252 - val_acc: 0.8095\n",
      "Epoch 75/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4222 - acc: 0.8168 - val_loss: 0.4219 - val_acc: 0.8128\n",
      "Epoch 76/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4228 - acc: 0.8186 - val_loss: 0.4233 - val_acc: 0.8112\n",
      "Epoch 77/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4239 - acc: 0.8137 - val_loss: 0.4285 - val_acc: 0.8079\n",
      "Epoch 78/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4200 - acc: 0.8159 - val_loss: 0.4246 - val_acc: 0.8095\n",
      "Epoch 79/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4190 - acc: 0.8205 - val_loss: 0.4216 - val_acc: 0.8112\n",
      "Epoch 80/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4183 - acc: 0.8205 - val_loss: 0.4233 - val_acc: 0.8079\n",
      "Epoch 81/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4192 - acc: 0.8185 - val_loss: 0.4244 - val_acc: 0.8095\n",
      "Epoch 82/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4163 - acc: 0.8194 - val_loss: 0.4215 - val_acc: 0.8144\n",
      "Epoch 83/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4155 - acc: 0.8181 - val_loss: 0.4231 - val_acc: 0.8128\n",
      "Epoch 84/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4187 - acc: 0.8186 - val_loss: 0.4256 - val_acc: 0.8112\n",
      "Epoch 85/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4140 - acc: 0.8181 - val_loss: 0.4209 - val_acc: 0.8161\n",
      "Epoch 86/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4126 - acc: 0.8210 - val_loss: 0.4164 - val_acc: 0.8194\n",
      "Epoch 87/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4154 - acc: 0.8208 - val_loss: 0.4157 - val_acc: 0.8177\n",
      "Epoch 88/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4124 - acc: 0.8217 - val_loss: 0.4196 - val_acc: 0.8161\n",
      "Epoch 89/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4094 - acc: 0.8236 - val_loss: 0.4217 - val_acc: 0.8095\n",
      "Epoch 90/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4096 - acc: 0.8248 - val_loss: 0.4170 - val_acc: 0.8177\n",
      "Epoch 91/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4121 - acc: 0.8197 - val_loss: 0.4150 - val_acc: 0.8161\n",
      "Epoch 92/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4111 - acc: 0.8210 - val_loss: 0.4175 - val_acc: 0.8194\n",
      "Epoch 93/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4096 - acc: 0.8201 - val_loss: 0.4212 - val_acc: 0.8112\n",
      "Epoch 94/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4061 - acc: 0.8212 - val_loss: 0.4195 - val_acc: 0.8144\n",
      "Epoch 95/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4052 - acc: 0.8267 - val_loss: 0.4146 - val_acc: 0.8227\n",
      "Epoch 96/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4095 - acc: 0.8243 - val_loss: 0.4132 - val_acc: 0.8161\n",
      "Epoch 97/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4052 - acc: 0.8269 - val_loss: 0.4155 - val_acc: 0.8227\n",
      "Epoch 98/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4055 - acc: 0.8214 - val_loss: 0.4184 - val_acc: 0.8161\n",
      "Epoch 99/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4025 - acc: 0.8281 - val_loss: 0.4157 - val_acc: 0.8243\n",
      "Epoch 100/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3994 - acc: 0.8281 - val_loss: 0.4133 - val_acc: 0.8177\n",
      "Epoch 101/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4042 - acc: 0.8290 - val_loss: 0.4152 - val_acc: 0.8243\n",
      "Training with parameters {'batch_size': 2500, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_34 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.7616 - acc: 0.4353 - val_loss: 0.7510 - val_acc: 0.3892\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.7254 - acc: 0.4461 - val_loss: 0.7149 - val_acc: 0.4319\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6985 - acc: 0.4899 - val_loss: 0.6882 - val_acc: 0.5255\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6814 - acc: 0.5694 - val_loss: 0.6692 - val_acc: 0.6601\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6695 - acc: 0.6229 - val_loss: 0.6557 - val_acc: 0.6814\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6601 - acc: 0.6293 - val_loss: 0.6458 - val_acc: 0.6568\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6521 - acc: 0.6335 - val_loss: 0.6379 - val_acc: 0.6486\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6461 - acc: 0.6351 - val_loss: 0.6310 - val_acc: 0.6617\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6401 - acc: 0.6439 - val_loss: 0.6249 - val_acc: 0.6913\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6332 - acc: 0.6608 - val_loss: 0.6194 - val_acc: 0.6929\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6255 - acc: 0.6831 - val_loss: 0.6147 - val_acc: 0.7126\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6179 - acc: 0.6990 - val_loss: 0.6105 - val_acc: 0.7077\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6111 - acc: 0.7066 - val_loss: 0.6065 - val_acc: 0.6962\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6041 - acc: 0.7130 - val_loss: 0.6024 - val_acc: 0.6864\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.5986 - acc: 0.7048 - val_loss: 0.5972 - val_acc: 0.6929\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.5938 - acc: 0.7179 - val_loss: 0.5911 - val_acc: 0.6946\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5858 - acc: 0.7238 - val_loss: 0.5848 - val_acc: 0.7077\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5803 - acc: 0.7283 - val_loss: 0.5790 - val_acc: 0.7143\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.5773 - acc: 0.7272 - val_loss: 0.5733 - val_acc: 0.7159\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5706 - acc: 0.7331 - val_loss: 0.5681 - val_acc: 0.7176\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.5624 - acc: 0.7400 - val_loss: 0.5635 - val_acc: 0.7241\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5587 - acc: 0.7417 - val_loss: 0.5585 - val_acc: 0.7241\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5529 - acc: 0.7398 - val_loss: 0.5532 - val_acc: 0.7307\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5476 - acc: 0.7491 - val_loss: 0.5471 - val_acc: 0.7323\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5405 - acc: 0.7546 - val_loss: 0.5411 - val_acc: 0.7389\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5382 - acc: 0.7575 - val_loss: 0.5360 - val_acc: 0.7406\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5342 - acc: 0.7537 - val_loss: 0.5311 - val_acc: 0.7438\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5240 - acc: 0.7603 - val_loss: 0.5266 - val_acc: 0.7406\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5215 - acc: 0.7617 - val_loss: 0.5224 - val_acc: 0.7438\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5157 - acc: 0.7612 - val_loss: 0.5177 - val_acc: 0.7488\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.5110 - acc: 0.7710 - val_loss: 0.5113 - val_acc: 0.7504\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5066 - acc: 0.7696 - val_loss: 0.5045 - val_acc: 0.7619\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5048 - acc: 0.7718 - val_loss: 0.4996 - val_acc: 0.7668\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4993 - acc: 0.7758 - val_loss: 0.4966 - val_acc: 0.7685\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4966 - acc: 0.7780 - val_loss: 0.4947 - val_acc: 0.7635\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4882 - acc: 0.7761 - val_loss: 0.4913 - val_acc: 0.7718\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4876 - acc: 0.7842 - val_loss: 0.4859 - val_acc: 0.7750\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4871 - acc: 0.7778 - val_loss: 0.4817 - val_acc: 0.7783\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4838 - acc: 0.7829 - val_loss: 0.4783 - val_acc: 0.7833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4803 - acc: 0.7838 - val_loss: 0.4758 - val_acc: 0.7833\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4790 - acc: 0.7842 - val_loss: 0.4738 - val_acc: 0.7865\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4741 - acc: 0.7920 - val_loss: 0.4735 - val_acc: 0.7800\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4740 - acc: 0.7860 - val_loss: 0.4742 - val_acc: 0.7783\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4692 - acc: 0.7924 - val_loss: 0.4715 - val_acc: 0.7833\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4711 - acc: 0.7893 - val_loss: 0.4665 - val_acc: 0.7915\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4665 - acc: 0.7929 - val_loss: 0.4618 - val_acc: 0.7947\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4633 - acc: 0.7969 - val_loss: 0.4592 - val_acc: 0.7964\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4617 - acc: 0.7949 - val_loss: 0.4597 - val_acc: 0.7931\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4578 - acc: 0.7904 - val_loss: 0.4620 - val_acc: 0.7947\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4558 - acc: 0.8017 - val_loss: 0.4584 - val_acc: 0.7947\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4571 - acc: 0.7955 - val_loss: 0.4542 - val_acc: 0.7964\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4518 - acc: 0.8070 - val_loss: 0.4498 - val_acc: 0.7997\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4510 - acc: 0.8048 - val_loss: 0.4482 - val_acc: 0.8013\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4494 - acc: 0.8039 - val_loss: 0.4502 - val_acc: 0.7980\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4494 - acc: 0.7995 - val_loss: 0.4519 - val_acc: 0.7980\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4476 - acc: 0.8072 - val_loss: 0.4486 - val_acc: 0.7997\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4465 - acc: 0.8037 - val_loss: 0.4454 - val_acc: 0.8013\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4429 - acc: 0.8066 - val_loss: 0.4432 - val_acc: 0.8046\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4415 - acc: 0.8046 - val_loss: 0.4417 - val_acc: 0.8079\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4433 - acc: 0.8075 - val_loss: 0.4420 - val_acc: 0.8079\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4371 - acc: 0.8073 - val_loss: 0.4424 - val_acc: 0.8062\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4394 - acc: 0.8121 - val_loss: 0.4412 - val_acc: 0.8062\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4374 - acc: 0.8075 - val_loss: 0.4400 - val_acc: 0.8030\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4381 - acc: 0.8101 - val_loss: 0.4428 - val_acc: 0.8030\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4348 - acc: 0.8097 - val_loss: 0.4457 - val_acc: 0.8030\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4349 - acc: 0.8108 - val_loss: 0.4420 - val_acc: 0.8030\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.4323 - acc: 0.8121 - val_loss: 0.4367 - val_acc: 0.8079\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4299 - acc: 0.8159 - val_loss: 0.4347 - val_acc: 0.8079\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4312 - acc: 0.8101 - val_loss: 0.4361 - val_acc: 0.8062\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4308 - acc: 0.8124 - val_loss: 0.4373 - val_acc: 0.8079\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4279 - acc: 0.8139 - val_loss: 0.4333 - val_acc: 0.8062\n",
      "Epoch 72/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4264 - acc: 0.8155 - val_loss: 0.4322 - val_acc: 0.8112\n",
      "Epoch 73/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4234 - acc: 0.8168 - val_loss: 0.4357 - val_acc: 0.8079\n",
      "Epoch 74/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.4248 - acc: 0.8159 - val_loss: 0.4353 - val_acc: 0.8062\n",
      "Epoch 75/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4252 - acc: 0.8152 - val_loss: 0.4319 - val_acc: 0.8079\n",
      "Epoch 76/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4257 - acc: 0.8112 - val_loss: 0.4291 - val_acc: 0.8095\n",
      "Epoch 77/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4230 - acc: 0.8163 - val_loss: 0.4292 - val_acc: 0.8095\n",
      "Epoch 78/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4240 - acc: 0.8155 - val_loss: 0.4320 - val_acc: 0.8095\n",
      "Epoch 79/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4240 - acc: 0.8174 - val_loss: 0.4333 - val_acc: 0.8079\n",
      "Epoch 80/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4195 - acc: 0.8168 - val_loss: 0.4315 - val_acc: 0.8095\n",
      "Epoch 81/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4200 - acc: 0.8186 - val_loss: 0.4293 - val_acc: 0.8112\n",
      "Training with parameters {'batch_size': 2500, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_35 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.8302 - acc: 0.4355 - val_loss: 0.7910 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.7629 - acc: 0.4404 - val_loss: 0.7279 - val_acc: 0.3924\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.7171 - acc: 0.4671 - val_loss: 0.6858 - val_acc: 0.5780\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6874 - acc: 0.5486 - val_loss: 0.6599 - val_acc: 0.6404\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6690 - acc: 0.6083 - val_loss: 0.6440 - val_acc: 0.6256\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6588 - acc: 0.5992 - val_loss: 0.6330 - val_acc: 0.6190\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6496 - acc: 0.5964 - val_loss: 0.6236 - val_acc: 0.6240\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6420 - acc: 0.6068 - val_loss: 0.6149 - val_acc: 0.6371\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6282 - acc: 0.6411 - val_loss: 0.6075 - val_acc: 0.6995\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6182 - acc: 0.6856 - val_loss: 0.6023 - val_acc: 0.7307\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.6088 - acc: 0.7074 - val_loss: 0.5986 - val_acc: 0.7011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.6027 - acc: 0.7017 - val_loss: 0.5947 - val_acc: 0.6946\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.5949 - acc: 0.7004 - val_loss: 0.5894 - val_acc: 0.6946\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5859 - acc: 0.7084 - val_loss: 0.5816 - val_acc: 0.6995\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.5803 - acc: 0.7070 - val_loss: 0.5731 - val_acc: 0.7028\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.5709 - acc: 0.7225 - val_loss: 0.5648 - val_acc: 0.7176\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5670 - acc: 0.7251 - val_loss: 0.5575 - val_acc: 0.7192\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5584 - acc: 0.7329 - val_loss: 0.5504 - val_acc: 0.7225\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5538 - acc: 0.7426 - val_loss: 0.5435 - val_acc: 0.7258\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5472 - acc: 0.7424 - val_loss: 0.5373 - val_acc: 0.7307\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5404 - acc: 0.7464 - val_loss: 0.5312 - val_acc: 0.7291\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5321 - acc: 0.7491 - val_loss: 0.5253 - val_acc: 0.7373\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.5278 - acc: 0.7513 - val_loss: 0.5177 - val_acc: 0.7406\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.5231 - acc: 0.7617 - val_loss: 0.5090 - val_acc: 0.7438\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5141 - acc: 0.7676 - val_loss: 0.5022 - val_acc: 0.7537\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5060 - acc: 0.7719 - val_loss: 0.4977 - val_acc: 0.7537\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.5024 - acc: 0.7703 - val_loss: 0.4929 - val_acc: 0.7553\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4939 - acc: 0.7772 - val_loss: 0.4866 - val_acc: 0.7619\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4936 - acc: 0.7772 - val_loss: 0.4802 - val_acc: 0.7685\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.4893 - acc: 0.7803 - val_loss: 0.4766 - val_acc: 0.7718\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.4830 - acc: 0.7807 - val_loss: 0.4725 - val_acc: 0.7750\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.4801 - acc: 0.7869 - val_loss: 0.4658 - val_acc: 0.7800\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4742 - acc: 0.7891 - val_loss: 0.4623 - val_acc: 0.7865\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4722 - acc: 0.7898 - val_loss: 0.4605 - val_acc: 0.7849\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4682 - acc: 0.7867 - val_loss: 0.4577 - val_acc: 0.7865\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4662 - acc: 0.7874 - val_loss: 0.4517 - val_acc: 0.7931\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4619 - acc: 0.7957 - val_loss: 0.4487 - val_acc: 0.7980\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4577 - acc: 0.7980 - val_loss: 0.4487 - val_acc: 0.7947\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4560 - acc: 0.7968 - val_loss: 0.4485 - val_acc: 0.7947\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.4524 - acc: 0.7993 - val_loss: 0.4443 - val_acc: 0.7997\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.4504 - acc: 0.8002 - val_loss: 0.4413 - val_acc: 0.8030\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.4480 - acc: 0.8022 - val_loss: 0.4362 - val_acc: 0.8062\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4460 - acc: 0.8062 - val_loss: 0.4345 - val_acc: 0.8062\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.4418 - acc: 0.8092 - val_loss: 0.4390 - val_acc: 0.8013\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.4461 - acc: 0.8020 - val_loss: 0.4376 - val_acc: 0.8013\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4414 - acc: 0.8064 - val_loss: 0.4288 - val_acc: 0.8112\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.4400 - acc: 0.8072 - val_loss: 0.4276 - val_acc: 0.8112\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.4359 - acc: 0.8124 - val_loss: 0.4304 - val_acc: 0.8062\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4362 - acc: 0.8073 - val_loss: 0.4295 - val_acc: 0.8062\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4334 - acc: 0.8106 - val_loss: 0.4266 - val_acc: 0.8062\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4316 - acc: 0.8110 - val_loss: 0.4259 - val_acc: 0.8062\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4332 - acc: 0.8073 - val_loss: 0.4261 - val_acc: 0.8062\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.4305 - acc: 0.8121 - val_loss: 0.4250 - val_acc: 0.8079\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.4267 - acc: 0.8123 - val_loss: 0.4247 - val_acc: 0.8095\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4237 - acc: 0.8139 - val_loss: 0.4258 - val_acc: 0.8112\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4248 - acc: 0.8168 - val_loss: 0.4240 - val_acc: 0.8095\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4237 - acc: 0.8177 - val_loss: 0.4197 - val_acc: 0.8112\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4211 - acc: 0.8146 - val_loss: 0.4209 - val_acc: 0.8079\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4226 - acc: 0.8181 - val_loss: 0.4185 - val_acc: 0.8128\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4202 - acc: 0.8161 - val_loss: 0.4165 - val_acc: 0.8161\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4199 - acc: 0.8163 - val_loss: 0.4168 - val_acc: 0.8128\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4156 - acc: 0.8228 - val_loss: 0.4154 - val_acc: 0.8161\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4181 - acc: 0.8179 - val_loss: 0.4178 - val_acc: 0.8144\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4146 - acc: 0.8179 - val_loss: 0.4184 - val_acc: 0.8128\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4125 - acc: 0.8192 - val_loss: 0.4136 - val_acc: 0.8144\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4114 - acc: 0.8205 - val_loss: 0.4113 - val_acc: 0.8194\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4113 - acc: 0.8203 - val_loss: 0.4162 - val_acc: 0.8144\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4105 - acc: 0.8196 - val_loss: 0.4133 - val_acc: 0.8144\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4076 - acc: 0.8228 - val_loss: 0.4103 - val_acc: 0.8161\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4062 - acc: 0.8203 - val_loss: 0.4195 - val_acc: 0.8112\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4100 - acc: 0.8194 - val_loss: 0.4173 - val_acc: 0.8144\n",
      "Epoch 72/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4063 - acc: 0.8239 - val_loss: 0.4096 - val_acc: 0.8210\n",
      "Epoch 73/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4050 - acc: 0.8285 - val_loss: 0.4114 - val_acc: 0.8177\n",
      "Epoch 74/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4038 - acc: 0.8267 - val_loss: 0.4158 - val_acc: 0.8128\n",
      "Epoch 75/300\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4044 - acc: 0.8238 - val_loss: 0.4120 - val_acc: 0.8194\n",
      "Epoch 76/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4019 - acc: 0.8250 - val_loss: 0.4119 - val_acc: 0.8194\n",
      "Epoch 77/300\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4041 - acc: 0.8236 - val_loss: 0.4131 - val_acc: 0.8177\n",
      "Training with parameters {'batch_size': 3000, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_36 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 1s 135ms/step - loss: 0.7719 - acc: 0.4355 - val_loss: 0.7313 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.7172 - acc: 0.4501 - val_loss: 0.6846 - val_acc: 0.5928\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6881 - acc: 0.5375 - val_loss: 0.6594 - val_acc: 0.6092\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6730 - acc: 0.5740 - val_loss: 0.6459 - val_acc: 0.6125\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6637 - acc: 0.5733 - val_loss: 0.6359 - val_acc: 0.6108\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6567 - acc: 0.5727 - val_loss: 0.6261 - val_acc: 0.6108\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6433 - acc: 0.5818 - val_loss: 0.6166 - val_acc: 0.6289\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6305 - acc: 0.6236 - val_loss: 0.6090 - val_acc: 0.7274\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6159 - acc: 0.6791 - val_loss: 0.6044 - val_acc: 0.7061\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6077 - acc: 0.7006 - val_loss: 0.6020 - val_acc: 0.6732\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6003 - acc: 0.7006 - val_loss: 0.5996 - val_acc: 0.6585\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5942 - acc: 0.6982 - val_loss: 0.5945 - val_acc: 0.6568\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5869 - acc: 0.7004 - val_loss: 0.5860 - val_acc: 0.6749\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5786 - acc: 0.7086 - val_loss: 0.5756 - val_acc: 0.6880\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5689 - acc: 0.7282 - val_loss: 0.5656 - val_acc: 0.7028\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5620 - acc: 0.7340 - val_loss: 0.5566 - val_acc: 0.7258\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5562 - acc: 0.7424 - val_loss: 0.5493 - val_acc: 0.7323\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5485 - acc: 0.7460 - val_loss: 0.5432 - val_acc: 0.7389\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5399 - acc: 0.7542 - val_loss: 0.5379 - val_acc: 0.7340\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5324 - acc: 0.7511 - val_loss: 0.5328 - val_acc: 0.7323\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5278 - acc: 0.7548 - val_loss: 0.5274 - val_acc: 0.7323\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5186 - acc: 0.7584 - val_loss: 0.5200 - val_acc: 0.7521\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5127 - acc: 0.7670 - val_loss: 0.5114 - val_acc: 0.7619\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5081 - acc: 0.7699 - val_loss: 0.5037 - val_acc: 0.7668\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5024 - acc: 0.7734 - val_loss: 0.4974 - val_acc: 0.7718\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4988 - acc: 0.7785 - val_loss: 0.4928 - val_acc: 0.7800\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4926 - acc: 0.7800 - val_loss: 0.4896 - val_acc: 0.7833\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4857 - acc: 0.7843 - val_loss: 0.4861 - val_acc: 0.7833\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4817 - acc: 0.7869 - val_loss: 0.4804 - val_acc: 0.7800\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4757 - acc: 0.7893 - val_loss: 0.4751 - val_acc: 0.7865\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4752 - acc: 0.7926 - val_loss: 0.4710 - val_acc: 0.7882\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4689 - acc: 0.7968 - val_loss: 0.4675 - val_acc: 0.7865\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4660 - acc: 0.7916 - val_loss: 0.4649 - val_acc: 0.7882\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4612 - acc: 0.7968 - val_loss: 0.4609 - val_acc: 0.7882\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4575 - acc: 0.7951 - val_loss: 0.4571 - val_acc: 0.7898\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4559 - acc: 0.7968 - val_loss: 0.4535 - val_acc: 0.7915\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4530 - acc: 0.8030 - val_loss: 0.4506 - val_acc: 0.7964\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4500 - acc: 0.8037 - val_loss: 0.4475 - val_acc: 0.7980\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4456 - acc: 0.8079 - val_loss: 0.4431 - val_acc: 0.8030\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4427 - acc: 0.8051 - val_loss: 0.4419 - val_acc: 0.7997\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4408 - acc: 0.8059 - val_loss: 0.4400 - val_acc: 0.8030\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4359 - acc: 0.8104 - val_loss: 0.4385 - val_acc: 0.8046\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4354 - acc: 0.8106 - val_loss: 0.4355 - val_acc: 0.8079\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4351 - acc: 0.8130 - val_loss: 0.4338 - val_acc: 0.8079\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4304 - acc: 0.8154 - val_loss: 0.4329 - val_acc: 0.8046\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4310 - acc: 0.8119 - val_loss: 0.4305 - val_acc: 0.8079\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4288 - acc: 0.8179 - val_loss: 0.4276 - val_acc: 0.8112\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4234 - acc: 0.8166 - val_loss: 0.4265 - val_acc: 0.8112\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4242 - acc: 0.8170 - val_loss: 0.4263 - val_acc: 0.8095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4222 - acc: 0.8150 - val_loss: 0.4242 - val_acc: 0.8095\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4197 - acc: 0.8212 - val_loss: 0.4233 - val_acc: 0.8095\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4180 - acc: 0.8196 - val_loss: 0.4220 - val_acc: 0.8128\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4182 - acc: 0.8185 - val_loss: 0.4213 - val_acc: 0.8144\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4135 - acc: 0.8207 - val_loss: 0.4182 - val_acc: 0.8177\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4115 - acc: 0.8205 - val_loss: 0.4197 - val_acc: 0.8144\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4108 - acc: 0.8223 - val_loss: 0.4212 - val_acc: 0.8177\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4098 - acc: 0.8219 - val_loss: 0.4171 - val_acc: 0.8144\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4078 - acc: 0.8223 - val_loss: 0.4158 - val_acc: 0.8177\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4053 - acc: 0.8292 - val_loss: 0.4160 - val_acc: 0.8161\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4034 - acc: 0.8267 - val_loss: 0.4177 - val_acc: 0.8177\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4058 - acc: 0.8254 - val_loss: 0.4154 - val_acc: 0.8161\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4020 - acc: 0.8263 - val_loss: 0.4120 - val_acc: 0.8227\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4003 - acc: 0.8296 - val_loss: 0.4132 - val_acc: 0.8177\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4021 - acc: 0.8250 - val_loss: 0.4156 - val_acc: 0.8210\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3983 - acc: 0.8278 - val_loss: 0.4118 - val_acc: 0.8210\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3973 - acc: 0.8283 - val_loss: 0.4105 - val_acc: 0.8227\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3967 - acc: 0.8343 - val_loss: 0.4129 - val_acc: 0.8210\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3934 - acc: 0.8318 - val_loss: 0.4125 - val_acc: 0.8210\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3924 - acc: 0.8327 - val_loss: 0.4099 - val_acc: 0.8194\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3915 - acc: 0.8316 - val_loss: 0.4099 - val_acc: 0.8194\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3894 - acc: 0.8338 - val_loss: 0.4115 - val_acc: 0.8227\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3883 - acc: 0.8336 - val_loss: 0.4100 - val_acc: 0.8210\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3885 - acc: 0.8311 - val_loss: 0.4087 - val_acc: 0.8194\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3853 - acc: 0.8351 - val_loss: 0.4091 - val_acc: 0.8227\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3847 - acc: 0.8365 - val_loss: 0.4107 - val_acc: 0.8194\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3863 - acc: 0.8347 - val_loss: 0.4086 - val_acc: 0.8227\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3827 - acc: 0.8387 - val_loss: 0.4077 - val_acc: 0.8210\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3796 - acc: 0.8373 - val_loss: 0.4090 - val_acc: 0.8194\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3793 - acc: 0.8389 - val_loss: 0.4089 - val_acc: 0.8210\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3793 - acc: 0.8378 - val_loss: 0.4084 - val_acc: 0.8210\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3772 - acc: 0.8402 - val_loss: 0.4098 - val_acc: 0.8194\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3765 - acc: 0.8373 - val_loss: 0.4080 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': 3000, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_37 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.8393 - acc: 0.4333 - val_loss: 0.8175 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.7865 - acc: 0.4432 - val_loss: 0.7673 - val_acc: 0.3892\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.7485 - acc: 0.4448 - val_loss: 0.7276 - val_acc: 0.3924\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.7184 - acc: 0.4625 - val_loss: 0.6973 - val_acc: 0.4828\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6969 - acc: 0.5182 - val_loss: 0.6752 - val_acc: 0.6256\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.6825 - acc: 0.5703 - val_loss: 0.6595 - val_acc: 0.6355\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.6712 - acc: 0.6003 - val_loss: 0.6481 - val_acc: 0.6305\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.6621 - acc: 0.6090 - val_loss: 0.6394 - val_acc: 0.6190\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.6564 - acc: 0.5993 - val_loss: 0.6322 - val_acc: 0.6190\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.6507 - acc: 0.6006 - val_loss: 0.6257 - val_acc: 0.6223\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.6460 - acc: 0.6043 - val_loss: 0.6196 - val_acc: 0.6305\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.6376 - acc: 0.6189 - val_loss: 0.6139 - val_acc: 0.6420\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.6300 - acc: 0.6424 - val_loss: 0.6088 - val_acc: 0.6847\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.6225 - acc: 0.6732 - val_loss: 0.6044 - val_acc: 0.7274\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.6131 - acc: 0.7043 - val_loss: 0.6007 - val_acc: 0.7258\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.6089 - acc: 0.7105 - val_loss: 0.5975 - val_acc: 0.7077\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.6039 - acc: 0.7035 - val_loss: 0.5943 - val_acc: 0.6946\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.5969 - acc: 0.6997 - val_loss: 0.5909 - val_acc: 0.6979\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5943 - acc: 0.7001 - val_loss: 0.5869 - val_acc: 0.7028\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.5866 - acc: 0.7134 - val_loss: 0.5823 - val_acc: 0.7011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.5811 - acc: 0.7121 - val_loss: 0.5771 - val_acc: 0.7011\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.5754 - acc: 0.7194 - val_loss: 0.5715 - val_acc: 0.7061\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5716 - acc: 0.7201 - val_loss: 0.5656 - val_acc: 0.7126\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.5678 - acc: 0.7209 - val_loss: 0.5599 - val_acc: 0.7192\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5618 - acc: 0.7296 - val_loss: 0.5548 - val_acc: 0.7176\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5571 - acc: 0.7342 - val_loss: 0.5498 - val_acc: 0.7225\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.5506 - acc: 0.7367 - val_loss: 0.5451 - val_acc: 0.7225\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.5467 - acc: 0.7418 - val_loss: 0.5405 - val_acc: 0.7291\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.5412 - acc: 0.7440 - val_loss: 0.5359 - val_acc: 0.7274\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.5352 - acc: 0.7517 - val_loss: 0.5310 - val_acc: 0.7323\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5326 - acc: 0.7526 - val_loss: 0.5256 - val_acc: 0.7356\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.5226 - acc: 0.7614 - val_loss: 0.5199 - val_acc: 0.7389\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5214 - acc: 0.7546 - val_loss: 0.5142 - val_acc: 0.7422\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.5190 - acc: 0.7606 - val_loss: 0.5088 - val_acc: 0.7488\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5146 - acc: 0.7639 - val_loss: 0.5037 - val_acc: 0.7488\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5068 - acc: 0.7721 - val_loss: 0.4991 - val_acc: 0.7553\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5062 - acc: 0.7690 - val_loss: 0.4947 - val_acc: 0.7603\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5018 - acc: 0.7708 - val_loss: 0.4905 - val_acc: 0.7603\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4966 - acc: 0.7749 - val_loss: 0.4865 - val_acc: 0.7635\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4944 - acc: 0.7741 - val_loss: 0.4827 - val_acc: 0.7652\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4884 - acc: 0.7838 - val_loss: 0.4785 - val_acc: 0.7701\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4869 - acc: 0.7871 - val_loss: 0.4744 - val_acc: 0.7750\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4842 - acc: 0.7805 - val_loss: 0.4711 - val_acc: 0.7783\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.4755 - acc: 0.7865 - val_loss: 0.4688 - val_acc: 0.7783\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4786 - acc: 0.7843 - val_loss: 0.4672 - val_acc: 0.7800\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4715 - acc: 0.7871 - val_loss: 0.4637 - val_acc: 0.7783\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.4680 - acc: 0.7902 - val_loss: 0.4601 - val_acc: 0.7833\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4655 - acc: 0.7922 - val_loss: 0.4568 - val_acc: 0.7882\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4624 - acc: 0.7927 - val_loss: 0.4543 - val_acc: 0.7898\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4630 - acc: 0.7931 - val_loss: 0.4522 - val_acc: 0.7964\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.4586 - acc: 0.7984 - val_loss: 0.4499 - val_acc: 0.7980\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4566 - acc: 0.7984 - val_loss: 0.4479 - val_acc: 0.7997\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4543 - acc: 0.7937 - val_loss: 0.4471 - val_acc: 0.7980\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4554 - acc: 0.7969 - val_loss: 0.4446 - val_acc: 0.8013\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4499 - acc: 0.8011 - val_loss: 0.4418 - val_acc: 0.8046\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4524 - acc: 0.7986 - val_loss: 0.4392 - val_acc: 0.8095\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4489 - acc: 0.8037 - val_loss: 0.4378 - val_acc: 0.8079\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4447 - acc: 0.8015 - val_loss: 0.4375 - val_acc: 0.8030\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.4458 - acc: 0.8044 - val_loss: 0.4368 - val_acc: 0.8030\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4460 - acc: 0.8044 - val_loss: 0.4339 - val_acc: 0.8079\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4383 - acc: 0.8057 - val_loss: 0.4310 - val_acc: 0.8112\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4388 - acc: 0.8081 - val_loss: 0.4311 - val_acc: 0.8144\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4393 - acc: 0.8104 - val_loss: 0.4307 - val_acc: 0.8095\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4353 - acc: 0.8126 - val_loss: 0.4306 - val_acc: 0.8079\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.4339 - acc: 0.8082 - val_loss: 0.4272 - val_acc: 0.8144\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4344 - acc: 0.8097 - val_loss: 0.4258 - val_acc: 0.8128\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4299 - acc: 0.8124 - val_loss: 0.4258 - val_acc: 0.8112\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4307 - acc: 0.8086 - val_loss: 0.4272 - val_acc: 0.8079\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4298 - acc: 0.8139 - val_loss: 0.4262 - val_acc: 0.8095\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4282 - acc: 0.8139 - val_loss: 0.4233 - val_acc: 0.8095\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4283 - acc: 0.8126 - val_loss: 0.4208 - val_acc: 0.8128\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4253 - acc: 0.8143 - val_loss: 0.4208 - val_acc: 0.8128\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4230 - acc: 0.8099 - val_loss: 0.4225 - val_acc: 0.8095\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.4267 - acc: 0.8165 - val_loss: 0.4225 - val_acc: 0.8095\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4210 - acc: 0.8176 - val_loss: 0.4207 - val_acc: 0.8112\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4208 - acc: 0.8137 - val_loss: 0.4179 - val_acc: 0.8177\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4184 - acc: 0.8210 - val_loss: 0.4167 - val_acc: 0.8144\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4176 - acc: 0.8192 - val_loss: 0.4184 - val_acc: 0.8128\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.4185 - acc: 0.8186 - val_loss: 0.4188 - val_acc: 0.8161\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4185 - acc: 0.8183 - val_loss: 0.4189 - val_acc: 0.8144\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4155 - acc: 0.8154 - val_loss: 0.4157 - val_acc: 0.8161\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.4155 - acc: 0.8186 - val_loss: 0.4135 - val_acc: 0.8210\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4130 - acc: 0.8223 - val_loss: 0.4152 - val_acc: 0.8128\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.4110 - acc: 0.8216 - val_loss: 0.4189 - val_acc: 0.8144\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4109 - acc: 0.8197 - val_loss: 0.4164 - val_acc: 0.8177\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4113 - acc: 0.8192 - val_loss: 0.4124 - val_acc: 0.8210\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4103 - acc: 0.8199 - val_loss: 0.4113 - val_acc: 0.8210\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4100 - acc: 0.8250 - val_loss: 0.4137 - val_acc: 0.8144\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.4076 - acc: 0.8250 - val_loss: 0.4166 - val_acc: 0.8144\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.4107 - acc: 0.8208 - val_loss: 0.4141 - val_acc: 0.8144\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4021 - acc: 0.8238 - val_loss: 0.4113 - val_acc: 0.8177\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4067 - acc: 0.8245 - val_loss: 0.4109 - val_acc: 0.8194\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.4024 - acc: 0.8272 - val_loss: 0.4122 - val_acc: 0.8161\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.4026 - acc: 0.8214 - val_loss: 0.4136 - val_acc: 0.8128\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4016 - acc: 0.8256 - val_loss: 0.4130 - val_acc: 0.8128\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4027 - acc: 0.8227 - val_loss: 0.4108 - val_acc: 0.8210\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.3997 - acc: 0.8272 - val_loss: 0.4102 - val_acc: 0.8194\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4016 - acc: 0.8223 - val_loss: 0.4116 - val_acc: 0.8194\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.3979 - acc: 0.8239 - val_loss: 0.4136 - val_acc: 0.8128\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3986 - acc: 0.8263 - val_loss: 0.4117 - val_acc: 0.8177\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3936 - acc: 0.8303 - val_loss: 0.4098 - val_acc: 0.8210\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3955 - acc: 0.8281 - val_loss: 0.4100 - val_acc: 0.8210\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3948 - acc: 0.8269 - val_loss: 0.4107 - val_acc: 0.8194\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3931 - acc: 0.8287 - val_loss: 0.4118 - val_acc: 0.8161\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.3914 - acc: 0.8314 - val_loss: 0.4112 - val_acc: 0.8161\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3922 - acc: 0.8342 - val_loss: 0.4102 - val_acc: 0.8177\n",
      "Training with parameters {'batch_size': 3000, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_38 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.6716 - acc: 0.5995 - val_loss: 0.6647 - val_acc: 0.5895\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6583 - acc: 0.6318 - val_loss: 0.6504 - val_acc: 0.6338\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6481 - acc: 0.6459 - val_loss: 0.6399 - val_acc: 0.6585\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6420 - acc: 0.6440 - val_loss: 0.6319 - val_acc: 0.6683\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6349 - acc: 0.6499 - val_loss: 0.6254 - val_acc: 0.6700\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6286 - acc: 0.6658 - val_loss: 0.6198 - val_acc: 0.6864\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6228 - acc: 0.6760 - val_loss: 0.6151 - val_acc: 0.6782\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6162 - acc: 0.6829 - val_loss: 0.6111 - val_acc: 0.6749\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6129 - acc: 0.6970 - val_loss: 0.6074 - val_acc: 0.6847\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6058 - acc: 0.7017 - val_loss: 0.6041 - val_acc: 0.6814\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6005 - acc: 0.7050 - val_loss: 0.6008 - val_acc: 0.6782\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5962 - acc: 0.7081 - val_loss: 0.5974 - val_acc: 0.6831\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5899 - acc: 0.7108 - val_loss: 0.5934 - val_acc: 0.6864\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5864 - acc: 0.7148 - val_loss: 0.5893 - val_acc: 0.6847\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5819 - acc: 0.7154 - val_loss: 0.5848 - val_acc: 0.6897\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5779 - acc: 0.7210 - val_loss: 0.5804 - val_acc: 0.6946\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5732 - acc: 0.7243 - val_loss: 0.5760 - val_acc: 0.7061\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5696 - acc: 0.7316 - val_loss: 0.5718 - val_acc: 0.7110\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5667 - acc: 0.7303 - val_loss: 0.5680 - val_acc: 0.7126\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5634 - acc: 0.7334 - val_loss: 0.5644 - val_acc: 0.7126\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5582 - acc: 0.7384 - val_loss: 0.5610 - val_acc: 0.7143\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5533 - acc: 0.7417 - val_loss: 0.5578 - val_acc: 0.7176\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5504 - acc: 0.7431 - val_loss: 0.5546 - val_acc: 0.7241\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5494 - acc: 0.7460 - val_loss: 0.5513 - val_acc: 0.7241\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5435 - acc: 0.7477 - val_loss: 0.5479 - val_acc: 0.7291\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5412 - acc: 0.7535 - val_loss: 0.5447 - val_acc: 0.7307\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5374 - acc: 0.7568 - val_loss: 0.5414 - val_acc: 0.7373\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5355 - acc: 0.7537 - val_loss: 0.5382 - val_acc: 0.7422\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5330 - acc: 0.7553 - val_loss: 0.5349 - val_acc: 0.7422\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5282 - acc: 0.7573 - val_loss: 0.5319 - val_acc: 0.7455\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5267 - acc: 0.7628 - val_loss: 0.5290 - val_acc: 0.7471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5228 - acc: 0.7657 - val_loss: 0.5261 - val_acc: 0.7504\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5214 - acc: 0.7621 - val_loss: 0.5233 - val_acc: 0.7521\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5171 - acc: 0.7690 - val_loss: 0.5207 - val_acc: 0.7537\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5159 - acc: 0.7710 - val_loss: 0.5181 - val_acc: 0.7537\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5131 - acc: 0.7758 - val_loss: 0.5155 - val_acc: 0.7570\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5112 - acc: 0.7739 - val_loss: 0.5131 - val_acc: 0.7570\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5077 - acc: 0.7732 - val_loss: 0.5108 - val_acc: 0.7619\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5058 - acc: 0.7683 - val_loss: 0.5085 - val_acc: 0.7652\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5048 - acc: 0.7727 - val_loss: 0.5060 - val_acc: 0.7668\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5021 - acc: 0.7798 - val_loss: 0.5038 - val_acc: 0.7635\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5002 - acc: 0.7783 - val_loss: 0.5017 - val_acc: 0.7652\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4977 - acc: 0.7789 - val_loss: 0.4995 - val_acc: 0.7685\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4970 - acc: 0.7805 - val_loss: 0.4972 - val_acc: 0.7685\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4933 - acc: 0.7823 - val_loss: 0.4952 - val_acc: 0.7701\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4901 - acc: 0.7862 - val_loss: 0.4937 - val_acc: 0.7685\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4905 - acc: 0.7823 - val_loss: 0.4922 - val_acc: 0.7701\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4865 - acc: 0.7843 - val_loss: 0.4906 - val_acc: 0.7718\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4850 - acc: 0.7856 - val_loss: 0.4891 - val_acc: 0.7718\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4845 - acc: 0.7862 - val_loss: 0.4876 - val_acc: 0.7718\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4845 - acc: 0.7854 - val_loss: 0.4858 - val_acc: 0.7734\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4801 - acc: 0.7907 - val_loss: 0.4837 - val_acc: 0.7734\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4797 - acc: 0.7905 - val_loss: 0.4820 - val_acc: 0.7767\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4780 - acc: 0.7927 - val_loss: 0.4805 - val_acc: 0.7800\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4769 - acc: 0.7929 - val_loss: 0.4797 - val_acc: 0.7816\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4756 - acc: 0.7929 - val_loss: 0.4787 - val_acc: 0.7833\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4734 - acc: 0.7924 - val_loss: 0.4774 - val_acc: 0.7849\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4714 - acc: 0.7922 - val_loss: 0.4761 - val_acc: 0.7865\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4710 - acc: 0.7931 - val_loss: 0.4743 - val_acc: 0.7882\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4692 - acc: 0.7940 - val_loss: 0.4724 - val_acc: 0.7882\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4697 - acc: 0.7929 - val_loss: 0.4709 - val_acc: 0.7915\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4681 - acc: 0.7971 - val_loss: 0.4702 - val_acc: 0.7931\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4667 - acc: 0.7968 - val_loss: 0.4694 - val_acc: 0.7947\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4658 - acc: 0.7951 - val_loss: 0.4683 - val_acc: 0.7980\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4641 - acc: 0.7982 - val_loss: 0.4675 - val_acc: 0.7980\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4633 - acc: 0.7997 - val_loss: 0.4661 - val_acc: 0.7980\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4621 - acc: 0.7989 - val_loss: 0.4647 - val_acc: 0.7980\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4606 - acc: 0.8006 - val_loss: 0.4637 - val_acc: 0.7964\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4598 - acc: 0.8011 - val_loss: 0.4625 - val_acc: 0.7931\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4584 - acc: 0.7993 - val_loss: 0.4615 - val_acc: 0.7931\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4571 - acc: 0.8008 - val_loss: 0.4607 - val_acc: 0.7947\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4570 - acc: 0.8028 - val_loss: 0.4599 - val_acc: 0.7947\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4559 - acc: 0.8000 - val_loss: 0.4594 - val_acc: 0.7947\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4559 - acc: 0.8035 - val_loss: 0.4583 - val_acc: 0.7947\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4551 - acc: 0.8028 - val_loss: 0.4574 - val_acc: 0.7947\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4519 - acc: 0.8033 - val_loss: 0.4564 - val_acc: 0.7947\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4519 - acc: 0.8039 - val_loss: 0.4555 - val_acc: 0.7964\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4506 - acc: 0.8046 - val_loss: 0.4542 - val_acc: 0.7997\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4498 - acc: 0.8059 - val_loss: 0.4533 - val_acc: 0.7980\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4501 - acc: 0.8041 - val_loss: 0.4528 - val_acc: 0.7997\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4482 - acc: 0.8073 - val_loss: 0.4524 - val_acc: 0.7997\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4483 - acc: 0.8053 - val_loss: 0.4518 - val_acc: 0.7997\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4468 - acc: 0.8077 - val_loss: 0.4508 - val_acc: 0.7997\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4467 - acc: 0.8084 - val_loss: 0.4500 - val_acc: 0.7997\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4453 - acc: 0.8092 - val_loss: 0.4488 - val_acc: 0.7997\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4445 - acc: 0.8108 - val_loss: 0.4478 - val_acc: 0.8013\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4433 - acc: 0.8093 - val_loss: 0.4472 - val_acc: 0.8013\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4432 - acc: 0.8110 - val_loss: 0.4472 - val_acc: 0.8030\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4442 - acc: 0.8082 - val_loss: 0.4466 - val_acc: 0.8046\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4422 - acc: 0.8084 - val_loss: 0.4451 - val_acc: 0.8046\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4414 - acc: 0.8084 - val_loss: 0.4442 - val_acc: 0.8046\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4409 - acc: 0.8097 - val_loss: 0.4437 - val_acc: 0.8046\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4401 - acc: 0.8082 - val_loss: 0.4436 - val_acc: 0.8046\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4394 - acc: 0.8134 - val_loss: 0.4433 - val_acc: 0.8046\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4380 - acc: 0.8124 - val_loss: 0.4432 - val_acc: 0.8062\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4388 - acc: 0.8101 - val_loss: 0.4422 - val_acc: 0.8046\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4372 - acc: 0.8148 - val_loss: 0.4414 - val_acc: 0.8046\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4361 - acc: 0.8124 - val_loss: 0.4410 - val_acc: 0.8062\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4362 - acc: 0.8124 - val_loss: 0.4410 - val_acc: 0.8062\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4356 - acc: 0.8121 - val_loss: 0.4404 - val_acc: 0.8079\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4357 - acc: 0.8121 - val_loss: 0.4396 - val_acc: 0.8095\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4353 - acc: 0.8088 - val_loss: 0.4392 - val_acc: 0.8095\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4334 - acc: 0.8139 - val_loss: 0.4385 - val_acc: 0.8079\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4336 - acc: 0.8132 - val_loss: 0.4378 - val_acc: 0.8079\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4321 - acc: 0.8154 - val_loss: 0.4373 - val_acc: 0.8079\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4335 - acc: 0.8139 - val_loss: 0.4369 - val_acc: 0.8095\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4318 - acc: 0.8148 - val_loss: 0.4367 - val_acc: 0.8095\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4316 - acc: 0.8148 - val_loss: 0.4367 - val_acc: 0.8095\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4300 - acc: 0.8152 - val_loss: 0.4364 - val_acc: 0.8095\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4298 - acc: 0.8134 - val_loss: 0.4352 - val_acc: 0.8095\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4298 - acc: 0.8152 - val_loss: 0.4345 - val_acc: 0.8095\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4291 - acc: 0.8166 - val_loss: 0.4341 - val_acc: 0.8095\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4282 - acc: 0.8155 - val_loss: 0.4344 - val_acc: 0.8112\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4298 - acc: 0.8161 - val_loss: 0.4347 - val_acc: 0.8095\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4272 - acc: 0.8161 - val_loss: 0.4347 - val_acc: 0.8095\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4281 - acc: 0.8150 - val_loss: 0.4338 - val_acc: 0.8095\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4254 - acc: 0.8185 - val_loss: 0.4328 - val_acc: 0.8112\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4244 - acc: 0.8185 - val_loss: 0.4324 - val_acc: 0.8128\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4269 - acc: 0.8190 - val_loss: 0.4319 - val_acc: 0.8128\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4247 - acc: 0.8179 - val_loss: 0.4318 - val_acc: 0.8128\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4217 - acc: 0.8207 - val_loss: 0.4313 - val_acc: 0.8112\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4241 - acc: 0.8225 - val_loss: 0.4306 - val_acc: 0.8112\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4235 - acc: 0.8186 - val_loss: 0.4311 - val_acc: 0.8112\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4232 - acc: 0.8176 - val_loss: 0.4317 - val_acc: 0.8112\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4219 - acc: 0.8197 - val_loss: 0.4315 - val_acc: 0.8112\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4216 - acc: 0.8203 - val_loss: 0.4309 - val_acc: 0.8112\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4205 - acc: 0.8159 - val_loss: 0.4299 - val_acc: 0.8128\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4208 - acc: 0.8176 - val_loss: 0.4284 - val_acc: 0.8144\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4218 - acc: 0.8185 - val_loss: 0.4281 - val_acc: 0.8161\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4194 - acc: 0.8219 - val_loss: 0.4281 - val_acc: 0.8144\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4208 - acc: 0.8192 - val_loss: 0.4290 - val_acc: 0.8144\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4192 - acc: 0.8197 - val_loss: 0.4289 - val_acc: 0.8144\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4191 - acc: 0.8197 - val_loss: 0.4286 - val_acc: 0.8161\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4184 - acc: 0.8239 - val_loss: 0.4272 - val_acc: 0.8161\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4180 - acc: 0.8228 - val_loss: 0.4266 - val_acc: 0.8161\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4178 - acc: 0.8196 - val_loss: 0.4265 - val_acc: 0.8161\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4179 - acc: 0.8192 - val_loss: 0.4268 - val_acc: 0.8161\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4164 - acc: 0.8217 - val_loss: 0.4266 - val_acc: 0.8161\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4166 - acc: 0.8248 - val_loss: 0.4263 - val_acc: 0.8161\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4150 - acc: 0.8210 - val_loss: 0.4259 - val_acc: 0.8161\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4158 - acc: 0.8243 - val_loss: 0.4257 - val_acc: 0.8161\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4151 - acc: 0.8217 - val_loss: 0.4261 - val_acc: 0.8161\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4148 - acc: 0.8212 - val_loss: 0.4262 - val_acc: 0.8161\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4149 - acc: 0.8250 - val_loss: 0.4262 - val_acc: 0.8161\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4148 - acc: 0.8225 - val_loss: 0.4254 - val_acc: 0.8161\n",
      "Epoch 146/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4138 - acc: 0.8225 - val_loss: 0.4248 - val_acc: 0.8161\n",
      "Epoch 147/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4139 - acc: 0.8216 - val_loss: 0.4242 - val_acc: 0.8177\n",
      "Epoch 148/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4127 - acc: 0.8263 - val_loss: 0.4239 - val_acc: 0.8177\n",
      "Epoch 149/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4113 - acc: 0.8232 - val_loss: 0.4240 - val_acc: 0.8161\n",
      "Epoch 150/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4138 - acc: 0.8197 - val_loss: 0.4243 - val_acc: 0.8161\n",
      "Epoch 151/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4111 - acc: 0.8243 - val_loss: 0.4241 - val_acc: 0.8161\n",
      "Epoch 152/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4125 - acc: 0.8247 - val_loss: 0.4237 - val_acc: 0.8161\n",
      "Epoch 153/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4113 - acc: 0.8243 - val_loss: 0.4232 - val_acc: 0.8177\n",
      "Epoch 154/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4101 - acc: 0.8245 - val_loss: 0.4231 - val_acc: 0.8177\n",
      "Epoch 155/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4095 - acc: 0.8252 - val_loss: 0.4226 - val_acc: 0.8177\n",
      "Epoch 156/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4104 - acc: 0.8256 - val_loss: 0.4228 - val_acc: 0.8161\n",
      "Epoch 157/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4103 - acc: 0.8228 - val_loss: 0.4231 - val_acc: 0.8161\n",
      "Epoch 158/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4081 - acc: 0.8270 - val_loss: 0.4228 - val_acc: 0.8161\n",
      "Epoch 159/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4075 - acc: 0.8234 - val_loss: 0.4221 - val_acc: 0.8177\n",
      "Epoch 160/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4086 - acc: 0.8254 - val_loss: 0.4216 - val_acc: 0.8177\n",
      "Epoch 161/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4070 - acc: 0.8239 - val_loss: 0.4223 - val_acc: 0.8161\n",
      "Epoch 162/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4071 - acc: 0.8248 - val_loss: 0.4222 - val_acc: 0.8161\n",
      "Epoch 163/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4081 - acc: 0.8250 - val_loss: 0.4220 - val_acc: 0.8161\n",
      "Epoch 164/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4079 - acc: 0.8234 - val_loss: 0.4218 - val_acc: 0.8161\n",
      "Epoch 165/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4069 - acc: 0.8278 - val_loss: 0.4213 - val_acc: 0.8177\n",
      "Epoch 166/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4065 - acc: 0.8245 - val_loss: 0.4210 - val_acc: 0.8177\n",
      "Epoch 167/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4057 - acc: 0.8283 - val_loss: 0.4206 - val_acc: 0.8177\n",
      "Epoch 168/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4060 - acc: 0.8263 - val_loss: 0.4208 - val_acc: 0.8177\n",
      "Epoch 169/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4043 - acc: 0.8248 - val_loss: 0.4213 - val_acc: 0.8161\n",
      "Epoch 170/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4067 - acc: 0.8261 - val_loss: 0.4210 - val_acc: 0.8161\n",
      "Epoch 171/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4042 - acc: 0.8265 - val_loss: 0.4207 - val_acc: 0.8161\n",
      "Epoch 172/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4044 - acc: 0.8259 - val_loss: 0.4201 - val_acc: 0.8177\n",
      "Epoch 173/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4063 - acc: 0.8247 - val_loss: 0.4193 - val_acc: 0.8194\n",
      "Epoch 174/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4051 - acc: 0.8276 - val_loss: 0.4196 - val_acc: 0.8194\n",
      "Epoch 175/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4031 - acc: 0.8252 - val_loss: 0.4205 - val_acc: 0.8194\n",
      "Epoch 176/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4037 - acc: 0.8259 - val_loss: 0.4197 - val_acc: 0.8194\n",
      "Epoch 177/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4003 - acc: 0.8283 - val_loss: 0.4195 - val_acc: 0.8194\n",
      "Epoch 178/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4036 - acc: 0.8247 - val_loss: 0.4199 - val_acc: 0.8194\n",
      "Training with parameters {'batch_size': 3000, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_39 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 1s 133ms/step - loss: 0.8437 - acc: 0.4350 - val_loss: 0.8016 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.7717 - acc: 0.4346 - val_loss: 0.7344 - val_acc: 0.3908\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.7220 - acc: 0.4514 - val_loss: 0.6913 - val_acc: 0.5271\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6930 - acc: 0.5189 - val_loss: 0.6661 - val_acc: 0.6108\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6765 - acc: 0.5625 - val_loss: 0.6513 - val_acc: 0.6125\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6670 - acc: 0.5764 - val_loss: 0.6410 - val_acc: 0.6158\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6601 - acc: 0.5756 - val_loss: 0.6318 - val_acc: 0.6174\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6503 - acc: 0.5822 - val_loss: 0.6228 - val_acc: 0.6223\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6381 - acc: 0.5984 - val_loss: 0.6146 - val_acc: 0.6552\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6278 - acc: 0.6357 - val_loss: 0.6081 - val_acc: 0.6979\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6172 - acc: 0.6707 - val_loss: 0.6040 - val_acc: 0.7061\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6064 - acc: 0.7021 - val_loss: 0.6017 - val_acc: 0.6749\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6001 - acc: 0.7046 - val_loss: 0.5999 - val_acc: 0.6667\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.5913 - acc: 0.7022 - val_loss: 0.5968 - val_acc: 0.6683\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5876 - acc: 0.6957 - val_loss: 0.5914 - val_acc: 0.6683\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5819 - acc: 0.7011 - val_loss: 0.5835 - val_acc: 0.6716\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5747 - acc: 0.7088 - val_loss: 0.5744 - val_acc: 0.6880\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5678 - acc: 0.7216 - val_loss: 0.5653 - val_acc: 0.6946\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5604 - acc: 0.7334 - val_loss: 0.5571 - val_acc: 0.7110\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5525 - acc: 0.7418 - val_loss: 0.5499 - val_acc: 0.7126\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5488 - acc: 0.7415 - val_loss: 0.5435 - val_acc: 0.7159\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5401 - acc: 0.7491 - val_loss: 0.5377 - val_acc: 0.7209\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5351 - acc: 0.7557 - val_loss: 0.5329 - val_acc: 0.7209\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5277 - acc: 0.7541 - val_loss: 0.5275 - val_acc: 0.7241\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5236 - acc: 0.7528 - val_loss: 0.5208 - val_acc: 0.7356\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5171 - acc: 0.7645 - val_loss: 0.5132 - val_acc: 0.7504\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5096 - acc: 0.7679 - val_loss: 0.5057 - val_acc: 0.7603\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5067 - acc: 0.7721 - val_loss: 0.4998 - val_acc: 0.7652\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4997 - acc: 0.7812 - val_loss: 0.4956 - val_acc: 0.7701\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4996 - acc: 0.7789 - val_loss: 0.4922 - val_acc: 0.7734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4924 - acc: 0.7814 - val_loss: 0.4884 - val_acc: 0.7701\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4887 - acc: 0.7840 - val_loss: 0.4835 - val_acc: 0.7734\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4842 - acc: 0.7891 - val_loss: 0.4789 - val_acc: 0.7783\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4810 - acc: 0.7889 - val_loss: 0.4745 - val_acc: 0.7833\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4774 - acc: 0.7880 - val_loss: 0.4714 - val_acc: 0.7833\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4730 - acc: 0.7896 - val_loss: 0.4686 - val_acc: 0.7833\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4706 - acc: 0.7898 - val_loss: 0.4654 - val_acc: 0.7882\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4679 - acc: 0.7957 - val_loss: 0.4631 - val_acc: 0.7898\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4653 - acc: 0.7938 - val_loss: 0.4594 - val_acc: 0.7947\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4576 - acc: 0.8004 - val_loss: 0.4556 - val_acc: 0.7964\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4561 - acc: 0.7989 - val_loss: 0.4523 - val_acc: 0.8013\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4544 - acc: 0.8022 - val_loss: 0.4504 - val_acc: 0.8030\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4508 - acc: 0.8077 - val_loss: 0.4486 - val_acc: 0.7997\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4492 - acc: 0.8044 - val_loss: 0.4455 - val_acc: 0.8030\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4466 - acc: 0.8103 - val_loss: 0.4427 - val_acc: 0.8030\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4414 - acc: 0.8119 - val_loss: 0.4399 - val_acc: 0.8046\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4416 - acc: 0.8103 - val_loss: 0.4381 - val_acc: 0.8046\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4403 - acc: 0.8086 - val_loss: 0.4382 - val_acc: 0.8030\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4394 - acc: 0.8112 - val_loss: 0.4371 - val_acc: 0.8030\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4379 - acc: 0.8134 - val_loss: 0.4332 - val_acc: 0.8030\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4362 - acc: 0.8099 - val_loss: 0.4313 - val_acc: 0.8046\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4316 - acc: 0.8135 - val_loss: 0.4309 - val_acc: 0.8046\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4314 - acc: 0.8148 - val_loss: 0.4296 - val_acc: 0.8062\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4296 - acc: 0.8177 - val_loss: 0.4284 - val_acc: 0.8062\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4268 - acc: 0.8134 - val_loss: 0.4272 - val_acc: 0.8079\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4245 - acc: 0.8203 - val_loss: 0.4261 - val_acc: 0.8095\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4240 - acc: 0.8163 - val_loss: 0.4260 - val_acc: 0.8079\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4224 - acc: 0.8163 - val_loss: 0.4237 - val_acc: 0.8128\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4203 - acc: 0.8166 - val_loss: 0.4214 - val_acc: 0.8112\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4185 - acc: 0.8183 - val_loss: 0.4223 - val_acc: 0.8095\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4179 - acc: 0.8223 - val_loss: 0.4223 - val_acc: 0.8079\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4136 - acc: 0.8210 - val_loss: 0.4205 - val_acc: 0.8112\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4168 - acc: 0.8192 - val_loss: 0.4193 - val_acc: 0.8128\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4146 - acc: 0.8241 - val_loss: 0.4183 - val_acc: 0.8128\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4129 - acc: 0.8212 - val_loss: 0.4187 - val_acc: 0.8112\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4097 - acc: 0.8228 - val_loss: 0.4188 - val_acc: 0.8161\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4092 - acc: 0.8216 - val_loss: 0.4174 - val_acc: 0.8144\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4091 - acc: 0.8263 - val_loss: 0.4153 - val_acc: 0.8079\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4048 - acc: 0.8270 - val_loss: 0.4134 - val_acc: 0.8144\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4064 - acc: 0.8287 - val_loss: 0.4148 - val_acc: 0.8128\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4015 - acc: 0.8307 - val_loss: 0.4172 - val_acc: 0.8194\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4047 - acc: 0.8289 - val_loss: 0.4152 - val_acc: 0.8161\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4046 - acc: 0.8278 - val_loss: 0.4128 - val_acc: 0.8144\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4031 - acc: 0.8256 - val_loss: 0.4122 - val_acc: 0.8144\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4000 - acc: 0.8283 - val_loss: 0.4148 - val_acc: 0.8161\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.3987 - acc: 0.8316 - val_loss: 0.4138 - val_acc: 0.8177\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3972 - acc: 0.8309 - val_loss: 0.4116 - val_acc: 0.8194\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.3953 - acc: 0.8323 - val_loss: 0.4097 - val_acc: 0.8161\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.3952 - acc: 0.8298 - val_loss: 0.4128 - val_acc: 0.8210\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.3943 - acc: 0.8287 - val_loss: 0.4131 - val_acc: 0.8210\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.3929 - acc: 0.8312 - val_loss: 0.4101 - val_acc: 0.8194\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.3912 - acc: 0.8345 - val_loss: 0.4102 - val_acc: 0.8194\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.3911 - acc: 0.8321 - val_loss: 0.4108 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': 3000, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_40 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 122ms/step - loss: 0.8732 - acc: 0.4353 - val_loss: 0.8561 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.8212 - acc: 0.4353 - val_loss: 0.8027 - val_acc: 0.3875\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.7740 - acc: 0.4370 - val_loss: 0.7616 - val_acc: 0.3875\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.7411 - acc: 0.4450 - val_loss: 0.7296 - val_acc: 0.3924\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.7167 - acc: 0.4605 - val_loss: 0.7050 - val_acc: 0.4302\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6972 - acc: 0.5008 - val_loss: 0.6863 - val_acc: 0.5501\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6834 - acc: 0.5629 - val_loss: 0.6721 - val_acc: 0.6338\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6738 - acc: 0.6052 - val_loss: 0.6607 - val_acc: 0.6880\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6693 - acc: 0.6054 - val_loss: 0.6515 - val_acc: 0.6864\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6599 - acc: 0.6329 - val_loss: 0.6437 - val_acc: 0.6798\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6533 - acc: 0.6378 - val_loss: 0.6367 - val_acc: 0.6946\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6479 - acc: 0.6526 - val_loss: 0.6303 - val_acc: 0.7159\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6397 - acc: 0.6663 - val_loss: 0.6247 - val_acc: 0.7356\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6352 - acc: 0.6736 - val_loss: 0.6197 - val_acc: 0.7521\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6273 - acc: 0.6917 - val_loss: 0.6153 - val_acc: 0.7323\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6204 - acc: 0.7035 - val_loss: 0.6116 - val_acc: 0.7126\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6140 - acc: 0.7125 - val_loss: 0.6083 - val_acc: 0.7094\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6107 - acc: 0.7132 - val_loss: 0.6053 - val_acc: 0.6995\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6063 - acc: 0.7090 - val_loss: 0.6022 - val_acc: 0.6979\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6012 - acc: 0.7110 - val_loss: 0.5989 - val_acc: 0.6946\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5951 - acc: 0.7156 - val_loss: 0.5952 - val_acc: 0.6962\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5931 - acc: 0.7123 - val_loss: 0.5907 - val_acc: 0.6979\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5908 - acc: 0.7136 - val_loss: 0.5860 - val_acc: 0.6995\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5830 - acc: 0.7225 - val_loss: 0.5811 - val_acc: 0.7011\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5794 - acc: 0.7225 - val_loss: 0.5763 - val_acc: 0.7061\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5773 - acc: 0.7227 - val_loss: 0.5717 - val_acc: 0.7094\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.5700 - acc: 0.7331 - val_loss: 0.5675 - val_acc: 0.7126\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5694 - acc: 0.7342 - val_loss: 0.5635 - val_acc: 0.7061\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.5679 - acc: 0.7378 - val_loss: 0.5598 - val_acc: 0.7094\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5575 - acc: 0.7406 - val_loss: 0.5564 - val_acc: 0.7110\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5519 - acc: 0.7422 - val_loss: 0.5530 - val_acc: 0.7126\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.5530 - acc: 0.7398 - val_loss: 0.5495 - val_acc: 0.7143\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5467 - acc: 0.7484 - val_loss: 0.5459 - val_acc: 0.7143\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5422 - acc: 0.7493 - val_loss: 0.5417 - val_acc: 0.7110\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5417 - acc: 0.7515 - val_loss: 0.5375 - val_acc: 0.7192\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5397 - acc: 0.7460 - val_loss: 0.5330 - val_acc: 0.7307\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5358 - acc: 0.7541 - val_loss: 0.5288 - val_acc: 0.7340\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5307 - acc: 0.7561 - val_loss: 0.5248 - val_acc: 0.7340\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.5276 - acc: 0.7601 - val_loss: 0.5212 - val_acc: 0.7389\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5220 - acc: 0.7599 - val_loss: 0.5179 - val_acc: 0.7406\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.5179 - acc: 0.7676 - val_loss: 0.5144 - val_acc: 0.7438\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5177 - acc: 0.7661 - val_loss: 0.5107 - val_acc: 0.7504\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5131 - acc: 0.7679 - val_loss: 0.5074 - val_acc: 0.7521\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5109 - acc: 0.7674 - val_loss: 0.5038 - val_acc: 0.7537\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.5077 - acc: 0.7781 - val_loss: 0.5011 - val_acc: 0.7521\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5061 - acc: 0.7729 - val_loss: 0.4980 - val_acc: 0.7537\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5007 - acc: 0.7719 - val_loss: 0.4952 - val_acc: 0.7553\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4989 - acc: 0.7752 - val_loss: 0.4926 - val_acc: 0.7570\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4967 - acc: 0.7739 - val_loss: 0.4895 - val_acc: 0.7586\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4950 - acc: 0.7781 - val_loss: 0.4864 - val_acc: 0.7635\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4881 - acc: 0.7833 - val_loss: 0.4834 - val_acc: 0.7701\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4890 - acc: 0.7769 - val_loss: 0.4806 - val_acc: 0.7718\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4854 - acc: 0.7873 - val_loss: 0.4786 - val_acc: 0.7701\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4819 - acc: 0.7838 - val_loss: 0.4766 - val_acc: 0.7734\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.4830 - acc: 0.7840 - val_loss: 0.4752 - val_acc: 0.7734\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4807 - acc: 0.7885 - val_loss: 0.4728 - val_acc: 0.7734\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4781 - acc: 0.7843 - val_loss: 0.4704 - val_acc: 0.7734\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4758 - acc: 0.7884 - val_loss: 0.4677 - val_acc: 0.7767\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4765 - acc: 0.7885 - val_loss: 0.4657 - val_acc: 0.7783\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4721 - acc: 0.7902 - val_loss: 0.4636 - val_acc: 0.7833\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4695 - acc: 0.7891 - val_loss: 0.4615 - val_acc: 0.7849\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4660 - acc: 0.7947 - val_loss: 0.4599 - val_acc: 0.7865\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4668 - acc: 0.7980 - val_loss: 0.4587 - val_acc: 0.7898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4644 - acc: 0.7966 - val_loss: 0.4582 - val_acc: 0.7882\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4627 - acc: 0.7955 - val_loss: 0.4562 - val_acc: 0.7882\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4592 - acc: 0.7975 - val_loss: 0.4535 - val_acc: 0.7882\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4582 - acc: 0.7978 - val_loss: 0.4515 - val_acc: 0.7915\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4616 - acc: 0.7937 - val_loss: 0.4506 - val_acc: 0.7931\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.4546 - acc: 0.8019 - val_loss: 0.4493 - val_acc: 0.7931\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4554 - acc: 0.7995 - val_loss: 0.4488 - val_acc: 0.7947\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4542 - acc: 0.7980 - val_loss: 0.4476 - val_acc: 0.7947\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4512 - acc: 0.8009 - val_loss: 0.4461 - val_acc: 0.7964\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4485 - acc: 0.8041 - val_loss: 0.4445 - val_acc: 0.7980\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4501 - acc: 0.8028 - val_loss: 0.4431 - val_acc: 0.7980\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4465 - acc: 0.8053 - val_loss: 0.4428 - val_acc: 0.7997\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4452 - acc: 0.8059 - val_loss: 0.4420 - val_acc: 0.7997\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4438 - acc: 0.8061 - val_loss: 0.4411 - val_acc: 0.7980\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4445 - acc: 0.8024 - val_loss: 0.4396 - val_acc: 0.7997\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4428 - acc: 0.8055 - val_loss: 0.4381 - val_acc: 0.8013\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4407 - acc: 0.8086 - val_loss: 0.4366 - val_acc: 0.8030\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4424 - acc: 0.8106 - val_loss: 0.4358 - val_acc: 0.8046\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4396 - acc: 0.8082 - val_loss: 0.4354 - val_acc: 0.8062\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4371 - acc: 0.8101 - val_loss: 0.4358 - val_acc: 0.8046\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4351 - acc: 0.8099 - val_loss: 0.4348 - val_acc: 0.8046\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4369 - acc: 0.8110 - val_loss: 0.4329 - val_acc: 0.8062\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4346 - acc: 0.8108 - val_loss: 0.4323 - val_acc: 0.8062\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4333 - acc: 0.8097 - val_loss: 0.4319 - val_acc: 0.8062\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4348 - acc: 0.8112 - val_loss: 0.4315 - val_acc: 0.8062\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4329 - acc: 0.8139 - val_loss: 0.4308 - val_acc: 0.8062\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4310 - acc: 0.8148 - val_loss: 0.4297 - val_acc: 0.8079\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4310 - acc: 0.8141 - val_loss: 0.4282 - val_acc: 0.8079\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4248 - acc: 0.8144 - val_loss: 0.4278 - val_acc: 0.8079\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4288 - acc: 0.8119 - val_loss: 0.4282 - val_acc: 0.8079\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4274 - acc: 0.8146 - val_loss: 0.4276 - val_acc: 0.8079\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4215 - acc: 0.8196 - val_loss: 0.4263 - val_acc: 0.8079\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4241 - acc: 0.8148 - val_loss: 0.4257 - val_acc: 0.8079\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4226 - acc: 0.8203 - val_loss: 0.4256 - val_acc: 0.8079\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4242 - acc: 0.8157 - val_loss: 0.4256 - val_acc: 0.8112\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4218 - acc: 0.8157 - val_loss: 0.4250 - val_acc: 0.8112\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4210 - acc: 0.8181 - val_loss: 0.4236 - val_acc: 0.8112\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4175 - acc: 0.8159 - val_loss: 0.4229 - val_acc: 0.8128\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4170 - acc: 0.8207 - val_loss: 0.4227 - val_acc: 0.8128\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.4177 - acc: 0.8190 - val_loss: 0.4229 - val_acc: 0.8128\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4186 - acc: 0.8166 - val_loss: 0.4219 - val_acc: 0.8144\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.4194 - acc: 0.8124 - val_loss: 0.4207 - val_acc: 0.8144\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.4172 - acc: 0.8192 - val_loss: 0.4198 - val_acc: 0.8177\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4162 - acc: 0.8208 - val_loss: 0.4191 - val_acc: 0.8194\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4154 - acc: 0.8174 - val_loss: 0.4207 - val_acc: 0.8128\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.4166 - acc: 0.8181 - val_loss: 0.4214 - val_acc: 0.8144\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4113 - acc: 0.8210 - val_loss: 0.4207 - val_acc: 0.8128\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4141 - acc: 0.8236 - val_loss: 0.4183 - val_acc: 0.8194\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4128 - acc: 0.8172 - val_loss: 0.4181 - val_acc: 0.8194\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4090 - acc: 0.8241 - val_loss: 0.4191 - val_acc: 0.8144\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4104 - acc: 0.8254 - val_loss: 0.4199 - val_acc: 0.8161\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4100 - acc: 0.8232 - val_loss: 0.4184 - val_acc: 0.8144\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4069 - acc: 0.8250 - val_loss: 0.4169 - val_acc: 0.8177\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4066 - acc: 0.8208 - val_loss: 0.4166 - val_acc: 0.8177\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4042 - acc: 0.8267 - val_loss: 0.4181 - val_acc: 0.8161\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4057 - acc: 0.8247 - val_loss: 0.4185 - val_acc: 0.8144\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4063 - acc: 0.8278 - val_loss: 0.4161 - val_acc: 0.8177\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4066 - acc: 0.8241 - val_loss: 0.4146 - val_acc: 0.8194\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4033 - acc: 0.8305 - val_loss: 0.4156 - val_acc: 0.8161\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4032 - acc: 0.8243 - val_loss: 0.4171 - val_acc: 0.8161\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4030 - acc: 0.8239 - val_loss: 0.4169 - val_acc: 0.8177\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4039 - acc: 0.8270 - val_loss: 0.4160 - val_acc: 0.8177\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.4011 - acc: 0.8290 - val_loss: 0.4144 - val_acc: 0.8227\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4035 - acc: 0.8250 - val_loss: 0.4143 - val_acc: 0.8243\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.3993 - acc: 0.8274 - val_loss: 0.4150 - val_acc: 0.8210\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3975 - acc: 0.8340 - val_loss: 0.4152 - val_acc: 0.8194\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.3965 - acc: 0.8301 - val_loss: 0.4151 - val_acc: 0.8194\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.4008 - acc: 0.8254 - val_loss: 0.4133 - val_acc: 0.8227\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3968 - acc: 0.8320 - val_loss: 0.4138 - val_acc: 0.8227\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3955 - acc: 0.8303 - val_loss: 0.4149 - val_acc: 0.8177\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3974 - acc: 0.8300 - val_loss: 0.4146 - val_acc: 0.8177\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3942 - acc: 0.8303 - val_loss: 0.4146 - val_acc: 0.8177\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3921 - acc: 0.8342 - val_loss: 0.4139 - val_acc: 0.8227\n",
      "Training with parameters {'batch_size': 3000, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_41 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.7644 - acc: 0.4355 - val_loss: 0.7653 - val_acc: 0.3892\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.7381 - acc: 0.4402 - val_loss: 0.7376 - val_acc: 0.3990\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.7157 - acc: 0.4556 - val_loss: 0.7146 - val_acc: 0.4319\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7008 - acc: 0.4880 - val_loss: 0.6956 - val_acc: 0.4778\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6851 - acc: 0.5455 - val_loss: 0.6803 - val_acc: 0.5714\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6758 - acc: 0.5951 - val_loss: 0.6683 - val_acc: 0.6650\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6694 - acc: 0.6249 - val_loss: 0.6588 - val_acc: 0.6814\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6638 - acc: 0.6282 - val_loss: 0.6511 - val_acc: 0.6732\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6555 - acc: 0.6464 - val_loss: 0.6447 - val_acc: 0.6519\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6521 - acc: 0.6267 - val_loss: 0.6391 - val_acc: 0.6470\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6465 - acc: 0.6318 - val_loss: 0.6342 - val_acc: 0.6502\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.6448 - acc: 0.6291 - val_loss: 0.6296 - val_acc: 0.6568\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6390 - acc: 0.6424 - val_loss: 0.6253 - val_acc: 0.6782\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6324 - acc: 0.6575 - val_loss: 0.6212 - val_acc: 0.6979\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6281 - acc: 0.6752 - val_loss: 0.6174 - val_acc: 0.6946\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6242 - acc: 0.6866 - val_loss: 0.6140 - val_acc: 0.7028\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.6174 - acc: 0.6993 - val_loss: 0.6108 - val_acc: 0.7126\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.6152 - acc: 0.7030 - val_loss: 0.6079 - val_acc: 0.7061\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6091 - acc: 0.7123 - val_loss: 0.6051 - val_acc: 0.7011\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6047 - acc: 0.7092 - val_loss: 0.6023 - val_acc: 0.6962\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6012 - acc: 0.7088 - val_loss: 0.5992 - val_acc: 0.6913\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.5955 - acc: 0.7141 - val_loss: 0.5958 - val_acc: 0.6913\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5908 - acc: 0.7152 - val_loss: 0.5922 - val_acc: 0.6913\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5880 - acc: 0.7185 - val_loss: 0.5884 - val_acc: 0.6962\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5853 - acc: 0.7139 - val_loss: 0.5844 - val_acc: 0.7044\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5791 - acc: 0.7232 - val_loss: 0.5803 - val_acc: 0.7077\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5752 - acc: 0.7271 - val_loss: 0.5763 - val_acc: 0.7094\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.5705 - acc: 0.7278 - val_loss: 0.5723 - val_acc: 0.7110\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5669 - acc: 0.7354 - val_loss: 0.5684 - val_acc: 0.7159\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5614 - acc: 0.7402 - val_loss: 0.5645 - val_acc: 0.7159\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5595 - acc: 0.7407 - val_loss: 0.5608 - val_acc: 0.7192\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5538 - acc: 0.7433 - val_loss: 0.5568 - val_acc: 0.7209\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5503 - acc: 0.7448 - val_loss: 0.5528 - val_acc: 0.7258\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5466 - acc: 0.7455 - val_loss: 0.5487 - val_acc: 0.7274\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5441 - acc: 0.7424 - val_loss: 0.5447 - val_acc: 0.7291\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5399 - acc: 0.7522 - val_loss: 0.5406 - val_acc: 0.7340\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5353 - acc: 0.7557 - val_loss: 0.5361 - val_acc: 0.7406\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5297 - acc: 0.7579 - val_loss: 0.5313 - val_acc: 0.7488\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5272 - acc: 0.7604 - val_loss: 0.5266 - val_acc: 0.7488\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5232 - acc: 0.7619 - val_loss: 0.5222 - val_acc: 0.7471\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5195 - acc: 0.7665 - val_loss: 0.5184 - val_acc: 0.7488\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5151 - acc: 0.7654 - val_loss: 0.5145 - val_acc: 0.7504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5150 - acc: 0.7668 - val_loss: 0.5116 - val_acc: 0.7537\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5081 - acc: 0.7679 - val_loss: 0.5083 - val_acc: 0.7537\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5058 - acc: 0.7694 - val_loss: 0.5049 - val_acc: 0.7586\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5015 - acc: 0.7758 - val_loss: 0.5016 - val_acc: 0.7586\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4993 - acc: 0.7758 - val_loss: 0.4983 - val_acc: 0.7603\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4982 - acc: 0.7781 - val_loss: 0.4957 - val_acc: 0.7603\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4956 - acc: 0.7765 - val_loss: 0.4932 - val_acc: 0.7652\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4905 - acc: 0.7827 - val_loss: 0.4909 - val_acc: 0.7668\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4875 - acc: 0.7822 - val_loss: 0.4885 - val_acc: 0.7668\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4864 - acc: 0.7838 - val_loss: 0.4854 - val_acc: 0.7718\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4863 - acc: 0.7811 - val_loss: 0.4829 - val_acc: 0.7750\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4807 - acc: 0.7847 - val_loss: 0.4803 - val_acc: 0.7800\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4795 - acc: 0.7836 - val_loss: 0.4787 - val_acc: 0.7816\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4766 - acc: 0.7885 - val_loss: 0.4775 - val_acc: 0.7750\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4773 - acc: 0.7876 - val_loss: 0.4761 - val_acc: 0.7767\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4742 - acc: 0.7878 - val_loss: 0.4739 - val_acc: 0.7865\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4711 - acc: 0.7935 - val_loss: 0.4712 - val_acc: 0.7915\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4712 - acc: 0.7942 - val_loss: 0.4689 - val_acc: 0.7931\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4686 - acc: 0.7907 - val_loss: 0.4671 - val_acc: 0.7931\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4660 - acc: 0.7909 - val_loss: 0.4656 - val_acc: 0.7915\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4647 - acc: 0.7931 - val_loss: 0.4641 - val_acc: 0.7931\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4616 - acc: 0.7988 - val_loss: 0.4618 - val_acc: 0.7931\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4619 - acc: 0.7944 - val_loss: 0.4604 - val_acc: 0.7915\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4606 - acc: 0.7995 - val_loss: 0.4592 - val_acc: 0.7915\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4554 - acc: 0.7968 - val_loss: 0.4571 - val_acc: 0.7931\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4557 - acc: 0.7999 - val_loss: 0.4556 - val_acc: 0.7931\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4561 - acc: 0.8019 - val_loss: 0.4542 - val_acc: 0.7931\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4537 - acc: 0.8008 - val_loss: 0.4539 - val_acc: 0.7964\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4513 - acc: 0.8019 - val_loss: 0.4519 - val_acc: 0.7947\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4499 - acc: 0.8002 - val_loss: 0.4498 - val_acc: 0.7980\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4497 - acc: 0.8073 - val_loss: 0.4486 - val_acc: 0.7997\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4486 - acc: 0.8026 - val_loss: 0.4477 - val_acc: 0.8030\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4441 - acc: 0.8055 - val_loss: 0.4468 - val_acc: 0.8046\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4460 - acc: 0.8068 - val_loss: 0.4457 - val_acc: 0.8046\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4432 - acc: 0.8059 - val_loss: 0.4444 - val_acc: 0.8046\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4408 - acc: 0.8112 - val_loss: 0.4441 - val_acc: 0.8046\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4417 - acc: 0.8061 - val_loss: 0.4434 - val_acc: 0.8030\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4392 - acc: 0.8059 - val_loss: 0.4420 - val_acc: 0.8062\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4379 - acc: 0.8101 - val_loss: 0.4398 - val_acc: 0.8079\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4390 - acc: 0.8086 - val_loss: 0.4391 - val_acc: 0.8079\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4374 - acc: 0.8115 - val_loss: 0.4390 - val_acc: 0.8062\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4366 - acc: 0.8059 - val_loss: 0.4389 - val_acc: 0.8030\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4339 - acc: 0.8095 - val_loss: 0.4385 - val_acc: 0.8030\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4300 - acc: 0.8157 - val_loss: 0.4369 - val_acc: 0.8062\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4334 - acc: 0.8117 - val_loss: 0.4355 - val_acc: 0.8079\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4316 - acc: 0.8101 - val_loss: 0.4354 - val_acc: 0.8046\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4303 - acc: 0.8128 - val_loss: 0.4344 - val_acc: 0.8046\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4283 - acc: 0.8144 - val_loss: 0.4340 - val_acc: 0.8046\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4323 - acc: 0.8113 - val_loss: 0.4323 - val_acc: 0.8062\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4262 - acc: 0.8139 - val_loss: 0.4313 - val_acc: 0.8079\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4281 - acc: 0.8135 - val_loss: 0.4309 - val_acc: 0.8079\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4272 - acc: 0.8144 - val_loss: 0.4319 - val_acc: 0.8079\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4268 - acc: 0.8134 - val_loss: 0.4312 - val_acc: 0.8079\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4268 - acc: 0.8146 - val_loss: 0.4295 - val_acc: 0.8062\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4253 - acc: 0.8161 - val_loss: 0.4285 - val_acc: 0.8062\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4222 - acc: 0.8143 - val_loss: 0.4285 - val_acc: 0.8079\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4228 - acc: 0.8130 - val_loss: 0.4287 - val_acc: 0.8079\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4194 - acc: 0.8199 - val_loss: 0.4295 - val_acc: 0.8079\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4223 - acc: 0.8177 - val_loss: 0.4289 - val_acc: 0.8062\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4167 - acc: 0.8194 - val_loss: 0.4263 - val_acc: 0.8095\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4195 - acc: 0.8186 - val_loss: 0.4251 - val_acc: 0.8112\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4203 - acc: 0.8185 - val_loss: 0.4264 - val_acc: 0.8112\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4173 - acc: 0.8179 - val_loss: 0.4276 - val_acc: 0.8062\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4171 - acc: 0.8216 - val_loss: 0.4278 - val_acc: 0.8046\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4162 - acc: 0.8188 - val_loss: 0.4253 - val_acc: 0.8112\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4153 - acc: 0.8228 - val_loss: 0.4232 - val_acc: 0.8161\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4153 - acc: 0.8210 - val_loss: 0.4230 - val_acc: 0.8161\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4169 - acc: 0.8250 - val_loss: 0.4235 - val_acc: 0.8128\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4147 - acc: 0.8177 - val_loss: 0.4238 - val_acc: 0.8128\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4135 - acc: 0.8225 - val_loss: 0.4240 - val_acc: 0.8128\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4125 - acc: 0.8219 - val_loss: 0.4239 - val_acc: 0.8128\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4137 - acc: 0.8207 - val_loss: 0.4218 - val_acc: 0.8128\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4089 - acc: 0.8216 - val_loss: 0.4208 - val_acc: 0.8161\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4107 - acc: 0.8248 - val_loss: 0.4215 - val_acc: 0.8161\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4088 - acc: 0.8232 - val_loss: 0.4223 - val_acc: 0.8144\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4094 - acc: 0.8265 - val_loss: 0.4214 - val_acc: 0.8161\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4075 - acc: 0.8212 - val_loss: 0.4212 - val_acc: 0.8177\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4076 - acc: 0.8216 - val_loss: 0.4204 - val_acc: 0.8161\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4058 - acc: 0.8232 - val_loss: 0.4197 - val_acc: 0.8161\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4037 - acc: 0.8248 - val_loss: 0.4201 - val_acc: 0.8161\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4044 - acc: 0.8285 - val_loss: 0.4208 - val_acc: 0.8161\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4040 - acc: 0.8267 - val_loss: 0.4207 - val_acc: 0.8144\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4036 - acc: 0.8287 - val_loss: 0.4202 - val_acc: 0.8161\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4030 - acc: 0.8294 - val_loss: 0.4197 - val_acc: 0.8177\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4010 - acc: 0.8314 - val_loss: 0.4191 - val_acc: 0.8177\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4054 - acc: 0.8208 - val_loss: 0.4199 - val_acc: 0.8161\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4018 - acc: 0.8259 - val_loss: 0.4193 - val_acc: 0.8161\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 208ms/step - loss: 0.4008 - acc: 0.8280 - val_loss: 0.4187 - val_acc: 0.8177\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3996 - acc: 0.8248 - val_loss: 0.4190 - val_acc: 0.8161\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3997 - acc: 0.8269 - val_loss: 0.4201 - val_acc: 0.8177\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3963 - acc: 0.8303 - val_loss: 0.4191 - val_acc: 0.8194\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3940 - acc: 0.8296 - val_loss: 0.4172 - val_acc: 0.8177\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3972 - acc: 0.8307 - val_loss: 0.4168 - val_acc: 0.8177\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3961 - acc: 0.8292 - val_loss: 0.4188 - val_acc: 0.8194\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3945 - acc: 0.8325 - val_loss: 0.4199 - val_acc: 0.8210\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3956 - acc: 0.8298 - val_loss: 0.4179 - val_acc: 0.8194\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3932 - acc: 0.8316 - val_loss: 0.4165 - val_acc: 0.8161\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3936 - acc: 0.8320 - val_loss: 0.4173 - val_acc: 0.8194\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3944 - acc: 0.8278 - val_loss: 0.4185 - val_acc: 0.8210\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3935 - acc: 0.8274 - val_loss: 0.4178 - val_acc: 0.8210\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.3946 - acc: 0.8296 - val_loss: 0.4165 - val_acc: 0.8227\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.3923 - acc: 0.8292 - val_loss: 0.4151 - val_acc: 0.8194\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.3908 - acc: 0.8342 - val_loss: 0.4159 - val_acc: 0.8210\n",
      "Epoch 146/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3889 - acc: 0.8321 - val_loss: 0.4171 - val_acc: 0.8243\n",
      "Epoch 147/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.3912 - acc: 0.8332 - val_loss: 0.4177 - val_acc: 0.8227\n",
      "Epoch 148/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3904 - acc: 0.8332 - val_loss: 0.4172 - val_acc: 0.8243\n",
      "Epoch 149/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3897 - acc: 0.8349 - val_loss: 0.4163 - val_acc: 0.8243\n",
      "Training with parameters {'batch_size': 3000, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_42 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.8381 - acc: 0.4368 - val_loss: 0.8174 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.7884 - acc: 0.4390 - val_loss: 0.7672 - val_acc: 0.3892\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7500 - acc: 0.4430 - val_loss: 0.7275 - val_acc: 0.3924\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7166 - acc: 0.4641 - val_loss: 0.6974 - val_acc: 0.4811\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6970 - acc: 0.5125 - val_loss: 0.6754 - val_acc: 0.6240\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6837 - acc: 0.5738 - val_loss: 0.6596 - val_acc: 0.6371\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6707 - acc: 0.6072 - val_loss: 0.6482 - val_acc: 0.6322\n",
      "Epoch 8/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6625 - acc: 0.5993 - val_loss: 0.6394 - val_acc: 0.6223\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6587 - acc: 0.5959 - val_loss: 0.6322 - val_acc: 0.6190\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6496 - acc: 0.5984 - val_loss: 0.6257 - val_acc: 0.6240\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6413 - acc: 0.6030 - val_loss: 0.6195 - val_acc: 0.6322\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6370 - acc: 0.6158 - val_loss: 0.6137 - val_acc: 0.6437\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6275 - acc: 0.6398 - val_loss: 0.6085 - val_acc: 0.6798\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6221 - acc: 0.6694 - val_loss: 0.6040 - val_acc: 0.7307\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6132 - acc: 0.6960 - val_loss: 0.6002 - val_acc: 0.7307\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6090 - acc: 0.7017 - val_loss: 0.5970 - val_acc: 0.7094\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.6044 - acc: 0.7041 - val_loss: 0.5939 - val_acc: 0.6979\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.5990 - acc: 0.7037 - val_loss: 0.5906 - val_acc: 0.6995\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5952 - acc: 0.6999 - val_loss: 0.5866 - val_acc: 0.7011\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5873 - acc: 0.7066 - val_loss: 0.5820 - val_acc: 0.7011\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.5819 - acc: 0.7115 - val_loss: 0.5768 - val_acc: 0.7011\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5792 - acc: 0.7106 - val_loss: 0.5711 - val_acc: 0.7077\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.5726 - acc: 0.7205 - val_loss: 0.5653 - val_acc: 0.7126\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5670 - acc: 0.7267 - val_loss: 0.5596 - val_acc: 0.7176\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.5650 - acc: 0.7285 - val_loss: 0.5542 - val_acc: 0.7176\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5580 - acc: 0.7362 - val_loss: 0.5493 - val_acc: 0.7241\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.5546 - acc: 0.7373 - val_loss: 0.5446 - val_acc: 0.7274\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.5479 - acc: 0.7422 - val_loss: 0.5402 - val_acc: 0.7274\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5431 - acc: 0.7435 - val_loss: 0.5361 - val_acc: 0.7258\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5353 - acc: 0.7490 - val_loss: 0.5316 - val_acc: 0.7307\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5302 - acc: 0.7526 - val_loss: 0.5265 - val_acc: 0.7373\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.5288 - acc: 0.7544 - val_loss: 0.5209 - val_acc: 0.7406\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5225 - acc: 0.7572 - val_loss: 0.5150 - val_acc: 0.7438\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.5144 - acc: 0.7639 - val_loss: 0.5092 - val_acc: 0.7438\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5152 - acc: 0.7659 - val_loss: 0.5039 - val_acc: 0.7504\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5110 - acc: 0.7668 - val_loss: 0.4993 - val_acc: 0.7537\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5062 - acc: 0.7701 - val_loss: 0.4948 - val_acc: 0.7603\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5024 - acc: 0.7718 - val_loss: 0.4909 - val_acc: 0.7603\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4979 - acc: 0.7719 - val_loss: 0.4876 - val_acc: 0.7603\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4934 - acc: 0.7796 - val_loss: 0.4837 - val_acc: 0.7668\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4908 - acc: 0.7800 - val_loss: 0.4796 - val_acc: 0.7718\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4861 - acc: 0.7794 - val_loss: 0.4753 - val_acc: 0.7718\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4825 - acc: 0.7847 - val_loss: 0.4718 - val_acc: 0.7767\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4795 - acc: 0.7838 - val_loss: 0.4686 - val_acc: 0.7800\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4747 - acc: 0.7871 - val_loss: 0.4663 - val_acc: 0.7800\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4735 - acc: 0.7898 - val_loss: 0.4636 - val_acc: 0.7849\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.4704 - acc: 0.7869 - val_loss: 0.4607 - val_acc: 0.7865\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4668 - acc: 0.7944 - val_loss: 0.4575 - val_acc: 0.7865\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.4619 - acc: 0.7949 - val_loss: 0.4544 - val_acc: 0.7947\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4622 - acc: 0.7957 - val_loss: 0.4521 - val_acc: 0.7964\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4617 - acc: 0.7909 - val_loss: 0.4512 - val_acc: 0.7964\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4589 - acc: 0.7955 - val_loss: 0.4489 - val_acc: 0.7980\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4568 - acc: 0.7977 - val_loss: 0.4467 - val_acc: 0.7980\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4504 - acc: 0.8008 - val_loss: 0.4435 - val_acc: 0.8079\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4519 - acc: 0.8031 - val_loss: 0.4414 - val_acc: 0.8079\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4499 - acc: 0.8008 - val_loss: 0.4405 - val_acc: 0.8079\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.4488 - acc: 0.7973 - val_loss: 0.4383 - val_acc: 0.8095\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4460 - acc: 0.8033 - val_loss: 0.4373 - val_acc: 0.8079\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4443 - acc: 0.8053 - val_loss: 0.4360 - val_acc: 0.8046\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4415 - acc: 0.8061 - val_loss: 0.4342 - val_acc: 0.8062\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4397 - acc: 0.8072 - val_loss: 0.4323 - val_acc: 0.8128\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4369 - acc: 0.8072 - val_loss: 0.4312 - val_acc: 0.8177\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.4357 - acc: 0.8088 - val_loss: 0.4306 - val_acc: 0.8112\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4343 - acc: 0.8061 - val_loss: 0.4305 - val_acc: 0.8079\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4333 - acc: 0.8082 - val_loss: 0.4285 - val_acc: 0.8095\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4341 - acc: 0.8101 - val_loss: 0.4269 - val_acc: 0.8128\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4309 - acc: 0.8110 - val_loss: 0.4262 - val_acc: 0.8112\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4293 - acc: 0.8099 - val_loss: 0.4258 - val_acc: 0.8079\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4284 - acc: 0.8099 - val_loss: 0.4247 - val_acc: 0.8079\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4278 - acc: 0.8134 - val_loss: 0.4235 - val_acc: 0.8095\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.4279 - acc: 0.8139 - val_loss: 0.4233 - val_acc: 0.8079\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4285 - acc: 0.8117 - val_loss: 0.4221 - val_acc: 0.8095\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4267 - acc: 0.8130 - val_loss: 0.4206 - val_acc: 0.8161\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4243 - acc: 0.8157 - val_loss: 0.4201 - val_acc: 0.8161\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4229 - acc: 0.8159 - val_loss: 0.4210 - val_acc: 0.8112\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4203 - acc: 0.8170 - val_loss: 0.4202 - val_acc: 0.8112\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4179 - acc: 0.8186 - val_loss: 0.4186 - val_acc: 0.8161\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4183 - acc: 0.8194 - val_loss: 0.4181 - val_acc: 0.8144\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4173 - acc: 0.8168 - val_loss: 0.4167 - val_acc: 0.8144\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4163 - acc: 0.8165 - val_loss: 0.4176 - val_acc: 0.8128\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4124 - acc: 0.8161 - val_loss: 0.4182 - val_acc: 0.8128\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4139 - acc: 0.8172 - val_loss: 0.4163 - val_acc: 0.8144\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4128 - acc: 0.8216 - val_loss: 0.4142 - val_acc: 0.8161\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4082 - acc: 0.8208 - val_loss: 0.4146 - val_acc: 0.8128\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4110 - acc: 0.8208 - val_loss: 0.4152 - val_acc: 0.8161\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4084 - acc: 0.8219 - val_loss: 0.4155 - val_acc: 0.8177\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4110 - acc: 0.8186 - val_loss: 0.4139 - val_acc: 0.8144\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.4033 - acc: 0.8256 - val_loss: 0.4125 - val_acc: 0.8177\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4071 - acc: 0.8238 - val_loss: 0.4140 - val_acc: 0.8161\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4046 - acc: 0.8197 - val_loss: 0.4148 - val_acc: 0.8161\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4034 - acc: 0.8228 - val_loss: 0.4126 - val_acc: 0.8161\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4029 - acc: 0.8252 - val_loss: 0.4117 - val_acc: 0.8194\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4021 - acc: 0.8267 - val_loss: 0.4119 - val_acc: 0.8177\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4019 - acc: 0.8250 - val_loss: 0.4135 - val_acc: 0.8128\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4017 - acc: 0.8236 - val_loss: 0.4128 - val_acc: 0.8177\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3989 - acc: 0.8278 - val_loss: 0.4105 - val_acc: 0.8210\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4033 - acc: 0.8241 - val_loss: 0.4108 - val_acc: 0.8194\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.3987 - acc: 0.8272 - val_loss: 0.4111 - val_acc: 0.8177\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.3948 - acc: 0.8321 - val_loss: 0.4117 - val_acc: 0.8177\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.3956 - acc: 0.8269 - val_loss: 0.4104 - val_acc: 0.8177\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.3966 - acc: 0.8258 - val_loss: 0.4104 - val_acc: 0.8177\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3929 - acc: 0.8289 - val_loss: 0.4111 - val_acc: 0.8177\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.3942 - acc: 0.8294 - val_loss: 0.4099 - val_acc: 0.8177\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.3919 - acc: 0.8296 - val_loss: 0.4094 - val_acc: 0.8194\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3940 - acc: 0.8316 - val_loss: 0.4095 - val_acc: 0.8161\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3884 - acc: 0.8303 - val_loss: 0.4110 - val_acc: 0.8194\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.3902 - acc: 0.8351 - val_loss: 0.4104 - val_acc: 0.8194\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3908 - acc: 0.8292 - val_loss: 0.4090 - val_acc: 0.8161\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3863 - acc: 0.8329 - val_loss: 0.4094 - val_acc: 0.8210\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3890 - acc: 0.8312 - val_loss: 0.4095 - val_acc: 0.8210\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.3833 - acc: 0.8329 - val_loss: 0.4106 - val_acc: 0.8210\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.3848 - acc: 0.8312 - val_loss: 0.4106 - val_acc: 0.8210\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.3851 - acc: 0.8318 - val_loss: 0.4077 - val_acc: 0.8227\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3852 - acc: 0.8289 - val_loss: 0.4090 - val_acc: 0.8227\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.3833 - acc: 0.8321 - val_loss: 0.4096 - val_acc: 0.8210\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.3821 - acc: 0.8360 - val_loss: 0.4088 - val_acc: 0.8227\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3802 - acc: 0.8369 - val_loss: 0.4071 - val_acc: 0.8227\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.3818 - acc: 0.8352 - val_loss: 0.4065 - val_acc: 0.8243\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.3805 - acc: 0.8376 - val_loss: 0.4108 - val_acc: 0.8210\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.3785 - acc: 0.8362 - val_loss: 0.4087 - val_acc: 0.8243\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.3777 - acc: 0.8362 - val_loss: 0.4061 - val_acc: 0.8227\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.3767 - acc: 0.8374 - val_loss: 0.4086 - val_acc: 0.8243\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.3770 - acc: 0.8407 - val_loss: 0.4112 - val_acc: 0.8227\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.3749 - acc: 0.8393 - val_loss: 0.4078 - val_acc: 0.8243\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3713 - acc: 0.8413 - val_loss: 0.4084 - val_acc: 0.8243\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3717 - acc: 0.8391 - val_loss: 0.4087 - val_acc: 0.8243\n",
      "Training with parameters {'batch_size': 3500, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_42\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_43 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.7757 - acc: 0.4351 - val_loss: 0.7315 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.7177 - acc: 0.4574 - val_loss: 0.6849 - val_acc: 0.5944\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6867 - acc: 0.5373 - val_loss: 0.6596 - val_acc: 0.6092\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6740 - acc: 0.5725 - val_loss: 0.6460 - val_acc: 0.6125\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6639 - acc: 0.5698 - val_loss: 0.6361 - val_acc: 0.6125\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6566 - acc: 0.5707 - val_loss: 0.6263 - val_acc: 0.6125\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6445 - acc: 0.5840 - val_loss: 0.6166 - val_acc: 0.6273\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6334 - acc: 0.6178 - val_loss: 0.6088 - val_acc: 0.7258\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6188 - acc: 0.6709 - val_loss: 0.6040 - val_acc: 0.7061\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6077 - acc: 0.7046 - val_loss: 0.6015 - val_acc: 0.6732\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6014 - acc: 0.7030 - val_loss: 0.5990 - val_acc: 0.6585\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5926 - acc: 0.6975 - val_loss: 0.5942 - val_acc: 0.6601\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5868 - acc: 0.6984 - val_loss: 0.5863 - val_acc: 0.6732\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.5782 - acc: 0.7052 - val_loss: 0.5764 - val_acc: 0.6814\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5702 - acc: 0.7154 - val_loss: 0.5660 - val_acc: 0.6913\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5614 - acc: 0.7393 - val_loss: 0.5569 - val_acc: 0.7209\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5553 - acc: 0.7449 - val_loss: 0.5494 - val_acc: 0.7307\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5491 - acc: 0.7457 - val_loss: 0.5425 - val_acc: 0.7406\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5400 - acc: 0.7522 - val_loss: 0.5365 - val_acc: 0.7373\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5352 - acc: 0.7526 - val_loss: 0.5317 - val_acc: 0.7406\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5256 - acc: 0.7550 - val_loss: 0.5268 - val_acc: 0.7373\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5214 - acc: 0.7584 - val_loss: 0.5203 - val_acc: 0.7537\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5141 - acc: 0.7648 - val_loss: 0.5117 - val_acc: 0.7570\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.5079 - acc: 0.7701 - val_loss: 0.5036 - val_acc: 0.7668\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5029 - acc: 0.7758 - val_loss: 0.4975 - val_acc: 0.7734\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4972 - acc: 0.7789 - val_loss: 0.4932 - val_acc: 0.7783\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4934 - acc: 0.7770 - val_loss: 0.4895 - val_acc: 0.7816\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4861 - acc: 0.7825 - val_loss: 0.4854 - val_acc: 0.7833\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4819 - acc: 0.7878 - val_loss: 0.4805 - val_acc: 0.7816\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4783 - acc: 0.7905 - val_loss: 0.4767 - val_acc: 0.7849\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4733 - acc: 0.7918 - val_loss: 0.4722 - val_acc: 0.7865\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4692 - acc: 0.7927 - val_loss: 0.4675 - val_acc: 0.7865\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4658 - acc: 0.7927 - val_loss: 0.4643 - val_acc: 0.7882\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4600 - acc: 0.8017 - val_loss: 0.4623 - val_acc: 0.7849\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4572 - acc: 0.7960 - val_loss: 0.4586 - val_acc: 0.7882\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4536 - acc: 0.8020 - val_loss: 0.4537 - val_acc: 0.7947\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4522 - acc: 0.8024 - val_loss: 0.4511 - val_acc: 0.7947\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4505 - acc: 0.8031 - val_loss: 0.4502 - val_acc: 0.7980\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4470 - acc: 0.8062 - val_loss: 0.4485 - val_acc: 0.7964\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4454 - acc: 0.8046 - val_loss: 0.4429 - val_acc: 0.7997\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4428 - acc: 0.8099 - val_loss: 0.4398 - val_acc: 0.8030\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4366 - acc: 0.8090 - val_loss: 0.4397 - val_acc: 0.7997\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4367 - acc: 0.8126 - val_loss: 0.4390 - val_acc: 0.8013\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4332 - acc: 0.8119 - val_loss: 0.4358 - val_acc: 0.8030\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4303 - acc: 0.8166 - val_loss: 0.4322 - val_acc: 0.8046\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4298 - acc: 0.8134 - val_loss: 0.4305 - val_acc: 0.8062\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4265 - acc: 0.8117 - val_loss: 0.4309 - val_acc: 0.8079\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4274 - acc: 0.8152 - val_loss: 0.4302 - val_acc: 0.8095\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4241 - acc: 0.8146 - val_loss: 0.4255 - val_acc: 0.8128\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4201 - acc: 0.8177 - val_loss: 0.4235 - val_acc: 0.8128\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4200 - acc: 0.8176 - val_loss: 0.4234 - val_acc: 0.8112\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4185 - acc: 0.8212 - val_loss: 0.4232 - val_acc: 0.8112\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4155 - acc: 0.8186 - val_loss: 0.4208 - val_acc: 0.8161\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4152 - acc: 0.8207 - val_loss: 0.4201 - val_acc: 0.8144\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4133 - acc: 0.8197 - val_loss: 0.4213 - val_acc: 0.8161\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4151 - acc: 0.8214 - val_loss: 0.4195 - val_acc: 0.8144\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4103 - acc: 0.8219 - val_loss: 0.4181 - val_acc: 0.8161\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4102 - acc: 0.8252 - val_loss: 0.4171 - val_acc: 0.8161\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4099 - acc: 0.8227 - val_loss: 0.4173 - val_acc: 0.8144\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4059 - acc: 0.8258 - val_loss: 0.4193 - val_acc: 0.8161\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4064 - acc: 0.8241 - val_loss: 0.4154 - val_acc: 0.8161\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4040 - acc: 0.8261 - val_loss: 0.4129 - val_acc: 0.8194\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4013 - acc: 0.8272 - val_loss: 0.4144 - val_acc: 0.8177\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4019 - acc: 0.8278 - val_loss: 0.4157 - val_acc: 0.8194\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3983 - acc: 0.8283 - val_loss: 0.4119 - val_acc: 0.8227\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3988 - acc: 0.8296 - val_loss: 0.4122 - val_acc: 0.8227\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3997 - acc: 0.8294 - val_loss: 0.4147 - val_acc: 0.8227\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3933 - acc: 0.8300 - val_loss: 0.4136 - val_acc: 0.8227\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3949 - acc: 0.8325 - val_loss: 0.4111 - val_acc: 0.8227\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3943 - acc: 0.8305 - val_loss: 0.4113 - val_acc: 0.8210\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3917 - acc: 0.8332 - val_loss: 0.4141 - val_acc: 0.8227\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3907 - acc: 0.8318 - val_loss: 0.4125 - val_acc: 0.8227\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3886 - acc: 0.8336 - val_loss: 0.4086 - val_acc: 0.8210\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3879 - acc: 0.8318 - val_loss: 0.4101 - val_acc: 0.8210\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3863 - acc: 0.8358 - val_loss: 0.4125 - val_acc: 0.8227\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3870 - acc: 0.8365 - val_loss: 0.4097 - val_acc: 0.8194\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3851 - acc: 0.8340 - val_loss: 0.4079 - val_acc: 0.8227\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3818 - acc: 0.8365 - val_loss: 0.4094 - val_acc: 0.8194\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3825 - acc: 0.8358 - val_loss: 0.4100 - val_acc: 0.8194\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3823 - acc: 0.8360 - val_loss: 0.4084 - val_acc: 0.8210\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3793 - acc: 0.8369 - val_loss: 0.4105 - val_acc: 0.8177\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.3780 - acc: 0.8391 - val_loss: 0.4074 - val_acc: 0.8210\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3795 - acc: 0.8393 - val_loss: 0.4068 - val_acc: 0.8227\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3764 - acc: 0.8358 - val_loss: 0.4108 - val_acc: 0.8177\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3751 - acc: 0.8411 - val_loss: 0.4119 - val_acc: 0.8161\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3713 - acc: 0.8418 - val_loss: 0.4088 - val_acc: 0.8161\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3732 - acc: 0.8411 - val_loss: 0.4083 - val_acc: 0.8161\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3707 - acc: 0.8404 - val_loss: 0.4097 - val_acc: 0.8161\n",
      "Training with parameters {'batch_size': 3500, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_44 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_123 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.8416 - acc: 0.4351 - val_loss: 0.8173 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.7897 - acc: 0.4392 - val_loss: 0.7672 - val_acc: 0.3892\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.7512 - acc: 0.4399 - val_loss: 0.7275 - val_acc: 0.3924\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.7187 - acc: 0.4605 - val_loss: 0.6974 - val_acc: 0.4795\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6968 - acc: 0.5059 - val_loss: 0.6753 - val_acc: 0.6240\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6811 - acc: 0.5762 - val_loss: 0.6595 - val_acc: 0.6371\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6697 - acc: 0.6048 - val_loss: 0.6480 - val_acc: 0.6305\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6624 - acc: 0.6003 - val_loss: 0.6393 - val_acc: 0.6207\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6566 - acc: 0.6004 - val_loss: 0.6321 - val_acc: 0.6190\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6506 - acc: 0.5911 - val_loss: 0.6256 - val_acc: 0.6223\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6442 - acc: 0.6072 - val_loss: 0.6195 - val_acc: 0.6305\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6375 - acc: 0.6174 - val_loss: 0.6137 - val_acc: 0.6437\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6286 - acc: 0.6422 - val_loss: 0.6085 - val_acc: 0.6782\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6224 - acc: 0.6709 - val_loss: 0.6041 - val_acc: 0.7291\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6141 - acc: 0.6986 - val_loss: 0.6004 - val_acc: 0.7258\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6079 - acc: 0.7039 - val_loss: 0.5970 - val_acc: 0.7094\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.6017 - acc: 0.7117 - val_loss: 0.5937 - val_acc: 0.6995\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5963 - acc: 0.7057 - val_loss: 0.5902 - val_acc: 0.6962\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5946 - acc: 0.6980 - val_loss: 0.5863 - val_acc: 0.6979\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5861 - acc: 0.7105 - val_loss: 0.5818 - val_acc: 0.7011\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5844 - acc: 0.7066 - val_loss: 0.5769 - val_acc: 0.7011\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5763 - acc: 0.7148 - val_loss: 0.5716 - val_acc: 0.7061\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5724 - acc: 0.7183 - val_loss: 0.5659 - val_acc: 0.7126\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5671 - acc: 0.7269 - val_loss: 0.5603 - val_acc: 0.7159\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.5636 - acc: 0.7225 - val_loss: 0.5548 - val_acc: 0.7176\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5590 - acc: 0.7349 - val_loss: 0.5496 - val_acc: 0.7176\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.5515 - acc: 0.7387 - val_loss: 0.5447 - val_acc: 0.7209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5484 - acc: 0.7431 - val_loss: 0.5398 - val_acc: 0.7307\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5414 - acc: 0.7462 - val_loss: 0.5349 - val_acc: 0.7356\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.5393 - acc: 0.7471 - val_loss: 0.5299 - val_acc: 0.7323\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5286 - acc: 0.7553 - val_loss: 0.5246 - val_acc: 0.7373\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5260 - acc: 0.7579 - val_loss: 0.5189 - val_acc: 0.7422\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5221 - acc: 0.7630 - val_loss: 0.5133 - val_acc: 0.7438\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5170 - acc: 0.7625 - val_loss: 0.5076 - val_acc: 0.7471\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5155 - acc: 0.7634 - val_loss: 0.5025 - val_acc: 0.7488\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5068 - acc: 0.7683 - val_loss: 0.4979 - val_acc: 0.7553\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.5029 - acc: 0.7725 - val_loss: 0.4942 - val_acc: 0.7603\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4991 - acc: 0.7763 - val_loss: 0.4899 - val_acc: 0.7603\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4968 - acc: 0.7721 - val_loss: 0.4851 - val_acc: 0.7652\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4932 - acc: 0.7756 - val_loss: 0.4805 - val_acc: 0.7701\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4898 - acc: 0.7796 - val_loss: 0.4775 - val_acc: 0.7734\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4846 - acc: 0.7829 - val_loss: 0.4751 - val_acc: 0.7734\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4800 - acc: 0.7851 - val_loss: 0.4724 - val_acc: 0.7767\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4763 - acc: 0.7874 - val_loss: 0.4693 - val_acc: 0.7783\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4752 - acc: 0.7874 - val_loss: 0.4653 - val_acc: 0.7816\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4724 - acc: 0.7900 - val_loss: 0.4619 - val_acc: 0.7849\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4711 - acc: 0.7874 - val_loss: 0.4594 - val_acc: 0.7882\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4662 - acc: 0.7922 - val_loss: 0.4573 - val_acc: 0.7882\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4646 - acc: 0.7920 - val_loss: 0.4552 - val_acc: 0.7931\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4611 - acc: 0.7971 - val_loss: 0.4530 - val_acc: 0.7947\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4597 - acc: 0.7969 - val_loss: 0.4506 - val_acc: 0.7997\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4598 - acc: 0.7940 - val_loss: 0.4476 - val_acc: 0.8013\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4565 - acc: 0.7953 - val_loss: 0.4460 - val_acc: 0.8030\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4546 - acc: 0.7991 - val_loss: 0.4457 - val_acc: 0.7997\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4506 - acc: 0.8017 - val_loss: 0.4434 - val_acc: 0.8013\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4494 - acc: 0.8026 - val_loss: 0.4406 - val_acc: 0.8062\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4486 - acc: 0.8053 - val_loss: 0.4378 - val_acc: 0.8095\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4483 - acc: 0.8039 - val_loss: 0.4371 - val_acc: 0.8079\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.4471 - acc: 0.8015 - val_loss: 0.4366 - val_acc: 0.8013\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4442 - acc: 0.8044 - val_loss: 0.4358 - val_acc: 0.8013\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4403 - acc: 0.8048 - val_loss: 0.4331 - val_acc: 0.8095\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4418 - acc: 0.8081 - val_loss: 0.4307 - val_acc: 0.8128\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4393 - acc: 0.8026 - val_loss: 0.4306 - val_acc: 0.8128\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4358 - acc: 0.8086 - val_loss: 0.4310 - val_acc: 0.8095\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4345 - acc: 0.8079 - val_loss: 0.4291 - val_acc: 0.8079\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4341 - acc: 0.8086 - val_loss: 0.4266 - val_acc: 0.8128\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4319 - acc: 0.8099 - val_loss: 0.4255 - val_acc: 0.8128\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4291 - acc: 0.8117 - val_loss: 0.4251 - val_acc: 0.8079\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4312 - acc: 0.8128 - val_loss: 0.4252 - val_acc: 0.8095\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4282 - acc: 0.8117 - val_loss: 0.4237 - val_acc: 0.8079\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4251 - acc: 0.8135 - val_loss: 0.4230 - val_acc: 0.8079\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4303 - acc: 0.8124 - val_loss: 0.4226 - val_acc: 0.8095\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4246 - acc: 0.8148 - val_loss: 0.4230 - val_acc: 0.8079\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4264 - acc: 0.8143 - val_loss: 0.4230 - val_acc: 0.8095\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.4241 - acc: 0.8168 - val_loss: 0.4209 - val_acc: 0.8095\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4219 - acc: 0.8135 - val_loss: 0.4187 - val_acc: 0.8161\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4218 - acc: 0.8119 - val_loss: 0.4188 - val_acc: 0.8128\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.4180 - acc: 0.8192 - val_loss: 0.4189 - val_acc: 0.8112\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4147 - acc: 0.8199 - val_loss: 0.4177 - val_acc: 0.8112\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4154 - acc: 0.8190 - val_loss: 0.4174 - val_acc: 0.8112\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4164 - acc: 0.8170 - val_loss: 0.4165 - val_acc: 0.8095\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4154 - acc: 0.8177 - val_loss: 0.4179 - val_acc: 0.8112\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.4177 - acc: 0.8163 - val_loss: 0.4166 - val_acc: 0.8095\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.4163 - acc: 0.8146 - val_loss: 0.4145 - val_acc: 0.8128\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4125 - acc: 0.8225 - val_loss: 0.4148 - val_acc: 0.8112\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4072 - acc: 0.8245 - val_loss: 0.4157 - val_acc: 0.8128\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4100 - acc: 0.8197 - val_loss: 0.4159 - val_acc: 0.8144\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4094 - acc: 0.8234 - val_loss: 0.4139 - val_acc: 0.8144\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4065 - acc: 0.8221 - val_loss: 0.4124 - val_acc: 0.8177\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4105 - acc: 0.8170 - val_loss: 0.4139 - val_acc: 0.8128\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4046 - acc: 0.8227 - val_loss: 0.4149 - val_acc: 0.8128\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4055 - acc: 0.8217 - val_loss: 0.4128 - val_acc: 0.8144\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4042 - acc: 0.8243 - val_loss: 0.4124 - val_acc: 0.8144\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4042 - acc: 0.8250 - val_loss: 0.4120 - val_acc: 0.8194\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4044 - acc: 0.8269 - val_loss: 0.4125 - val_acc: 0.8144\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4038 - acc: 0.8252 - val_loss: 0.4128 - val_acc: 0.8161\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.3976 - acc: 0.8265 - val_loss: 0.4129 - val_acc: 0.8161\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4003 - acc: 0.8234 - val_loss: 0.4125 - val_acc: 0.8144\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3990 - acc: 0.8265 - val_loss: 0.4107 - val_acc: 0.8210\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.3946 - acc: 0.8321 - val_loss: 0.4117 - val_acc: 0.8161\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.3981 - acc: 0.8307 - val_loss: 0.4147 - val_acc: 0.8161\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.3963 - acc: 0.8256 - val_loss: 0.4134 - val_acc: 0.8161\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.3946 - acc: 0.8274 - val_loss: 0.4111 - val_acc: 0.8161\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.3960 - acc: 0.8276 - val_loss: 0.4104 - val_acc: 0.8161\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.3930 - acc: 0.8316 - val_loss: 0.4123 - val_acc: 0.8177\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.3918 - acc: 0.8296 - val_loss: 0.4114 - val_acc: 0.8161\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.3927 - acc: 0.8305 - val_loss: 0.4104 - val_acc: 0.8161\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.3912 - acc: 0.8298 - val_loss: 0.4106 - val_acc: 0.8161\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3897 - acc: 0.8332 - val_loss: 0.4124 - val_acc: 0.8194\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.3888 - acc: 0.8343 - val_loss: 0.4124 - val_acc: 0.8194\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.3895 - acc: 0.8314 - val_loss: 0.4087 - val_acc: 0.8194\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.3849 - acc: 0.8314 - val_loss: 0.4086 - val_acc: 0.8194\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.3871 - acc: 0.8347 - val_loss: 0.4101 - val_acc: 0.8210\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.3824 - acc: 0.8365 - val_loss: 0.4104 - val_acc: 0.8210\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.3818 - acc: 0.8371 - val_loss: 0.4099 - val_acc: 0.8210\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.3833 - acc: 0.8345 - val_loss: 0.4100 - val_acc: 0.8210\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.3816 - acc: 0.8321 - val_loss: 0.4106 - val_acc: 0.8243\n",
      "Training with parameters {'batch_size': 3500, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_45 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_126 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_127 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.6724 - acc: 0.6006 - val_loss: 0.6646 - val_acc: 0.5895\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6589 - acc: 0.6344 - val_loss: 0.6502 - val_acc: 0.6371\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6485 - acc: 0.6453 - val_loss: 0.6397 - val_acc: 0.6568\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6428 - acc: 0.6404 - val_loss: 0.6318 - val_acc: 0.6634\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6347 - acc: 0.6544 - val_loss: 0.6253 - val_acc: 0.6749\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6303 - acc: 0.6543 - val_loss: 0.6198 - val_acc: 0.6814\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6222 - acc: 0.6723 - val_loss: 0.6152 - val_acc: 0.6749\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6158 - acc: 0.6851 - val_loss: 0.6111 - val_acc: 0.6716\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6117 - acc: 0.6959 - val_loss: 0.6077 - val_acc: 0.6847\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6052 - acc: 0.6993 - val_loss: 0.6044 - val_acc: 0.6814\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5991 - acc: 0.7079 - val_loss: 0.6011 - val_acc: 0.6782\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5977 - acc: 0.7011 - val_loss: 0.5977 - val_acc: 0.6831\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5915 - acc: 0.7072 - val_loss: 0.5939 - val_acc: 0.6864\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5852 - acc: 0.7132 - val_loss: 0.5898 - val_acc: 0.6847\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5819 - acc: 0.7130 - val_loss: 0.5854 - val_acc: 0.6897\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5775 - acc: 0.7152 - val_loss: 0.5807 - val_acc: 0.6929\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5740 - acc: 0.7252 - val_loss: 0.5762 - val_acc: 0.7061\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5707 - acc: 0.7274 - val_loss: 0.5720 - val_acc: 0.7126\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5659 - acc: 0.7342 - val_loss: 0.5682 - val_acc: 0.7126\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5614 - acc: 0.7382 - val_loss: 0.5646 - val_acc: 0.7126\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5592 - acc: 0.7347 - val_loss: 0.5610 - val_acc: 0.7126\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5556 - acc: 0.7409 - val_loss: 0.5576 - val_acc: 0.7192\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5524 - acc: 0.7440 - val_loss: 0.5546 - val_acc: 0.7241\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5483 - acc: 0.7442 - val_loss: 0.5516 - val_acc: 0.7258\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5448 - acc: 0.7488 - val_loss: 0.5487 - val_acc: 0.7274\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5410 - acc: 0.7508 - val_loss: 0.5457 - val_acc: 0.7274\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5392 - acc: 0.7535 - val_loss: 0.5425 - val_acc: 0.7307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5351 - acc: 0.7586 - val_loss: 0.5388 - val_acc: 0.7373\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5321 - acc: 0.7557 - val_loss: 0.5354 - val_acc: 0.7406\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5297 - acc: 0.7590 - val_loss: 0.5320 - val_acc: 0.7455\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5258 - acc: 0.7661 - val_loss: 0.5290 - val_acc: 0.7488\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5240 - acc: 0.7648 - val_loss: 0.5262 - val_acc: 0.7521\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5209 - acc: 0.7665 - val_loss: 0.5236 - val_acc: 0.7521\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.5178 - acc: 0.7677 - val_loss: 0.5211 - val_acc: 0.7504\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5154 - acc: 0.7677 - val_loss: 0.5186 - val_acc: 0.7537\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.5129 - acc: 0.7690 - val_loss: 0.5162 - val_acc: 0.7553\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5105 - acc: 0.7725 - val_loss: 0.5139 - val_acc: 0.7603\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5071 - acc: 0.7743 - val_loss: 0.5115 - val_acc: 0.7635\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.5048 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7652\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.5026 - acc: 0.7756 - val_loss: 0.5065 - val_acc: 0.7652\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5022 - acc: 0.7767 - val_loss: 0.5040 - val_acc: 0.7635\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.5002 - acc: 0.7781 - val_loss: 0.5018 - val_acc: 0.7635\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4985 - acc: 0.7780 - val_loss: 0.4998 - val_acc: 0.7685\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4963 - acc: 0.7800 - val_loss: 0.4979 - val_acc: 0.7701\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4938 - acc: 0.7838 - val_loss: 0.4959 - val_acc: 0.7685\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4927 - acc: 0.7801 - val_loss: 0.4941 - val_acc: 0.7701\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4902 - acc: 0.7847 - val_loss: 0.4925 - val_acc: 0.7701\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4877 - acc: 0.7827 - val_loss: 0.4908 - val_acc: 0.7734\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4873 - acc: 0.7823 - val_loss: 0.4889 - val_acc: 0.7750\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4852 - acc: 0.7845 - val_loss: 0.4869 - val_acc: 0.7750\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4822 - acc: 0.7867 - val_loss: 0.4853 - val_acc: 0.7767\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4832 - acc: 0.7887 - val_loss: 0.4842 - val_acc: 0.7750\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4816 - acc: 0.7902 - val_loss: 0.4832 - val_acc: 0.7767\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4780 - acc: 0.7907 - val_loss: 0.4820 - val_acc: 0.7783\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4775 - acc: 0.7911 - val_loss: 0.4803 - val_acc: 0.7783\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4756 - acc: 0.7918 - val_loss: 0.4784 - val_acc: 0.7783\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4738 - acc: 0.7938 - val_loss: 0.4765 - val_acc: 0.7816\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4728 - acc: 0.7916 - val_loss: 0.4749 - val_acc: 0.7816\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4701 - acc: 0.7947 - val_loss: 0.4737 - val_acc: 0.7833\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4712 - acc: 0.7975 - val_loss: 0.4734 - val_acc: 0.7882\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4703 - acc: 0.7942 - val_loss: 0.4727 - val_acc: 0.7849\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4681 - acc: 0.7937 - val_loss: 0.4713 - val_acc: 0.7898\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4666 - acc: 0.7937 - val_loss: 0.4700 - val_acc: 0.7882\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4648 - acc: 0.7973 - val_loss: 0.4686 - val_acc: 0.7931\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4643 - acc: 0.8002 - val_loss: 0.4674 - val_acc: 0.7980\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4644 - acc: 0.7982 - val_loss: 0.4662 - val_acc: 0.7980\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4638 - acc: 0.7960 - val_loss: 0.4654 - val_acc: 0.7980\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4611 - acc: 0.7978 - val_loss: 0.4645 - val_acc: 0.7980\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4600 - acc: 0.7988 - val_loss: 0.4629 - val_acc: 0.7947\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4590 - acc: 0.8015 - val_loss: 0.4623 - val_acc: 0.7947\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4588 - acc: 0.8051 - val_loss: 0.4618 - val_acc: 0.7964\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4575 - acc: 0.7988 - val_loss: 0.4615 - val_acc: 0.7997\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4559 - acc: 0.7999 - val_loss: 0.4605 - val_acc: 0.7980\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4537 - acc: 0.8022 - val_loss: 0.4590 - val_acc: 0.7980\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4545 - acc: 0.8035 - val_loss: 0.4569 - val_acc: 0.7964\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4518 - acc: 0.8059 - val_loss: 0.4554 - val_acc: 0.7964\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4528 - acc: 0.8028 - val_loss: 0.4553 - val_acc: 0.7964\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4511 - acc: 0.8057 - val_loss: 0.4553 - val_acc: 0.7964\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4522 - acc: 0.8048 - val_loss: 0.4547 - val_acc: 0.7980\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4494 - acc: 0.8075 - val_loss: 0.4536 - val_acc: 0.7980\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4501 - acc: 0.8053 - val_loss: 0.4526 - val_acc: 0.7997\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4486 - acc: 0.8117 - val_loss: 0.4515 - val_acc: 0.7997\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4475 - acc: 0.8064 - val_loss: 0.4505 - val_acc: 0.7997\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4484 - acc: 0.8092 - val_loss: 0.4493 - val_acc: 0.8013\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4468 - acc: 0.8066 - val_loss: 0.4483 - val_acc: 0.8013\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4437 - acc: 0.8081 - val_loss: 0.4477 - val_acc: 0.8013\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4433 - acc: 0.8061 - val_loss: 0.4478 - val_acc: 0.8030\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4429 - acc: 0.8101 - val_loss: 0.4476 - val_acc: 0.8046\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4439 - acc: 0.8070 - val_loss: 0.4469 - val_acc: 0.8046\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4426 - acc: 0.8090 - val_loss: 0.4453 - val_acc: 0.8046\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4399 - acc: 0.8079 - val_loss: 0.4440 - val_acc: 0.8046\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4413 - acc: 0.8092 - val_loss: 0.4430 - val_acc: 0.8046\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4399 - acc: 0.8124 - val_loss: 0.4427 - val_acc: 0.8046\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4405 - acc: 0.8106 - val_loss: 0.4421 - val_acc: 0.8046\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4367 - acc: 0.8128 - val_loss: 0.4417 - val_acc: 0.8046\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4377 - acc: 0.8112 - val_loss: 0.4411 - val_acc: 0.8046\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4365 - acc: 0.8119 - val_loss: 0.4407 - val_acc: 0.8062\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4372 - acc: 0.8115 - val_loss: 0.4397 - val_acc: 0.8046\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4373 - acc: 0.8099 - val_loss: 0.4387 - val_acc: 0.8030\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4338 - acc: 0.8124 - val_loss: 0.4382 - val_acc: 0.8030\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4338 - acc: 0.8135 - val_loss: 0.4380 - val_acc: 0.8030\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4328 - acc: 0.8134 - val_loss: 0.4381 - val_acc: 0.8046\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4340 - acc: 0.8155 - val_loss: 0.4381 - val_acc: 0.8062\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4324 - acc: 0.8141 - val_loss: 0.4381 - val_acc: 0.8062\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4326 - acc: 0.8115 - val_loss: 0.4372 - val_acc: 0.8062\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4330 - acc: 0.8115 - val_loss: 0.4355 - val_acc: 0.8079\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4320 - acc: 0.8095 - val_loss: 0.4345 - val_acc: 0.8079\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4278 - acc: 0.8170 - val_loss: 0.4346 - val_acc: 0.8079\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4309 - acc: 0.8128 - val_loss: 0.4354 - val_acc: 0.8062\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4293 - acc: 0.8166 - val_loss: 0.4354 - val_acc: 0.8062\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4290 - acc: 0.8159 - val_loss: 0.4345 - val_acc: 0.8095\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4275 - acc: 0.8154 - val_loss: 0.4329 - val_acc: 0.8095\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4269 - acc: 0.8186 - val_loss: 0.4321 - val_acc: 0.8112\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4268 - acc: 0.8181 - val_loss: 0.4318 - val_acc: 0.8112\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4292 - acc: 0.8128 - val_loss: 0.4323 - val_acc: 0.8112\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4257 - acc: 0.8174 - val_loss: 0.4327 - val_acc: 0.8112\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4258 - acc: 0.8155 - val_loss: 0.4322 - val_acc: 0.8112\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4246 - acc: 0.8190 - val_loss: 0.4319 - val_acc: 0.8112\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4243 - acc: 0.8154 - val_loss: 0.4310 - val_acc: 0.8128\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4235 - acc: 0.8188 - val_loss: 0.4304 - val_acc: 0.8144\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4233 - acc: 0.8181 - val_loss: 0.4303 - val_acc: 0.8144\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4224 - acc: 0.8192 - val_loss: 0.4301 - val_acc: 0.8144\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4234 - acc: 0.8174 - val_loss: 0.4299 - val_acc: 0.8161\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4221 - acc: 0.8203 - val_loss: 0.4294 - val_acc: 0.8161\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4224 - acc: 0.8174 - val_loss: 0.4287 - val_acc: 0.8161\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4215 - acc: 0.8181 - val_loss: 0.4282 - val_acc: 0.8161\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4201 - acc: 0.8212 - val_loss: 0.4280 - val_acc: 0.8161\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4189 - acc: 0.8203 - val_loss: 0.4279 - val_acc: 0.8161\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4212 - acc: 0.8225 - val_loss: 0.4283 - val_acc: 0.8161\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4196 - acc: 0.8166 - val_loss: 0.4281 - val_acc: 0.8161\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4200 - acc: 0.8179 - val_loss: 0.4275 - val_acc: 0.8161\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4198 - acc: 0.8194 - val_loss: 0.4267 - val_acc: 0.8161\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4195 - acc: 0.8210 - val_loss: 0.4270 - val_acc: 0.8161\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4179 - acc: 0.8207 - val_loss: 0.4272 - val_acc: 0.8161\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4171 - acc: 0.8212 - val_loss: 0.4275 - val_acc: 0.8161\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4166 - acc: 0.8247 - val_loss: 0.4267 - val_acc: 0.8161\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4174 - acc: 0.8203 - val_loss: 0.4257 - val_acc: 0.8161\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4173 - acc: 0.8212 - val_loss: 0.4245 - val_acc: 0.8161\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4159 - acc: 0.8208 - val_loss: 0.4246 - val_acc: 0.8161\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4146 - acc: 0.8241 - val_loss: 0.4251 - val_acc: 0.8161\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4131 - acc: 0.8205 - val_loss: 0.4258 - val_acc: 0.8161\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4160 - acc: 0.8181 - val_loss: 0.4256 - val_acc: 0.8161\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4157 - acc: 0.8217 - val_loss: 0.4248 - val_acc: 0.8161\n",
      "Training with parameters {'batch_size': 3500, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_46 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_128 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_129 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_130 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.8437 - acc: 0.4351 - val_loss: 0.8017 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.7732 - acc: 0.4344 - val_loss: 0.7345 - val_acc: 0.3892\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.7240 - acc: 0.4517 - val_loss: 0.6914 - val_acc: 0.5271\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6929 - acc: 0.5203 - val_loss: 0.6662 - val_acc: 0.6108\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6774 - acc: 0.5641 - val_loss: 0.6514 - val_acc: 0.6125\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6694 - acc: 0.5738 - val_loss: 0.6409 - val_acc: 0.6158\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6624 - acc: 0.5747 - val_loss: 0.6319 - val_acc: 0.6174\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6513 - acc: 0.5815 - val_loss: 0.6230 - val_acc: 0.6207\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6404 - acc: 0.5957 - val_loss: 0.6148 - val_acc: 0.6552\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6310 - acc: 0.6262 - val_loss: 0.6085 - val_acc: 0.6962\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6169 - acc: 0.6740 - val_loss: 0.6045 - val_acc: 0.6995\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6103 - acc: 0.6966 - val_loss: 0.6021 - val_acc: 0.6765\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6000 - acc: 0.7041 - val_loss: 0.6004 - val_acc: 0.6650\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5937 - acc: 0.6948 - val_loss: 0.5975 - val_acc: 0.6683\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5877 - acc: 0.6975 - val_loss: 0.5923 - val_acc: 0.6667\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5840 - acc: 0.7015 - val_loss: 0.5846 - val_acc: 0.6700\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5749 - acc: 0.7132 - val_loss: 0.5753 - val_acc: 0.6880\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5664 - acc: 0.7216 - val_loss: 0.5661 - val_acc: 0.6913\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5600 - acc: 0.7309 - val_loss: 0.5580 - val_acc: 0.7126\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5549 - acc: 0.7420 - val_loss: 0.5510 - val_acc: 0.7143\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5481 - acc: 0.7429 - val_loss: 0.5447 - val_acc: 0.7143\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5422 - acc: 0.7537 - val_loss: 0.5394 - val_acc: 0.7192\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5347 - acc: 0.7531 - val_loss: 0.5345 - val_acc: 0.7176\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5298 - acc: 0.7528 - val_loss: 0.5291 - val_acc: 0.7225\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5250 - acc: 0.7524 - val_loss: 0.5227 - val_acc: 0.7323\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5169 - acc: 0.7592 - val_loss: 0.5146 - val_acc: 0.7488\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5134 - acc: 0.7650 - val_loss: 0.5065 - val_acc: 0.7619\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5080 - acc: 0.7716 - val_loss: 0.4998 - val_acc: 0.7668\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5016 - acc: 0.7752 - val_loss: 0.4956 - val_acc: 0.7701\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4989 - acc: 0.7767 - val_loss: 0.4931 - val_acc: 0.7701\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4924 - acc: 0.7811 - val_loss: 0.4902 - val_acc: 0.7668\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4893 - acc: 0.7783 - val_loss: 0.4860 - val_acc: 0.7750\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.4876 - acc: 0.7862 - val_loss: 0.4806 - val_acc: 0.7750\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.4823 - acc: 0.7874 - val_loss: 0.4760 - val_acc: 0.7833\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4778 - acc: 0.7909 - val_loss: 0.4723 - val_acc: 0.7849\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.4752 - acc: 0.7935 - val_loss: 0.4700 - val_acc: 0.7833\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4704 - acc: 0.7882 - val_loss: 0.4679 - val_acc: 0.7816\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4682 - acc: 0.7951 - val_loss: 0.4651 - val_acc: 0.7898\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4634 - acc: 0.7960 - val_loss: 0.4619 - val_acc: 0.7947\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.4615 - acc: 0.7995 - val_loss: 0.4582 - val_acc: 0.7931\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4597 - acc: 0.7989 - val_loss: 0.4539 - val_acc: 0.7980\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4567 - acc: 0.8015 - val_loss: 0.4506 - val_acc: 0.8030\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4538 - acc: 0.8028 - val_loss: 0.4490 - val_acc: 0.8013\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.4499 - acc: 0.8062 - val_loss: 0.4481 - val_acc: 0.7997\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4460 - acc: 0.8064 - val_loss: 0.4444 - val_acc: 0.8046\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4454 - acc: 0.8082 - val_loss: 0.4408 - val_acc: 0.8046\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4415 - acc: 0.8092 - val_loss: 0.4395 - val_acc: 0.8046\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.4420 - acc: 0.8104 - val_loss: 0.4398 - val_acc: 0.7997\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4385 - acc: 0.8099 - val_loss: 0.4380 - val_acc: 0.8013\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4363 - acc: 0.8141 - val_loss: 0.4356 - val_acc: 0.8030\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.4396 - acc: 0.8097 - val_loss: 0.4352 - val_acc: 0.8079\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.4329 - acc: 0.8135 - val_loss: 0.4343 - val_acc: 0.8046\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4314 - acc: 0.8108 - val_loss: 0.4314 - val_acc: 0.8062\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4306 - acc: 0.8155 - val_loss: 0.4292 - val_acc: 0.8062\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4297 - acc: 0.8159 - val_loss: 0.4281 - val_acc: 0.8062\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.4290 - acc: 0.8150 - val_loss: 0.4269 - val_acc: 0.8095\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.4233 - acc: 0.8179 - val_loss: 0.4252 - val_acc: 0.8095\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.4246 - acc: 0.8196 - val_loss: 0.4242 - val_acc: 0.8095\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.4213 - acc: 0.8194 - val_loss: 0.4247 - val_acc: 0.8112\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.4186 - acc: 0.8201 - val_loss: 0.4263 - val_acc: 0.8046\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4202 - acc: 0.8170 - val_loss: 0.4237 - val_acc: 0.8079\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4171 - acc: 0.8207 - val_loss: 0.4211 - val_acc: 0.8128\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4163 - acc: 0.8216 - val_loss: 0.4201 - val_acc: 0.8128\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4162 - acc: 0.8212 - val_loss: 0.4222 - val_acc: 0.8128\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4156 - acc: 0.8197 - val_loss: 0.4217 - val_acc: 0.8161\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4118 - acc: 0.8214 - val_loss: 0.4184 - val_acc: 0.8128\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4093 - acc: 0.8239 - val_loss: 0.4163 - val_acc: 0.8112\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.4112 - acc: 0.8214 - val_loss: 0.4159 - val_acc: 0.8095\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.4096 - acc: 0.8232 - val_loss: 0.4186 - val_acc: 0.8177\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4089 - acc: 0.8272 - val_loss: 0.4180 - val_acc: 0.8144\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4083 - acc: 0.8230 - val_loss: 0.4147 - val_acc: 0.8079\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4035 - acc: 0.8252 - val_loss: 0.4149 - val_acc: 0.8112\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4018 - acc: 0.8259 - val_loss: 0.4174 - val_acc: 0.8194\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4027 - acc: 0.8265 - val_loss: 0.4141 - val_acc: 0.8144\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4027 - acc: 0.8267 - val_loss: 0.4116 - val_acc: 0.8128\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.3998 - acc: 0.8320 - val_loss: 0.4115 - val_acc: 0.8144\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3988 - acc: 0.8318 - val_loss: 0.4137 - val_acc: 0.8177\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3979 - acc: 0.8301 - val_loss: 0.4131 - val_acc: 0.8177\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.3952 - acc: 0.8363 - val_loss: 0.4115 - val_acc: 0.8194\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.3938 - acc: 0.8336 - val_loss: 0.4121 - val_acc: 0.8194\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.3955 - acc: 0.8331 - val_loss: 0.4123 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': 3500, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_47 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_131 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_85 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_132 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_86 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.8756 - acc: 0.4350 - val_loss: 0.8560 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.8186 - acc: 0.4357 - val_loss: 0.8026 - val_acc: 0.3875\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.7757 - acc: 0.4359 - val_loss: 0.7616 - val_acc: 0.3875\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7429 - acc: 0.4444 - val_loss: 0.7296 - val_acc: 0.3924\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7199 - acc: 0.4548 - val_loss: 0.7049 - val_acc: 0.4335\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6992 - acc: 0.4988 - val_loss: 0.6862 - val_acc: 0.5550\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6835 - acc: 0.5588 - val_loss: 0.6720 - val_acc: 0.6388\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6753 - acc: 0.5853 - val_loss: 0.6607 - val_acc: 0.6864\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6677 - acc: 0.6178 - val_loss: 0.6515 - val_acc: 0.6847\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6589 - acc: 0.6406 - val_loss: 0.6438 - val_acc: 0.6814\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6532 - acc: 0.6364 - val_loss: 0.6368 - val_acc: 0.6913\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6445 - acc: 0.6588 - val_loss: 0.6304 - val_acc: 0.7126\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6431 - acc: 0.6566 - val_loss: 0.6247 - val_acc: 0.7373\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6338 - acc: 0.6836 - val_loss: 0.6198 - val_acc: 0.7471\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6271 - acc: 0.6887 - val_loss: 0.6154 - val_acc: 0.7323\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6202 - acc: 0.7041 - val_loss: 0.6116 - val_acc: 0.7094\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6178 - acc: 0.6991 - val_loss: 0.6082 - val_acc: 0.7110\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6113 - acc: 0.7022 - val_loss: 0.6052 - val_acc: 0.6962\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6068 - acc: 0.7028 - val_loss: 0.6022 - val_acc: 0.6979\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6024 - acc: 0.7145 - val_loss: 0.5988 - val_acc: 0.6979\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5964 - acc: 0.7123 - val_loss: 0.5949 - val_acc: 0.7011\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5922 - acc: 0.7168 - val_loss: 0.5906 - val_acc: 0.6995\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5883 - acc: 0.7165 - val_loss: 0.5860 - val_acc: 0.7011\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5834 - acc: 0.7249 - val_loss: 0.5815 - val_acc: 0.7028\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5790 - acc: 0.7331 - val_loss: 0.5771 - val_acc: 0.7077\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5793 - acc: 0.7300 - val_loss: 0.5729 - val_acc: 0.7077\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5723 - acc: 0.7351 - val_loss: 0.5690 - val_acc: 0.7094\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5679 - acc: 0.7360 - val_loss: 0.5651 - val_acc: 0.7044\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5636 - acc: 0.7353 - val_loss: 0.5613 - val_acc: 0.7061\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5591 - acc: 0.7429 - val_loss: 0.5575 - val_acc: 0.7094\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5552 - acc: 0.7398 - val_loss: 0.5538 - val_acc: 0.7110\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5540 - acc: 0.7440 - val_loss: 0.5501 - val_acc: 0.7143\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.5499 - acc: 0.7468 - val_loss: 0.5461 - val_acc: 0.7143\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5446 - acc: 0.7446 - val_loss: 0.5422 - val_acc: 0.7143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5416 - acc: 0.7504 - val_loss: 0.5383 - val_acc: 0.7192\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5373 - acc: 0.7522 - val_loss: 0.5345 - val_acc: 0.7274\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5348 - acc: 0.7599 - val_loss: 0.5305 - val_acc: 0.7340\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.5321 - acc: 0.7601 - val_loss: 0.5264 - val_acc: 0.7422\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5262 - acc: 0.7599 - val_loss: 0.5222 - val_acc: 0.7422\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5244 - acc: 0.7635 - val_loss: 0.5184 - val_acc: 0.7438\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5211 - acc: 0.7625 - val_loss: 0.5151 - val_acc: 0.7455\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5149 - acc: 0.7661 - val_loss: 0.5117 - val_acc: 0.7504\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5133 - acc: 0.7661 - val_loss: 0.5083 - val_acc: 0.7521\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5132 - acc: 0.7710 - val_loss: 0.5047 - val_acc: 0.7521\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5064 - acc: 0.7741 - val_loss: 0.5014 - val_acc: 0.7537\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5053 - acc: 0.7699 - val_loss: 0.4983 - val_acc: 0.7570\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5022 - acc: 0.7739 - val_loss: 0.4953 - val_acc: 0.7553\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4999 - acc: 0.7770 - val_loss: 0.4926 - val_acc: 0.7553\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4973 - acc: 0.7805 - val_loss: 0.4899 - val_acc: 0.7553\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4980 - acc: 0.7739 - val_loss: 0.4870 - val_acc: 0.7619\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4905 - acc: 0.7818 - val_loss: 0.4841 - val_acc: 0.7652\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4876 - acc: 0.7825 - val_loss: 0.4815 - val_acc: 0.7652\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4861 - acc: 0.7785 - val_loss: 0.4788 - val_acc: 0.7701\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4871 - acc: 0.7847 - val_loss: 0.4768 - val_acc: 0.7734\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4827 - acc: 0.7865 - val_loss: 0.4746 - val_acc: 0.7750\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4810 - acc: 0.7849 - val_loss: 0.4721 - val_acc: 0.7750\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4780 - acc: 0.7809 - val_loss: 0.4699 - val_acc: 0.7750\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4784 - acc: 0.7856 - val_loss: 0.4684 - val_acc: 0.7750\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4763 - acc: 0.7880 - val_loss: 0.4670 - val_acc: 0.7767\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4704 - acc: 0.7907 - val_loss: 0.4645 - val_acc: 0.7783\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4719 - acc: 0.7915 - val_loss: 0.4621 - val_acc: 0.7816\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4670 - acc: 0.7927 - val_loss: 0.4599 - val_acc: 0.7849\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4660 - acc: 0.7982 - val_loss: 0.4575 - val_acc: 0.7849\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4624 - acc: 0.7999 - val_loss: 0.4555 - val_acc: 0.7915\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4640 - acc: 0.7973 - val_loss: 0.4543 - val_acc: 0.7915\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4608 - acc: 0.8004 - val_loss: 0.4534 - val_acc: 0.7915\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4552 - acc: 0.7997 - val_loss: 0.4523 - val_acc: 0.7915\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4579 - acc: 0.7971 - val_loss: 0.4509 - val_acc: 0.7931\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4554 - acc: 0.7973 - val_loss: 0.4493 - val_acc: 0.7931\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4553 - acc: 0.7989 - val_loss: 0.4473 - val_acc: 0.7964\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4511 - acc: 0.8009 - val_loss: 0.4458 - val_acc: 0.7964\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4529 - acc: 0.8006 - val_loss: 0.4451 - val_acc: 0.7997\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4494 - acc: 0.8008 - val_loss: 0.4440 - val_acc: 0.7997\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4488 - acc: 0.8017 - val_loss: 0.4430 - val_acc: 0.7980\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4476 - acc: 0.8008 - val_loss: 0.4414 - val_acc: 0.7997\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4450 - acc: 0.8031 - val_loss: 0.4404 - val_acc: 0.7997\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4418 - acc: 0.8077 - val_loss: 0.4398 - val_acc: 0.7997\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4439 - acc: 0.8048 - val_loss: 0.4387 - val_acc: 0.7997\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4413 - acc: 0.8050 - val_loss: 0.4383 - val_acc: 0.7997\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4385 - acc: 0.8090 - val_loss: 0.4367 - val_acc: 0.8013\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.4372 - acc: 0.8088 - val_loss: 0.4342 - val_acc: 0.8013\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4379 - acc: 0.8130 - val_loss: 0.4334 - val_acc: 0.8030\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4338 - acc: 0.8124 - val_loss: 0.4332 - val_acc: 0.8046\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4356 - acc: 0.8086 - val_loss: 0.4328 - val_acc: 0.8062\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4351 - acc: 0.8082 - val_loss: 0.4318 - val_acc: 0.8062\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4361 - acc: 0.8112 - val_loss: 0.4312 - val_acc: 0.8062\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4340 - acc: 0.8090 - val_loss: 0.4308 - val_acc: 0.8062\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4336 - acc: 0.8097 - val_loss: 0.4307 - val_acc: 0.8062\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4301 - acc: 0.8166 - val_loss: 0.4302 - val_acc: 0.8062\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4318 - acc: 0.8163 - val_loss: 0.4286 - val_acc: 0.8062\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4309 - acc: 0.8103 - val_loss: 0.4272 - val_acc: 0.8095\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4276 - acc: 0.8126 - val_loss: 0.4260 - val_acc: 0.8128\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4265 - acc: 0.8117 - val_loss: 0.4260 - val_acc: 0.8112\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4264 - acc: 0.8110 - val_loss: 0.4259 - val_acc: 0.8095\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4259 - acc: 0.8137 - val_loss: 0.4263 - val_acc: 0.8079\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4255 - acc: 0.8141 - val_loss: 0.4258 - val_acc: 0.8079\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4207 - acc: 0.8170 - val_loss: 0.4241 - val_acc: 0.8128\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4243 - acc: 0.8174 - val_loss: 0.4231 - val_acc: 0.8128\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.4192 - acc: 0.8176 - val_loss: 0.4235 - val_acc: 0.8144\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4239 - acc: 0.8123 - val_loss: 0.4235 - val_acc: 0.8144\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4180 - acc: 0.8210 - val_loss: 0.4232 - val_acc: 0.8128\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4209 - acc: 0.8192 - val_loss: 0.4231 - val_acc: 0.8128\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4184 - acc: 0.8165 - val_loss: 0.4222 - val_acc: 0.8128\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4177 - acc: 0.8185 - val_loss: 0.4206 - val_acc: 0.8161\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4173 - acc: 0.8197 - val_loss: 0.4199 - val_acc: 0.8144\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4151 - acc: 0.8181 - val_loss: 0.4208 - val_acc: 0.8144\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4155 - acc: 0.8205 - val_loss: 0.4221 - val_acc: 0.8112\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4144 - acc: 0.8181 - val_loss: 0.4215 - val_acc: 0.8095\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4152 - acc: 0.8183 - val_loss: 0.4206 - val_acc: 0.8128\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4127 - acc: 0.8210 - val_loss: 0.4192 - val_acc: 0.8194\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4118 - acc: 0.8203 - val_loss: 0.4183 - val_acc: 0.8194\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4109 - acc: 0.8210 - val_loss: 0.4184 - val_acc: 0.8194\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4096 - acc: 0.8281 - val_loss: 0.4196 - val_acc: 0.8144\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4131 - acc: 0.8236 - val_loss: 0.4212 - val_acc: 0.8128\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4081 - acc: 0.8232 - val_loss: 0.4195 - val_acc: 0.8144\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4097 - acc: 0.8214 - val_loss: 0.4167 - val_acc: 0.8194\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4088 - acc: 0.8225 - val_loss: 0.4158 - val_acc: 0.8194\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4096 - acc: 0.8238 - val_loss: 0.4173 - val_acc: 0.8177\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4088 - acc: 0.8239 - val_loss: 0.4204 - val_acc: 0.8128\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4056 - acc: 0.8270 - val_loss: 0.4213 - val_acc: 0.8112\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4057 - acc: 0.8232 - val_loss: 0.4183 - val_acc: 0.8112\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4063 - acc: 0.8230 - val_loss: 0.4145 - val_acc: 0.8194\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4061 - acc: 0.8248 - val_loss: 0.4141 - val_acc: 0.8194\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4016 - acc: 0.8263 - val_loss: 0.4168 - val_acc: 0.8177\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4014 - acc: 0.8272 - val_loss: 0.4188 - val_acc: 0.8112\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.3977 - acc: 0.8281 - val_loss: 0.4173 - val_acc: 0.8144\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4010 - acc: 0.8287 - val_loss: 0.4147 - val_acc: 0.8227\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4016 - acc: 0.8285 - val_loss: 0.4134 - val_acc: 0.8227\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4001 - acc: 0.8283 - val_loss: 0.4140 - val_acc: 0.8210\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4008 - acc: 0.8276 - val_loss: 0.4166 - val_acc: 0.8144\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.3997 - acc: 0.8298 - val_loss: 0.4169 - val_acc: 0.8128\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4002 - acc: 0.8283 - val_loss: 0.4148 - val_acc: 0.8227\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.3967 - acc: 0.8287 - val_loss: 0.4133 - val_acc: 0.8227\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.3964 - acc: 0.8309 - val_loss: 0.4139 - val_acc: 0.8227\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.3956 - acc: 0.8307 - val_loss: 0.4139 - val_acc: 0.8227\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.3959 - acc: 0.8316 - val_loss: 0.4143 - val_acc: 0.8243\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.3933 - acc: 0.8303 - val_loss: 0.4143 - val_acc: 0.8227\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.3947 - acc: 0.8320 - val_loss: 0.4131 - val_acc: 0.8243\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.3954 - acc: 0.8300 - val_loss: 0.4129 - val_acc: 0.8243\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.3935 - acc: 0.8325 - val_loss: 0.4132 - val_acc: 0.8243\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.3945 - acc: 0.8300 - val_loss: 0.4142 - val_acc: 0.8210\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3887 - acc: 0.8374 - val_loss: 0.4137 - val_acc: 0.8227\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.3897 - acc: 0.8331 - val_loss: 0.4137 - val_acc: 0.8243\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.3924 - acc: 0.8356 - val_loss: 0.4142 - val_acc: 0.8243\n",
      "Training with parameters {'batch_size': 3500, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_48 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.7664 - acc: 0.4353 - val_loss: 0.7653 - val_acc: 0.3892\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.7391 - acc: 0.4384 - val_loss: 0.7376 - val_acc: 0.3990\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7190 - acc: 0.4528 - val_loss: 0.7146 - val_acc: 0.4319\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6994 - acc: 0.4824 - val_loss: 0.6958 - val_acc: 0.4795\n",
      "Epoch 5/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6885 - acc: 0.5355 - val_loss: 0.6806 - val_acc: 0.5681\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6781 - acc: 0.5833 - val_loss: 0.6686 - val_acc: 0.6650\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6689 - acc: 0.6227 - val_loss: 0.6591 - val_acc: 0.6716\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6626 - acc: 0.6347 - val_loss: 0.6514 - val_acc: 0.6749\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6554 - acc: 0.6428 - val_loss: 0.6449 - val_acc: 0.6552\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6516 - acc: 0.6362 - val_loss: 0.6393 - val_acc: 0.6502\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6490 - acc: 0.6273 - val_loss: 0.6343 - val_acc: 0.6519\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6430 - acc: 0.6366 - val_loss: 0.6296 - val_acc: 0.6601\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6379 - acc: 0.6455 - val_loss: 0.6253 - val_acc: 0.6782\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6342 - acc: 0.6539 - val_loss: 0.6212 - val_acc: 0.6979\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6291 - acc: 0.6710 - val_loss: 0.6174 - val_acc: 0.6880\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6237 - acc: 0.6858 - val_loss: 0.6140 - val_acc: 0.7011\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6203 - acc: 0.6922 - val_loss: 0.6108 - val_acc: 0.7110\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.6148 - acc: 0.6975 - val_loss: 0.6078 - val_acc: 0.7077\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6079 - acc: 0.7099 - val_loss: 0.6050 - val_acc: 0.7011\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6057 - acc: 0.7068 - val_loss: 0.6022 - val_acc: 0.6946\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6002 - acc: 0.7101 - val_loss: 0.5991 - val_acc: 0.6979\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5955 - acc: 0.7161 - val_loss: 0.5958 - val_acc: 0.6897\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5923 - acc: 0.7148 - val_loss: 0.5923 - val_acc: 0.6946\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5868 - acc: 0.7165 - val_loss: 0.5885 - val_acc: 0.6995\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5826 - acc: 0.7251 - val_loss: 0.5844 - val_acc: 0.7028\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5784 - acc: 0.7209 - val_loss: 0.5802 - val_acc: 0.7077\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5762 - acc: 0.7229 - val_loss: 0.5758 - val_acc: 0.7077\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5711 - acc: 0.7331 - val_loss: 0.5716 - val_acc: 0.7176\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5661 - acc: 0.7331 - val_loss: 0.5676 - val_acc: 0.7192\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5623 - acc: 0.7417 - val_loss: 0.5637 - val_acc: 0.7176\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5599 - acc: 0.7356 - val_loss: 0.5599 - val_acc: 0.7209\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5556 - acc: 0.7400 - val_loss: 0.5564 - val_acc: 0.7225\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5496 - acc: 0.7437 - val_loss: 0.5527 - val_acc: 0.7274\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5456 - acc: 0.7475 - val_loss: 0.5486 - val_acc: 0.7274\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5420 - acc: 0.7495 - val_loss: 0.5443 - val_acc: 0.7340\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5395 - acc: 0.7521 - val_loss: 0.5396 - val_acc: 0.7406\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5337 - acc: 0.7541 - val_loss: 0.5349 - val_acc: 0.7455\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5301 - acc: 0.7604 - val_loss: 0.5303 - val_acc: 0.7471\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5249 - acc: 0.7617 - val_loss: 0.5261 - val_acc: 0.7471\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5223 - acc: 0.7639 - val_loss: 0.5222 - val_acc: 0.7488\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5204 - acc: 0.7645 - val_loss: 0.5186 - val_acc: 0.7504\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5168 - acc: 0.7661 - val_loss: 0.5154 - val_acc: 0.7521\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5107 - acc: 0.7670 - val_loss: 0.5118 - val_acc: 0.7504\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5095 - acc: 0.7668 - val_loss: 0.5085 - val_acc: 0.7521\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5070 - acc: 0.7705 - val_loss: 0.5048 - val_acc: 0.7586\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5011 - acc: 0.7798 - val_loss: 0.5009 - val_acc: 0.7619\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5012 - acc: 0.7716 - val_loss: 0.4979 - val_acc: 0.7652\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4950 - acc: 0.7783 - val_loss: 0.4954 - val_acc: 0.7652\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4925 - acc: 0.7785 - val_loss: 0.4933 - val_acc: 0.7652\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4918 - acc: 0.7798 - val_loss: 0.4914 - val_acc: 0.7652\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4892 - acc: 0.7783 - val_loss: 0.4886 - val_acc: 0.7685\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4860 - acc: 0.7831 - val_loss: 0.4855 - val_acc: 0.7750\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4838 - acc: 0.7822 - val_loss: 0.4830 - val_acc: 0.7767\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4791 - acc: 0.7847 - val_loss: 0.4807 - val_acc: 0.7800\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4803 - acc: 0.7847 - val_loss: 0.4789 - val_acc: 0.7800\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4768 - acc: 0.7853 - val_loss: 0.4781 - val_acc: 0.7800\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4732 - acc: 0.7884 - val_loss: 0.4767 - val_acc: 0.7833\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4738 - acc: 0.7895 - val_loss: 0.4746 - val_acc: 0.7849\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4697 - acc: 0.7918 - val_loss: 0.4718 - val_acc: 0.7915\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4704 - acc: 0.7927 - val_loss: 0.4682 - val_acc: 0.7931\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4678 - acc: 0.7920 - val_loss: 0.4656 - val_acc: 0.7947\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4672 - acc: 0.7891 - val_loss: 0.4641 - val_acc: 0.7931\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4653 - acc: 0.7927 - val_loss: 0.4636 - val_acc: 0.7931\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4617 - acc: 0.7947 - val_loss: 0.4639 - val_acc: 0.7947\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4598 - acc: 0.7977 - val_loss: 0.4641 - val_acc: 0.7915\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4618 - acc: 0.7966 - val_loss: 0.4628 - val_acc: 0.7931\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4563 - acc: 0.8013 - val_loss: 0.4595 - val_acc: 0.7931\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4571 - acc: 0.7951 - val_loss: 0.4555 - val_acc: 0.7915\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4543 - acc: 0.8026 - val_loss: 0.4534 - val_acc: 0.7964\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4530 - acc: 0.8009 - val_loss: 0.4527 - val_acc: 0.7931\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4508 - acc: 0.7964 - val_loss: 0.4532 - val_acc: 0.7964\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4503 - acc: 0.7995 - val_loss: 0.4536 - val_acc: 0.7964\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4514 - acc: 0.8011 - val_loss: 0.4519 - val_acc: 0.7947\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4500 - acc: 0.8031 - val_loss: 0.4494 - val_acc: 0.8013\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4465 - acc: 0.8072 - val_loss: 0.4476 - val_acc: 0.8013\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4449 - acc: 0.8051 - val_loss: 0.4462 - val_acc: 0.8013\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4444 - acc: 0.8037 - val_loss: 0.4459 - val_acc: 0.8030\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4430 - acc: 0.8086 - val_loss: 0.4450 - val_acc: 0.8046\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4417 - acc: 0.8061 - val_loss: 0.4443 - val_acc: 0.8046\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4404 - acc: 0.8075 - val_loss: 0.4431 - val_acc: 0.8046\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4392 - acc: 0.8075 - val_loss: 0.4408 - val_acc: 0.8079\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4377 - acc: 0.8082 - val_loss: 0.4396 - val_acc: 0.8079\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4350 - acc: 0.8117 - val_loss: 0.4405 - val_acc: 0.8030\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4362 - acc: 0.8072 - val_loss: 0.4411 - val_acc: 0.8046\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4370 - acc: 0.8068 - val_loss: 0.4407 - val_acc: 0.8046\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4326 - acc: 0.8084 - val_loss: 0.4385 - val_acc: 0.8046\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4337 - acc: 0.8079 - val_loss: 0.4357 - val_acc: 0.8079\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4316 - acc: 0.8139 - val_loss: 0.4342 - val_acc: 0.8062\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4320 - acc: 0.8119 - val_loss: 0.4346 - val_acc: 0.8062\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4295 - acc: 0.8137 - val_loss: 0.4364 - val_acc: 0.8046\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4303 - acc: 0.8165 - val_loss: 0.4369 - val_acc: 0.8030\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4266 - acc: 0.8135 - val_loss: 0.4341 - val_acc: 0.8079\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4283 - acc: 0.8172 - val_loss: 0.4316 - val_acc: 0.8079\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4278 - acc: 0.8152 - val_loss: 0.4311 - val_acc: 0.8079\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4236 - acc: 0.8143 - val_loss: 0.4316 - val_acc: 0.8095\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4246 - acc: 0.8157 - val_loss: 0.4319 - val_acc: 0.8095\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4226 - acc: 0.8176 - val_loss: 0.4324 - val_acc: 0.8062\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4235 - acc: 0.8148 - val_loss: 0.4312 - val_acc: 0.8095\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4222 - acc: 0.8170 - val_loss: 0.4294 - val_acc: 0.8095\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4246 - acc: 0.8185 - val_loss: 0.4286 - val_acc: 0.8095\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4213 - acc: 0.8161 - val_loss: 0.4284 - val_acc: 0.8079\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4218 - acc: 0.8188 - val_loss: 0.4281 - val_acc: 0.8079\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4172 - acc: 0.8174 - val_loss: 0.4275 - val_acc: 0.8095\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4171 - acc: 0.8185 - val_loss: 0.4270 - val_acc: 0.8112\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4180 - acc: 0.8210 - val_loss: 0.4268 - val_acc: 0.8112\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4168 - acc: 0.8197 - val_loss: 0.4260 - val_acc: 0.8128\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4173 - acc: 0.8194 - val_loss: 0.4262 - val_acc: 0.8112\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4166 - acc: 0.8199 - val_loss: 0.4264 - val_acc: 0.8112\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4154 - acc: 0.8214 - val_loss: 0.4265 - val_acc: 0.8112\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4133 - acc: 0.8217 - val_loss: 0.4257 - val_acc: 0.8128\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4127 - acc: 0.8205 - val_loss: 0.4244 - val_acc: 0.8161\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4118 - acc: 0.8197 - val_loss: 0.4243 - val_acc: 0.8161\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4133 - acc: 0.8194 - val_loss: 0.4248 - val_acc: 0.8177\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4087 - acc: 0.8265 - val_loss: 0.4247 - val_acc: 0.8177\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4123 - acc: 0.8228 - val_loss: 0.4233 - val_acc: 0.8177\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4086 - acc: 0.8234 - val_loss: 0.4226 - val_acc: 0.8177\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4077 - acc: 0.8210 - val_loss: 0.4226 - val_acc: 0.8177\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4112 - acc: 0.8243 - val_loss: 0.4228 - val_acc: 0.8177\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4061 - acc: 0.8225 - val_loss: 0.4231 - val_acc: 0.8177\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4067 - acc: 0.8250 - val_loss: 0.4221 - val_acc: 0.8161\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4079 - acc: 0.8247 - val_loss: 0.4209 - val_acc: 0.8161\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4075 - acc: 0.8234 - val_loss: 0.4205 - val_acc: 0.8177\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4043 - acc: 0.8294 - val_loss: 0.4199 - val_acc: 0.8177\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4066 - acc: 0.8280 - val_loss: 0.4201 - val_acc: 0.8177\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4048 - acc: 0.8280 - val_loss: 0.4211 - val_acc: 0.8177\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4043 - acc: 0.8267 - val_loss: 0.4213 - val_acc: 0.8177\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4047 - acc: 0.8239 - val_loss: 0.4206 - val_acc: 0.8177\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4030 - acc: 0.8223 - val_loss: 0.4193 - val_acc: 0.8177\n",
      "Epoch 129/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4038 - acc: 0.8274 - val_loss: 0.4181 - val_acc: 0.8177\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4020 - acc: 0.8280 - val_loss: 0.4182 - val_acc: 0.8177\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.3985 - acc: 0.8290 - val_loss: 0.4184 - val_acc: 0.8177\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4004 - acc: 0.8298 - val_loss: 0.4196 - val_acc: 0.8177\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3954 - acc: 0.8307 - val_loss: 0.4198 - val_acc: 0.8194\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3983 - acc: 0.8289 - val_loss: 0.4185 - val_acc: 0.8177\n",
      "Training with parameters {'batch_size': 3500, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_49 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_89 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_138 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_90 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 1s 253ms/step - loss: 0.8407 - acc: 0.4344 - val_loss: 0.8174 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.7871 - acc: 0.4393 - val_loss: 0.7672 - val_acc: 0.3892\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.7487 - acc: 0.4466 - val_loss: 0.7275 - val_acc: 0.3924\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.7187 - acc: 0.4600 - val_loss: 0.6973 - val_acc: 0.4844\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.7003 - acc: 0.5019 - val_loss: 0.6752 - val_acc: 0.6289\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6819 - acc: 0.5784 - val_loss: 0.6594 - val_acc: 0.6371\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6688 - acc: 0.6074 - val_loss: 0.6480 - val_acc: 0.6305\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.6635 - acc: 0.6074 - val_loss: 0.6393 - val_acc: 0.6190\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.6567 - acc: 0.5972 - val_loss: 0.6322 - val_acc: 0.6190\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.6538 - acc: 0.5973 - val_loss: 0.6257 - val_acc: 0.6223\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.6435 - acc: 0.6028 - val_loss: 0.6196 - val_acc: 0.6289\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.6362 - acc: 0.6216 - val_loss: 0.6138 - val_acc: 0.6404\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.6301 - acc: 0.6420 - val_loss: 0.6086 - val_acc: 0.6765\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.6248 - acc: 0.6674 - val_loss: 0.6041 - val_acc: 0.7291\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6152 - acc: 0.6926 - val_loss: 0.6004 - val_acc: 0.7258\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6092 - acc: 0.7074 - val_loss: 0.5972 - val_acc: 0.7094\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6038 - acc: 0.7086 - val_loss: 0.5942 - val_acc: 0.6946\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.5983 - acc: 0.7019 - val_loss: 0.5910 - val_acc: 0.6979\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5931 - acc: 0.7052 - val_loss: 0.5871 - val_acc: 0.7028\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5891 - acc: 0.7008 - val_loss: 0.5825 - val_acc: 0.7028\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.5839 - acc: 0.7081 - val_loss: 0.5773 - val_acc: 0.7011\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.5782 - acc: 0.7128 - val_loss: 0.5718 - val_acc: 0.7077\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5728 - acc: 0.7176 - val_loss: 0.5660 - val_acc: 0.7110\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.5667 - acc: 0.7196 - val_loss: 0.5602 - val_acc: 0.7176\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.5599 - acc: 0.7320 - val_loss: 0.5547 - val_acc: 0.7176\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5589 - acc: 0.7303 - val_loss: 0.5497 - val_acc: 0.7209\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.5532 - acc: 0.7418 - val_loss: 0.5451 - val_acc: 0.7241\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5492 - acc: 0.7418 - val_loss: 0.5407 - val_acc: 0.7258\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.5401 - acc: 0.7438 - val_loss: 0.5363 - val_acc: 0.7258\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.5392 - acc: 0.7455 - val_loss: 0.5316 - val_acc: 0.7307\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.5342 - acc: 0.7482 - val_loss: 0.5264 - val_acc: 0.7373\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.5271 - acc: 0.7524 - val_loss: 0.5206 - val_acc: 0.7406\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5224 - acc: 0.7606 - val_loss: 0.5149 - val_acc: 0.7422\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5185 - acc: 0.7628 - val_loss: 0.5093 - val_acc: 0.7438\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5152 - acc: 0.7656 - val_loss: 0.5041 - val_acc: 0.7504\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.5111 - acc: 0.7668 - val_loss: 0.4997 - val_acc: 0.7504\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5054 - acc: 0.7677 - val_loss: 0.4955 - val_acc: 0.7586\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5033 - acc: 0.7696 - val_loss: 0.4907 - val_acc: 0.7603\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5000 - acc: 0.7760 - val_loss: 0.4859 - val_acc: 0.7652\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4955 - acc: 0.7752 - val_loss: 0.4825 - val_acc: 0.7668\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4916 - acc: 0.7738 - val_loss: 0.4795 - val_acc: 0.7685\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4885 - acc: 0.7811 - val_loss: 0.4770 - val_acc: 0.7685\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4852 - acc: 0.7814 - val_loss: 0.4741 - val_acc: 0.7701\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4815 - acc: 0.7867 - val_loss: 0.4704 - val_acc: 0.7718\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.4755 - acc: 0.7889 - val_loss: 0.4664 - val_acc: 0.7800\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4747 - acc: 0.7900 - val_loss: 0.4629 - val_acc: 0.7849\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4727 - acc: 0.7920 - val_loss: 0.4604 - val_acc: 0.7882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4678 - acc: 0.7905 - val_loss: 0.4589 - val_acc: 0.7865\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4652 - acc: 0.7895 - val_loss: 0.4575 - val_acc: 0.7898\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4648 - acc: 0.7933 - val_loss: 0.4555 - val_acc: 0.7882\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4634 - acc: 0.7946 - val_loss: 0.4525 - val_acc: 0.7931\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.4605 - acc: 0.7973 - val_loss: 0.4488 - val_acc: 0.7980\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4568 - acc: 0.8000 - val_loss: 0.4465 - val_acc: 0.8030\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4530 - acc: 0.7982 - val_loss: 0.4444 - val_acc: 0.8046\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4509 - acc: 0.7958 - val_loss: 0.4441 - val_acc: 0.8046\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4506 - acc: 0.8042 - val_loss: 0.4431 - val_acc: 0.8030\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4488 - acc: 0.8024 - val_loss: 0.4401 - val_acc: 0.8062\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4450 - acc: 0.8042 - val_loss: 0.4373 - val_acc: 0.8079\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4421 - acc: 0.8048 - val_loss: 0.4360 - val_acc: 0.8079\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4437 - acc: 0.8057 - val_loss: 0.4347 - val_acc: 0.8079\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4409 - acc: 0.8044 - val_loss: 0.4343 - val_acc: 0.8079\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4406 - acc: 0.8051 - val_loss: 0.4340 - val_acc: 0.8095\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4388 - acc: 0.8072 - val_loss: 0.4329 - val_acc: 0.8095\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4348 - acc: 0.8084 - val_loss: 0.4300 - val_acc: 0.8144\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4359 - acc: 0.8088 - val_loss: 0.4271 - val_acc: 0.8128\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4356 - acc: 0.8101 - val_loss: 0.4268 - val_acc: 0.8144\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4321 - acc: 0.8090 - val_loss: 0.4277 - val_acc: 0.8079\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4310 - acc: 0.8126 - val_loss: 0.4267 - val_acc: 0.8079\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4325 - acc: 0.8124 - val_loss: 0.4244 - val_acc: 0.8144\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4281 - acc: 0.8126 - val_loss: 0.4238 - val_acc: 0.8144\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4259 - acc: 0.8141 - val_loss: 0.4239 - val_acc: 0.8095\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4262 - acc: 0.8134 - val_loss: 0.4241 - val_acc: 0.8046\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4273 - acc: 0.8117 - val_loss: 0.4235 - val_acc: 0.8062\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4244 - acc: 0.8166 - val_loss: 0.4220 - val_acc: 0.8095\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4213 - acc: 0.8157 - val_loss: 0.4205 - val_acc: 0.8112\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4243 - acc: 0.8134 - val_loss: 0.4198 - val_acc: 0.8128\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4211 - acc: 0.8132 - val_loss: 0.4190 - val_acc: 0.8144\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4210 - acc: 0.8132 - val_loss: 0.4188 - val_acc: 0.8161\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4189 - acc: 0.8177 - val_loss: 0.4194 - val_acc: 0.8128\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.4171 - acc: 0.8135 - val_loss: 0.4191 - val_acc: 0.8128\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.4157 - acc: 0.8166 - val_loss: 0.4177 - val_acc: 0.8144\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.4148 - acc: 0.8201 - val_loss: 0.4162 - val_acc: 0.8144\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4140 - acc: 0.8185 - val_loss: 0.4157 - val_acc: 0.8128\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.4145 - acc: 0.8170 - val_loss: 0.4160 - val_acc: 0.8144\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4131 - acc: 0.8194 - val_loss: 0.4167 - val_acc: 0.8177\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4102 - acc: 0.8199 - val_loss: 0.4159 - val_acc: 0.8177\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4111 - acc: 0.8194 - val_loss: 0.4146 - val_acc: 0.8161\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.4104 - acc: 0.8219 - val_loss: 0.4138 - val_acc: 0.8194\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4089 - acc: 0.8208 - val_loss: 0.4149 - val_acc: 0.8144\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4084 - acc: 0.8254 - val_loss: 0.4158 - val_acc: 0.8144\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.4043 - acc: 0.8250 - val_loss: 0.4136 - val_acc: 0.8177\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.4054 - acc: 0.8250 - val_loss: 0.4114 - val_acc: 0.8227\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.4045 - acc: 0.8248 - val_loss: 0.4117 - val_acc: 0.8227\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4046 - acc: 0.8252 - val_loss: 0.4153 - val_acc: 0.8144\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4027 - acc: 0.8214 - val_loss: 0.4165 - val_acc: 0.8144\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4031 - acc: 0.8208 - val_loss: 0.4117 - val_acc: 0.8194\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.3995 - acc: 0.8245 - val_loss: 0.4100 - val_acc: 0.8227\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.3991 - acc: 0.8238 - val_loss: 0.4119 - val_acc: 0.8177\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4013 - acc: 0.8258 - val_loss: 0.4141 - val_acc: 0.8128\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.3974 - acc: 0.8323 - val_loss: 0.4125 - val_acc: 0.8161\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.3962 - acc: 0.8289 - val_loss: 0.4102 - val_acc: 0.8210\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3970 - acc: 0.8283 - val_loss: 0.4100 - val_acc: 0.8210\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3967 - acc: 0.8281 - val_loss: 0.4125 - val_acc: 0.8161\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3939 - acc: 0.8259 - val_loss: 0.4132 - val_acc: 0.8177\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.3940 - acc: 0.8294 - val_loss: 0.4105 - val_acc: 0.8194\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.3936 - acc: 0.8296 - val_loss: 0.4097 - val_acc: 0.8227\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.3915 - acc: 0.8280 - val_loss: 0.4124 - val_acc: 0.8177\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.3912 - acc: 0.8280 - val_loss: 0.4138 - val_acc: 0.8194\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.3910 - acc: 0.8336 - val_loss: 0.4097 - val_acc: 0.8227\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.3885 - acc: 0.8336 - val_loss: 0.4088 - val_acc: 0.8243\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3881 - acc: 0.8334 - val_loss: 0.4118 - val_acc: 0.8161\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.3885 - acc: 0.8343 - val_loss: 0.4158 - val_acc: 0.8210\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.3864 - acc: 0.8307 - val_loss: 0.4120 - val_acc: 0.8177\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.3851 - acc: 0.8340 - val_loss: 0.4083 - val_acc: 0.8243\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.3863 - acc: 0.8334 - val_loss: 0.4095 - val_acc: 0.8227\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.3841 - acc: 0.8342 - val_loss: 0.4137 - val_acc: 0.8227\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.3832 - acc: 0.8345 - val_loss: 0.4119 - val_acc: 0.8243\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.3838 - acc: 0.8307 - val_loss: 0.4086 - val_acc: 0.8227\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.3798 - acc: 0.8343 - val_loss: 0.4092 - val_acc: 0.8227\n",
      "Training with parameters {'batch_size': 4000, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_50 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_140 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_91 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_141 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_92 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.7791 - acc: 0.4353 - val_loss: 0.7318 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.7230 - acc: 0.4492 - val_loss: 0.6850 - val_acc: 0.5944\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6901 - acc: 0.5296 - val_loss: 0.6595 - val_acc: 0.6125\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6734 - acc: 0.5762 - val_loss: 0.6454 - val_acc: 0.6141\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6650 - acc: 0.5754 - val_loss: 0.6352 - val_acc: 0.6108\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6537 - acc: 0.5762 - val_loss: 0.6252 - val_acc: 0.6125\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6451 - acc: 0.5862 - val_loss: 0.6156 - val_acc: 0.6437\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6288 - acc: 0.6225 - val_loss: 0.6080 - val_acc: 0.7258\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6193 - acc: 0.6798 - val_loss: 0.6033 - val_acc: 0.7094\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6077 - acc: 0.7043 - val_loss: 0.6005 - val_acc: 0.6765\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6009 - acc: 0.7030 - val_loss: 0.5971 - val_acc: 0.6650\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.5936 - acc: 0.6973 - val_loss: 0.5913 - val_acc: 0.6667\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.5842 - acc: 0.6982 - val_loss: 0.5832 - val_acc: 0.6782\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5780 - acc: 0.7094 - val_loss: 0.5739 - val_acc: 0.6864\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5697 - acc: 0.7187 - val_loss: 0.5645 - val_acc: 0.6979\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5619 - acc: 0.7329 - val_loss: 0.5561 - val_acc: 0.7143\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5539 - acc: 0.7435 - val_loss: 0.5487 - val_acc: 0.7258\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.5480 - acc: 0.7488 - val_loss: 0.5419 - val_acc: 0.7373\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5391 - acc: 0.7517 - val_loss: 0.5356 - val_acc: 0.7389\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.5323 - acc: 0.7537 - val_loss: 0.5301 - val_acc: 0.7422\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5259 - acc: 0.7553 - val_loss: 0.5251 - val_acc: 0.7389\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.5194 - acc: 0.7604 - val_loss: 0.5190 - val_acc: 0.7537\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5124 - acc: 0.7656 - val_loss: 0.5109 - val_acc: 0.7553\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.5082 - acc: 0.7677 - val_loss: 0.5029 - val_acc: 0.7652\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5000 - acc: 0.7754 - val_loss: 0.4968 - val_acc: 0.7734\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4956 - acc: 0.7805 - val_loss: 0.4922 - val_acc: 0.7783\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4914 - acc: 0.7840 - val_loss: 0.4881 - val_acc: 0.7816\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4874 - acc: 0.7831 - val_loss: 0.4843 - val_acc: 0.7833\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4812 - acc: 0.7803 - val_loss: 0.4801 - val_acc: 0.7849\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4796 - acc: 0.7891 - val_loss: 0.4761 - val_acc: 0.7915\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4719 - acc: 0.7918 - val_loss: 0.4719 - val_acc: 0.7915\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4697 - acc: 0.7931 - val_loss: 0.4690 - val_acc: 0.7865\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4651 - acc: 0.7962 - val_loss: 0.4656 - val_acc: 0.7865\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4644 - acc: 0.7940 - val_loss: 0.4619 - val_acc: 0.7849\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4586 - acc: 0.7977 - val_loss: 0.4596 - val_acc: 0.7849\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4577 - acc: 0.7973 - val_loss: 0.4572 - val_acc: 0.7882\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4521 - acc: 0.7984 - val_loss: 0.4523 - val_acc: 0.7931\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4504 - acc: 0.8031 - val_loss: 0.4480 - val_acc: 0.7980\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4459 - acc: 0.8084 - val_loss: 0.4452 - val_acc: 0.7997\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4451 - acc: 0.8030 - val_loss: 0.4426 - val_acc: 0.7997\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4405 - acc: 0.8101 - val_loss: 0.4404 - val_acc: 0.7980\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4396 - acc: 0.8092 - val_loss: 0.4376 - val_acc: 0.8062\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4362 - acc: 0.8082 - val_loss: 0.4353 - val_acc: 0.8079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4345 - acc: 0.8110 - val_loss: 0.4356 - val_acc: 0.8062\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4308 - acc: 0.8143 - val_loss: 0.4342 - val_acc: 0.8062\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4324 - acc: 0.8128 - val_loss: 0.4302 - val_acc: 0.8046\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4275 - acc: 0.8112 - val_loss: 0.4284 - val_acc: 0.8095\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4218 - acc: 0.8176 - val_loss: 0.4299 - val_acc: 0.8095\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4253 - acc: 0.8135 - val_loss: 0.4298 - val_acc: 0.8112\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4219 - acc: 0.8165 - val_loss: 0.4244 - val_acc: 0.8112\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4212 - acc: 0.8150 - val_loss: 0.4221 - val_acc: 0.8128\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4186 - acc: 0.8190 - val_loss: 0.4225 - val_acc: 0.8128\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4175 - acc: 0.8185 - val_loss: 0.4244 - val_acc: 0.8144\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4171 - acc: 0.8201 - val_loss: 0.4208 - val_acc: 0.8161\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4142 - acc: 0.8201 - val_loss: 0.4178 - val_acc: 0.8161\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4145 - acc: 0.8216 - val_loss: 0.4186 - val_acc: 0.8161\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4116 - acc: 0.8225 - val_loss: 0.4203 - val_acc: 0.8161\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4108 - acc: 0.8238 - val_loss: 0.4190 - val_acc: 0.8161\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4083 - acc: 0.8230 - val_loss: 0.4175 - val_acc: 0.8161\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4093 - acc: 0.8247 - val_loss: 0.4190 - val_acc: 0.8161\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4085 - acc: 0.8219 - val_loss: 0.4194 - val_acc: 0.8161\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4044 - acc: 0.8259 - val_loss: 0.4163 - val_acc: 0.8194\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4037 - acc: 0.8258 - val_loss: 0.4137 - val_acc: 0.8194\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4009 - acc: 0.8305 - val_loss: 0.4134 - val_acc: 0.8210\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4015 - acc: 0.8283 - val_loss: 0.4152 - val_acc: 0.8194\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4013 - acc: 0.8294 - val_loss: 0.4152 - val_acc: 0.8210\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4001 - acc: 0.8276 - val_loss: 0.4131 - val_acc: 0.8210\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3958 - acc: 0.8280 - val_loss: 0.4133 - val_acc: 0.8210\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3953 - acc: 0.8272 - val_loss: 0.4119 - val_acc: 0.8210\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3942 - acc: 0.8321 - val_loss: 0.4116 - val_acc: 0.8210\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.3929 - acc: 0.8325 - val_loss: 0.4131 - val_acc: 0.8227\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3918 - acc: 0.8298 - val_loss: 0.4139 - val_acc: 0.8227\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.3916 - acc: 0.8312 - val_loss: 0.4135 - val_acc: 0.8227\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3884 - acc: 0.8334 - val_loss: 0.4122 - val_acc: 0.8227\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3880 - acc: 0.8340 - val_loss: 0.4098 - val_acc: 0.8210\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3881 - acc: 0.8343 - val_loss: 0.4106 - val_acc: 0.8210\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3858 - acc: 0.8384 - val_loss: 0.4099 - val_acc: 0.8210\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3851 - acc: 0.8367 - val_loss: 0.4100 - val_acc: 0.8194\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3830 - acc: 0.8363 - val_loss: 0.4110 - val_acc: 0.8194\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.3799 - acc: 0.8369 - val_loss: 0.4117 - val_acc: 0.8194\n",
      "Training with parameters {'batch_size': 4000, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_51 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_93 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_144 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_94 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_145 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.8440 - acc: 0.4355 - val_loss: 0.8175 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.7930 - acc: 0.4371 - val_loss: 0.7674 - val_acc: 0.3892\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.7527 - acc: 0.4371 - val_loss: 0.7276 - val_acc: 0.3924\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.7203 - acc: 0.4590 - val_loss: 0.6973 - val_acc: 0.4795\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6973 - acc: 0.5114 - val_loss: 0.6751 - val_acc: 0.6305\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6798 - acc: 0.5816 - val_loss: 0.6594 - val_acc: 0.6371\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6685 - acc: 0.6138 - val_loss: 0.6481 - val_acc: 0.6305\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6632 - acc: 0.6017 - val_loss: 0.6394 - val_acc: 0.6190\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6581 - acc: 0.5986 - val_loss: 0.6322 - val_acc: 0.6190\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6489 - acc: 0.5937 - val_loss: 0.6258 - val_acc: 0.6240\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6465 - acc: 0.6057 - val_loss: 0.6197 - val_acc: 0.6305\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6354 - acc: 0.6170 - val_loss: 0.6140 - val_acc: 0.6437\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6321 - acc: 0.6424 - val_loss: 0.6089 - val_acc: 0.6864\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6226 - acc: 0.6718 - val_loss: 0.6044 - val_acc: 0.7323\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6145 - acc: 0.6949 - val_loss: 0.6008 - val_acc: 0.7274\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6067 - acc: 0.7088 - val_loss: 0.5976 - val_acc: 0.7126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6029 - acc: 0.7044 - val_loss: 0.5945 - val_acc: 0.6979\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5984 - acc: 0.7066 - val_loss: 0.5912 - val_acc: 0.6979\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5933 - acc: 0.7024 - val_loss: 0.5871 - val_acc: 0.7044\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5872 - acc: 0.7010 - val_loss: 0.5824 - val_acc: 0.7011\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5831 - acc: 0.7119 - val_loss: 0.5772 - val_acc: 0.6995\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5787 - acc: 0.7172 - val_loss: 0.5716 - val_acc: 0.7077\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5747 - acc: 0.7181 - val_loss: 0.5660 - val_acc: 0.7143\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5705 - acc: 0.7190 - val_loss: 0.5606 - val_acc: 0.7159\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5644 - acc: 0.7296 - val_loss: 0.5555 - val_acc: 0.7159\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5571 - acc: 0.7358 - val_loss: 0.5505 - val_acc: 0.7192\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5511 - acc: 0.7404 - val_loss: 0.5456 - val_acc: 0.7241\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5472 - acc: 0.7415 - val_loss: 0.5409 - val_acc: 0.7274\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5437 - acc: 0.7420 - val_loss: 0.5363 - val_acc: 0.7274\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5369 - acc: 0.7448 - val_loss: 0.5314 - val_acc: 0.7323\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5330 - acc: 0.7524 - val_loss: 0.5267 - val_acc: 0.7373\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5256 - acc: 0.7588 - val_loss: 0.5216 - val_acc: 0.7406\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.5238 - acc: 0.7584 - val_loss: 0.5163 - val_acc: 0.7438\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5199 - acc: 0.7623 - val_loss: 0.5109 - val_acc: 0.7455\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5134 - acc: 0.7645 - val_loss: 0.5053 - val_acc: 0.7471\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5114 - acc: 0.7628 - val_loss: 0.4998 - val_acc: 0.7537\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5048 - acc: 0.7714 - val_loss: 0.4950 - val_acc: 0.7586\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5010 - acc: 0.7750 - val_loss: 0.4912 - val_acc: 0.7603\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4986 - acc: 0.7776 - val_loss: 0.4878 - val_acc: 0.7603\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4927 - acc: 0.7791 - val_loss: 0.4841 - val_acc: 0.7668\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4894 - acc: 0.7761 - val_loss: 0.4794 - val_acc: 0.7718\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4890 - acc: 0.7789 - val_loss: 0.4745 - val_acc: 0.7750\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4847 - acc: 0.7816 - val_loss: 0.4706 - val_acc: 0.7816\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4806 - acc: 0.7873 - val_loss: 0.4678 - val_acc: 0.7816\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4765 - acc: 0.7895 - val_loss: 0.4659 - val_acc: 0.7816\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4738 - acc: 0.7867 - val_loss: 0.4637 - val_acc: 0.7816\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4725 - acc: 0.7896 - val_loss: 0.4609 - val_acc: 0.7833\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4685 - acc: 0.7938 - val_loss: 0.4576 - val_acc: 0.7882\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4653 - acc: 0.7960 - val_loss: 0.4551 - val_acc: 0.7898\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4653 - acc: 0.7947 - val_loss: 0.4529 - val_acc: 0.7915\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4606 - acc: 0.7947 - val_loss: 0.4516 - val_acc: 0.7980\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4603 - acc: 0.7927 - val_loss: 0.4502 - val_acc: 0.7997\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4589 - acc: 0.7968 - val_loss: 0.4471 - val_acc: 0.8013\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4533 - acc: 0.7991 - val_loss: 0.4440 - val_acc: 0.8046\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4499 - acc: 0.8020 - val_loss: 0.4436 - val_acc: 0.8030\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4499 - acc: 0.8006 - val_loss: 0.4437 - val_acc: 0.8013\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4484 - acc: 0.8031 - val_loss: 0.4415 - val_acc: 0.8013\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4424 - acc: 0.8053 - val_loss: 0.4373 - val_acc: 0.8079\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.4479 - acc: 0.8009 - val_loss: 0.4344 - val_acc: 0.8079\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4464 - acc: 0.8024 - val_loss: 0.4334 - val_acc: 0.8095\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4412 - acc: 0.8066 - val_loss: 0.4330 - val_acc: 0.8095\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4421 - acc: 0.8061 - val_loss: 0.4328 - val_acc: 0.8062\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4386 - acc: 0.8090 - val_loss: 0.4314 - val_acc: 0.8095\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4368 - acc: 0.8112 - val_loss: 0.4304 - val_acc: 0.8095\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4374 - acc: 0.8066 - val_loss: 0.4294 - val_acc: 0.8079\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4366 - acc: 0.8082 - val_loss: 0.4286 - val_acc: 0.8062\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4303 - acc: 0.8104 - val_loss: 0.4269 - val_acc: 0.8079\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4313 - acc: 0.8128 - val_loss: 0.4251 - val_acc: 0.8095\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4342 - acc: 0.8084 - val_loss: 0.4244 - val_acc: 0.8095\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4285 - acc: 0.8113 - val_loss: 0.4252 - val_acc: 0.8062\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4285 - acc: 0.8123 - val_loss: 0.4257 - val_acc: 0.8079\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4277 - acc: 0.8170 - val_loss: 0.4237 - val_acc: 0.8046\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4285 - acc: 0.8132 - val_loss: 0.4201 - val_acc: 0.8128\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4246 - acc: 0.8135 - val_loss: 0.4180 - val_acc: 0.8177\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4235 - acc: 0.8163 - val_loss: 0.4188 - val_acc: 0.8112\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4220 - acc: 0.8165 - val_loss: 0.4217 - val_acc: 0.8079\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4183 - acc: 0.8185 - val_loss: 0.4211 - val_acc: 0.8095\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4222 - acc: 0.8155 - val_loss: 0.4175 - val_acc: 0.8128\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4205 - acc: 0.8174 - val_loss: 0.4169 - val_acc: 0.8128\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4187 - acc: 0.8166 - val_loss: 0.4180 - val_acc: 0.8128\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4164 - acc: 0.8176 - val_loss: 0.4177 - val_acc: 0.8128\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4153 - acc: 0.8205 - val_loss: 0.4164 - val_acc: 0.8144\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4162 - acc: 0.8188 - val_loss: 0.4168 - val_acc: 0.8112\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4132 - acc: 0.8205 - val_loss: 0.4168 - val_acc: 0.8095\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4121 - acc: 0.8241 - val_loss: 0.4157 - val_acc: 0.8128\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4112 - acc: 0.8194 - val_loss: 0.4139 - val_acc: 0.8177\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4093 - acc: 0.8230 - val_loss: 0.4121 - val_acc: 0.8194\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4145 - acc: 0.8196 - val_loss: 0.4132 - val_acc: 0.8194\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4081 - acc: 0.8228 - val_loss: 0.4160 - val_acc: 0.8128\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4063 - acc: 0.8238 - val_loss: 0.4155 - val_acc: 0.8112\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4059 - acc: 0.8239 - val_loss: 0.4130 - val_acc: 0.8128\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4076 - acc: 0.8270 - val_loss: 0.4114 - val_acc: 0.8194\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4067 - acc: 0.8230 - val_loss: 0.4124 - val_acc: 0.8128\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4067 - acc: 0.8281 - val_loss: 0.4138 - val_acc: 0.8095\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4029 - acc: 0.8270 - val_loss: 0.4124 - val_acc: 0.8112\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4019 - acc: 0.8252 - val_loss: 0.4116 - val_acc: 0.8128\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4018 - acc: 0.8241 - val_loss: 0.4116 - val_acc: 0.8128\n",
      "Training with parameters {'batch_size': 4000, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_52 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_146 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_95 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_147 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.6729 - acc: 0.5930 - val_loss: 0.6648 - val_acc: 0.5895\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6597 - acc: 0.6307 - val_loss: 0.6506 - val_acc: 0.6322\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6494 - acc: 0.6382 - val_loss: 0.6401 - val_acc: 0.6535\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6409 - acc: 0.6535 - val_loss: 0.6319 - val_acc: 0.6683\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6363 - acc: 0.6519 - val_loss: 0.6253 - val_acc: 0.6732\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6291 - acc: 0.6586 - val_loss: 0.6197 - val_acc: 0.6831\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6234 - acc: 0.6716 - val_loss: 0.6150 - val_acc: 0.6765\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6185 - acc: 0.6783 - val_loss: 0.6109 - val_acc: 0.6716\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6120 - acc: 0.6975 - val_loss: 0.6073 - val_acc: 0.6847\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6053 - acc: 0.7099 - val_loss: 0.6041 - val_acc: 0.6814\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6008 - acc: 0.7070 - val_loss: 0.6010 - val_acc: 0.6782\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.5948 - acc: 0.7094 - val_loss: 0.5977 - val_acc: 0.6831\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5915 - acc: 0.7092 - val_loss: 0.5937 - val_acc: 0.6864\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5862 - acc: 0.7123 - val_loss: 0.5894 - val_acc: 0.6864\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5827 - acc: 0.7183 - val_loss: 0.5852 - val_acc: 0.6897\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5774 - acc: 0.7223 - val_loss: 0.5810 - val_acc: 0.6962\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5733 - acc: 0.7258 - val_loss: 0.5769 - val_acc: 0.7011\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5695 - acc: 0.7285 - val_loss: 0.5728 - val_acc: 0.7110\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5669 - acc: 0.7325 - val_loss: 0.5685 - val_acc: 0.7126\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5626 - acc: 0.7360 - val_loss: 0.5644 - val_acc: 0.7126\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5590 - acc: 0.7382 - val_loss: 0.5608 - val_acc: 0.7143\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5545 - acc: 0.7411 - val_loss: 0.5577 - val_acc: 0.7192\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5510 - acc: 0.7424 - val_loss: 0.5546 - val_acc: 0.7241\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5504 - acc: 0.7418 - val_loss: 0.5514 - val_acc: 0.7274\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5458 - acc: 0.7453 - val_loss: 0.5481 - val_acc: 0.7291\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5422 - acc: 0.7490 - val_loss: 0.5449 - val_acc: 0.7340\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5380 - acc: 0.7542 - val_loss: 0.5418 - val_acc: 0.7340\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5348 - acc: 0.7586 - val_loss: 0.5389 - val_acc: 0.7373\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5324 - acc: 0.7579 - val_loss: 0.5360 - val_acc: 0.7406\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5309 - acc: 0.7566 - val_loss: 0.5329 - val_acc: 0.7422\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5277 - acc: 0.7590 - val_loss: 0.5299 - val_acc: 0.7455\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5253 - acc: 0.7626 - val_loss: 0.5273 - val_acc: 0.7488\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5234 - acc: 0.7654 - val_loss: 0.5249 - val_acc: 0.7521\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5188 - acc: 0.7643 - val_loss: 0.5227 - val_acc: 0.7471\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5161 - acc: 0.7707 - val_loss: 0.5202 - val_acc: 0.7488\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5161 - acc: 0.7685 - val_loss: 0.5177 - val_acc: 0.7504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5113 - acc: 0.7723 - val_loss: 0.5149 - val_acc: 0.7570\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5107 - acc: 0.7705 - val_loss: 0.5123 - val_acc: 0.7619\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5081 - acc: 0.7743 - val_loss: 0.5099 - val_acc: 0.7635\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5047 - acc: 0.7756 - val_loss: 0.5077 - val_acc: 0.7635\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5038 - acc: 0.7787 - val_loss: 0.5054 - val_acc: 0.7652\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5006 - acc: 0.7750 - val_loss: 0.5033 - val_acc: 0.7635\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4992 - acc: 0.7791 - val_loss: 0.5009 - val_acc: 0.7668\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4957 - acc: 0.7820 - val_loss: 0.4986 - val_acc: 0.7685\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4948 - acc: 0.7792 - val_loss: 0.4969 - val_acc: 0.7701\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4932 - acc: 0.7833 - val_loss: 0.4956 - val_acc: 0.7685\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4923 - acc: 0.7807 - val_loss: 0.4946 - val_acc: 0.7685\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4889 - acc: 0.7829 - val_loss: 0.4934 - val_acc: 0.7668\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4879 - acc: 0.7874 - val_loss: 0.4915 - val_acc: 0.7685\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4868 - acc: 0.7871 - val_loss: 0.4894 - val_acc: 0.7701\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4856 - acc: 0.7874 - val_loss: 0.4875 - val_acc: 0.7734\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4835 - acc: 0.7871 - val_loss: 0.4855 - val_acc: 0.7734\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4838 - acc: 0.7874 - val_loss: 0.4840 - val_acc: 0.7734\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4799 - acc: 0.7882 - val_loss: 0.4823 - val_acc: 0.7750\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4785 - acc: 0.7922 - val_loss: 0.4808 - val_acc: 0.7767\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4788 - acc: 0.7874 - val_loss: 0.4795 - val_acc: 0.7833\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4766 - acc: 0.7911 - val_loss: 0.4787 - val_acc: 0.7816\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4744 - acc: 0.7964 - val_loss: 0.4778 - val_acc: 0.7865\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4726 - acc: 0.7971 - val_loss: 0.4770 - val_acc: 0.7865\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4745 - acc: 0.7924 - val_loss: 0.4758 - val_acc: 0.7882\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4715 - acc: 0.7966 - val_loss: 0.4741 - val_acc: 0.7898\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4690 - acc: 0.7978 - val_loss: 0.4725 - val_acc: 0.7882\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4678 - acc: 0.7975 - val_loss: 0.4711 - val_acc: 0.7931\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4668 - acc: 0.7997 - val_loss: 0.4699 - val_acc: 0.7947\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4681 - acc: 0.7973 - val_loss: 0.4688 - val_acc: 0.7947\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4653 - acc: 0.7978 - val_loss: 0.4678 - val_acc: 0.7947\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4646 - acc: 0.7971 - val_loss: 0.4677 - val_acc: 0.7964\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4627 - acc: 0.7989 - val_loss: 0.4676 - val_acc: 0.7964\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4633 - acc: 0.7975 - val_loss: 0.4670 - val_acc: 0.7964\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4629 - acc: 0.7960 - val_loss: 0.4655 - val_acc: 0.7997\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4606 - acc: 0.7975 - val_loss: 0.4633 - val_acc: 0.7997\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4589 - acc: 0.8041 - val_loss: 0.4613 - val_acc: 0.7931\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4584 - acc: 0.8035 - val_loss: 0.4601 - val_acc: 0.7931\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4586 - acc: 0.7982 - val_loss: 0.4595 - val_acc: 0.7931\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4578 - acc: 0.8009 - val_loss: 0.4590 - val_acc: 0.7931\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4560 - acc: 0.8028 - val_loss: 0.4583 - val_acc: 0.7947\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4573 - acc: 0.7991 - val_loss: 0.4573 - val_acc: 0.7964\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4543 - acc: 0.8017 - val_loss: 0.4565 - val_acc: 0.7980\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4538 - acc: 0.8020 - val_loss: 0.4559 - val_acc: 0.7980\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4530 - acc: 0.8020 - val_loss: 0.4554 - val_acc: 0.7980\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4515 - acc: 0.8064 - val_loss: 0.4547 - val_acc: 0.7997\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4509 - acc: 0.8042 - val_loss: 0.4536 - val_acc: 0.8013\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4500 - acc: 0.8057 - val_loss: 0.4526 - val_acc: 0.8013\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4477 - acc: 0.8035 - val_loss: 0.4519 - val_acc: 0.8013\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4484 - acc: 0.8075 - val_loss: 0.4515 - val_acc: 0.7997\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4453 - acc: 0.8092 - val_loss: 0.4507 - val_acc: 0.7997\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4479 - acc: 0.8097 - val_loss: 0.4496 - val_acc: 0.7997\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4475 - acc: 0.8077 - val_loss: 0.4489 - val_acc: 0.7997\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4456 - acc: 0.8079 - val_loss: 0.4482 - val_acc: 0.8013\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4438 - acc: 0.8068 - val_loss: 0.4474 - val_acc: 0.8013\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4435 - acc: 0.8092 - val_loss: 0.4468 - val_acc: 0.8013\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4431 - acc: 0.8115 - val_loss: 0.4461 - val_acc: 0.8013\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4429 - acc: 0.8104 - val_loss: 0.4454 - val_acc: 0.8046\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4412 - acc: 0.8072 - val_loss: 0.4450 - val_acc: 0.8046\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4408 - acc: 0.8101 - val_loss: 0.4446 - val_acc: 0.8046\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4382 - acc: 0.8126 - val_loss: 0.4436 - val_acc: 0.8062\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4396 - acc: 0.8104 - val_loss: 0.4423 - val_acc: 0.8062\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4389 - acc: 0.8103 - val_loss: 0.4413 - val_acc: 0.8062\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4375 - acc: 0.8121 - val_loss: 0.4410 - val_acc: 0.8062\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4376 - acc: 0.8121 - val_loss: 0.4411 - val_acc: 0.8062\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4383 - acc: 0.8103 - val_loss: 0.4415 - val_acc: 0.8079\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4368 - acc: 0.8106 - val_loss: 0.4413 - val_acc: 0.8095\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4362 - acc: 0.8134 - val_loss: 0.4403 - val_acc: 0.8079\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4357 - acc: 0.8144 - val_loss: 0.4390 - val_acc: 0.8079\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4351 - acc: 0.8104 - val_loss: 0.4391 - val_acc: 0.8079\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4338 - acc: 0.8119 - val_loss: 0.4402 - val_acc: 0.8095\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4327 - acc: 0.8121 - val_loss: 0.4403 - val_acc: 0.8079\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4334 - acc: 0.8141 - val_loss: 0.4390 - val_acc: 0.8095\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4335 - acc: 0.8113 - val_loss: 0.4370 - val_acc: 0.8095\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4307 - acc: 0.8143 - val_loss: 0.4356 - val_acc: 0.8095\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4321 - acc: 0.8115 - val_loss: 0.4349 - val_acc: 0.8128\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4308 - acc: 0.8132 - val_loss: 0.4351 - val_acc: 0.8095\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4297 - acc: 0.8126 - val_loss: 0.4358 - val_acc: 0.8112\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4290 - acc: 0.8141 - val_loss: 0.4365 - val_acc: 0.8079\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4294 - acc: 0.8150 - val_loss: 0.4363 - val_acc: 0.8079\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4299 - acc: 0.8143 - val_loss: 0.4355 - val_acc: 0.8112\n",
      "Training with parameters {'batch_size': 4000, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_53 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_148 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_96 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_97 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_150 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.8509 - acc: 0.4351 - val_loss: 0.8015 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.7749 - acc: 0.4359 - val_loss: 0.7344 - val_acc: 0.3908\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7239 - acc: 0.4510 - val_loss: 0.6913 - val_acc: 0.5238\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6952 - acc: 0.5127 - val_loss: 0.6662 - val_acc: 0.6092\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6795 - acc: 0.5647 - val_loss: 0.6515 - val_acc: 0.6125\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6702 - acc: 0.5725 - val_loss: 0.6412 - val_acc: 0.6158\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6620 - acc: 0.5764 - val_loss: 0.6320 - val_acc: 0.6174\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6507 - acc: 0.5767 - val_loss: 0.6230 - val_acc: 0.6207\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6395 - acc: 0.5957 - val_loss: 0.6148 - val_acc: 0.6617\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6289 - acc: 0.6307 - val_loss: 0.6083 - val_acc: 0.7011\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6181 - acc: 0.6661 - val_loss: 0.6040 - val_acc: 0.7061\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6093 - acc: 0.6962 - val_loss: 0.6016 - val_acc: 0.6798\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6026 - acc: 0.6991 - val_loss: 0.5996 - val_acc: 0.6683\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5954 - acc: 0.6988 - val_loss: 0.5964 - val_acc: 0.6650\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5900 - acc: 0.6939 - val_loss: 0.5908 - val_acc: 0.6683\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5789 - acc: 0.7077 - val_loss: 0.5829 - val_acc: 0.6749\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5747 - acc: 0.7114 - val_loss: 0.5740 - val_acc: 0.6880\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5670 - acc: 0.7229 - val_loss: 0.5651 - val_acc: 0.6979\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5609 - acc: 0.7362 - val_loss: 0.5571 - val_acc: 0.7159\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5563 - acc: 0.7440 - val_loss: 0.5504 - val_acc: 0.7143\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5488 - acc: 0.7480 - val_loss: 0.5445 - val_acc: 0.7159\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5426 - acc: 0.7462 - val_loss: 0.5395 - val_acc: 0.7209\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5363 - acc: 0.7473 - val_loss: 0.5349 - val_acc: 0.7159\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5301 - acc: 0.7521 - val_loss: 0.5300 - val_acc: 0.7258\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5242 - acc: 0.7573 - val_loss: 0.5236 - val_acc: 0.7323\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5177 - acc: 0.7619 - val_loss: 0.5159 - val_acc: 0.7471\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5143 - acc: 0.7630 - val_loss: 0.5078 - val_acc: 0.7619\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5100 - acc: 0.7710 - val_loss: 0.5009 - val_acc: 0.7668\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5024 - acc: 0.7756 - val_loss: 0.4963 - val_acc: 0.7668\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4993 - acc: 0.7789 - val_loss: 0.4937 - val_acc: 0.7734\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4948 - acc: 0.7791 - val_loss: 0.4909 - val_acc: 0.7701\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4928 - acc: 0.7825 - val_loss: 0.4862 - val_acc: 0.7718\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4893 - acc: 0.7812 - val_loss: 0.4815 - val_acc: 0.7750\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4838 - acc: 0.7842 - val_loss: 0.4770 - val_acc: 0.7767\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4805 - acc: 0.7902 - val_loss: 0.4733 - val_acc: 0.7800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4765 - acc: 0.7838 - val_loss: 0.4698 - val_acc: 0.7816\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4720 - acc: 0.7931 - val_loss: 0.4671 - val_acc: 0.7849\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4689 - acc: 0.7902 - val_loss: 0.4644 - val_acc: 0.7849\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4658 - acc: 0.7940 - val_loss: 0.4609 - val_acc: 0.7849\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4625 - acc: 0.7958 - val_loss: 0.4575 - val_acc: 0.7915\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4586 - acc: 0.8004 - val_loss: 0.4545 - val_acc: 0.7980\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4575 - acc: 0.8000 - val_loss: 0.4522 - val_acc: 0.8013\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4565 - acc: 0.8006 - val_loss: 0.4506 - val_acc: 0.8013\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4515 - acc: 0.8009 - val_loss: 0.4489 - val_acc: 0.8013\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4488 - acc: 0.8057 - val_loss: 0.4450 - val_acc: 0.8013\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4447 - acc: 0.8072 - val_loss: 0.4424 - val_acc: 0.8013\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4428 - acc: 0.8092 - val_loss: 0.4417 - val_acc: 0.8013\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4421 - acc: 0.8073 - val_loss: 0.4406 - val_acc: 0.7997\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4418 - acc: 0.8075 - val_loss: 0.4397 - val_acc: 0.7997\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4379 - acc: 0.8079 - val_loss: 0.4380 - val_acc: 0.8013\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4373 - acc: 0.8062 - val_loss: 0.4350 - val_acc: 0.8013\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4338 - acc: 0.8104 - val_loss: 0.4332 - val_acc: 0.8046\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4329 - acc: 0.8157 - val_loss: 0.4316 - val_acc: 0.8046\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4298 - acc: 0.8159 - val_loss: 0.4291 - val_acc: 0.8062\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4315 - acc: 0.8137 - val_loss: 0.4269 - val_acc: 0.8079\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4250 - acc: 0.8232 - val_loss: 0.4273 - val_acc: 0.8062\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4233 - acc: 0.8203 - val_loss: 0.4280 - val_acc: 0.8046\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4248 - acc: 0.8159 - val_loss: 0.4258 - val_acc: 0.8079\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4221 - acc: 0.8214 - val_loss: 0.4236 - val_acc: 0.8095\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4241 - acc: 0.8172 - val_loss: 0.4225 - val_acc: 0.8095\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4206 - acc: 0.8165 - val_loss: 0.4231 - val_acc: 0.8079\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4201 - acc: 0.8207 - val_loss: 0.4228 - val_acc: 0.8079\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4186 - acc: 0.8176 - val_loss: 0.4206 - val_acc: 0.8062\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4155 - acc: 0.8219 - val_loss: 0.4179 - val_acc: 0.8144\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4131 - acc: 0.8241 - val_loss: 0.4194 - val_acc: 0.8079\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4131 - acc: 0.8210 - val_loss: 0.4215 - val_acc: 0.8128\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4110 - acc: 0.8228 - val_loss: 0.4207 - val_acc: 0.8161\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4100 - acc: 0.8241 - val_loss: 0.4183 - val_acc: 0.8095\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4080 - acc: 0.8294 - val_loss: 0.4153 - val_acc: 0.8112\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4107 - acc: 0.8263 - val_loss: 0.4158 - val_acc: 0.8095\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4099 - acc: 0.8259 - val_loss: 0.4191 - val_acc: 0.8144\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4084 - acc: 0.8263 - val_loss: 0.4174 - val_acc: 0.8128\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4073 - acc: 0.8238 - val_loss: 0.4140 - val_acc: 0.8112\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4056 - acc: 0.8241 - val_loss: 0.4146 - val_acc: 0.8112\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4011 - acc: 0.8292 - val_loss: 0.4154 - val_acc: 0.8144\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4017 - acc: 0.8289 - val_loss: 0.4145 - val_acc: 0.8144\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4008 - acc: 0.8292 - val_loss: 0.4125 - val_acc: 0.8144\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4011 - acc: 0.8267 - val_loss: 0.4126 - val_acc: 0.8144\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4029 - acc: 0.8263 - val_loss: 0.4127 - val_acc: 0.8177\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.3991 - acc: 0.8265 - val_loss: 0.4131 - val_acc: 0.8177\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.3953 - acc: 0.8342 - val_loss: 0.4107 - val_acc: 0.8161\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3962 - acc: 0.8294 - val_loss: 0.4110 - val_acc: 0.8177\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.3971 - acc: 0.8294 - val_loss: 0.4125 - val_acc: 0.8177\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.3937 - acc: 0.8323 - val_loss: 0.4121 - val_acc: 0.8177\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.3924 - acc: 0.8311 - val_loss: 0.4104 - val_acc: 0.8177\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3921 - acc: 0.8316 - val_loss: 0.4093 - val_acc: 0.8194\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.3929 - acc: 0.8325 - val_loss: 0.4112 - val_acc: 0.8210\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.3912 - acc: 0.8301 - val_loss: 0.4125 - val_acc: 0.8227\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.3893 - acc: 0.8360 - val_loss: 0.4090 - val_acc: 0.8194\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.3892 - acc: 0.8303 - val_loss: 0.4096 - val_acc: 0.8210\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3879 - acc: 0.8365 - val_loss: 0.4123 - val_acc: 0.8227\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.3854 - acc: 0.8347 - val_loss: 0.4101 - val_acc: 0.8210\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3847 - acc: 0.8334 - val_loss: 0.4083 - val_acc: 0.8177\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.3831 - acc: 0.8387 - val_loss: 0.4100 - val_acc: 0.8210\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3842 - acc: 0.8365 - val_loss: 0.4120 - val_acc: 0.8227\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3849 - acc: 0.8312 - val_loss: 0.4089 - val_acc: 0.8177\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3799 - acc: 0.8365 - val_loss: 0.4078 - val_acc: 0.8194\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.3811 - acc: 0.8367 - val_loss: 0.4102 - val_acc: 0.8177\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.3785 - acc: 0.8385 - val_loss: 0.4126 - val_acc: 0.8243\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3802 - acc: 0.8367 - val_loss: 0.4103 - val_acc: 0.8210\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.3771 - acc: 0.8352 - val_loss: 0.4076 - val_acc: 0.8210\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3745 - acc: 0.8402 - val_loss: 0.4098 - val_acc: 0.8210\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3757 - acc: 0.8384 - val_loss: 0.4114 - val_acc: 0.8194\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3739 - acc: 0.8427 - val_loss: 0.4114 - val_acc: 0.8210\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3727 - acc: 0.8393 - val_loss: 0.4102 - val_acc: 0.8194\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.3731 - acc: 0.8416 - val_loss: 0.4111 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': 4000, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_54 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_98 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_99 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.8797 - acc: 0.4351 - val_loss: 0.8562 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.8234 - acc: 0.4353 - val_loss: 0.8028 - val_acc: 0.3875\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7763 - acc: 0.4355 - val_loss: 0.7617 - val_acc: 0.3875\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.7445 - acc: 0.4408 - val_loss: 0.7298 - val_acc: 0.3924\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.7190 - acc: 0.4589 - val_loss: 0.7052 - val_acc: 0.4319\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7003 - acc: 0.4891 - val_loss: 0.6866 - val_acc: 0.5484\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6860 - acc: 0.5415 - val_loss: 0.6724 - val_acc: 0.6388\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6750 - acc: 0.5990 - val_loss: 0.6611 - val_acc: 0.6831\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6679 - acc: 0.6159 - val_loss: 0.6519 - val_acc: 0.6847\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6615 - acc: 0.6238 - val_loss: 0.6442 - val_acc: 0.6814\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6534 - acc: 0.6415 - val_loss: 0.6372 - val_acc: 0.6913\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6467 - acc: 0.6497 - val_loss: 0.6308 - val_acc: 0.7143\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6427 - acc: 0.6570 - val_loss: 0.6251 - val_acc: 0.7340\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6350 - acc: 0.6745 - val_loss: 0.6201 - val_acc: 0.7471\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6285 - acc: 0.6886 - val_loss: 0.6159 - val_acc: 0.7307\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6242 - acc: 0.6982 - val_loss: 0.6121 - val_acc: 0.7110\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6172 - acc: 0.7074 - val_loss: 0.6087 - val_acc: 0.7094\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.6113 - acc: 0.7117 - val_loss: 0.6056 - val_acc: 0.6979\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6076 - acc: 0.7075 - val_loss: 0.6025 - val_acc: 0.6995\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6009 - acc: 0.7077 - val_loss: 0.5991 - val_acc: 0.6962\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5980 - acc: 0.7152 - val_loss: 0.5955 - val_acc: 0.6995\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5937 - acc: 0.7088 - val_loss: 0.5914 - val_acc: 0.7011\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5889 - acc: 0.7156 - val_loss: 0.5869 - val_acc: 0.6995\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5849 - acc: 0.7181 - val_loss: 0.5820 - val_acc: 0.7044\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5818 - acc: 0.7221 - val_loss: 0.5770 - val_acc: 0.7061\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.5766 - acc: 0.7322 - val_loss: 0.5722 - val_acc: 0.7094\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5728 - acc: 0.7340 - val_loss: 0.5679 - val_acc: 0.7110\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5681 - acc: 0.7334 - val_loss: 0.5641 - val_acc: 0.7110\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5618 - acc: 0.7345 - val_loss: 0.5604 - val_acc: 0.7094\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5615 - acc: 0.7420 - val_loss: 0.5571 - val_acc: 0.7077\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5574 - acc: 0.7400 - val_loss: 0.5539 - val_acc: 0.7094\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5559 - acc: 0.7395 - val_loss: 0.5508 - val_acc: 0.7110\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5501 - acc: 0.7455 - val_loss: 0.5474 - val_acc: 0.7126\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5473 - acc: 0.7442 - val_loss: 0.5433 - val_acc: 0.7126\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5436 - acc: 0.7431 - val_loss: 0.5390 - val_acc: 0.7176\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5396 - acc: 0.7506 - val_loss: 0.5348 - val_acc: 0.7258\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5350 - acc: 0.7553 - val_loss: 0.5306 - val_acc: 0.7373\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5336 - acc: 0.7539 - val_loss: 0.5268 - val_acc: 0.7373\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5267 - acc: 0.7650 - val_loss: 0.5234 - val_acc: 0.7389\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5251 - acc: 0.7604 - val_loss: 0.5199 - val_acc: 0.7455\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5232 - acc: 0.7652 - val_loss: 0.5163 - val_acc: 0.7471\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5168 - acc: 0.7676 - val_loss: 0.5127 - val_acc: 0.7488\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5151 - acc: 0.7635 - val_loss: 0.5092 - val_acc: 0.7488\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5122 - acc: 0.7734 - val_loss: 0.5059 - val_acc: 0.7521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5111 - acc: 0.7648 - val_loss: 0.5029 - val_acc: 0.7521\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5084 - acc: 0.7730 - val_loss: 0.5002 - val_acc: 0.7521\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5029 - acc: 0.7783 - val_loss: 0.4977 - val_acc: 0.7537\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4997 - acc: 0.7734 - val_loss: 0.4943 - val_acc: 0.7553\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4993 - acc: 0.7734 - val_loss: 0.4903 - val_acc: 0.7635\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4946 - acc: 0.7781 - val_loss: 0.4865 - val_acc: 0.7718\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4939 - acc: 0.7765 - val_loss: 0.4838 - val_acc: 0.7701\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4908 - acc: 0.7789 - val_loss: 0.4815 - val_acc: 0.7701\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4895 - acc: 0.7789 - val_loss: 0.4798 - val_acc: 0.7701\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4843 - acc: 0.7862 - val_loss: 0.4780 - val_acc: 0.7734\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4830 - acc: 0.7849 - val_loss: 0.4759 - val_acc: 0.7750\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4837 - acc: 0.7818 - val_loss: 0.4740 - val_acc: 0.7767\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4800 - acc: 0.7842 - val_loss: 0.4721 - val_acc: 0.7767\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4768 - acc: 0.7957 - val_loss: 0.4702 - val_acc: 0.7767\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4777 - acc: 0.7889 - val_loss: 0.4690 - val_acc: 0.7750\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4752 - acc: 0.7918 - val_loss: 0.4682 - val_acc: 0.7783\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4718 - acc: 0.7918 - val_loss: 0.4662 - val_acc: 0.7783\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4716 - acc: 0.7882 - val_loss: 0.4628 - val_acc: 0.7816\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4684 - acc: 0.7913 - val_loss: 0.4600 - val_acc: 0.7865\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4698 - acc: 0.7909 - val_loss: 0.4588 - val_acc: 0.7849\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4682 - acc: 0.7947 - val_loss: 0.4589 - val_acc: 0.7849\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4610 - acc: 0.7997 - val_loss: 0.4589 - val_acc: 0.7849\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4583 - acc: 0.7995 - val_loss: 0.4585 - val_acc: 0.7865\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4637 - acc: 0.7975 - val_loss: 0.4568 - val_acc: 0.7882\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4578 - acc: 0.7982 - val_loss: 0.4541 - val_acc: 0.7898\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4563 - acc: 0.7995 - val_loss: 0.4507 - val_acc: 0.7898\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4570 - acc: 0.7958 - val_loss: 0.4484 - val_acc: 0.7947\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4555 - acc: 0.8013 - val_loss: 0.4477 - val_acc: 0.7931\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4534 - acc: 0.8028 - val_loss: 0.4484 - val_acc: 0.7898\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4518 - acc: 0.8035 - val_loss: 0.4483 - val_acc: 0.7915\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4500 - acc: 0.7993 - val_loss: 0.4470 - val_acc: 0.7898\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4483 - acc: 0.8017 - val_loss: 0.4447 - val_acc: 0.7947\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4478 - acc: 0.8039 - val_loss: 0.4424 - val_acc: 0.7980\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4469 - acc: 0.8009 - val_loss: 0.4405 - val_acc: 0.7997\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4484 - acc: 0.8081 - val_loss: 0.4399 - val_acc: 0.7997\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4465 - acc: 0.8035 - val_loss: 0.4409 - val_acc: 0.8013\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4440 - acc: 0.8068 - val_loss: 0.4405 - val_acc: 0.8013\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4409 - acc: 0.8048 - val_loss: 0.4384 - val_acc: 0.8013\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4391 - acc: 0.8077 - val_loss: 0.4362 - val_acc: 0.8013\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4397 - acc: 0.8106 - val_loss: 0.4349 - val_acc: 0.8046\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4406 - acc: 0.8144 - val_loss: 0.4350 - val_acc: 0.8013\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4381 - acc: 0.8081 - val_loss: 0.4351 - val_acc: 0.8030\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4363 - acc: 0.8097 - val_loss: 0.4341 - val_acc: 0.8030\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4371 - acc: 0.8119 - val_loss: 0.4329 - val_acc: 0.8030\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4361 - acc: 0.8064 - val_loss: 0.4318 - val_acc: 0.8046\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4345 - acc: 0.8097 - val_loss: 0.4311 - val_acc: 0.8046\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4313 - acc: 0.8106 - val_loss: 0.4307 - val_acc: 0.8046\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4329 - acc: 0.8110 - val_loss: 0.4301 - val_acc: 0.8046\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4350 - acc: 0.8082 - val_loss: 0.4291 - val_acc: 0.8062\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4301 - acc: 0.8132 - val_loss: 0.4289 - val_acc: 0.8046\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4315 - acc: 0.8117 - val_loss: 0.4283 - val_acc: 0.8062\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4312 - acc: 0.8132 - val_loss: 0.4281 - val_acc: 0.8046\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4296 - acc: 0.8137 - val_loss: 0.4273 - val_acc: 0.8079\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4267 - acc: 0.8126 - val_loss: 0.4266 - val_acc: 0.8095\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4275 - acc: 0.8110 - val_loss: 0.4267 - val_acc: 0.8062\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4238 - acc: 0.8146 - val_loss: 0.4264 - val_acc: 0.8062\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4225 - acc: 0.8159 - val_loss: 0.4256 - val_acc: 0.8095\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4217 - acc: 0.8186 - val_loss: 0.4249 - val_acc: 0.8112\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4221 - acc: 0.8172 - val_loss: 0.4253 - val_acc: 0.8095\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4236 - acc: 0.8163 - val_loss: 0.4253 - val_acc: 0.8095\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4200 - acc: 0.8205 - val_loss: 0.4255 - val_acc: 0.8079\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4204 - acc: 0.8170 - val_loss: 0.4257 - val_acc: 0.8095\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4192 - acc: 0.8177 - val_loss: 0.4247 - val_acc: 0.8079\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4175 - acc: 0.8190 - val_loss: 0.4237 - val_acc: 0.8112\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4190 - acc: 0.8181 - val_loss: 0.4228 - val_acc: 0.8128\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4196 - acc: 0.8181 - val_loss: 0.4224 - val_acc: 0.8128\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4178 - acc: 0.8185 - val_loss: 0.4222 - val_acc: 0.8144\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4143 - acc: 0.8203 - val_loss: 0.4210 - val_acc: 0.8144\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4115 - acc: 0.8245 - val_loss: 0.4194 - val_acc: 0.8177\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4126 - acc: 0.8203 - val_loss: 0.4194 - val_acc: 0.8161\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4146 - acc: 0.8210 - val_loss: 0.4206 - val_acc: 0.8144\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4117 - acc: 0.8210 - val_loss: 0.4205 - val_acc: 0.8161\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4129 - acc: 0.8163 - val_loss: 0.4194 - val_acc: 0.8177\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4137 - acc: 0.8205 - val_loss: 0.4187 - val_acc: 0.8177\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4119 - acc: 0.8207 - val_loss: 0.4192 - val_acc: 0.8177\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4083 - acc: 0.8250 - val_loss: 0.4203 - val_acc: 0.8144\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4104 - acc: 0.8197 - val_loss: 0.4199 - val_acc: 0.8144\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4061 - acc: 0.8245 - val_loss: 0.4179 - val_acc: 0.8144\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4080 - acc: 0.8205 - val_loss: 0.4163 - val_acc: 0.8177\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4061 - acc: 0.8252 - val_loss: 0.4171 - val_acc: 0.8161\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4062 - acc: 0.8265 - val_loss: 0.4184 - val_acc: 0.8128\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4068 - acc: 0.8199 - val_loss: 0.4198 - val_acc: 0.8144\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4042 - acc: 0.8280 - val_loss: 0.4187 - val_acc: 0.8128\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4087 - acc: 0.8216 - val_loss: 0.4162 - val_acc: 0.8177\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4069 - acc: 0.8254 - val_loss: 0.4141 - val_acc: 0.8210\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4040 - acc: 0.8261 - val_loss: 0.4141 - val_acc: 0.8194\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4036 - acc: 0.8278 - val_loss: 0.4153 - val_acc: 0.8194\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4025 - acc: 0.8274 - val_loss: 0.4170 - val_acc: 0.8128\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4001 - acc: 0.8261 - val_loss: 0.4184 - val_acc: 0.8144\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4005 - acc: 0.8320 - val_loss: 0.4185 - val_acc: 0.8161\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.3995 - acc: 0.8280 - val_loss: 0.4163 - val_acc: 0.8177\n",
      "Training with parameters {'batch_size': 4000, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_55 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_154 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_100 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_155 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_101 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_156 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.7664 - acc: 0.4359 - val_loss: 0.7652 - val_acc: 0.3892\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7398 - acc: 0.4375 - val_loss: 0.7375 - val_acc: 0.3990\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7179 - acc: 0.4521 - val_loss: 0.7146 - val_acc: 0.4319\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7003 - acc: 0.4835 - val_loss: 0.6956 - val_acc: 0.4795\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6878 - acc: 0.5295 - val_loss: 0.6803 - val_acc: 0.5698\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6772 - acc: 0.5864 - val_loss: 0.6683 - val_acc: 0.6585\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6682 - acc: 0.6309 - val_loss: 0.6590 - val_acc: 0.6864\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6637 - acc: 0.6367 - val_loss: 0.6514 - val_acc: 0.6683\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6567 - acc: 0.6358 - val_loss: 0.6450 - val_acc: 0.6519\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6537 - acc: 0.6231 - val_loss: 0.6395 - val_acc: 0.6437\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6489 - acc: 0.6258 - val_loss: 0.6346 - val_acc: 0.6486\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6441 - acc: 0.6296 - val_loss: 0.6301 - val_acc: 0.6568\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6392 - acc: 0.6426 - val_loss: 0.6258 - val_acc: 0.6765\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6345 - acc: 0.6517 - val_loss: 0.6218 - val_acc: 0.6946\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6304 - acc: 0.6623 - val_loss: 0.6182 - val_acc: 0.6946\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6236 - acc: 0.6772 - val_loss: 0.6148 - val_acc: 0.6962\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6192 - acc: 0.6948 - val_loss: 0.6117 - val_acc: 0.7110\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6159 - acc: 0.7019 - val_loss: 0.6088 - val_acc: 0.7094\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6088 - acc: 0.7090 - val_loss: 0.6060 - val_acc: 0.7028\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6074 - acc: 0.7086 - val_loss: 0.6030 - val_acc: 0.6962\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6022 - acc: 0.7101 - val_loss: 0.5997 - val_acc: 0.6995\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5969 - acc: 0.7172 - val_loss: 0.5963 - val_acc: 0.6962\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5960 - acc: 0.7126 - val_loss: 0.5926 - val_acc: 0.6979\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5884 - acc: 0.7205 - val_loss: 0.5887 - val_acc: 0.7028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5836 - acc: 0.7245 - val_loss: 0.5847 - val_acc: 0.7077\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5806 - acc: 0.7216 - val_loss: 0.5806 - val_acc: 0.7094\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5749 - acc: 0.7265 - val_loss: 0.5768 - val_acc: 0.7126\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5722 - acc: 0.7333 - val_loss: 0.5729 - val_acc: 0.7176\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.5705 - acc: 0.7362 - val_loss: 0.5691 - val_acc: 0.7192\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5662 - acc: 0.7375 - val_loss: 0.5655 - val_acc: 0.7176\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5605 - acc: 0.7395 - val_loss: 0.5619 - val_acc: 0.7225\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5570 - acc: 0.7393 - val_loss: 0.5583 - val_acc: 0.7241\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5520 - acc: 0.7469 - val_loss: 0.5545 - val_acc: 0.7291\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5486 - acc: 0.7400 - val_loss: 0.5503 - val_acc: 0.7291\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5430 - acc: 0.7486 - val_loss: 0.5459 - val_acc: 0.7291\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5387 - acc: 0.7535 - val_loss: 0.5410 - val_acc: 0.7389\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.5346 - acc: 0.7553 - val_loss: 0.5362 - val_acc: 0.7438\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5319 - acc: 0.7594 - val_loss: 0.5318 - val_acc: 0.7455\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5307 - acc: 0.7597 - val_loss: 0.5282 - val_acc: 0.7471\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5252 - acc: 0.7623 - val_loss: 0.5244 - val_acc: 0.7438\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5218 - acc: 0.7628 - val_loss: 0.5206 - val_acc: 0.7471\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5166 - acc: 0.7676 - val_loss: 0.5170 - val_acc: 0.7488\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5159 - acc: 0.7652 - val_loss: 0.5134 - val_acc: 0.7471\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.5091 - acc: 0.7698 - val_loss: 0.5101 - val_acc: 0.7504\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5080 - acc: 0.7734 - val_loss: 0.5071 - val_acc: 0.7521\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5036 - acc: 0.7699 - val_loss: 0.5040 - val_acc: 0.7570\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5001 - acc: 0.7765 - val_loss: 0.5009 - val_acc: 0.7586\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4979 - acc: 0.7778 - val_loss: 0.4976 - val_acc: 0.7603\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4932 - acc: 0.7796 - val_loss: 0.4942 - val_acc: 0.7635\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4937 - acc: 0.7803 - val_loss: 0.4915 - val_acc: 0.7635\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4894 - acc: 0.7747 - val_loss: 0.4899 - val_acc: 0.7668\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4874 - acc: 0.7805 - val_loss: 0.4880 - val_acc: 0.7685\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4856 - acc: 0.7836 - val_loss: 0.4851 - val_acc: 0.7718\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4833 - acc: 0.7822 - val_loss: 0.4824 - val_acc: 0.7767\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4814 - acc: 0.7840 - val_loss: 0.4800 - val_acc: 0.7734\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4768 - acc: 0.7849 - val_loss: 0.4780 - val_acc: 0.7783\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4768 - acc: 0.7898 - val_loss: 0.4768 - val_acc: 0.7783\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4727 - acc: 0.7893 - val_loss: 0.4759 - val_acc: 0.7849\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4737 - acc: 0.7876 - val_loss: 0.4736 - val_acc: 0.7833\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4716 - acc: 0.7926 - val_loss: 0.4712 - val_acc: 0.7931\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4717 - acc: 0.7909 - val_loss: 0.4684 - val_acc: 0.7947\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4677 - acc: 0.7905 - val_loss: 0.4663 - val_acc: 0.7947\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4650 - acc: 0.7938 - val_loss: 0.4654 - val_acc: 0.7931\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4663 - acc: 0.7966 - val_loss: 0.4649 - val_acc: 0.7947\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4614 - acc: 0.7978 - val_loss: 0.4637 - val_acc: 0.7947\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4622 - acc: 0.7944 - val_loss: 0.4618 - val_acc: 0.7947\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4555 - acc: 0.8020 - val_loss: 0.4591 - val_acc: 0.7931\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4599 - acc: 0.7971 - val_loss: 0.4570 - val_acc: 0.7931\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4573 - acc: 0.7977 - val_loss: 0.4554 - val_acc: 0.7915\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4555 - acc: 0.7995 - val_loss: 0.4552 - val_acc: 0.7915\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4520 - acc: 0.8041 - val_loss: 0.4557 - val_acc: 0.7931\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4556 - acc: 0.7971 - val_loss: 0.4553 - val_acc: 0.7931\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4520 - acc: 0.8011 - val_loss: 0.4524 - val_acc: 0.7947\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4486 - acc: 0.8041 - val_loss: 0.4499 - val_acc: 0.7980\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4451 - acc: 0.8048 - val_loss: 0.4480 - val_acc: 0.7997\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4457 - acc: 0.8015 - val_loss: 0.4471 - val_acc: 0.7997\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4460 - acc: 0.8037 - val_loss: 0.4486 - val_acc: 0.8013\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4420 - acc: 0.8070 - val_loss: 0.4490 - val_acc: 0.7997\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4436 - acc: 0.8068 - val_loss: 0.4461 - val_acc: 0.8013\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4414 - acc: 0.8041 - val_loss: 0.4425 - val_acc: 0.8046\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4415 - acc: 0.8048 - val_loss: 0.4408 - val_acc: 0.8062\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4390 - acc: 0.8101 - val_loss: 0.4407 - val_acc: 0.8062\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4393 - acc: 0.8088 - val_loss: 0.4415 - val_acc: 0.8046\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4383 - acc: 0.8126 - val_loss: 0.4418 - val_acc: 0.8046\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4361 - acc: 0.8081 - val_loss: 0.4398 - val_acc: 0.8046\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4339 - acc: 0.8123 - val_loss: 0.4372 - val_acc: 0.8095\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4410 - acc: 0.8068 - val_loss: 0.4353 - val_acc: 0.8079\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4361 - acc: 0.8097 - val_loss: 0.4353 - val_acc: 0.8062\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4343 - acc: 0.8119 - val_loss: 0.4359 - val_acc: 0.8062\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4339 - acc: 0.8124 - val_loss: 0.4364 - val_acc: 0.8013\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4321 - acc: 0.8141 - val_loss: 0.4365 - val_acc: 0.8013\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4291 - acc: 0.8101 - val_loss: 0.4349 - val_acc: 0.8030\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4285 - acc: 0.8159 - val_loss: 0.4340 - val_acc: 0.8030\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4278 - acc: 0.8135 - val_loss: 0.4335 - val_acc: 0.8030\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4288 - acc: 0.8135 - val_loss: 0.4328 - val_acc: 0.8046\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4264 - acc: 0.8128 - val_loss: 0.4327 - val_acc: 0.8030\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4253 - acc: 0.8132 - val_loss: 0.4331 - val_acc: 0.8030\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4279 - acc: 0.8163 - val_loss: 0.4323 - val_acc: 0.8030\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4244 - acc: 0.8143 - val_loss: 0.4318 - val_acc: 0.8030\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4253 - acc: 0.8148 - val_loss: 0.4303 - val_acc: 0.8062\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4213 - acc: 0.8154 - val_loss: 0.4293 - val_acc: 0.8079\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4200 - acc: 0.8194 - val_loss: 0.4296 - val_acc: 0.8062\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4220 - acc: 0.8203 - val_loss: 0.4296 - val_acc: 0.8046\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4220 - acc: 0.8170 - val_loss: 0.4288 - val_acc: 0.8062\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4180 - acc: 0.8201 - val_loss: 0.4271 - val_acc: 0.8079\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4189 - acc: 0.8183 - val_loss: 0.4265 - val_acc: 0.8095\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4219 - acc: 0.8177 - val_loss: 0.4257 - val_acc: 0.8095\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4177 - acc: 0.8196 - val_loss: 0.4250 - val_acc: 0.8128\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4177 - acc: 0.8188 - val_loss: 0.4263 - val_acc: 0.8095\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4164 - acc: 0.8192 - val_loss: 0.4267 - val_acc: 0.8046\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4162 - acc: 0.8185 - val_loss: 0.4255 - val_acc: 0.8144\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4152 - acc: 0.8214 - val_loss: 0.4239 - val_acc: 0.8161\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4137 - acc: 0.8194 - val_loss: 0.4230 - val_acc: 0.8144\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4134 - acc: 0.8238 - val_loss: 0.4246 - val_acc: 0.8161\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4165 - acc: 0.8144 - val_loss: 0.4272 - val_acc: 0.8062\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4132 - acc: 0.8203 - val_loss: 0.4265 - val_acc: 0.8062\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4123 - acc: 0.8223 - val_loss: 0.4236 - val_acc: 0.8161\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4103 - acc: 0.8228 - val_loss: 0.4207 - val_acc: 0.8112\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4120 - acc: 0.8219 - val_loss: 0.4199 - val_acc: 0.8144\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4096 - acc: 0.8290 - val_loss: 0.4217 - val_acc: 0.8161\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4091 - acc: 0.8238 - val_loss: 0.4246 - val_acc: 0.8177\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4101 - acc: 0.8243 - val_loss: 0.4237 - val_acc: 0.8194\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4092 - acc: 0.8238 - val_loss: 0.4207 - val_acc: 0.8144\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4057 - acc: 0.8261 - val_loss: 0.4195 - val_acc: 0.8144\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4074 - acc: 0.8238 - val_loss: 0.4201 - val_acc: 0.8161\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4065 - acc: 0.8281 - val_loss: 0.4218 - val_acc: 0.8194\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4031 - acc: 0.8263 - val_loss: 0.4216 - val_acc: 0.8194\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4036 - acc: 0.8261 - val_loss: 0.4198 - val_acc: 0.8161\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4032 - acc: 0.8292 - val_loss: 0.4182 - val_acc: 0.8161\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4021 - acc: 0.8267 - val_loss: 0.4196 - val_acc: 0.8161\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4027 - acc: 0.8252 - val_loss: 0.4226 - val_acc: 0.8177\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4019 - acc: 0.8247 - val_loss: 0.4217 - val_acc: 0.8210\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4024 - acc: 0.8245 - val_loss: 0.4183 - val_acc: 0.8161\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4022 - acc: 0.8269 - val_loss: 0.4166 - val_acc: 0.8161\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3992 - acc: 0.8305 - val_loss: 0.4166 - val_acc: 0.8161\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4017 - acc: 0.8236 - val_loss: 0.4182 - val_acc: 0.8194\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4006 - acc: 0.8287 - val_loss: 0.4202 - val_acc: 0.8177\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3960 - acc: 0.8290 - val_loss: 0.4196 - val_acc: 0.8194\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3996 - acc: 0.8296 - val_loss: 0.4165 - val_acc: 0.8161\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.3983 - acc: 0.8292 - val_loss: 0.4152 - val_acc: 0.8177\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3991 - acc: 0.8248 - val_loss: 0.4165 - val_acc: 0.8161\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.3967 - acc: 0.8312 - val_loss: 0.4181 - val_acc: 0.8210\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3959 - acc: 0.8303 - val_loss: 0.4195 - val_acc: 0.8210\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.3969 - acc: 0.8289 - val_loss: 0.4192 - val_acc: 0.8210\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3945 - acc: 0.8272 - val_loss: 0.4163 - val_acc: 0.8177\n",
      "Training with parameters {'batch_size': 4000, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_56 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_157 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_102 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_158 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_103 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_159 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.8447 - acc: 0.4348 - val_loss: 0.8174 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.7913 - acc: 0.4350 - val_loss: 0.7671 - val_acc: 0.3892\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7516 - acc: 0.4441 - val_loss: 0.7273 - val_acc: 0.3924\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7208 - acc: 0.4610 - val_loss: 0.6972 - val_acc: 0.4828\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6979 - acc: 0.5096 - val_loss: 0.6753 - val_acc: 0.6207\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6813 - acc: 0.5749 - val_loss: 0.6596 - val_acc: 0.6355\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6710 - acc: 0.6088 - val_loss: 0.6481 - val_acc: 0.6273\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6645 - acc: 0.6004 - val_loss: 0.6394 - val_acc: 0.6207\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6591 - acc: 0.5973 - val_loss: 0.6322 - val_acc: 0.6190\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6521 - acc: 0.5953 - val_loss: 0.6257 - val_acc: 0.6223\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6451 - acc: 0.5995 - val_loss: 0.6197 - val_acc: 0.6289\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6385 - acc: 0.6119 - val_loss: 0.6139 - val_acc: 0.6404\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6312 - acc: 0.6342 - val_loss: 0.6088 - val_acc: 0.6749\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6227 - acc: 0.6732 - val_loss: 0.6043 - val_acc: 0.7291\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6148 - acc: 0.6939 - val_loss: 0.6006 - val_acc: 0.7241\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6087 - acc: 0.7079 - val_loss: 0.5973 - val_acc: 0.7159\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6037 - acc: 0.7086 - val_loss: 0.5941 - val_acc: 0.7011\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5992 - acc: 0.7002 - val_loss: 0.5906 - val_acc: 0.6979\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5896 - acc: 0.7086 - val_loss: 0.5867 - val_acc: 0.7011\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5898 - acc: 0.7044 - val_loss: 0.5822 - val_acc: 0.6995\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5814 - acc: 0.7115 - val_loss: 0.5774 - val_acc: 0.7011\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.5786 - acc: 0.7108 - val_loss: 0.5720 - val_acc: 0.7061\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5738 - acc: 0.7150 - val_loss: 0.5664 - val_acc: 0.7126\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5684 - acc: 0.7230 - val_loss: 0.5609 - val_acc: 0.7176\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5614 - acc: 0.7367 - val_loss: 0.5556 - val_acc: 0.7176\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5584 - acc: 0.7396 - val_loss: 0.5507 - val_acc: 0.7192\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5541 - acc: 0.7396 - val_loss: 0.5461 - val_acc: 0.7241\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5486 - acc: 0.7437 - val_loss: 0.5416 - val_acc: 0.7258\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5438 - acc: 0.7420 - val_loss: 0.5368 - val_acc: 0.7241\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.5384 - acc: 0.7521 - val_loss: 0.5318 - val_acc: 0.7340\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5335 - acc: 0.7524 - val_loss: 0.5269 - val_acc: 0.7389\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5292 - acc: 0.7555 - val_loss: 0.5216 - val_acc: 0.7406\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5230 - acc: 0.7570 - val_loss: 0.5162 - val_acc: 0.7438\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5209 - acc: 0.7594 - val_loss: 0.5105 - val_acc: 0.7471\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5181 - acc: 0.7632 - val_loss: 0.5052 - val_acc: 0.7471\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5112 - acc: 0.7681 - val_loss: 0.5008 - val_acc: 0.7521\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5086 - acc: 0.7645 - val_loss: 0.4963 - val_acc: 0.7570\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5029 - acc: 0.7714 - val_loss: 0.4925 - val_acc: 0.7586\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5010 - acc: 0.7710 - val_loss: 0.4891 - val_acc: 0.7586\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4937 - acc: 0.7769 - val_loss: 0.4853 - val_acc: 0.7652\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4896 - acc: 0.7811 - val_loss: 0.4808 - val_acc: 0.7701\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4864 - acc: 0.7789 - val_loss: 0.4758 - val_acc: 0.7734\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4838 - acc: 0.7823 - val_loss: 0.4716 - val_acc: 0.7767\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4820 - acc: 0.7812 - val_loss: 0.4687 - val_acc: 0.7767\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4813 - acc: 0.7854 - val_loss: 0.4667 - val_acc: 0.7800\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4734 - acc: 0.7893 - val_loss: 0.4642 - val_acc: 0.7833\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4689 - acc: 0.7913 - val_loss: 0.4602 - val_acc: 0.7816\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4713 - acc: 0.7882 - val_loss: 0.4569 - val_acc: 0.7865\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4652 - acc: 0.7951 - val_loss: 0.4553 - val_acc: 0.7882\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4643 - acc: 0.7927 - val_loss: 0.4539 - val_acc: 0.7915\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4624 - acc: 0.7926 - val_loss: 0.4515 - val_acc: 0.8013\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4568 - acc: 0.7931 - val_loss: 0.4498 - val_acc: 0.7997\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4609 - acc: 0.7915 - val_loss: 0.4472 - val_acc: 0.8013\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4524 - acc: 0.7988 - val_loss: 0.4450 - val_acc: 0.8046\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4523 - acc: 0.7993 - val_loss: 0.4429 - val_acc: 0.8046\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4490 - acc: 0.8017 - val_loss: 0.4404 - val_acc: 0.8079\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4482 - acc: 0.8026 - val_loss: 0.4379 - val_acc: 0.8095\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4466 - acc: 0.8020 - val_loss: 0.4375 - val_acc: 0.8079\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4471 - acc: 0.8013 - val_loss: 0.4364 - val_acc: 0.8062\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4425 - acc: 0.8059 - val_loss: 0.4350 - val_acc: 0.8046\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4438 - acc: 0.8008 - val_loss: 0.4339 - val_acc: 0.8079\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4407 - acc: 0.8055 - val_loss: 0.4337 - val_acc: 0.8046\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4388 - acc: 0.8103 - val_loss: 0.4318 - val_acc: 0.8079\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4347 - acc: 0.8119 - val_loss: 0.4300 - val_acc: 0.8079\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4377 - acc: 0.8103 - val_loss: 0.4288 - val_acc: 0.8112\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4360 - acc: 0.8068 - val_loss: 0.4277 - val_acc: 0.8128\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4310 - acc: 0.8104 - val_loss: 0.4282 - val_acc: 0.8062\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4341 - acc: 0.8095 - val_loss: 0.4293 - val_acc: 0.8079\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4301 - acc: 0.8161 - val_loss: 0.4289 - val_acc: 0.8095\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4294 - acc: 0.8115 - val_loss: 0.4247 - val_acc: 0.8079\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4307 - acc: 0.8108 - val_loss: 0.4211 - val_acc: 0.8144\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4293 - acc: 0.8104 - val_loss: 0.4203 - val_acc: 0.8144\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4279 - acc: 0.8155 - val_loss: 0.4215 - val_acc: 0.8095\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4282 - acc: 0.8103 - val_loss: 0.4232 - val_acc: 0.8095\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4238 - acc: 0.8146 - val_loss: 0.4234 - val_acc: 0.8079\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4231 - acc: 0.8137 - val_loss: 0.4194 - val_acc: 0.8128\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4204 - acc: 0.8161 - val_loss: 0.4173 - val_acc: 0.8144\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4198 - acc: 0.8181 - val_loss: 0.4188 - val_acc: 0.8161\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4207 - acc: 0.8146 - val_loss: 0.4202 - val_acc: 0.8095\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4178 - acc: 0.8165 - val_loss: 0.4200 - val_acc: 0.8095\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4195 - acc: 0.8163 - val_loss: 0.4186 - val_acc: 0.8112\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4148 - acc: 0.8212 - val_loss: 0.4170 - val_acc: 0.8128\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4146 - acc: 0.8203 - val_loss: 0.4170 - val_acc: 0.8095\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4153 - acc: 0.8181 - val_loss: 0.4168 - val_acc: 0.8112\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4125 - acc: 0.8172 - val_loss: 0.4152 - val_acc: 0.8144\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.4150 - acc: 0.8192 - val_loss: 0.4138 - val_acc: 0.8177\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4123 - acc: 0.8205 - val_loss: 0.4136 - val_acc: 0.8177\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4085 - acc: 0.8245 - val_loss: 0.4147 - val_acc: 0.8144\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4088 - acc: 0.8261 - val_loss: 0.4155 - val_acc: 0.8128\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4070 - acc: 0.8252 - val_loss: 0.4143 - val_acc: 0.8112\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4060 - acc: 0.8256 - val_loss: 0.4120 - val_acc: 0.8161\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4049 - acc: 0.8263 - val_loss: 0.4121 - val_acc: 0.8161\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4071 - acc: 0.8205 - val_loss: 0.4142 - val_acc: 0.8095\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4060 - acc: 0.8243 - val_loss: 0.4141 - val_acc: 0.8095\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4074 - acc: 0.8232 - val_loss: 0.4105 - val_acc: 0.8194\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4042 - acc: 0.8265 - val_loss: 0.4106 - val_acc: 0.8177\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4037 - acc: 0.8261 - val_loss: 0.4123 - val_acc: 0.8112\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4012 - acc: 0.8289 - val_loss: 0.4114 - val_acc: 0.8161\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4011 - acc: 0.8250 - val_loss: 0.4096 - val_acc: 0.8177\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.3992 - acc: 0.8289 - val_loss: 0.4095 - val_acc: 0.8177\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.3978 - acc: 0.8270 - val_loss: 0.4108 - val_acc: 0.8128\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.3974 - acc: 0.8301 - val_loss: 0.4112 - val_acc: 0.8144\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.3996 - acc: 0.8238 - val_loss: 0.4098 - val_acc: 0.8161\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.3965 - acc: 0.8272 - val_loss: 0.4097 - val_acc: 0.8161\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.3955 - acc: 0.8292 - val_loss: 0.4113 - val_acc: 0.8161\n",
      "Training with parameters {'batch_size': 4500, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_57 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_160 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_104 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_105 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_162 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 1s 115ms/step - loss: 0.7816 - acc: 0.4353 - val_loss: 0.7316 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.7237 - acc: 0.4452 - val_loss: 0.6850 - val_acc: 0.5977\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6898 - acc: 0.5318 - val_loss: 0.6595 - val_acc: 0.6108\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6728 - acc: 0.5751 - val_loss: 0.6458 - val_acc: 0.6125\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6641 - acc: 0.5725 - val_loss: 0.6359 - val_acc: 0.6108\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6565 - acc: 0.5727 - val_loss: 0.6261 - val_acc: 0.6108\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6454 - acc: 0.5838 - val_loss: 0.6165 - val_acc: 0.6273\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6326 - acc: 0.6134 - val_loss: 0.6086 - val_acc: 0.7274\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6208 - acc: 0.6721 - val_loss: 0.6035 - val_acc: 0.7094\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6094 - acc: 0.7044 - val_loss: 0.6007 - val_acc: 0.6765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6016 - acc: 0.7068 - val_loss: 0.5981 - val_acc: 0.6650\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5939 - acc: 0.6984 - val_loss: 0.5934 - val_acc: 0.6601\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5859 - acc: 0.7013 - val_loss: 0.5857 - val_acc: 0.6765\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5794 - acc: 0.7033 - val_loss: 0.5764 - val_acc: 0.6831\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5697 - acc: 0.7199 - val_loss: 0.5669 - val_acc: 0.6913\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5612 - acc: 0.7314 - val_loss: 0.5575 - val_acc: 0.7110\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5543 - acc: 0.7435 - val_loss: 0.5491 - val_acc: 0.7291\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.5504 - acc: 0.7475 - val_loss: 0.5423 - val_acc: 0.7406\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5425 - acc: 0.7513 - val_loss: 0.5367 - val_acc: 0.7356\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5350 - acc: 0.7544 - val_loss: 0.5320 - val_acc: 0.7406\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5285 - acc: 0.7537 - val_loss: 0.5274 - val_acc: 0.7323\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5213 - acc: 0.7570 - val_loss: 0.5210 - val_acc: 0.7455\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5159 - acc: 0.7595 - val_loss: 0.5135 - val_acc: 0.7570\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5082 - acc: 0.7635 - val_loss: 0.5053 - val_acc: 0.7603\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5045 - acc: 0.7698 - val_loss: 0.4979 - val_acc: 0.7718\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4979 - acc: 0.7796 - val_loss: 0.4928 - val_acc: 0.7734\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4937 - acc: 0.7814 - val_loss: 0.4887 - val_acc: 0.7800\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4879 - acc: 0.7862 - val_loss: 0.4847 - val_acc: 0.7833\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4864 - acc: 0.7842 - val_loss: 0.4811 - val_acc: 0.7833\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4808 - acc: 0.7847 - val_loss: 0.4786 - val_acc: 0.7833\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4773 - acc: 0.7896 - val_loss: 0.4759 - val_acc: 0.7833\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4732 - acc: 0.7885 - val_loss: 0.4709 - val_acc: 0.7882\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4689 - acc: 0.7935 - val_loss: 0.4654 - val_acc: 0.7882\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4641 - acc: 0.7949 - val_loss: 0.4617 - val_acc: 0.7898\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4619 - acc: 0.7978 - val_loss: 0.4600 - val_acc: 0.7882\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4577 - acc: 0.8015 - val_loss: 0.4599 - val_acc: 0.7833\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4531 - acc: 0.8028 - val_loss: 0.4580 - val_acc: 0.7882\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4531 - acc: 0.7988 - val_loss: 0.4526 - val_acc: 0.7980\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4511 - acc: 0.8051 - val_loss: 0.4475 - val_acc: 0.7980\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4453 - acc: 0.8046 - val_loss: 0.4439 - val_acc: 0.8030\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4456 - acc: 0.8079 - val_loss: 0.4419 - val_acc: 0.8030\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4408 - acc: 0.8103 - val_loss: 0.4429 - val_acc: 0.7980\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4409 - acc: 0.8093 - val_loss: 0.4430 - val_acc: 0.7964\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4382 - acc: 0.8051 - val_loss: 0.4387 - val_acc: 0.7980\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4357 - acc: 0.8113 - val_loss: 0.4330 - val_acc: 0.8095\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4326 - acc: 0.8137 - val_loss: 0.4306 - val_acc: 0.8079\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4321 - acc: 0.8113 - val_loss: 0.4320 - val_acc: 0.8095\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4301 - acc: 0.8126 - val_loss: 0.4335 - val_acc: 0.8062\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4290 - acc: 0.8139 - val_loss: 0.4309 - val_acc: 0.8095\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4269 - acc: 0.8137 - val_loss: 0.4277 - val_acc: 0.8095\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4246 - acc: 0.8181 - val_loss: 0.4260 - val_acc: 0.8112\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4241 - acc: 0.8152 - val_loss: 0.4282 - val_acc: 0.8095\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4217 - acc: 0.8174 - val_loss: 0.4289 - val_acc: 0.8128\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4195 - acc: 0.8152 - val_loss: 0.4242 - val_acc: 0.8128\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4156 - acc: 0.8205 - val_loss: 0.4203 - val_acc: 0.8177\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4163 - acc: 0.8214 - val_loss: 0.4212 - val_acc: 0.8144\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4173 - acc: 0.8207 - val_loss: 0.4274 - val_acc: 0.8095\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4136 - acc: 0.8166 - val_loss: 0.4255 - val_acc: 0.8112\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4128 - acc: 0.8225 - val_loss: 0.4192 - val_acc: 0.8161\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4134 - acc: 0.8188 - val_loss: 0.4180 - val_acc: 0.8161\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4094 - acc: 0.8225 - val_loss: 0.4191 - val_acc: 0.8177\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4088 - acc: 0.8225 - val_loss: 0.4212 - val_acc: 0.8161\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4090 - acc: 0.8196 - val_loss: 0.4201 - val_acc: 0.8161\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4076 - acc: 0.8234 - val_loss: 0.4172 - val_acc: 0.8194\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4071 - acc: 0.8241 - val_loss: 0.4163 - val_acc: 0.8194\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4056 - acc: 0.8274 - val_loss: 0.4174 - val_acc: 0.8210\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4027 - acc: 0.8272 - val_loss: 0.4185 - val_acc: 0.8227\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4017 - acc: 0.8267 - val_loss: 0.4137 - val_acc: 0.8210\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.3993 - acc: 0.8272 - val_loss: 0.4109 - val_acc: 0.8177\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4002 - acc: 0.8280 - val_loss: 0.4156 - val_acc: 0.8227\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3996 - acc: 0.8294 - val_loss: 0.4210 - val_acc: 0.8227\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3996 - acc: 0.8274 - val_loss: 0.4128 - val_acc: 0.8227\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3942 - acc: 0.8309 - val_loss: 0.4084 - val_acc: 0.8177\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3941 - acc: 0.8312 - val_loss: 0.4101 - val_acc: 0.8227\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.3949 - acc: 0.8305 - val_loss: 0.4135 - val_acc: 0.8227\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3941 - acc: 0.8316 - val_loss: 0.4114 - val_acc: 0.8243\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.3898 - acc: 0.8369 - val_loss: 0.4081 - val_acc: 0.8194\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.3935 - acc: 0.8340 - val_loss: 0.4107 - val_acc: 0.8210\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3894 - acc: 0.8300 - val_loss: 0.4169 - val_acc: 0.8243\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3877 - acc: 0.8334 - val_loss: 0.4134 - val_acc: 0.8243\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3872 - acc: 0.8349 - val_loss: 0.4099 - val_acc: 0.8210\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.3875 - acc: 0.8354 - val_loss: 0.4112 - val_acc: 0.8210\n",
      "Training with parameters {'batch_size': 4500, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_58 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_163 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_106 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_164 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_107 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_165 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.8457 - acc: 0.4346 - val_loss: 0.8175 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.7945 - acc: 0.4375 - val_loss: 0.7674 - val_acc: 0.3892\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.7555 - acc: 0.4413 - val_loss: 0.7277 - val_acc: 0.3924\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.7249 - acc: 0.4496 - val_loss: 0.6977 - val_acc: 0.4811\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6974 - acc: 0.5125 - val_loss: 0.6757 - val_acc: 0.6223\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6816 - acc: 0.5712 - val_loss: 0.6598 - val_acc: 0.6404\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6716 - acc: 0.6015 - val_loss: 0.6484 - val_acc: 0.6305\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6621 - acc: 0.6026 - val_loss: 0.6397 - val_acc: 0.6223\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6588 - acc: 0.5975 - val_loss: 0.6325 - val_acc: 0.6190\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6502 - acc: 0.5961 - val_loss: 0.6261 - val_acc: 0.6223\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6453 - acc: 0.5975 - val_loss: 0.6200 - val_acc: 0.6305\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6375 - acc: 0.6156 - val_loss: 0.6143 - val_acc: 0.6404\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6299 - acc: 0.6422 - val_loss: 0.6091 - val_acc: 0.6765\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6223 - acc: 0.6714 - val_loss: 0.6048 - val_acc: 0.7307\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6171 - acc: 0.6940 - val_loss: 0.6010 - val_acc: 0.7241\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6069 - acc: 0.7105 - val_loss: 0.5977 - val_acc: 0.7126\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6016 - acc: 0.7074 - val_loss: 0.5947 - val_acc: 0.6962\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5987 - acc: 0.7070 - val_loss: 0.5915 - val_acc: 0.6962\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5931 - acc: 0.7041 - val_loss: 0.5877 - val_acc: 0.7011\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5877 - acc: 0.7044 - val_loss: 0.5832 - val_acc: 0.7011\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5856 - acc: 0.7064 - val_loss: 0.5782 - val_acc: 0.7028\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5803 - acc: 0.7035 - val_loss: 0.5727 - val_acc: 0.7044\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5752 - acc: 0.7137 - val_loss: 0.5673 - val_acc: 0.7110\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5674 - acc: 0.7223 - val_loss: 0.5620 - val_acc: 0.7176\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5612 - acc: 0.7298 - val_loss: 0.5570 - val_acc: 0.7159\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5586 - acc: 0.7347 - val_loss: 0.5521 - val_acc: 0.7225\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5542 - acc: 0.7329 - val_loss: 0.5473 - val_acc: 0.7192\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5520 - acc: 0.7347 - val_loss: 0.5425 - val_acc: 0.7241\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5472 - acc: 0.7400 - val_loss: 0.5376 - val_acc: 0.7356\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5409 - acc: 0.7473 - val_loss: 0.5327 - val_acc: 0.7356\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5392 - acc: 0.7497 - val_loss: 0.5279 - val_acc: 0.7406\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5312 - acc: 0.7541 - val_loss: 0.5233 - val_acc: 0.7373\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5275 - acc: 0.7552 - val_loss: 0.5185 - val_acc: 0.7438\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5222 - acc: 0.7575 - val_loss: 0.5131 - val_acc: 0.7438\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5162 - acc: 0.7641 - val_loss: 0.5071 - val_acc: 0.7438\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5141 - acc: 0.7681 - val_loss: 0.5011 - val_acc: 0.7570\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5091 - acc: 0.7690 - val_loss: 0.4962 - val_acc: 0.7553\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5049 - acc: 0.7732 - val_loss: 0.4926 - val_acc: 0.7619\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5026 - acc: 0.7749 - val_loss: 0.4904 - val_acc: 0.7603\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4993 - acc: 0.7727 - val_loss: 0.4881 - val_acc: 0.7603\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4924 - acc: 0.7710 - val_loss: 0.4838 - val_acc: 0.7652\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4924 - acc: 0.7736 - val_loss: 0.4777 - val_acc: 0.7734\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4895 - acc: 0.7834 - val_loss: 0.4730 - val_acc: 0.7750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4860 - acc: 0.7842 - val_loss: 0.4704 - val_acc: 0.7783\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4788 - acc: 0.7880 - val_loss: 0.4694 - val_acc: 0.7750\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4780 - acc: 0.7882 - val_loss: 0.4678 - val_acc: 0.7783\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4732 - acc: 0.7882 - val_loss: 0.4648 - val_acc: 0.7767\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4736 - acc: 0.7849 - val_loss: 0.4617 - val_acc: 0.7800\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4720 - acc: 0.7900 - val_loss: 0.4591 - val_acc: 0.7816\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4672 - acc: 0.7937 - val_loss: 0.4570 - val_acc: 0.7865\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4654 - acc: 0.7904 - val_loss: 0.4555 - val_acc: 0.7882\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4626 - acc: 0.7957 - val_loss: 0.4542 - val_acc: 0.7898\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4596 - acc: 0.7988 - val_loss: 0.4509 - val_acc: 0.7964\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4557 - acc: 0.7978 - val_loss: 0.4470 - val_acc: 0.7980\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4555 - acc: 0.8013 - val_loss: 0.4443 - val_acc: 0.8030\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4525 - acc: 0.8017 - val_loss: 0.4441 - val_acc: 0.8030\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4540 - acc: 0.8022 - val_loss: 0.4451 - val_acc: 0.7997\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4495 - acc: 0.8002 - val_loss: 0.4446 - val_acc: 0.8030\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4493 - acc: 0.8002 - val_loss: 0.4412 - val_acc: 0.7997\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4478 - acc: 0.8037 - val_loss: 0.4372 - val_acc: 0.8095\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4478 - acc: 0.8022 - val_loss: 0.4352 - val_acc: 0.8062\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4486 - acc: 0.8050 - val_loss: 0.4353 - val_acc: 0.8079\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4422 - acc: 0.8064 - val_loss: 0.4370 - val_acc: 0.8013\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4421 - acc: 0.8073 - val_loss: 0.4359 - val_acc: 0.8046\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4417 - acc: 0.8062 - val_loss: 0.4332 - val_acc: 0.8144\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4372 - acc: 0.8106 - val_loss: 0.4314 - val_acc: 0.8161\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4410 - acc: 0.8053 - val_loss: 0.4316 - val_acc: 0.8112\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4333 - acc: 0.8121 - val_loss: 0.4339 - val_acc: 0.8030\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4352 - acc: 0.8048 - val_loss: 0.4348 - val_acc: 0.8030\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4364 - acc: 0.8106 - val_loss: 0.4300 - val_acc: 0.8046\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4320 - acc: 0.8081 - val_loss: 0.4261 - val_acc: 0.8128\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4313 - acc: 0.8123 - val_loss: 0.4254 - val_acc: 0.8128\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4297 - acc: 0.8123 - val_loss: 0.4260 - val_acc: 0.8112\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4309 - acc: 0.8112 - val_loss: 0.4258 - val_acc: 0.8079\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4288 - acc: 0.8134 - val_loss: 0.4240 - val_acc: 0.8095\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4256 - acc: 0.8119 - val_loss: 0.4218 - val_acc: 0.8128\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4268 - acc: 0.8126 - val_loss: 0.4208 - val_acc: 0.8144\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4278 - acc: 0.8152 - val_loss: 0.4227 - val_acc: 0.8062\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4234 - acc: 0.8146 - val_loss: 0.4255 - val_acc: 0.8079\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4235 - acc: 0.8135 - val_loss: 0.4227 - val_acc: 0.8112\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4234 - acc: 0.8161 - val_loss: 0.4191 - val_acc: 0.8144\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4232 - acc: 0.8154 - val_loss: 0.4184 - val_acc: 0.8144\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4191 - acc: 0.8181 - val_loss: 0.4191 - val_acc: 0.8112\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4187 - acc: 0.8168 - val_loss: 0.4177 - val_acc: 0.8095\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4167 - acc: 0.8201 - val_loss: 0.4153 - val_acc: 0.8194\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4182 - acc: 0.8183 - val_loss: 0.4148 - val_acc: 0.8177\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4176 - acc: 0.8176 - val_loss: 0.4171 - val_acc: 0.8112\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4142 - acc: 0.8228 - val_loss: 0.4180 - val_acc: 0.8128\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.4155 - acc: 0.8186 - val_loss: 0.4159 - val_acc: 0.8144\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4152 - acc: 0.8194 - val_loss: 0.4159 - val_acc: 0.8144\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4116 - acc: 0.8228 - val_loss: 0.4173 - val_acc: 0.8161\n",
      "Training with parameters {'batch_size': 4500, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_59 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_166 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_108 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_167 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.6732 - acc: 0.5878 - val_loss: 0.6650 - val_acc: 0.5862\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.6602 - acc: 0.6273 - val_loss: 0.6509 - val_acc: 0.6273\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6484 - acc: 0.6479 - val_loss: 0.6402 - val_acc: 0.6502\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6430 - acc: 0.6437 - val_loss: 0.6318 - val_acc: 0.6634\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6364 - acc: 0.6521 - val_loss: 0.6253 - val_acc: 0.6700\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6312 - acc: 0.6564 - val_loss: 0.6200 - val_acc: 0.6814\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6244 - acc: 0.6709 - val_loss: 0.6155 - val_acc: 0.6798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6185 - acc: 0.6825 - val_loss: 0.6116 - val_acc: 0.6716\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6131 - acc: 0.6929 - val_loss: 0.6083 - val_acc: 0.6831\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6078 - acc: 0.6953 - val_loss: 0.6054 - val_acc: 0.6765\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6015 - acc: 0.7011 - val_loss: 0.6025 - val_acc: 0.6765\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5965 - acc: 0.7033 - val_loss: 0.5995 - val_acc: 0.6732\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5933 - acc: 0.7068 - val_loss: 0.5961 - val_acc: 0.6814\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5876 - acc: 0.7105 - val_loss: 0.5921 - val_acc: 0.6831\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.5843 - acc: 0.7094 - val_loss: 0.5877 - val_acc: 0.6831\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5804 - acc: 0.7132 - val_loss: 0.5831 - val_acc: 0.6880\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5760 - acc: 0.7219 - val_loss: 0.5784 - val_acc: 0.6979\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5718 - acc: 0.7261 - val_loss: 0.5742 - val_acc: 0.7110\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5684 - acc: 0.7298 - val_loss: 0.5705 - val_acc: 0.7110\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5646 - acc: 0.7302 - val_loss: 0.5670 - val_acc: 0.7143\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5621 - acc: 0.7369 - val_loss: 0.5636 - val_acc: 0.7143\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5578 - acc: 0.7373 - val_loss: 0.5603 - val_acc: 0.7159\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5535 - acc: 0.7418 - val_loss: 0.5569 - val_acc: 0.7192\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5507 - acc: 0.7446 - val_loss: 0.5538 - val_acc: 0.7225\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5477 - acc: 0.7475 - val_loss: 0.5511 - val_acc: 0.7274\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5453 - acc: 0.7484 - val_loss: 0.5479 - val_acc: 0.7307\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5420 - acc: 0.7515 - val_loss: 0.5445 - val_acc: 0.7340\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5374 - acc: 0.7583 - val_loss: 0.5412 - val_acc: 0.7356\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5366 - acc: 0.7562 - val_loss: 0.5379 - val_acc: 0.7373\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5336 - acc: 0.7583 - val_loss: 0.5350 - val_acc: 0.7422\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5306 - acc: 0.7610 - val_loss: 0.5323 - val_acc: 0.7455\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5275 - acc: 0.7625 - val_loss: 0.5295 - val_acc: 0.7504\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5263 - acc: 0.7661 - val_loss: 0.5268 - val_acc: 0.7504\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5227 - acc: 0.7670 - val_loss: 0.5245 - val_acc: 0.7521\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5199 - acc: 0.7676 - val_loss: 0.5229 - val_acc: 0.7471\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5192 - acc: 0.7659 - val_loss: 0.5213 - val_acc: 0.7521\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5156 - acc: 0.7679 - val_loss: 0.5190 - val_acc: 0.7521\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5140 - acc: 0.7701 - val_loss: 0.5166 - val_acc: 0.7521\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5109 - acc: 0.7701 - val_loss: 0.5138 - val_acc: 0.7586\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5083 - acc: 0.7745 - val_loss: 0.5107 - val_acc: 0.7586\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5060 - acc: 0.7783 - val_loss: 0.5076 - val_acc: 0.7652\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5048 - acc: 0.7778 - val_loss: 0.5050 - val_acc: 0.7635\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5019 - acc: 0.7778 - val_loss: 0.5032 - val_acc: 0.7652\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4998 - acc: 0.7818 - val_loss: 0.5021 - val_acc: 0.7635\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4985 - acc: 0.7778 - val_loss: 0.5010 - val_acc: 0.7668\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4955 - acc: 0.7864 - val_loss: 0.4992 - val_acc: 0.7668\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4949 - acc: 0.7829 - val_loss: 0.4971 - val_acc: 0.7668\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4920 - acc: 0.7842 - val_loss: 0.4955 - val_acc: 0.7685\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4912 - acc: 0.7823 - val_loss: 0.4943 - val_acc: 0.7685\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4892 - acc: 0.7849 - val_loss: 0.4929 - val_acc: 0.7701\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4875 - acc: 0.7822 - val_loss: 0.4911 - val_acc: 0.7701\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4856 - acc: 0.7853 - val_loss: 0.4887 - val_acc: 0.7734\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4862 - acc: 0.7864 - val_loss: 0.4865 - val_acc: 0.7734\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4835 - acc: 0.7884 - val_loss: 0.4850 - val_acc: 0.7750\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4822 - acc: 0.7891 - val_loss: 0.4838 - val_acc: 0.7750\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4803 - acc: 0.7909 - val_loss: 0.4830 - val_acc: 0.7750\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4799 - acc: 0.7905 - val_loss: 0.4829 - val_acc: 0.7750\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4790 - acc: 0.7944 - val_loss: 0.4827 - val_acc: 0.7783\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4785 - acc: 0.7887 - val_loss: 0.4822 - val_acc: 0.7783\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4749 - acc: 0.7900 - val_loss: 0.4802 - val_acc: 0.7783\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4737 - acc: 0.7927 - val_loss: 0.4771 - val_acc: 0.7833\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4721 - acc: 0.7913 - val_loss: 0.4747 - val_acc: 0.7833\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4727 - acc: 0.7924 - val_loss: 0.4731 - val_acc: 0.7816\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4708 - acc: 0.7909 - val_loss: 0.4718 - val_acc: 0.7833\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4695 - acc: 0.7940 - val_loss: 0.4708 - val_acc: 0.7865\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4683 - acc: 0.7962 - val_loss: 0.4702 - val_acc: 0.7898\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4677 - acc: 0.7968 - val_loss: 0.4699 - val_acc: 0.7915\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4662 - acc: 0.7971 - val_loss: 0.4694 - val_acc: 0.7964\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4642 - acc: 0.7984 - val_loss: 0.4686 - val_acc: 0.7964\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4645 - acc: 0.7968 - val_loss: 0.4674 - val_acc: 0.7980\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4660 - acc: 0.7968 - val_loss: 0.4663 - val_acc: 0.7980\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4624 - acc: 0.7957 - val_loss: 0.4651 - val_acc: 0.7980\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4626 - acc: 0.7995 - val_loss: 0.4641 - val_acc: 0.7980\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4625 - acc: 0.7962 - val_loss: 0.4633 - val_acc: 0.7980\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4584 - acc: 0.8044 - val_loss: 0.4622 - val_acc: 0.7931\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4575 - acc: 0.8028 - val_loss: 0.4612 - val_acc: 0.7931\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4591 - acc: 0.8041 - val_loss: 0.4606 - val_acc: 0.7931\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4580 - acc: 0.8017 - val_loss: 0.4601 - val_acc: 0.7964\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4565 - acc: 0.8051 - val_loss: 0.4592 - val_acc: 0.7947\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4574 - acc: 0.8046 - val_loss: 0.4579 - val_acc: 0.7964\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4553 - acc: 0.8015 - val_loss: 0.4566 - val_acc: 0.7964\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4545 - acc: 0.8050 - val_loss: 0.4551 - val_acc: 0.7980\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4530 - acc: 0.8033 - val_loss: 0.4545 - val_acc: 0.7997\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4532 - acc: 0.8057 - val_loss: 0.4546 - val_acc: 0.7980\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4508 - acc: 0.8041 - val_loss: 0.4544 - val_acc: 0.7997\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4516 - acc: 0.8044 - val_loss: 0.4535 - val_acc: 0.7997\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4500 - acc: 0.8079 - val_loss: 0.4527 - val_acc: 0.8013\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4490 - acc: 0.8064 - val_loss: 0.4523 - val_acc: 0.7997\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4481 - acc: 0.8051 - val_loss: 0.4517 - val_acc: 0.7997\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4470 - acc: 0.8072 - val_loss: 0.4506 - val_acc: 0.8013\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4486 - acc: 0.8028 - val_loss: 0.4494 - val_acc: 0.8013\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4474 - acc: 0.8086 - val_loss: 0.4485 - val_acc: 0.8013\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4462 - acc: 0.8070 - val_loss: 0.4482 - val_acc: 0.8013\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4447 - acc: 0.8073 - val_loss: 0.4484 - val_acc: 0.8030\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4453 - acc: 0.8081 - val_loss: 0.4489 - val_acc: 0.8013\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4442 - acc: 0.8084 - val_loss: 0.4484 - val_acc: 0.8030\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4433 - acc: 0.8090 - val_loss: 0.4469 - val_acc: 0.8030\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4432 - acc: 0.8093 - val_loss: 0.4454 - val_acc: 0.8046\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4419 - acc: 0.8093 - val_loss: 0.4442 - val_acc: 0.8062\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4420 - acc: 0.8061 - val_loss: 0.4439 - val_acc: 0.8062\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4402 - acc: 0.8053 - val_loss: 0.4439 - val_acc: 0.8046\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4397 - acc: 0.8124 - val_loss: 0.4438 - val_acc: 0.8046\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4392 - acc: 0.8099 - val_loss: 0.4433 - val_acc: 0.8046\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4377 - acc: 0.8086 - val_loss: 0.4425 - val_acc: 0.8062\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4392 - acc: 0.8095 - val_loss: 0.4413 - val_acc: 0.8062\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4369 - acc: 0.8115 - val_loss: 0.4400 - val_acc: 0.8079\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4355 - acc: 0.8119 - val_loss: 0.4389 - val_acc: 0.8079\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4352 - acc: 0.8139 - val_loss: 0.4383 - val_acc: 0.8079\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4352 - acc: 0.8097 - val_loss: 0.4388 - val_acc: 0.8095\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4357 - acc: 0.8121 - val_loss: 0.4394 - val_acc: 0.8079\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4320 - acc: 0.8143 - val_loss: 0.4396 - val_acc: 0.8095\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4327 - acc: 0.8110 - val_loss: 0.4389 - val_acc: 0.8079\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4319 - acc: 0.8137 - val_loss: 0.4371 - val_acc: 0.8095\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4332 - acc: 0.8110 - val_loss: 0.4361 - val_acc: 0.8079\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4355 - acc: 0.8166 - val_loss: 0.4366 - val_acc: 0.8062\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4325 - acc: 0.8124 - val_loss: 0.4376 - val_acc: 0.8095\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4317 - acc: 0.8144 - val_loss: 0.4385 - val_acc: 0.8095\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4312 - acc: 0.8117 - val_loss: 0.4378 - val_acc: 0.8112\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4305 - acc: 0.8117 - val_loss: 0.4359 - val_acc: 0.8079\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4309 - acc: 0.8150 - val_loss: 0.4342 - val_acc: 0.8079\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4316 - acc: 0.8146 - val_loss: 0.4332 - val_acc: 0.8095\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4309 - acc: 0.8152 - val_loss: 0.4332 - val_acc: 0.8079\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4299 - acc: 0.8134 - val_loss: 0.4340 - val_acc: 0.8079\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4288 - acc: 0.8163 - val_loss: 0.4341 - val_acc: 0.8112\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4266 - acc: 0.8166 - val_loss: 0.4331 - val_acc: 0.8079\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4275 - acc: 0.8144 - val_loss: 0.4326 - val_acc: 0.8079\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4278 - acc: 0.8134 - val_loss: 0.4325 - val_acc: 0.8079\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4276 - acc: 0.8188 - val_loss: 0.4315 - val_acc: 0.8112\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4265 - acc: 0.8146 - val_loss: 0.4307 - val_acc: 0.8144\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4238 - acc: 0.8170 - val_loss: 0.4310 - val_acc: 0.8128\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4261 - acc: 0.8146 - val_loss: 0.4328 - val_acc: 0.8095\n",
      "Epoch 132/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4249 - acc: 0.8168 - val_loss: 0.4351 - val_acc: 0.8112\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4254 - acc: 0.8165 - val_loss: 0.4351 - val_acc: 0.8112\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4256 - acc: 0.8163 - val_loss: 0.4322 - val_acc: 0.8095\n",
      "Training with parameters {'batch_size': 4500, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_60 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_168 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_109 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_169 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_110 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_170 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.8566 - acc: 0.4351 - val_loss: 0.8016 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.7769 - acc: 0.4348 - val_loss: 0.7345 - val_acc: 0.3908\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7256 - acc: 0.4559 - val_loss: 0.6917 - val_acc: 0.5271\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6974 - acc: 0.5028 - val_loss: 0.6668 - val_acc: 0.6108\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6801 - acc: 0.5654 - val_loss: 0.6523 - val_acc: 0.6125\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6723 - acc: 0.5681 - val_loss: 0.6423 - val_acc: 0.6141\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6625 - acc: 0.5720 - val_loss: 0.6333 - val_acc: 0.6158\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6550 - acc: 0.5767 - val_loss: 0.6244 - val_acc: 0.6207\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6428 - acc: 0.5913 - val_loss: 0.6162 - val_acc: 0.6519\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6320 - acc: 0.6172 - val_loss: 0.6097 - val_acc: 0.6946\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6184 - acc: 0.6597 - val_loss: 0.6054 - val_acc: 0.6979\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6078 - acc: 0.7024 - val_loss: 0.6029 - val_acc: 0.6798\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6026 - acc: 0.7043 - val_loss: 0.6011 - val_acc: 0.6683\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5967 - acc: 0.6991 - val_loss: 0.5986 - val_acc: 0.6716\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5880 - acc: 0.6973 - val_loss: 0.5943 - val_acc: 0.6667\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5850 - acc: 0.6986 - val_loss: 0.5879 - val_acc: 0.6716\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5775 - acc: 0.7017 - val_loss: 0.5799 - val_acc: 0.6765\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5706 - acc: 0.7168 - val_loss: 0.5710 - val_acc: 0.6864\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5629 - acc: 0.7254 - val_loss: 0.5622 - val_acc: 0.6979\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5588 - acc: 0.7302 - val_loss: 0.5542 - val_acc: 0.7143\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5523 - acc: 0.7440 - val_loss: 0.5477 - val_acc: 0.7143\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5451 - acc: 0.7458 - val_loss: 0.5421 - val_acc: 0.7176\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5403 - acc: 0.7427 - val_loss: 0.5374 - val_acc: 0.7192\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5319 - acc: 0.7511 - val_loss: 0.5327 - val_acc: 0.7143\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5288 - acc: 0.7544 - val_loss: 0.5266 - val_acc: 0.7241\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5220 - acc: 0.7581 - val_loss: 0.5182 - val_acc: 0.7406\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5155 - acc: 0.7628 - val_loss: 0.5095 - val_acc: 0.7570\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5103 - acc: 0.7712 - val_loss: 0.5031 - val_acc: 0.7668\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5080 - acc: 0.7741 - val_loss: 0.4993 - val_acc: 0.7668\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5025 - acc: 0.7714 - val_loss: 0.4971 - val_acc: 0.7652\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4979 - acc: 0.7770 - val_loss: 0.4946 - val_acc: 0.7635\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4918 - acc: 0.7831 - val_loss: 0.4910 - val_acc: 0.7635\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4914 - acc: 0.7800 - val_loss: 0.4844 - val_acc: 0.7685\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4869 - acc: 0.7803 - val_loss: 0.4777 - val_acc: 0.7767\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4823 - acc: 0.7864 - val_loss: 0.4736 - val_acc: 0.7750\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4787 - acc: 0.7916 - val_loss: 0.4724 - val_acc: 0.7783\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4723 - acc: 0.7933 - val_loss: 0.4743 - val_acc: 0.7783\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4725 - acc: 0.7896 - val_loss: 0.4729 - val_acc: 0.7833\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4694 - acc: 0.7895 - val_loss: 0.4664 - val_acc: 0.7865\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4666 - acc: 0.7920 - val_loss: 0.4605 - val_acc: 0.7882\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4640 - acc: 0.7986 - val_loss: 0.4572 - val_acc: 0.7964\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4585 - acc: 0.7973 - val_loss: 0.4553 - val_acc: 0.7980\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4559 - acc: 0.8033 - val_loss: 0.4530 - val_acc: 0.8013\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4564 - acc: 0.7991 - val_loss: 0.4507 - val_acc: 0.7997\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4491 - acc: 0.8066 - val_loss: 0.4498 - val_acc: 0.7964\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4482 - acc: 0.8048 - val_loss: 0.4491 - val_acc: 0.7964\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4492 - acc: 0.8055 - val_loss: 0.4451 - val_acc: 0.7964\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4464 - acc: 0.8079 - val_loss: 0.4404 - val_acc: 0.8030\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4433 - acc: 0.8134 - val_loss: 0.4378 - val_acc: 0.8095\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4414 - acc: 0.8112 - val_loss: 0.4383 - val_acc: 0.8062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4382 - acc: 0.8155 - val_loss: 0.4399 - val_acc: 0.8013\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4370 - acc: 0.8082 - val_loss: 0.4370 - val_acc: 0.8030\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4349 - acc: 0.8154 - val_loss: 0.4325 - val_acc: 0.8046\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4348 - acc: 0.8113 - val_loss: 0.4310 - val_acc: 0.8046\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4332 - acc: 0.8130 - val_loss: 0.4339 - val_acc: 0.8013\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4347 - acc: 0.8115 - val_loss: 0.4347 - val_acc: 0.8013\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4298 - acc: 0.8144 - val_loss: 0.4298 - val_acc: 0.8062\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4300 - acc: 0.8141 - val_loss: 0.4256 - val_acc: 0.8095\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4300 - acc: 0.8163 - val_loss: 0.4251 - val_acc: 0.8095\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4263 - acc: 0.8172 - val_loss: 0.4289 - val_acc: 0.8062\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4224 - acc: 0.8177 - val_loss: 0.4313 - val_acc: 0.8046\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4221 - acc: 0.8190 - val_loss: 0.4285 - val_acc: 0.8062\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4217 - acc: 0.8188 - val_loss: 0.4223 - val_acc: 0.8112\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4186 - acc: 0.8208 - val_loss: 0.4194 - val_acc: 0.8144\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4197 - acc: 0.8223 - val_loss: 0.4215 - val_acc: 0.8062\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4163 - acc: 0.8221 - val_loss: 0.4254 - val_acc: 0.8112\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4181 - acc: 0.8210 - val_loss: 0.4243 - val_acc: 0.8112\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4155 - acc: 0.8203 - val_loss: 0.4199 - val_acc: 0.8062\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4142 - acc: 0.8179 - val_loss: 0.4184 - val_acc: 0.8095\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4153 - acc: 0.8212 - val_loss: 0.4200 - val_acc: 0.8095\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4135 - acc: 0.8223 - val_loss: 0.4211 - val_acc: 0.8128\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4129 - acc: 0.8223 - val_loss: 0.4183 - val_acc: 0.8112\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4101 - acc: 0.8239 - val_loss: 0.4160 - val_acc: 0.8128\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4083 - acc: 0.8294 - val_loss: 0.4171 - val_acc: 0.8095\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4074 - acc: 0.8241 - val_loss: 0.4206 - val_acc: 0.8144\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4080 - acc: 0.8247 - val_loss: 0.4214 - val_acc: 0.8144\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4083 - acc: 0.8234 - val_loss: 0.4175 - val_acc: 0.8128\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4060 - acc: 0.8272 - val_loss: 0.4143 - val_acc: 0.8128\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4055 - acc: 0.8272 - val_loss: 0.4147 - val_acc: 0.8128\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4025 - acc: 0.8267 - val_loss: 0.4178 - val_acc: 0.8144\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4033 - acc: 0.8245 - val_loss: 0.4195 - val_acc: 0.8144\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4017 - acc: 0.8278 - val_loss: 0.4174 - val_acc: 0.8161\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3996 - acc: 0.8280 - val_loss: 0.4143 - val_acc: 0.8177\n",
      "Training with parameters {'batch_size': 4500, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_60\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_61 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_171 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_111 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_172 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_112 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_173 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.8797 - acc: 0.4351 - val_loss: 0.8561 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.8253 - acc: 0.4351 - val_loss: 0.8028 - val_acc: 0.3875\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7812 - acc: 0.4371 - val_loss: 0.7618 - val_acc: 0.3875\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7476 - acc: 0.4364 - val_loss: 0.7297 - val_acc: 0.3924\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7210 - acc: 0.4536 - val_loss: 0.7050 - val_acc: 0.4368\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.7021 - acc: 0.4890 - val_loss: 0.6863 - val_acc: 0.5534\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6867 - acc: 0.5395 - val_loss: 0.6722 - val_acc: 0.6289\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6757 - acc: 0.5962 - val_loss: 0.6610 - val_acc: 0.6880\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6683 - acc: 0.6119 - val_loss: 0.6520 - val_acc: 0.6864\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6609 - acc: 0.6263 - val_loss: 0.6444 - val_acc: 0.6798\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6536 - acc: 0.6404 - val_loss: 0.6377 - val_acc: 0.6847\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6508 - acc: 0.6398 - val_loss: 0.6315 - val_acc: 0.7110\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6420 - acc: 0.6543 - val_loss: 0.6261 - val_acc: 0.7323\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6362 - acc: 0.6762 - val_loss: 0.6215 - val_acc: 0.7422\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6298 - acc: 0.6895 - val_loss: 0.6174 - val_acc: 0.7258\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6233 - acc: 0.7017 - val_loss: 0.6138 - val_acc: 0.7126\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6203 - acc: 0.7028 - val_loss: 0.6106 - val_acc: 0.7044\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6136 - acc: 0.7112 - val_loss: 0.6075 - val_acc: 0.6979\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6094 - acc: 0.7117 - val_loss: 0.6042 - val_acc: 0.6979\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6021 - acc: 0.7115 - val_loss: 0.6008 - val_acc: 0.6979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5996 - acc: 0.7077 - val_loss: 0.5970 - val_acc: 0.6979\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5960 - acc: 0.7121 - val_loss: 0.5931 - val_acc: 0.6995\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5901 - acc: 0.7165 - val_loss: 0.5891 - val_acc: 0.6995\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5891 - acc: 0.7139 - val_loss: 0.5851 - val_acc: 0.7011\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5837 - acc: 0.7174 - val_loss: 0.5809 - val_acc: 0.7061\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5817 - acc: 0.7203 - val_loss: 0.5764 - val_acc: 0.7044\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5761 - acc: 0.7298 - val_loss: 0.5721 - val_acc: 0.7044\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5725 - acc: 0.7276 - val_loss: 0.5681 - val_acc: 0.7044\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5666 - acc: 0.7333 - val_loss: 0.5644 - val_acc: 0.7044\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5654 - acc: 0.7327 - val_loss: 0.5609 - val_acc: 0.7044\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5613 - acc: 0.7351 - val_loss: 0.5576 - val_acc: 0.7044\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5543 - acc: 0.7420 - val_loss: 0.5539 - val_acc: 0.7126\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5528 - acc: 0.7393 - val_loss: 0.5501 - val_acc: 0.7126\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5488 - acc: 0.7466 - val_loss: 0.5462 - val_acc: 0.7110\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5439 - acc: 0.7499 - val_loss: 0.5423 - val_acc: 0.7126\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5389 - acc: 0.7541 - val_loss: 0.5383 - val_acc: 0.7209\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5347 - acc: 0.7583 - val_loss: 0.5345 - val_acc: 0.7291\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5363 - acc: 0.7528 - val_loss: 0.5311 - val_acc: 0.7356\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5312 - acc: 0.7557 - val_loss: 0.5277 - val_acc: 0.7389\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5290 - acc: 0.7606 - val_loss: 0.5240 - val_acc: 0.7422\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5292 - acc: 0.7590 - val_loss: 0.5203 - val_acc: 0.7455\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5237 - acc: 0.7630 - val_loss: 0.5169 - val_acc: 0.7455\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5233 - acc: 0.7610 - val_loss: 0.5137 - val_acc: 0.7455\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5169 - acc: 0.7687 - val_loss: 0.5109 - val_acc: 0.7438\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5134 - acc: 0.7708 - val_loss: 0.5086 - val_acc: 0.7471\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5113 - acc: 0.7683 - val_loss: 0.5064 - val_acc: 0.7471\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5073 - acc: 0.7739 - val_loss: 0.5043 - val_acc: 0.7488\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5080 - acc: 0.7652 - val_loss: 0.5014 - val_acc: 0.7504\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5031 - acc: 0.7690 - val_loss: 0.4976 - val_acc: 0.7537\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5028 - acc: 0.7716 - val_loss: 0.4935 - val_acc: 0.7570\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4955 - acc: 0.7760 - val_loss: 0.4892 - val_acc: 0.7685\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4979 - acc: 0.7758 - val_loss: 0.4855 - val_acc: 0.7718\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4937 - acc: 0.7807 - val_loss: 0.4826 - val_acc: 0.7734\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4882 - acc: 0.7818 - val_loss: 0.4811 - val_acc: 0.7701\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4872 - acc: 0.7843 - val_loss: 0.4808 - val_acc: 0.7685\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4867 - acc: 0.7838 - val_loss: 0.4805 - val_acc: 0.7685\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4808 - acc: 0.7840 - val_loss: 0.4794 - val_acc: 0.7652\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4790 - acc: 0.7880 - val_loss: 0.4768 - val_acc: 0.7701\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4793 - acc: 0.7851 - val_loss: 0.4735 - val_acc: 0.7734\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4777 - acc: 0.7895 - val_loss: 0.4706 - val_acc: 0.7767\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4731 - acc: 0.7902 - val_loss: 0.4683 - val_acc: 0.7800\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4764 - acc: 0.7909 - val_loss: 0.4667 - val_acc: 0.7833\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4708 - acc: 0.7907 - val_loss: 0.4654 - val_acc: 0.7833\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4663 - acc: 0.7929 - val_loss: 0.4641 - val_acc: 0.7849\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4672 - acc: 0.7922 - val_loss: 0.4622 - val_acc: 0.7849\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4643 - acc: 0.7960 - val_loss: 0.4611 - val_acc: 0.7849\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4654 - acc: 0.7955 - val_loss: 0.4597 - val_acc: 0.7833\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4676 - acc: 0.7909 - val_loss: 0.4580 - val_acc: 0.7865\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4603 - acc: 0.7957 - val_loss: 0.4568 - val_acc: 0.7882\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4583 - acc: 0.7975 - val_loss: 0.4553 - val_acc: 0.7882\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4605 - acc: 0.7999 - val_loss: 0.4529 - val_acc: 0.7898\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4540 - acc: 0.8015 - val_loss: 0.4505 - val_acc: 0.7947\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4570 - acc: 0.7977 - val_loss: 0.4489 - val_acc: 0.7980\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4571 - acc: 0.8022 - val_loss: 0.4487 - val_acc: 0.7980\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4521 - acc: 0.8033 - val_loss: 0.4491 - val_acc: 0.7931\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4530 - acc: 0.8042 - val_loss: 0.4491 - val_acc: 0.7931\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4518 - acc: 0.8041 - val_loss: 0.4470 - val_acc: 0.7964\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4489 - acc: 0.8008 - val_loss: 0.4439 - val_acc: 0.7964\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4465 - acc: 0.8059 - val_loss: 0.4418 - val_acc: 0.7997\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4486 - acc: 0.8055 - val_loss: 0.4416 - val_acc: 0.8013\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4455 - acc: 0.8039 - val_loss: 0.4436 - val_acc: 0.7997\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4481 - acc: 0.8009 - val_loss: 0.4444 - val_acc: 0.7947\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4436 - acc: 0.8059 - val_loss: 0.4417 - val_acc: 0.8013\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4425 - acc: 0.8072 - val_loss: 0.4376 - val_acc: 0.8030\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4428 - acc: 0.8055 - val_loss: 0.4352 - val_acc: 0.8079\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4429 - acc: 0.8051 - val_loss: 0.4343 - val_acc: 0.8079\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4406 - acc: 0.8093 - val_loss: 0.4352 - val_acc: 0.8046\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4380 - acc: 0.8132 - val_loss: 0.4372 - val_acc: 0.7997\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4358 - acc: 0.8101 - val_loss: 0.4387 - val_acc: 0.7997\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4389 - acc: 0.8090 - val_loss: 0.4376 - val_acc: 0.7997\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4381 - acc: 0.8097 - val_loss: 0.4335 - val_acc: 0.8030\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4371 - acc: 0.8079 - val_loss: 0.4301 - val_acc: 0.8095\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4306 - acc: 0.8123 - val_loss: 0.4287 - val_acc: 0.8112\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4348 - acc: 0.8106 - val_loss: 0.4290 - val_acc: 0.8079\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4327 - acc: 0.8126 - val_loss: 0.4310 - val_acc: 0.8046\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4313 - acc: 0.8117 - val_loss: 0.4324 - val_acc: 0.8030\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4285 - acc: 0.8097 - val_loss: 0.4309 - val_acc: 0.8046\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4280 - acc: 0.8168 - val_loss: 0.4286 - val_acc: 0.8046\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4306 - acc: 0.8101 - val_loss: 0.4272 - val_acc: 0.8095\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4299 - acc: 0.8130 - val_loss: 0.4270 - val_acc: 0.8095\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4287 - acc: 0.8155 - val_loss: 0.4276 - val_acc: 0.8046\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4237 - acc: 0.8185 - val_loss: 0.4284 - val_acc: 0.8046\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4271 - acc: 0.8148 - val_loss: 0.4287 - val_acc: 0.8046\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4279 - acc: 0.8128 - val_loss: 0.4275 - val_acc: 0.8046\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4263 - acc: 0.8119 - val_loss: 0.4249 - val_acc: 0.8095\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4222 - acc: 0.8174 - val_loss: 0.4235 - val_acc: 0.8112\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4240 - acc: 0.8117 - val_loss: 0.4238 - val_acc: 0.8095\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4220 - acc: 0.8190 - val_loss: 0.4251 - val_acc: 0.8079\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4195 - acc: 0.8170 - val_loss: 0.4253 - val_acc: 0.8112\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4216 - acc: 0.8155 - val_loss: 0.4241 - val_acc: 0.8095\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4197 - acc: 0.8161 - val_loss: 0.4227 - val_acc: 0.8112\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4171 - acc: 0.8170 - val_loss: 0.4222 - val_acc: 0.8112\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4182 - acc: 0.8197 - val_loss: 0.4222 - val_acc: 0.8128\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4174 - acc: 0.8203 - val_loss: 0.4224 - val_acc: 0.8144\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4170 - acc: 0.8183 - val_loss: 0.4221 - val_acc: 0.8161\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4130 - acc: 0.8239 - val_loss: 0.4213 - val_acc: 0.8144\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4185 - acc: 0.8177 - val_loss: 0.4202 - val_acc: 0.8144\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4160 - acc: 0.8225 - val_loss: 0.4198 - val_acc: 0.8161\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4129 - acc: 0.8208 - val_loss: 0.4205 - val_acc: 0.8161\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4160 - acc: 0.8199 - val_loss: 0.4210 - val_acc: 0.8177\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4103 - acc: 0.8241 - val_loss: 0.4206 - val_acc: 0.8177\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4132 - acc: 0.8241 - val_loss: 0.4203 - val_acc: 0.8177\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4105 - acc: 0.8208 - val_loss: 0.4200 - val_acc: 0.8177\n",
      "Training with parameters {'batch_size': 4500, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_61\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_62 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_174 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_175 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_114 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_176 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.7692 - acc: 0.4357 - val_loss: 0.7653 - val_acc: 0.3892\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7397 - acc: 0.4386 - val_loss: 0.7376 - val_acc: 0.3990\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.7201 - acc: 0.4499 - val_loss: 0.7146 - val_acc: 0.4319\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.7030 - acc: 0.4749 - val_loss: 0.6957 - val_acc: 0.4795\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6901 - acc: 0.5236 - val_loss: 0.6807 - val_acc: 0.5698\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6774 - acc: 0.5906 - val_loss: 0.6688 - val_acc: 0.6617\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6706 - acc: 0.6187 - val_loss: 0.6594 - val_acc: 0.6847\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6634 - acc: 0.6296 - val_loss: 0.6518 - val_acc: 0.6667\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6581 - acc: 0.6276 - val_loss: 0.6455 - val_acc: 0.6535\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6515 - acc: 0.6398 - val_loss: 0.6400 - val_acc: 0.6404\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6488 - acc: 0.6271 - val_loss: 0.6351 - val_acc: 0.6470\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6443 - acc: 0.6311 - val_loss: 0.6305 - val_acc: 0.6585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6392 - acc: 0.6408 - val_loss: 0.6262 - val_acc: 0.6716\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6365 - acc: 0.6492 - val_loss: 0.6222 - val_acc: 0.6962\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6306 - acc: 0.6627 - val_loss: 0.6184 - val_acc: 0.6946\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6260 - acc: 0.6787 - val_loss: 0.6150 - val_acc: 0.7011\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6205 - acc: 0.6931 - val_loss: 0.6119 - val_acc: 0.7143\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6168 - acc: 0.7033 - val_loss: 0.6089 - val_acc: 0.7110\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6103 - acc: 0.7081 - val_loss: 0.6061 - val_acc: 0.7011\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6062 - acc: 0.7123 - val_loss: 0.6034 - val_acc: 0.6995\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6024 - acc: 0.7081 - val_loss: 0.6007 - val_acc: 0.6913\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5975 - acc: 0.7103 - val_loss: 0.5977 - val_acc: 0.6897\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5944 - acc: 0.7168 - val_loss: 0.5943 - val_acc: 0.6929\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5898 - acc: 0.7126 - val_loss: 0.5906 - val_acc: 0.6979\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5842 - acc: 0.7172 - val_loss: 0.5867 - val_acc: 0.7011\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5812 - acc: 0.7201 - val_loss: 0.5826 - val_acc: 0.7044\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5762 - acc: 0.7247 - val_loss: 0.5786 - val_acc: 0.7061\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5742 - acc: 0.7236 - val_loss: 0.5745 - val_acc: 0.7143\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5708 - acc: 0.7305 - val_loss: 0.5706 - val_acc: 0.7110\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5669 - acc: 0.7356 - val_loss: 0.5667 - val_acc: 0.7192\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5604 - acc: 0.7371 - val_loss: 0.5625 - val_acc: 0.7225\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5604 - acc: 0.7409 - val_loss: 0.5584 - val_acc: 0.7241\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5538 - acc: 0.7449 - val_loss: 0.5546 - val_acc: 0.7241\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5494 - acc: 0.7510 - val_loss: 0.5509 - val_acc: 0.7274\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5460 - acc: 0.7479 - val_loss: 0.5471 - val_acc: 0.7291\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5403 - acc: 0.7510 - val_loss: 0.5436 - val_acc: 0.7291\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5387 - acc: 0.7484 - val_loss: 0.5401 - val_acc: 0.7340\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5355 - acc: 0.7541 - val_loss: 0.5362 - val_acc: 0.7323\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5290 - acc: 0.7575 - val_loss: 0.5323 - val_acc: 0.7406\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5272 - acc: 0.7599 - val_loss: 0.5283 - val_acc: 0.7389\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5260 - acc: 0.7608 - val_loss: 0.5238 - val_acc: 0.7438\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5183 - acc: 0.7608 - val_loss: 0.5192 - val_acc: 0.7504\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5174 - acc: 0.7634 - val_loss: 0.5150 - val_acc: 0.7488\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5114 - acc: 0.7688 - val_loss: 0.5119 - val_acc: 0.7504\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5094 - acc: 0.7712 - val_loss: 0.5099 - val_acc: 0.7504\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5071 - acc: 0.7692 - val_loss: 0.5081 - val_acc: 0.7488\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5029 - acc: 0.7752 - val_loss: 0.5051 - val_acc: 0.7521\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5006 - acc: 0.7725 - val_loss: 0.5013 - val_acc: 0.7570\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4975 - acc: 0.7736 - val_loss: 0.4977 - val_acc: 0.7635\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4952 - acc: 0.7761 - val_loss: 0.4948 - val_acc: 0.7619\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4931 - acc: 0.7805 - val_loss: 0.4926 - val_acc: 0.7635\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4900 - acc: 0.7778 - val_loss: 0.4905 - val_acc: 0.7668\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4869 - acc: 0.7805 - val_loss: 0.4875 - val_acc: 0.7685\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4859 - acc: 0.7838 - val_loss: 0.4845 - val_acc: 0.7750\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4837 - acc: 0.7853 - val_loss: 0.4819 - val_acc: 0.7800\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4838 - acc: 0.7847 - val_loss: 0.4796 - val_acc: 0.7800\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4824 - acc: 0.7849 - val_loss: 0.4789 - val_acc: 0.7767\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4783 - acc: 0.7871 - val_loss: 0.4795 - val_acc: 0.7718\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4781 - acc: 0.7853 - val_loss: 0.4796 - val_acc: 0.7800\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4744 - acc: 0.7891 - val_loss: 0.4775 - val_acc: 0.7800\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4714 - acc: 0.7849 - val_loss: 0.4740 - val_acc: 0.7849\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4692 - acc: 0.7893 - val_loss: 0.4704 - val_acc: 0.7947\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4679 - acc: 0.7951 - val_loss: 0.4674 - val_acc: 0.7931\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4696 - acc: 0.7891 - val_loss: 0.4655 - val_acc: 0.7947\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4667 - acc: 0.7968 - val_loss: 0.4651 - val_acc: 0.7947\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4631 - acc: 0.7969 - val_loss: 0.4651 - val_acc: 0.7931\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4631 - acc: 0.7955 - val_loss: 0.4647 - val_acc: 0.7947\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4617 - acc: 0.7922 - val_loss: 0.4636 - val_acc: 0.7947\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4617 - acc: 0.7986 - val_loss: 0.4616 - val_acc: 0.7947\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4574 - acc: 0.7988 - val_loss: 0.4585 - val_acc: 0.7931\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4560 - acc: 0.7973 - val_loss: 0.4567 - val_acc: 0.7931\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4573 - acc: 0.7957 - val_loss: 0.4552 - val_acc: 0.7931\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4537 - acc: 0.8035 - val_loss: 0.4541 - val_acc: 0.7931\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4525 - acc: 0.8004 - val_loss: 0.4536 - val_acc: 0.7931\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4489 - acc: 0.7999 - val_loss: 0.4526 - val_acc: 0.7947\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4490 - acc: 0.8046 - val_loss: 0.4504 - val_acc: 0.7947\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4502 - acc: 0.8015 - val_loss: 0.4486 - val_acc: 0.7964\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4484 - acc: 0.8028 - val_loss: 0.4483 - val_acc: 0.7997\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4458 - acc: 0.8037 - val_loss: 0.4487 - val_acc: 0.7980\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4457 - acc: 0.8039 - val_loss: 0.4479 - val_acc: 0.8013\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4460 - acc: 0.8048 - val_loss: 0.4463 - val_acc: 0.8030\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4436 - acc: 0.8086 - val_loss: 0.4443 - val_acc: 0.8030\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4423 - acc: 0.8082 - val_loss: 0.4431 - val_acc: 0.8030\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4402 - acc: 0.8077 - val_loss: 0.4414 - val_acc: 0.8046\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4395 - acc: 0.8075 - val_loss: 0.4402 - val_acc: 0.8046\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4386 - acc: 0.8075 - val_loss: 0.4406 - val_acc: 0.8030\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4390 - acc: 0.8112 - val_loss: 0.4404 - val_acc: 0.8046\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4349 - acc: 0.8121 - val_loss: 0.4394 - val_acc: 0.8046\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4325 - acc: 0.8119 - val_loss: 0.4389 - val_acc: 0.8046\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4376 - acc: 0.8068 - val_loss: 0.4385 - val_acc: 0.8046\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4344 - acc: 0.8106 - val_loss: 0.4378 - val_acc: 0.8062\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4333 - acc: 0.8095 - val_loss: 0.4359 - val_acc: 0.8079\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4353 - acc: 0.8081 - val_loss: 0.4340 - val_acc: 0.8112\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4309 - acc: 0.8112 - val_loss: 0.4333 - val_acc: 0.8112\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4311 - acc: 0.8123 - val_loss: 0.4355 - val_acc: 0.8079\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4288 - acc: 0.8132 - val_loss: 0.4373 - val_acc: 0.8046\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4306 - acc: 0.8135 - val_loss: 0.4360 - val_acc: 0.8062\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4273 - acc: 0.8154 - val_loss: 0.4332 - val_acc: 0.8112\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4275 - acc: 0.8139 - val_loss: 0.4309 - val_acc: 0.8112\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4270 - acc: 0.8143 - val_loss: 0.4307 - val_acc: 0.8112\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4254 - acc: 0.8139 - val_loss: 0.4326 - val_acc: 0.8095\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4267 - acc: 0.8144 - val_loss: 0.4360 - val_acc: 0.8013\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4260 - acc: 0.8155 - val_loss: 0.4373 - val_acc: 0.8013\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4265 - acc: 0.8101 - val_loss: 0.4338 - val_acc: 0.8030\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4254 - acc: 0.8172 - val_loss: 0.4296 - val_acc: 0.8128\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4248 - acc: 0.8117 - val_loss: 0.4278 - val_acc: 0.8112\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4265 - acc: 0.8128 - val_loss: 0.4283 - val_acc: 0.8112\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4223 - acc: 0.8119 - val_loss: 0.4303 - val_acc: 0.8062\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4203 - acc: 0.8183 - val_loss: 0.4314 - val_acc: 0.8046\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4213 - acc: 0.8155 - val_loss: 0.4311 - val_acc: 0.8046\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4206 - acc: 0.8163 - val_loss: 0.4287 - val_acc: 0.8079\n",
      "Training with parameters {'batch_size': 4500, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_62\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_63 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_177 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_115 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_178 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_116 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_179 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.8450 - acc: 0.4346 - val_loss: 0.8174 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7948 - acc: 0.4366 - val_loss: 0.7674 - val_acc: 0.3892\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.7521 - acc: 0.4439 - val_loss: 0.7279 - val_acc: 0.3924\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.7227 - acc: 0.4541 - val_loss: 0.6977 - val_acc: 0.4828\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.7007 - acc: 0.4990 - val_loss: 0.6756 - val_acc: 0.6273\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6831 - acc: 0.5698 - val_loss: 0.6598 - val_acc: 0.6355\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6707 - acc: 0.6014 - val_loss: 0.6486 - val_acc: 0.6305\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6624 - acc: 0.6054 - val_loss: 0.6399 - val_acc: 0.6223\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6567 - acc: 0.5977 - val_loss: 0.6328 - val_acc: 0.6190\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6528 - acc: 0.5926 - val_loss: 0.6264 - val_acc: 0.6223\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6462 - acc: 0.6010 - val_loss: 0.6203 - val_acc: 0.6273\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6389 - acc: 0.6125 - val_loss: 0.6146 - val_acc: 0.6404\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6316 - acc: 0.6382 - val_loss: 0.6096 - val_acc: 0.6749\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6227 - acc: 0.6714 - val_loss: 0.6053 - val_acc: 0.7258\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6165 - acc: 0.6860 - val_loss: 0.6018 - val_acc: 0.7241\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6096 - acc: 0.7084 - val_loss: 0.5988 - val_acc: 0.7143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6050 - acc: 0.7083 - val_loss: 0.5961 - val_acc: 0.7011\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5995 - acc: 0.7022 - val_loss: 0.5932 - val_acc: 0.6979\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5956 - acc: 0.7006 - val_loss: 0.5896 - val_acc: 0.6995\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5887 - acc: 0.7086 - val_loss: 0.5851 - val_acc: 0.6995\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5847 - acc: 0.7125 - val_loss: 0.5801 - val_acc: 0.7028\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5797 - acc: 0.7110 - val_loss: 0.5745 - val_acc: 0.6995\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5730 - acc: 0.7168 - val_loss: 0.5683 - val_acc: 0.7094\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5701 - acc: 0.7201 - val_loss: 0.5621 - val_acc: 0.7159\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5648 - acc: 0.7307 - val_loss: 0.5563 - val_acc: 0.7209\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5597 - acc: 0.7331 - val_loss: 0.5512 - val_acc: 0.7225\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5570 - acc: 0.7398 - val_loss: 0.5467 - val_acc: 0.7274\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5517 - acc: 0.7409 - val_loss: 0.5427 - val_acc: 0.7274\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5473 - acc: 0.7420 - val_loss: 0.5392 - val_acc: 0.7258\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5431 - acc: 0.7440 - val_loss: 0.5360 - val_acc: 0.7258\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5376 - acc: 0.7473 - val_loss: 0.5327 - val_acc: 0.7241\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5339 - acc: 0.7458 - val_loss: 0.5286 - val_acc: 0.7241\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5254 - acc: 0.7517 - val_loss: 0.5232 - val_acc: 0.7307\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5259 - acc: 0.7559 - val_loss: 0.5166 - val_acc: 0.7406\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5178 - acc: 0.7637 - val_loss: 0.5098 - val_acc: 0.7438\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5147 - acc: 0.7645 - val_loss: 0.5033 - val_acc: 0.7521\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5093 - acc: 0.7719 - val_loss: 0.4980 - val_acc: 0.7603\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.5070 - acc: 0.7712 - val_loss: 0.4941 - val_acc: 0.7603\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5001 - acc: 0.7772 - val_loss: 0.4915 - val_acc: 0.7586\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4970 - acc: 0.7809 - val_loss: 0.4895 - val_acc: 0.7570\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4917 - acc: 0.7778 - val_loss: 0.4864 - val_acc: 0.7619\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4917 - acc: 0.7780 - val_loss: 0.4821 - val_acc: 0.7685\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4889 - acc: 0.7798 - val_loss: 0.4772 - val_acc: 0.7718\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4847 - acc: 0.7829 - val_loss: 0.4732 - val_acc: 0.7734\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4803 - acc: 0.7880 - val_loss: 0.4704 - val_acc: 0.7750\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4790 - acc: 0.7849 - val_loss: 0.4684 - val_acc: 0.7783\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4749 - acc: 0.7847 - val_loss: 0.4670 - val_acc: 0.7783\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4734 - acc: 0.7900 - val_loss: 0.4654 - val_acc: 0.7816\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4720 - acc: 0.7856 - val_loss: 0.4618 - val_acc: 0.7849\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4689 - acc: 0.7907 - val_loss: 0.4575 - val_acc: 0.7882\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4653 - acc: 0.7944 - val_loss: 0.4543 - val_acc: 0.7915\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4603 - acc: 0.7988 - val_loss: 0.4524 - val_acc: 0.7964\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4618 - acc: 0.7966 - val_loss: 0.4510 - val_acc: 0.7964\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4572 - acc: 0.7982 - val_loss: 0.4490 - val_acc: 0.7964\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4580 - acc: 0.8000 - val_loss: 0.4461 - val_acc: 0.8013\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4572 - acc: 0.7973 - val_loss: 0.4436 - val_acc: 0.8030\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4490 - acc: 0.7989 - val_loss: 0.4423 - val_acc: 0.8013\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4524 - acc: 0.8000 - val_loss: 0.4433 - val_acc: 0.8013\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4493 - acc: 0.8037 - val_loss: 0.4439 - val_acc: 0.8030\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4473 - acc: 0.8030 - val_loss: 0.4405 - val_acc: 0.8030\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4469 - acc: 0.8044 - val_loss: 0.4360 - val_acc: 0.8062\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4447 - acc: 0.8048 - val_loss: 0.4333 - val_acc: 0.8095\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4410 - acc: 0.8070 - val_loss: 0.4334 - val_acc: 0.8095\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4416 - acc: 0.8097 - val_loss: 0.4348 - val_acc: 0.8030\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4404 - acc: 0.8044 - val_loss: 0.4337 - val_acc: 0.8046\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4383 - acc: 0.8044 - val_loss: 0.4299 - val_acc: 0.8095\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4363 - acc: 0.8066 - val_loss: 0.4264 - val_acc: 0.8144\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4357 - acc: 0.8108 - val_loss: 0.4257 - val_acc: 0.8128\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4356 - acc: 0.8113 - val_loss: 0.4269 - val_acc: 0.8095\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4329 - acc: 0.8117 - val_loss: 0.4288 - val_acc: 0.8079\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4347 - acc: 0.8086 - val_loss: 0.4276 - val_acc: 0.8062\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4310 - acc: 0.8072 - val_loss: 0.4246 - val_acc: 0.8095\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4287 - acc: 0.8108 - val_loss: 0.4233 - val_acc: 0.8128\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4295 - acc: 0.8130 - val_loss: 0.4244 - val_acc: 0.8079\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4285 - acc: 0.8128 - val_loss: 0.4270 - val_acc: 0.8095\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4280 - acc: 0.8086 - val_loss: 0.4271 - val_acc: 0.8095\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4229 - acc: 0.8150 - val_loss: 0.4222 - val_acc: 0.8112\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4270 - acc: 0.8130 - val_loss: 0.4179 - val_acc: 0.8210\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4230 - acc: 0.8176 - val_loss: 0.4175 - val_acc: 0.8210\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4249 - acc: 0.8161 - val_loss: 0.4216 - val_acc: 0.8095\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4195 - acc: 0.8166 - val_loss: 0.4248 - val_acc: 0.8095\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4229 - acc: 0.8177 - val_loss: 0.4211 - val_acc: 0.8095\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4188 - acc: 0.8183 - val_loss: 0.4170 - val_acc: 0.8112\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4161 - acc: 0.8207 - val_loss: 0.4157 - val_acc: 0.8161\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4175 - acc: 0.8205 - val_loss: 0.4183 - val_acc: 0.8112\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4178 - acc: 0.8174 - val_loss: 0.4221 - val_acc: 0.8095\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4180 - acc: 0.8199 - val_loss: 0.4189 - val_acc: 0.8144\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4155 - acc: 0.8181 - val_loss: 0.4143 - val_acc: 0.8161\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4147 - acc: 0.8203 - val_loss: 0.4130 - val_acc: 0.8194\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4156 - acc: 0.8194 - val_loss: 0.4144 - val_acc: 0.8112\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4099 - acc: 0.8203 - val_loss: 0.4154 - val_acc: 0.8144\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4115 - acc: 0.8203 - val_loss: 0.4137 - val_acc: 0.8128\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4101 - acc: 0.8228 - val_loss: 0.4114 - val_acc: 0.8194\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4141 - acc: 0.8205 - val_loss: 0.4108 - val_acc: 0.8194\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4086 - acc: 0.8258 - val_loss: 0.4119 - val_acc: 0.8144\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4099 - acc: 0.8197 - val_loss: 0.4157 - val_acc: 0.8161\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4056 - acc: 0.8221 - val_loss: 0.4184 - val_acc: 0.8112\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4082 - acc: 0.8188 - val_loss: 0.4141 - val_acc: 0.8161\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4067 - acc: 0.8239 - val_loss: 0.4101 - val_acc: 0.8194\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4040 - acc: 0.8270 - val_loss: 0.4089 - val_acc: 0.8227\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4044 - acc: 0.8298 - val_loss: 0.4104 - val_acc: 0.8177\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4053 - acc: 0.8248 - val_loss: 0.4148 - val_acc: 0.8128\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4059 - acc: 0.8245 - val_loss: 0.4155 - val_acc: 0.8112\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4018 - acc: 0.8265 - val_loss: 0.4133 - val_acc: 0.8128\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4003 - acc: 0.8252 - val_loss: 0.4112 - val_acc: 0.8194\n",
      "Training with parameters {'batch_size': 5000, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_64 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_180 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_117 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_181 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_118 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_182 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.7849 - acc: 0.4355 - val_loss: 0.7313 - val_acc: 0.3892\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.7251 - acc: 0.4457 - val_loss: 0.6850 - val_acc: 0.5829\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6902 - acc: 0.5311 - val_loss: 0.6607 - val_acc: 0.6092\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6770 - acc: 0.5639 - val_loss: 0.6484 - val_acc: 0.6125\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6699 - acc: 0.5711 - val_loss: 0.6394 - val_acc: 0.6125\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6648 - acc: 0.5694 - val_loss: 0.6300 - val_acc: 0.6125\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6520 - acc: 0.5742 - val_loss: 0.6203 - val_acc: 0.6174\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6409 - acc: 0.5858 - val_loss: 0.6119 - val_acc: 0.6683\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6271 - acc: 0.6322 - val_loss: 0.6063 - val_acc: 0.7159\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6134 - acc: 0.7008 - val_loss: 0.6037 - val_acc: 0.6782\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6063 - acc: 0.7081 - val_loss: 0.6015 - val_acc: 0.6683\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5993 - acc: 0.7019 - val_loss: 0.5969 - val_acc: 0.6683\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5921 - acc: 0.7004 - val_loss: 0.5892 - val_acc: 0.6765\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5845 - acc: 0.7059 - val_loss: 0.5794 - val_acc: 0.6897\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5765 - acc: 0.7214 - val_loss: 0.5700 - val_acc: 0.7044\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5686 - acc: 0.7309 - val_loss: 0.5621 - val_acc: 0.7241\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5638 - acc: 0.7364 - val_loss: 0.5558 - val_acc: 0.7323\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5584 - acc: 0.7427 - val_loss: 0.5504 - val_acc: 0.7291\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5500 - acc: 0.7455 - val_loss: 0.5459 - val_acc: 0.7274\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5435 - acc: 0.7479 - val_loss: 0.5421 - val_acc: 0.7176\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5353 - acc: 0.7444 - val_loss: 0.5369 - val_acc: 0.7159\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5316 - acc: 0.7488 - val_loss: 0.5298 - val_acc: 0.7340\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5256 - acc: 0.7504 - val_loss: 0.5220 - val_acc: 0.7488\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5172 - acc: 0.7641 - val_loss: 0.5150 - val_acc: 0.7537\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5102 - acc: 0.7674 - val_loss: 0.5084 - val_acc: 0.7652\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5077 - acc: 0.7725 - val_loss: 0.5021 - val_acc: 0.7701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5034 - acc: 0.7752 - val_loss: 0.4967 - val_acc: 0.7734\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4986 - acc: 0.7780 - val_loss: 0.4921 - val_acc: 0.7734\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4932 - acc: 0.7809 - val_loss: 0.4892 - val_acc: 0.7800\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4894 - acc: 0.7842 - val_loss: 0.4877 - val_acc: 0.7816\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4850 - acc: 0.7884 - val_loss: 0.4847 - val_acc: 0.7816\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4823 - acc: 0.7867 - val_loss: 0.4805 - val_acc: 0.7833\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4790 - acc: 0.7864 - val_loss: 0.4769 - val_acc: 0.7833\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4726 - acc: 0.7918 - val_loss: 0.4737 - val_acc: 0.7816\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4721 - acc: 0.7900 - val_loss: 0.4709 - val_acc: 0.7849\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4683 - acc: 0.7944 - val_loss: 0.4676 - val_acc: 0.7865\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4651 - acc: 0.7951 - val_loss: 0.4645 - val_acc: 0.7882\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4611 - acc: 0.7929 - val_loss: 0.4625 - val_acc: 0.7833\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4592 - acc: 0.7977 - val_loss: 0.4608 - val_acc: 0.7849\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4544 - acc: 0.7977 - val_loss: 0.4555 - val_acc: 0.7947\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4521 - acc: 0.8009 - val_loss: 0.4504 - val_acc: 0.7964\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4519 - acc: 0.8031 - val_loss: 0.4471 - val_acc: 0.8013\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4487 - acc: 0.8066 - val_loss: 0.4462 - val_acc: 0.7997\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4458 - acc: 0.8082 - val_loss: 0.4468 - val_acc: 0.7997\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4461 - acc: 0.8046 - val_loss: 0.4456 - val_acc: 0.7964\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4421 - acc: 0.8090 - val_loss: 0.4422 - val_acc: 0.8013\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4412 - acc: 0.8108 - val_loss: 0.4371 - val_acc: 0.8095\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4391 - acc: 0.8128 - val_loss: 0.4354 - val_acc: 0.8095\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4374 - acc: 0.8112 - val_loss: 0.4369 - val_acc: 0.8062\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4368 - acc: 0.8117 - val_loss: 0.4397 - val_acc: 0.8030\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4322 - acc: 0.8106 - val_loss: 0.4339 - val_acc: 0.8095\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4355 - acc: 0.8115 - val_loss: 0.4289 - val_acc: 0.8128\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4354 - acc: 0.8112 - val_loss: 0.4294 - val_acc: 0.8095\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4309 - acc: 0.8117 - val_loss: 0.4348 - val_acc: 0.8095\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4306 - acc: 0.8150 - val_loss: 0.4413 - val_acc: 0.8079\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4307 - acc: 0.8097 - val_loss: 0.4348 - val_acc: 0.8079\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4264 - acc: 0.8139 - val_loss: 0.4266 - val_acc: 0.8128\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4253 - acc: 0.8170 - val_loss: 0.4242 - val_acc: 0.8128\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4250 - acc: 0.8174 - val_loss: 0.4249 - val_acc: 0.8112\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4237 - acc: 0.8150 - val_loss: 0.4288 - val_acc: 0.8128\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4222 - acc: 0.8177 - val_loss: 0.4265 - val_acc: 0.8128\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4211 - acc: 0.8165 - val_loss: 0.4200 - val_acc: 0.8177\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4209 - acc: 0.8168 - val_loss: 0.4177 - val_acc: 0.8177\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4185 - acc: 0.8241 - val_loss: 0.4202 - val_acc: 0.8144\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4169 - acc: 0.8212 - val_loss: 0.4282 - val_acc: 0.8095\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4166 - acc: 0.8188 - val_loss: 0.4263 - val_acc: 0.8144\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4141 - acc: 0.8163 - val_loss: 0.4211 - val_acc: 0.8161\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4158 - acc: 0.8194 - val_loss: 0.4225 - val_acc: 0.8144\n",
      "Training with parameters {'batch_size': 5000, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_64\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_65 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_183 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_119 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_184 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_120 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_185 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.8483 - acc: 0.4353 - val_loss: 0.8178 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.7967 - acc: 0.4388 - val_loss: 0.7683 - val_acc: 0.3875\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7541 - acc: 0.4465 - val_loss: 0.7290 - val_acc: 0.3924\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.7227 - acc: 0.4583 - val_loss: 0.6986 - val_acc: 0.4745\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6989 - acc: 0.4972 - val_loss: 0.6760 - val_acc: 0.6273\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6822 - acc: 0.5712 - val_loss: 0.6599 - val_acc: 0.6420\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6707 - acc: 0.6077 - val_loss: 0.6483 - val_acc: 0.6338\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6619 - acc: 0.6072 - val_loss: 0.6395 - val_acc: 0.6223\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6583 - acc: 0.5977 - val_loss: 0.6324 - val_acc: 0.6190\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6529 - acc: 0.6001 - val_loss: 0.6261 - val_acc: 0.6240\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6463 - acc: 0.5981 - val_loss: 0.6202 - val_acc: 0.6305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6386 - acc: 0.6176 - val_loss: 0.6147 - val_acc: 0.6404\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6299 - acc: 0.6380 - val_loss: 0.6095 - val_acc: 0.6683\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6247 - acc: 0.6597 - val_loss: 0.6049 - val_acc: 0.7241\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6183 - acc: 0.6805 - val_loss: 0.6008 - val_acc: 0.7291\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6098 - acc: 0.7024 - val_loss: 0.5972 - val_acc: 0.7192\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6058 - acc: 0.7052 - val_loss: 0.5939 - val_acc: 0.7028\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6004 - acc: 0.7068 - val_loss: 0.5909 - val_acc: 0.6979\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5986 - acc: 0.7053 - val_loss: 0.5878 - val_acc: 0.6995\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5891 - acc: 0.7044 - val_loss: 0.5844 - val_acc: 0.7011\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5863 - acc: 0.7043 - val_loss: 0.5804 - val_acc: 0.6995\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5810 - acc: 0.7055 - val_loss: 0.5753 - val_acc: 0.6995\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5769 - acc: 0.7112 - val_loss: 0.5695 - val_acc: 0.7077\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5707 - acc: 0.7214 - val_loss: 0.5636 - val_acc: 0.7143\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5634 - acc: 0.7329 - val_loss: 0.5579 - val_acc: 0.7209\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5623 - acc: 0.7376 - val_loss: 0.5526 - val_acc: 0.7192\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5588 - acc: 0.7386 - val_loss: 0.5477 - val_acc: 0.7291\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5518 - acc: 0.7417 - val_loss: 0.5433 - val_acc: 0.7291\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5499 - acc: 0.7457 - val_loss: 0.5396 - val_acc: 0.7356\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5437 - acc: 0.7462 - val_loss: 0.5368 - val_acc: 0.7258\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5410 - acc: 0.7448 - val_loss: 0.5346 - val_acc: 0.7258\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5362 - acc: 0.7449 - val_loss: 0.5328 - val_acc: 0.7225\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5319 - acc: 0.7488 - val_loss: 0.5300 - val_acc: 0.7209\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5292 - acc: 0.7448 - val_loss: 0.5245 - val_acc: 0.7209\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5233 - acc: 0.7530 - val_loss: 0.5162 - val_acc: 0.7356\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5197 - acc: 0.7603 - val_loss: 0.5081 - val_acc: 0.7471\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5150 - acc: 0.7635 - val_loss: 0.5016 - val_acc: 0.7570\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5142 - acc: 0.7668 - val_loss: 0.4969 - val_acc: 0.7652\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5060 - acc: 0.7707 - val_loss: 0.4939 - val_acc: 0.7603\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5038 - acc: 0.7705 - val_loss: 0.4917 - val_acc: 0.7603\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4994 - acc: 0.7749 - val_loss: 0.4883 - val_acc: 0.7619\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4984 - acc: 0.7761 - val_loss: 0.4848 - val_acc: 0.7652\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4934 - acc: 0.7747 - val_loss: 0.4829 - val_acc: 0.7619\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4931 - acc: 0.7760 - val_loss: 0.4821 - val_acc: 0.7652\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4885 - acc: 0.7774 - val_loss: 0.4786 - val_acc: 0.7668\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4854 - acc: 0.7809 - val_loss: 0.4735 - val_acc: 0.7685\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4816 - acc: 0.7876 - val_loss: 0.4693 - val_acc: 0.7718\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4812 - acc: 0.7873 - val_loss: 0.4661 - val_acc: 0.7767\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4777 - acc: 0.7867 - val_loss: 0.4646 - val_acc: 0.7816\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4733 - acc: 0.7854 - val_loss: 0.4649 - val_acc: 0.7816\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4735 - acc: 0.7854 - val_loss: 0.4648 - val_acc: 0.7767\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4723 - acc: 0.7889 - val_loss: 0.4613 - val_acc: 0.7816\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4689 - acc: 0.7947 - val_loss: 0.4561 - val_acc: 0.7849\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4650 - acc: 0.7978 - val_loss: 0.4515 - val_acc: 0.7865\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4652 - acc: 0.7937 - val_loss: 0.4483 - val_acc: 0.7947\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4658 - acc: 0.7940 - val_loss: 0.4470 - val_acc: 0.7947\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4599 - acc: 0.7937 - val_loss: 0.4487 - val_acc: 0.7915\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4568 - acc: 0.7982 - val_loss: 0.4520 - val_acc: 0.7865\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4568 - acc: 0.7975 - val_loss: 0.4515 - val_acc: 0.7915\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4555 - acc: 0.7973 - val_loss: 0.4468 - val_acc: 0.7947\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4517 - acc: 0.8041 - val_loss: 0.4414 - val_acc: 0.8046\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4519 - acc: 0.8026 - val_loss: 0.4375 - val_acc: 0.8062\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4532 - acc: 0.8022 - val_loss: 0.4366 - val_acc: 0.8095\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4461 - acc: 0.8057 - val_loss: 0.4396 - val_acc: 0.8046\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4486 - acc: 0.8031 - val_loss: 0.4438 - val_acc: 0.7997\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4513 - acc: 0.7995 - val_loss: 0.4411 - val_acc: 0.7997\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4478 - acc: 0.7982 - val_loss: 0.4359 - val_acc: 0.8030\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4456 - acc: 0.8073 - val_loss: 0.4320 - val_acc: 0.8112\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4416 - acc: 0.8095 - val_loss: 0.4300 - val_acc: 0.8095\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4436 - acc: 0.8033 - val_loss: 0.4298 - val_acc: 0.8112\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4394 - acc: 0.8055 - val_loss: 0.4312 - val_acc: 0.8079\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4378 - acc: 0.8084 - val_loss: 0.4315 - val_acc: 0.8062\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4363 - acc: 0.8075 - val_loss: 0.4300 - val_acc: 0.8095\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4343 - acc: 0.8099 - val_loss: 0.4285 - val_acc: 0.8112\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4382 - acc: 0.8066 - val_loss: 0.4267 - val_acc: 0.8095\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4345 - acc: 0.8095 - val_loss: 0.4256 - val_acc: 0.8112\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4339 - acc: 0.8103 - val_loss: 0.4261 - val_acc: 0.8095\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4323 - acc: 0.8110 - val_loss: 0.4273 - val_acc: 0.8095\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4310 - acc: 0.8135 - val_loss: 0.4267 - val_acc: 0.8095\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4310 - acc: 0.8103 - val_loss: 0.4256 - val_acc: 0.8095\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4318 - acc: 0.8113 - val_loss: 0.4241 - val_acc: 0.8079\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4288 - acc: 0.8128 - val_loss: 0.4215 - val_acc: 0.8112\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4295 - acc: 0.8130 - val_loss: 0.4197 - val_acc: 0.8177\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4279 - acc: 0.8143 - val_loss: 0.4199 - val_acc: 0.8144\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4264 - acc: 0.8123 - val_loss: 0.4212 - val_acc: 0.8079\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4255 - acc: 0.8108 - val_loss: 0.4193 - val_acc: 0.8144\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4261 - acc: 0.8123 - val_loss: 0.4176 - val_acc: 0.8194\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4217 - acc: 0.8172 - val_loss: 0.4186 - val_acc: 0.8177\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4222 - acc: 0.8144 - val_loss: 0.4209 - val_acc: 0.8062\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4226 - acc: 0.8148 - val_loss: 0.4229 - val_acc: 0.8079\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4216 - acc: 0.8124 - val_loss: 0.4229 - val_acc: 0.8079\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4207 - acc: 0.8163 - val_loss: 0.4203 - val_acc: 0.8062\n",
      "Training with parameters {'batch_size': 5000, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_65\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_66 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_186 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_121 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_187 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.6746 - acc: 0.5886 - val_loss: 0.6647 - val_acc: 0.5911\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.6608 - acc: 0.6273 - val_loss: 0.6504 - val_acc: 0.6355\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6520 - acc: 0.6346 - val_loss: 0.6400 - val_acc: 0.6650\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6442 - acc: 0.6439 - val_loss: 0.6322 - val_acc: 0.6634\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6365 - acc: 0.6530 - val_loss: 0.6259 - val_acc: 0.6667\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.6318 - acc: 0.6530 - val_loss: 0.6207 - val_acc: 0.6782\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6259 - acc: 0.6670 - val_loss: 0.6161 - val_acc: 0.6814\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6182 - acc: 0.6844 - val_loss: 0.6121 - val_acc: 0.6716\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6145 - acc: 0.6900 - val_loss: 0.6086 - val_acc: 0.6782\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6065 - acc: 0.6991 - val_loss: 0.6056 - val_acc: 0.6782\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6026 - acc: 0.7021 - val_loss: 0.6030 - val_acc: 0.6732\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5987 - acc: 0.7001 - val_loss: 0.6002 - val_acc: 0.6765\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5933 - acc: 0.7055 - val_loss: 0.5970 - val_acc: 0.6732\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5905 - acc: 0.7063 - val_loss: 0.5929 - val_acc: 0.6831\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5859 - acc: 0.7106 - val_loss: 0.5882 - val_acc: 0.6864\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5822 - acc: 0.7156 - val_loss: 0.5839 - val_acc: 0.6897\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5760 - acc: 0.7223 - val_loss: 0.5799 - val_acc: 0.6946\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5736 - acc: 0.7225 - val_loss: 0.5763 - val_acc: 0.7011\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5707 - acc: 0.7278 - val_loss: 0.5727 - val_acc: 0.7077\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5665 - acc: 0.7316 - val_loss: 0.5695 - val_acc: 0.7110\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5635 - acc: 0.7331 - val_loss: 0.5663 - val_acc: 0.7126\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5606 - acc: 0.7336 - val_loss: 0.5629 - val_acc: 0.7143\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5566 - acc: 0.7406 - val_loss: 0.5593 - val_acc: 0.7176\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5532 - acc: 0.7448 - val_loss: 0.5559 - val_acc: 0.7176\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5504 - acc: 0.7413 - val_loss: 0.5531 - val_acc: 0.7209\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5482 - acc: 0.7468 - val_loss: 0.5508 - val_acc: 0.7241\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5428 - acc: 0.7513 - val_loss: 0.5484 - val_acc: 0.7274\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5417 - acc: 0.7490 - val_loss: 0.5456 - val_acc: 0.7291\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5389 - acc: 0.7493 - val_loss: 0.5426 - val_acc: 0.7307\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5373 - acc: 0.7548 - val_loss: 0.5396 - val_acc: 0.7340\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5320 - acc: 0.7592 - val_loss: 0.5366 - val_acc: 0.7373\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5325 - acc: 0.7608 - val_loss: 0.5341 - val_acc: 0.7422\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5273 - acc: 0.7623 - val_loss: 0.5323 - val_acc: 0.7422\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5266 - acc: 0.7606 - val_loss: 0.5305 - val_acc: 0.7455\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5235 - acc: 0.7637 - val_loss: 0.5284 - val_acc: 0.7438\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5210 - acc: 0.7663 - val_loss: 0.5261 - val_acc: 0.7406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5193 - acc: 0.7621 - val_loss: 0.5232 - val_acc: 0.7455\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5176 - acc: 0.7674 - val_loss: 0.5201 - val_acc: 0.7488\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5154 - acc: 0.7707 - val_loss: 0.5173 - val_acc: 0.7537\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5129 - acc: 0.7723 - val_loss: 0.5147 - val_acc: 0.7521\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5111 - acc: 0.7747 - val_loss: 0.5126 - val_acc: 0.7553\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5081 - acc: 0.7760 - val_loss: 0.5106 - val_acc: 0.7570\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5082 - acc: 0.7739 - val_loss: 0.5088 - val_acc: 0.7603\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.5046 - acc: 0.7780 - val_loss: 0.5081 - val_acc: 0.7603\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5059 - acc: 0.7727 - val_loss: 0.5069 - val_acc: 0.7619\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5012 - acc: 0.7781 - val_loss: 0.5044 - val_acc: 0.7603\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4999 - acc: 0.7798 - val_loss: 0.5023 - val_acc: 0.7652\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4986 - acc: 0.7778 - val_loss: 0.5012 - val_acc: 0.7635\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4968 - acc: 0.7803 - val_loss: 0.5011 - val_acc: 0.7603\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4947 - acc: 0.7820 - val_loss: 0.5002 - val_acc: 0.7619\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4934 - acc: 0.7825 - val_loss: 0.4980 - val_acc: 0.7619\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4930 - acc: 0.7833 - val_loss: 0.4956 - val_acc: 0.7668\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4904 - acc: 0.7853 - val_loss: 0.4942 - val_acc: 0.7701\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4867 - acc: 0.7856 - val_loss: 0.4932 - val_acc: 0.7652\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4904 - acc: 0.7860 - val_loss: 0.4920 - val_acc: 0.7652\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4861 - acc: 0.7874 - val_loss: 0.4908 - val_acc: 0.7718\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4859 - acc: 0.7854 - val_loss: 0.4891 - val_acc: 0.7718\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4857 - acc: 0.7874 - val_loss: 0.4877 - val_acc: 0.7734\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4817 - acc: 0.7893 - val_loss: 0.4866 - val_acc: 0.7734\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4819 - acc: 0.7902 - val_loss: 0.4851 - val_acc: 0.7783\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4808 - acc: 0.7898 - val_loss: 0.4836 - val_acc: 0.7783\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4774 - acc: 0.7962 - val_loss: 0.4824 - val_acc: 0.7767\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4793 - acc: 0.7916 - val_loss: 0.4822 - val_acc: 0.7800\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4792 - acc: 0.7893 - val_loss: 0.4822 - val_acc: 0.7783\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4753 - acc: 0.7922 - val_loss: 0.4811 - val_acc: 0.7800\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4768 - acc: 0.7905 - val_loss: 0.4793 - val_acc: 0.7800\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4747 - acc: 0.7940 - val_loss: 0.4778 - val_acc: 0.7800\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4720 - acc: 0.7933 - val_loss: 0.4760 - val_acc: 0.7783\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4717 - acc: 0.7958 - val_loss: 0.4744 - val_acc: 0.7800\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4705 - acc: 0.7938 - val_loss: 0.4737 - val_acc: 0.7800\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4696 - acc: 0.7944 - val_loss: 0.4731 - val_acc: 0.7833\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4688 - acc: 0.7916 - val_loss: 0.4727 - val_acc: 0.7915\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4675 - acc: 0.7982 - val_loss: 0.4720 - val_acc: 0.7898\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4666 - acc: 0.7960 - val_loss: 0.4707 - val_acc: 0.7915\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4669 - acc: 0.7969 - val_loss: 0.4691 - val_acc: 0.7898\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4648 - acc: 0.7958 - val_loss: 0.4678 - val_acc: 0.7882\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4650 - acc: 0.7978 - val_loss: 0.4667 - val_acc: 0.7865\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4642 - acc: 0.7975 - val_loss: 0.4656 - val_acc: 0.7882\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4624 - acc: 0.7962 - val_loss: 0.4646 - val_acc: 0.7882\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4620 - acc: 0.7975 - val_loss: 0.4637 - val_acc: 0.7898\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4632 - acc: 0.7975 - val_loss: 0.4629 - val_acc: 0.7882\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4613 - acc: 0.8020 - val_loss: 0.4618 - val_acc: 0.7898\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4596 - acc: 0.8028 - val_loss: 0.4611 - val_acc: 0.7898\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4600 - acc: 0.8011 - val_loss: 0.4613 - val_acc: 0.7931\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4585 - acc: 0.7999 - val_loss: 0.4616 - val_acc: 0.7947\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4577 - acc: 0.8004 - val_loss: 0.4626 - val_acc: 0.7947\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4566 - acc: 0.8006 - val_loss: 0.4631 - val_acc: 0.7964\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4580 - acc: 0.8013 - val_loss: 0.4615 - val_acc: 0.7947\n",
      "Training with parameters {'batch_size': 5000, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_66\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_67 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_188 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_122 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_189 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_123 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_190 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 1s 120ms/step - loss: 0.8577 - acc: 0.4351 - val_loss: 0.8017 - val_acc: 0.3875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7812 - acc: 0.4340 - val_loss: 0.7351 - val_acc: 0.3875\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7294 - acc: 0.4446 - val_loss: 0.6921 - val_acc: 0.5238\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6973 - acc: 0.5052 - val_loss: 0.6667 - val_acc: 0.6108\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6808 - acc: 0.5466 - val_loss: 0.6522 - val_acc: 0.6108\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6700 - acc: 0.5712 - val_loss: 0.6421 - val_acc: 0.6141\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6628 - acc: 0.5722 - val_loss: 0.6332 - val_acc: 0.6158\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6541 - acc: 0.5754 - val_loss: 0.6246 - val_acc: 0.6223\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6418 - acc: 0.5917 - val_loss: 0.6167 - val_acc: 0.6519\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6334 - acc: 0.6159 - val_loss: 0.6107 - val_acc: 0.6913\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6211 - acc: 0.6608 - val_loss: 0.6069 - val_acc: 0.6929\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6102 - acc: 0.6984 - val_loss: 0.6049 - val_acc: 0.6798\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6028 - acc: 0.7030 - val_loss: 0.6031 - val_acc: 0.6667\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5979 - acc: 0.6959 - val_loss: 0.5995 - val_acc: 0.6700\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5936 - acc: 0.6980 - val_loss: 0.5937 - val_acc: 0.6683\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5887 - acc: 0.6999 - val_loss: 0.5868 - val_acc: 0.6782\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5773 - acc: 0.7119 - val_loss: 0.5799 - val_acc: 0.6847\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5754 - acc: 0.7115 - val_loss: 0.5730 - val_acc: 0.6864\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5680 - acc: 0.7254 - val_loss: 0.5663 - val_acc: 0.6929\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5635 - acc: 0.7298 - val_loss: 0.5601 - val_acc: 0.7061\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5576 - acc: 0.7338 - val_loss: 0.5543 - val_acc: 0.7110\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5505 - acc: 0.7409 - val_loss: 0.5481 - val_acc: 0.7077\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5431 - acc: 0.7435 - val_loss: 0.5421 - val_acc: 0.7176\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5408 - acc: 0.7415 - val_loss: 0.5364 - val_acc: 0.7241\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5323 - acc: 0.7541 - val_loss: 0.5301 - val_acc: 0.7225\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5265 - acc: 0.7546 - val_loss: 0.5236 - val_acc: 0.7323\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5216 - acc: 0.7670 - val_loss: 0.5178 - val_acc: 0.7504\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5144 - acc: 0.7677 - val_loss: 0.5129 - val_acc: 0.7521\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5116 - acc: 0.7687 - val_loss: 0.5089 - val_acc: 0.7488\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5072 - acc: 0.7690 - val_loss: 0.5049 - val_acc: 0.7504\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5045 - acc: 0.7685 - val_loss: 0.4990 - val_acc: 0.7603\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4976 - acc: 0.7763 - val_loss: 0.4940 - val_acc: 0.7652\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4960 - acc: 0.7814 - val_loss: 0.4911 - val_acc: 0.7635\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4924 - acc: 0.7750 - val_loss: 0.4866 - val_acc: 0.7652\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4871 - acc: 0.7825 - val_loss: 0.4815 - val_acc: 0.7718\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4845 - acc: 0.7833 - val_loss: 0.4769 - val_acc: 0.7734\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4782 - acc: 0.7849 - val_loss: 0.4736 - val_acc: 0.7767\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4777 - acc: 0.7900 - val_loss: 0.4709 - val_acc: 0.7800\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4743 - acc: 0.7896 - val_loss: 0.4695 - val_acc: 0.7816\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4708 - acc: 0.7929 - val_loss: 0.4706 - val_acc: 0.7882\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4672 - acc: 0.7938 - val_loss: 0.4704 - val_acc: 0.7915\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4682 - acc: 0.7915 - val_loss: 0.4663 - val_acc: 0.7964\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4648 - acc: 0.7944 - val_loss: 0.4590 - val_acc: 0.7980\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4603 - acc: 0.7989 - val_loss: 0.4539 - val_acc: 0.7980\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4620 - acc: 0.7958 - val_loss: 0.4515 - val_acc: 0.8013\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4580 - acc: 0.8000 - val_loss: 0.4502 - val_acc: 0.8030\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4553 - acc: 0.8020 - val_loss: 0.4510 - val_acc: 0.8013\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4500 - acc: 0.8055 - val_loss: 0.4501 - val_acc: 0.7997\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4507 - acc: 0.8046 - val_loss: 0.4447 - val_acc: 0.8030\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4488 - acc: 0.8064 - val_loss: 0.4408 - val_acc: 0.8079\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4482 - acc: 0.8042 - val_loss: 0.4400 - val_acc: 0.8062\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4464 - acc: 0.8050 - val_loss: 0.4406 - val_acc: 0.8013\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4429 - acc: 0.8088 - val_loss: 0.4407 - val_acc: 0.7980\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4430 - acc: 0.8084 - val_loss: 0.4421 - val_acc: 0.7980\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4404 - acc: 0.8077 - val_loss: 0.4426 - val_acc: 0.8013\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4378 - acc: 0.8084 - val_loss: 0.4376 - val_acc: 0.8046\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4364 - acc: 0.8117 - val_loss: 0.4314 - val_acc: 0.8112\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4349 - acc: 0.8121 - val_loss: 0.4286 - val_acc: 0.8161\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4369 - acc: 0.8124 - val_loss: 0.4301 - val_acc: 0.8079\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4329 - acc: 0.8137 - val_loss: 0.4362 - val_acc: 0.8062\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4318 - acc: 0.8119 - val_loss: 0.4362 - val_acc: 0.8030\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4296 - acc: 0.8137 - val_loss: 0.4324 - val_acc: 0.8046\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4278 - acc: 0.8121 - val_loss: 0.4294 - val_acc: 0.8079\n",
      "Training with parameters {'batch_size': 5000, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_67\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_68 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_191 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_124 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_192 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_125 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_193 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.8858 - acc: 0.4353 - val_loss: 0.8561 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.8276 - acc: 0.4355 - val_loss: 0.8030 - val_acc: 0.3875\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7811 - acc: 0.4382 - val_loss: 0.7625 - val_acc: 0.3875\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.7487 - acc: 0.4395 - val_loss: 0.7310 - val_acc: 0.3924\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.7223 - acc: 0.4548 - val_loss: 0.7068 - val_acc: 0.4335\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.7036 - acc: 0.4860 - val_loss: 0.6883 - val_acc: 0.5320\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6881 - acc: 0.5406 - val_loss: 0.6741 - val_acc: 0.6190\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6771 - acc: 0.5855 - val_loss: 0.6630 - val_acc: 0.6831\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6695 - acc: 0.6139 - val_loss: 0.6536 - val_acc: 0.6929\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6622 - acc: 0.6251 - val_loss: 0.6457 - val_acc: 0.6814\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6564 - acc: 0.6406 - val_loss: 0.6387 - val_acc: 0.6897\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6503 - acc: 0.6433 - val_loss: 0.6322 - val_acc: 0.7143\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6452 - acc: 0.6526 - val_loss: 0.6264 - val_acc: 0.7323\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6382 - acc: 0.6692 - val_loss: 0.6213 - val_acc: 0.7455\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6303 - acc: 0.6838 - val_loss: 0.6166 - val_acc: 0.7455\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6255 - acc: 0.6933 - val_loss: 0.6124 - val_acc: 0.7258\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6193 - acc: 0.7043 - val_loss: 0.6087 - val_acc: 0.7110\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6129 - acc: 0.7156 - val_loss: 0.6055 - val_acc: 0.7110\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6108 - acc: 0.7086 - val_loss: 0.6022 - val_acc: 0.7061\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6039 - acc: 0.7152 - val_loss: 0.5990 - val_acc: 0.6962\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5987 - acc: 0.7132 - val_loss: 0.5961 - val_acc: 0.7011\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5946 - acc: 0.7152 - val_loss: 0.5930 - val_acc: 0.7011\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5929 - acc: 0.7176 - val_loss: 0.5895 - val_acc: 0.6995\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5878 - acc: 0.7223 - val_loss: 0.5859 - val_acc: 0.6979\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5830 - acc: 0.7179 - val_loss: 0.5822 - val_acc: 0.7044\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5777 - acc: 0.7236 - val_loss: 0.5786 - val_acc: 0.7028\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5767 - acc: 0.7276 - val_loss: 0.5746 - val_acc: 0.7028\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5746 - acc: 0.7287 - val_loss: 0.5702 - val_acc: 0.7061\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5703 - acc: 0.7300 - val_loss: 0.5658 - val_acc: 0.7077\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5658 - acc: 0.7393 - val_loss: 0.5618 - val_acc: 0.7143\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5612 - acc: 0.7398 - val_loss: 0.5584 - val_acc: 0.7143\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5617 - acc: 0.7376 - val_loss: 0.5553 - val_acc: 0.7110\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5577 - acc: 0.7417 - val_loss: 0.5526 - val_acc: 0.7094\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5515 - acc: 0.7407 - val_loss: 0.5506 - val_acc: 0.7126\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5501 - acc: 0.7444 - val_loss: 0.5484 - val_acc: 0.7110\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5436 - acc: 0.7490 - val_loss: 0.5448 - val_acc: 0.7126\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5426 - acc: 0.7453 - val_loss: 0.5404 - val_acc: 0.7176\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5356 - acc: 0.7535 - val_loss: 0.5359 - val_acc: 0.7225\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5334 - acc: 0.7515 - val_loss: 0.5312 - val_acc: 0.7274\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5305 - acc: 0.7573 - val_loss: 0.5265 - val_acc: 0.7389\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5288 - acc: 0.7548 - val_loss: 0.5221 - val_acc: 0.7356\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5279 - acc: 0.7614 - val_loss: 0.5184 - val_acc: 0.7422\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5233 - acc: 0.7654 - val_loss: 0.5157 - val_acc: 0.7455\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5192 - acc: 0.7666 - val_loss: 0.5133 - val_acc: 0.7455\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5161 - acc: 0.7666 - val_loss: 0.5113 - val_acc: 0.7488\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5182 - acc: 0.7694 - val_loss: 0.5094 - val_acc: 0.7504\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5132 - acc: 0.7666 - val_loss: 0.5058 - val_acc: 0.7521\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5102 - acc: 0.7694 - val_loss: 0.5013 - val_acc: 0.7521\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5072 - acc: 0.7734 - val_loss: 0.4982 - val_acc: 0.7537\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5021 - acc: 0.7770 - val_loss: 0.4966 - val_acc: 0.7586\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5044 - acc: 0.7750 - val_loss: 0.4960 - val_acc: 0.7537\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4986 - acc: 0.7801 - val_loss: 0.4951 - val_acc: 0.7504\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4971 - acc: 0.7772 - val_loss: 0.4931 - val_acc: 0.7504\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4973 - acc: 0.7727 - val_loss: 0.4903 - val_acc: 0.7504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4922 - acc: 0.7774 - val_loss: 0.4867 - val_acc: 0.7635\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4897 - acc: 0.7823 - val_loss: 0.4840 - val_acc: 0.7668\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4901 - acc: 0.7760 - val_loss: 0.4828 - val_acc: 0.7635\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4860 - acc: 0.7798 - val_loss: 0.4825 - val_acc: 0.7619\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4857 - acc: 0.7831 - val_loss: 0.4815 - val_acc: 0.7603\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4837 - acc: 0.7787 - val_loss: 0.4796 - val_acc: 0.7619\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4832 - acc: 0.7851 - val_loss: 0.4768 - val_acc: 0.7685\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4827 - acc: 0.7854 - val_loss: 0.4738 - val_acc: 0.7750\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4806 - acc: 0.7864 - val_loss: 0.4710 - val_acc: 0.7750\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4792 - acc: 0.7916 - val_loss: 0.4691 - val_acc: 0.7750\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4749 - acc: 0.7916 - val_loss: 0.4682 - val_acc: 0.7767\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4742 - acc: 0.7884 - val_loss: 0.4678 - val_acc: 0.7767\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4699 - acc: 0.7924 - val_loss: 0.4673 - val_acc: 0.7767\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4701 - acc: 0.7949 - val_loss: 0.4658 - val_acc: 0.7783\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4678 - acc: 0.7907 - val_loss: 0.4633 - val_acc: 0.7833\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4664 - acc: 0.7898 - val_loss: 0.4603 - val_acc: 0.7882\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4673 - acc: 0.7918 - val_loss: 0.4581 - val_acc: 0.7882\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4644 - acc: 0.7951 - val_loss: 0.4572 - val_acc: 0.7915\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4637 - acc: 0.7993 - val_loss: 0.4572 - val_acc: 0.7898\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4614 - acc: 0.7995 - val_loss: 0.4568 - val_acc: 0.7931\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4634 - acc: 0.7944 - val_loss: 0.4545 - val_acc: 0.7931\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4578 - acc: 0.7988 - val_loss: 0.4515 - val_acc: 0.7980\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4582 - acc: 0.7960 - val_loss: 0.4493 - val_acc: 0.7980\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4571 - acc: 0.7975 - val_loss: 0.4486 - val_acc: 0.7980\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4577 - acc: 0.7982 - val_loss: 0.4493 - val_acc: 0.7947\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4544 - acc: 0.7993 - val_loss: 0.4492 - val_acc: 0.7964\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4574 - acc: 0.8039 - val_loss: 0.4490 - val_acc: 0.7964\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4514 - acc: 0.8033 - val_loss: 0.4490 - val_acc: 0.7931\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4516 - acc: 0.8037 - val_loss: 0.4467 - val_acc: 0.7964\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4507 - acc: 0.7980 - val_loss: 0.4436 - val_acc: 0.8013\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4505 - acc: 0.8037 - val_loss: 0.4418 - val_acc: 0.7997\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4509 - acc: 0.8033 - val_loss: 0.4408 - val_acc: 0.8030\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4469 - acc: 0.8048 - val_loss: 0.4404 - val_acc: 0.8030\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4440 - acc: 0.8035 - val_loss: 0.4413 - val_acc: 0.8013\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4462 - acc: 0.8053 - val_loss: 0.4429 - val_acc: 0.7997\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4458 - acc: 0.8064 - val_loss: 0.4440 - val_acc: 0.7980\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4430 - acc: 0.8042 - val_loss: 0.4426 - val_acc: 0.7997\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4430 - acc: 0.8079 - val_loss: 0.4399 - val_acc: 0.8013\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4432 - acc: 0.8055 - val_loss: 0.4365 - val_acc: 0.8046\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4409 - acc: 0.8059 - val_loss: 0.4344 - val_acc: 0.8095\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4375 - acc: 0.8101 - val_loss: 0.4339 - val_acc: 0.8095\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4386 - acc: 0.8101 - val_loss: 0.4338 - val_acc: 0.8062\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4370 - acc: 0.8104 - val_loss: 0.4328 - val_acc: 0.8079\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4363 - acc: 0.8097 - val_loss: 0.4322 - val_acc: 0.8095\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4369 - acc: 0.8115 - val_loss: 0.4337 - val_acc: 0.8062\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4358 - acc: 0.8079 - val_loss: 0.4352 - val_acc: 0.8030\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4371 - acc: 0.8121 - val_loss: 0.4343 - val_acc: 0.8046\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4328 - acc: 0.8137 - val_loss: 0.4313 - val_acc: 0.8062\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4334 - acc: 0.8110 - val_loss: 0.4292 - val_acc: 0.8095\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4326 - acc: 0.8121 - val_loss: 0.4280 - val_acc: 0.8095\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4323 - acc: 0.8101 - val_loss: 0.4271 - val_acc: 0.8095\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4308 - acc: 0.8117 - val_loss: 0.4269 - val_acc: 0.8095\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4302 - acc: 0.8126 - val_loss: 0.4272 - val_acc: 0.8079\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4288 - acc: 0.8086 - val_loss: 0.4281 - val_acc: 0.8079\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4288 - acc: 0.8135 - val_loss: 0.4288 - val_acc: 0.8030\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4302 - acc: 0.8126 - val_loss: 0.4279 - val_acc: 0.8062\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4296 - acc: 0.8121 - val_loss: 0.4262 - val_acc: 0.8095\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4267 - acc: 0.8143 - val_loss: 0.4252 - val_acc: 0.8112\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4263 - acc: 0.8135 - val_loss: 0.4243 - val_acc: 0.8112\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4231 - acc: 0.8168 - val_loss: 0.4248 - val_acc: 0.8095\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4240 - acc: 0.8161 - val_loss: 0.4264 - val_acc: 0.8046\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4218 - acc: 0.8199 - val_loss: 0.4274 - val_acc: 0.8062\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4223 - acc: 0.8148 - val_loss: 0.4266 - val_acc: 0.8062\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4226 - acc: 0.8139 - val_loss: 0.4238 - val_acc: 0.8095\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4221 - acc: 0.8192 - val_loss: 0.4219 - val_acc: 0.8144\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4207 - acc: 0.8201 - val_loss: 0.4212 - val_acc: 0.8144\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4213 - acc: 0.8172 - val_loss: 0.4217 - val_acc: 0.8128\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4164 - acc: 0.8225 - val_loss: 0.4239 - val_acc: 0.8079\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4203 - acc: 0.8144 - val_loss: 0.4253 - val_acc: 0.8046\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4217 - acc: 0.8130 - val_loss: 0.4225 - val_acc: 0.8062\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4189 - acc: 0.8176 - val_loss: 0.4193 - val_acc: 0.8144\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4173 - acc: 0.8174 - val_loss: 0.4183 - val_acc: 0.8144\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4209 - acc: 0.8146 - val_loss: 0.4185 - val_acc: 0.8161\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4168 - acc: 0.8196 - val_loss: 0.4199 - val_acc: 0.8128\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4157 - acc: 0.8210 - val_loss: 0.4224 - val_acc: 0.8095\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4161 - acc: 0.8181 - val_loss: 0.4229 - val_acc: 0.8095\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4159 - acc: 0.8234 - val_loss: 0.4219 - val_acc: 0.8095\n",
      "Training with parameters {'batch_size': 5000, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_68\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_69 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_194 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_126 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_195 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_127 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_196 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.7686 - acc: 0.4361 - val_loss: 0.7655 - val_acc: 0.3892\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.7418 - acc: 0.4375 - val_loss: 0.7380 - val_acc: 0.3990\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7219 - acc: 0.4486 - val_loss: 0.7150 - val_acc: 0.4302\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.7024 - acc: 0.4822 - val_loss: 0.6963 - val_acc: 0.4745\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6896 - acc: 0.5289 - val_loss: 0.6815 - val_acc: 0.5649\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6793 - acc: 0.5793 - val_loss: 0.6698 - val_acc: 0.6486\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6719 - acc: 0.6132 - val_loss: 0.6606 - val_acc: 0.6700\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6646 - acc: 0.6338 - val_loss: 0.6530 - val_acc: 0.6831\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6593 - acc: 0.6340 - val_loss: 0.6464 - val_acc: 0.6650\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6531 - acc: 0.6367 - val_loss: 0.6407 - val_acc: 0.6552\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6484 - acc: 0.6353 - val_loss: 0.6357 - val_acc: 0.6535\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6449 - acc: 0.6316 - val_loss: 0.6309 - val_acc: 0.6634\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6412 - acc: 0.6406 - val_loss: 0.6266 - val_acc: 0.6732\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6360 - acc: 0.6444 - val_loss: 0.6226 - val_acc: 0.6897\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6308 - acc: 0.6592 - val_loss: 0.6188 - val_acc: 0.6962\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6269 - acc: 0.6683 - val_loss: 0.6153 - val_acc: 0.6962\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6211 - acc: 0.6811 - val_loss: 0.6120 - val_acc: 0.7011\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6191 - acc: 0.6907 - val_loss: 0.6088 - val_acc: 0.7094\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6142 - acc: 0.7002 - val_loss: 0.6059 - val_acc: 0.7126\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6090 - acc: 0.7081 - val_loss: 0.6031 - val_acc: 0.7094\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6060 - acc: 0.7112 - val_loss: 0.6006 - val_acc: 0.6979\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6007 - acc: 0.7192 - val_loss: 0.5981 - val_acc: 0.6979\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5962 - acc: 0.7128 - val_loss: 0.5953 - val_acc: 0.6962\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5927 - acc: 0.7170 - val_loss: 0.5929 - val_acc: 0.6962\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5877 - acc: 0.7159 - val_loss: 0.5902 - val_acc: 0.6946\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.5842 - acc: 0.7209 - val_loss: 0.5870 - val_acc: 0.6962\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5821 - acc: 0.7143 - val_loss: 0.5839 - val_acc: 0.6995\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5778 - acc: 0.7258 - val_loss: 0.5802 - val_acc: 0.7044\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5731 - acc: 0.7256 - val_loss: 0.5761 - val_acc: 0.7044\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5695 - acc: 0.7245 - val_loss: 0.5716 - val_acc: 0.7094\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5635 - acc: 0.7327 - val_loss: 0.5670 - val_acc: 0.7126\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5596 - acc: 0.7353 - val_loss: 0.5624 - val_acc: 0.7241\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5580 - acc: 0.7427 - val_loss: 0.5580 - val_acc: 0.7241\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5526 - acc: 0.7451 - val_loss: 0.5541 - val_acc: 0.7291\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5506 - acc: 0.7440 - val_loss: 0.5508 - val_acc: 0.7307\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5478 - acc: 0.7466 - val_loss: 0.5480 - val_acc: 0.7274\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5415 - acc: 0.7528 - val_loss: 0.5451 - val_acc: 0.7274\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5394 - acc: 0.7486 - val_loss: 0.5421 - val_acc: 0.7274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5371 - acc: 0.7542 - val_loss: 0.5387 - val_acc: 0.7258\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5308 - acc: 0.7542 - val_loss: 0.5344 - val_acc: 0.7373\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5311 - acc: 0.7562 - val_loss: 0.5303 - val_acc: 0.7422\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5285 - acc: 0.7595 - val_loss: 0.5261 - val_acc: 0.7455\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5230 - acc: 0.7628 - val_loss: 0.5224 - val_acc: 0.7471\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5206 - acc: 0.7674 - val_loss: 0.5197 - val_acc: 0.7488\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5182 - acc: 0.7672 - val_loss: 0.5169 - val_acc: 0.7521\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5135 - acc: 0.7676 - val_loss: 0.5138 - val_acc: 0.7504\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5126 - acc: 0.7692 - val_loss: 0.5107 - val_acc: 0.7504\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5085 - acc: 0.7672 - val_loss: 0.5075 - val_acc: 0.7553\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5054 - acc: 0.7741 - val_loss: 0.5045 - val_acc: 0.7619\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5034 - acc: 0.7696 - val_loss: 0.5015 - val_acc: 0.7619\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5022 - acc: 0.7699 - val_loss: 0.4988 - val_acc: 0.7685\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4985 - acc: 0.7780 - val_loss: 0.4969 - val_acc: 0.7668\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4975 - acc: 0.7763 - val_loss: 0.4949 - val_acc: 0.7668\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4940 - acc: 0.7814 - val_loss: 0.4931 - val_acc: 0.7668\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4897 - acc: 0.7856 - val_loss: 0.4918 - val_acc: 0.7718\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4926 - acc: 0.7778 - val_loss: 0.4898 - val_acc: 0.7701\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4863 - acc: 0.7871 - val_loss: 0.4870 - val_acc: 0.7734\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4884 - acc: 0.7780 - val_loss: 0.4833 - val_acc: 0.7734\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4836 - acc: 0.7847 - val_loss: 0.4804 - val_acc: 0.7783\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4806 - acc: 0.7843 - val_loss: 0.4788 - val_acc: 0.7800\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4830 - acc: 0.7869 - val_loss: 0.4775 - val_acc: 0.7833\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4799 - acc: 0.7896 - val_loss: 0.4765 - val_acc: 0.7800\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4758 - acc: 0.7882 - val_loss: 0.4741 - val_acc: 0.7833\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4730 - acc: 0.7887 - val_loss: 0.4711 - val_acc: 0.7915\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4708 - acc: 0.7920 - val_loss: 0.4693 - val_acc: 0.7898\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4696 - acc: 0.7902 - val_loss: 0.4686 - val_acc: 0.7915\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4684 - acc: 0.7905 - val_loss: 0.4689 - val_acc: 0.7931\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4668 - acc: 0.7938 - val_loss: 0.4690 - val_acc: 0.7865\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4702 - acc: 0.7913 - val_loss: 0.4680 - val_acc: 0.7865\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4658 - acc: 0.7944 - val_loss: 0.4653 - val_acc: 0.7947\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4637 - acc: 0.7931 - val_loss: 0.4623 - val_acc: 0.7947\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4617 - acc: 0.7973 - val_loss: 0.4606 - val_acc: 0.7947\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4594 - acc: 0.7975 - val_loss: 0.4596 - val_acc: 0.7947\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4599 - acc: 0.7953 - val_loss: 0.4573 - val_acc: 0.7947\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4577 - acc: 0.8028 - val_loss: 0.4558 - val_acc: 0.7947\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4574 - acc: 0.7962 - val_loss: 0.4547 - val_acc: 0.7947\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4571 - acc: 0.7986 - val_loss: 0.4533 - val_acc: 0.7947\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4550 - acc: 0.8035 - val_loss: 0.4522 - val_acc: 0.7964\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4561 - acc: 0.7980 - val_loss: 0.4516 - val_acc: 0.7964\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4525 - acc: 0.8019 - val_loss: 0.4527 - val_acc: 0.7964\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4510 - acc: 0.8017 - val_loss: 0.4529 - val_acc: 0.7964\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4538 - acc: 0.8013 - val_loss: 0.4515 - val_acc: 0.7964\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4489 - acc: 0.8041 - val_loss: 0.4498 - val_acc: 0.7964\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4472 - acc: 0.8004 - val_loss: 0.4476 - val_acc: 0.7997\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4474 - acc: 0.8030 - val_loss: 0.4453 - val_acc: 0.8013\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4491 - acc: 0.8008 - val_loss: 0.4445 - val_acc: 0.8030\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4471 - acc: 0.8097 - val_loss: 0.4450 - val_acc: 0.7997\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4462 - acc: 0.8064 - val_loss: 0.4468 - val_acc: 0.8013\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4433 - acc: 0.8046 - val_loss: 0.4485 - val_acc: 0.7997\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4445 - acc: 0.8019 - val_loss: 0.4478 - val_acc: 0.7980\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4402 - acc: 0.8037 - val_loss: 0.4442 - val_acc: 0.8013\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4388 - acc: 0.8055 - val_loss: 0.4408 - val_acc: 0.8030\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4413 - acc: 0.8013 - val_loss: 0.4403 - val_acc: 0.8046\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4428 - acc: 0.8048 - val_loss: 0.4419 - val_acc: 0.8046\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4375 - acc: 0.8082 - val_loss: 0.4439 - val_acc: 0.7997\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4378 - acc: 0.8086 - val_loss: 0.4446 - val_acc: 0.7997\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4406 - acc: 0.8086 - val_loss: 0.4423 - val_acc: 0.8013\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4368 - acc: 0.8088 - val_loss: 0.4381 - val_acc: 0.8079\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4340 - acc: 0.8106 - val_loss: 0.4351 - val_acc: 0.8095\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4382 - acc: 0.8108 - val_loss: 0.4344 - val_acc: 0.8095\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4356 - acc: 0.8103 - val_loss: 0.4356 - val_acc: 0.8095\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4331 - acc: 0.8068 - val_loss: 0.4367 - val_acc: 0.8112\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4317 - acc: 0.8079 - val_loss: 0.4364 - val_acc: 0.8112\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4302 - acc: 0.8112 - val_loss: 0.4356 - val_acc: 0.8112\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4319 - acc: 0.8159 - val_loss: 0.4351 - val_acc: 0.8112\n",
      "Training with parameters {'batch_size': 5000, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_69\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_70 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_197 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_128 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_198 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_129 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_199 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.8479 - acc: 0.4353 - val_loss: 0.8175 - val_acc: 0.3875\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.7953 - acc: 0.4393 - val_loss: 0.7675 - val_acc: 0.3892\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.7560 - acc: 0.4441 - val_loss: 0.7281 - val_acc: 0.3924\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.7233 - acc: 0.4565 - val_loss: 0.6981 - val_acc: 0.4795\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6992 - acc: 0.5003 - val_loss: 0.6761 - val_acc: 0.6174\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6840 - acc: 0.5627 - val_loss: 0.6604 - val_acc: 0.6371\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6751 - acc: 0.5886 - val_loss: 0.6492 - val_acc: 0.6305\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6664 - acc: 0.5966 - val_loss: 0.6406 - val_acc: 0.6223\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6597 - acc: 0.5968 - val_loss: 0.6335 - val_acc: 0.6190\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6535 - acc: 0.5979 - val_loss: 0.6271 - val_acc: 0.6207\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6476 - acc: 0.5972 - val_loss: 0.6210 - val_acc: 0.6305\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6397 - acc: 0.6158 - val_loss: 0.6153 - val_acc: 0.6404\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6315 - acc: 0.6358 - val_loss: 0.6103 - val_acc: 0.6732\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6227 - acc: 0.6667 - val_loss: 0.6060 - val_acc: 0.7291\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6159 - acc: 0.6977 - val_loss: 0.6026 - val_acc: 0.7258\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6097 - acc: 0.7083 - val_loss: 0.5999 - val_acc: 0.7061\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6078 - acc: 0.7008 - val_loss: 0.5974 - val_acc: 0.6979\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6020 - acc: 0.6995 - val_loss: 0.5944 - val_acc: 0.7044\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5969 - acc: 0.7022 - val_loss: 0.5905 - val_acc: 0.7028\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5929 - acc: 0.7044 - val_loss: 0.5858 - val_acc: 0.7011\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5857 - acc: 0.7035 - val_loss: 0.5804 - val_acc: 0.6995\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5823 - acc: 0.7106 - val_loss: 0.5746 - val_acc: 0.7077\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5785 - acc: 0.7198 - val_loss: 0.5687 - val_acc: 0.7126\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5698 - acc: 0.7260 - val_loss: 0.5633 - val_acc: 0.7209\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5682 - acc: 0.7261 - val_loss: 0.5586 - val_acc: 0.7176\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5624 - acc: 0.7323 - val_loss: 0.5542 - val_acc: 0.7209\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5597 - acc: 0.7409 - val_loss: 0.5501 - val_acc: 0.7225\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5557 - acc: 0.7402 - val_loss: 0.5460 - val_acc: 0.7274\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5498 - acc: 0.7429 - val_loss: 0.5422 - val_acc: 0.7323\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5442 - acc: 0.7479 - val_loss: 0.5387 - val_acc: 0.7291\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5416 - acc: 0.7475 - val_loss: 0.5355 - val_acc: 0.7258\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5372 - acc: 0.7491 - val_loss: 0.5321 - val_acc: 0.7241\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5333 - acc: 0.7493 - val_loss: 0.5278 - val_acc: 0.7258\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5292 - acc: 0.7486 - val_loss: 0.5228 - val_acc: 0.7406\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5235 - acc: 0.7552 - val_loss: 0.5170 - val_acc: 0.7438\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5203 - acc: 0.7590 - val_loss: 0.5114 - val_acc: 0.7406\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5178 - acc: 0.7634 - val_loss: 0.5065 - val_acc: 0.7438\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5108 - acc: 0.7690 - val_loss: 0.5019 - val_acc: 0.7504\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5082 - acc: 0.7707 - val_loss: 0.4979 - val_acc: 0.7537\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5059 - acc: 0.7683 - val_loss: 0.4952 - val_acc: 0.7553\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5024 - acc: 0.7701 - val_loss: 0.4935 - val_acc: 0.7521\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4950 - acc: 0.7774 - val_loss: 0.4909 - val_acc: 0.7537\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4941 - acc: 0.7732 - val_loss: 0.4862 - val_acc: 0.7570\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4887 - acc: 0.7809 - val_loss: 0.4805 - val_acc: 0.7685\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4859 - acc: 0.7842 - val_loss: 0.4754 - val_acc: 0.7718\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4846 - acc: 0.7847 - val_loss: 0.4714 - val_acc: 0.7734\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4819 - acc: 0.7880 - val_loss: 0.4688 - val_acc: 0.7767\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4787 - acc: 0.7885 - val_loss: 0.4671 - val_acc: 0.7800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4754 - acc: 0.7840 - val_loss: 0.4645 - val_acc: 0.7800\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4740 - acc: 0.7889 - val_loss: 0.4625 - val_acc: 0.7816\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4698 - acc: 0.7916 - val_loss: 0.4617 - val_acc: 0.7800\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4684 - acc: 0.7915 - val_loss: 0.4608 - val_acc: 0.7865\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4677 - acc: 0.7880 - val_loss: 0.4589 - val_acc: 0.7865\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4644 - acc: 0.7953 - val_loss: 0.4553 - val_acc: 0.7865\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4609 - acc: 0.7975 - val_loss: 0.4517 - val_acc: 0.7931\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4603 - acc: 0.7953 - val_loss: 0.4499 - val_acc: 0.7964\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4581 - acc: 0.7980 - val_loss: 0.4499 - val_acc: 0.7947\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4589 - acc: 0.8015 - val_loss: 0.4491 - val_acc: 0.7947\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4538 - acc: 0.8009 - val_loss: 0.4463 - val_acc: 0.7997\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4547 - acc: 0.7993 - val_loss: 0.4442 - val_acc: 0.8013\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4533 - acc: 0.8041 - val_loss: 0.4429 - val_acc: 0.8013\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4483 - acc: 0.8033 - val_loss: 0.4415 - val_acc: 0.8013\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4480 - acc: 0.8035 - val_loss: 0.4392 - val_acc: 0.8046\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4472 - acc: 0.8050 - val_loss: 0.4366 - val_acc: 0.8046\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4427 - acc: 0.8101 - val_loss: 0.4350 - val_acc: 0.8062\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4477 - acc: 0.8039 - val_loss: 0.4348 - val_acc: 0.8046\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4426 - acc: 0.8022 - val_loss: 0.4352 - val_acc: 0.8013\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4420 - acc: 0.8044 - val_loss: 0.4336 - val_acc: 0.8062\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4409 - acc: 0.8066 - val_loss: 0.4304 - val_acc: 0.8095\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4411 - acc: 0.8079 - val_loss: 0.4286 - val_acc: 0.8112\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4403 - acc: 0.8092 - val_loss: 0.4303 - val_acc: 0.8062\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4357 - acc: 0.8097 - val_loss: 0.4365 - val_acc: 0.8030\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4381 - acc: 0.8064 - val_loss: 0.4372 - val_acc: 0.8013\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4375 - acc: 0.8077 - val_loss: 0.4306 - val_acc: 0.8062\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4364 - acc: 0.8097 - val_loss: 0.4263 - val_acc: 0.8062\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4330 - acc: 0.8082 - val_loss: 0.4244 - val_acc: 0.8095\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4339 - acc: 0.8146 - val_loss: 0.4247 - val_acc: 0.8095\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4291 - acc: 0.8130 - val_loss: 0.4259 - val_acc: 0.8062\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4312 - acc: 0.8113 - val_loss: 0.4251 - val_acc: 0.8112\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4296 - acc: 0.8088 - val_loss: 0.4224 - val_acc: 0.8095\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4267 - acc: 0.8137 - val_loss: 0.4193 - val_acc: 0.8161\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4270 - acc: 0.8141 - val_loss: 0.4191 - val_acc: 0.8161\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4246 - acc: 0.8144 - val_loss: 0.4213 - val_acc: 0.8079\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4266 - acc: 0.8161 - val_loss: 0.4231 - val_acc: 0.8079\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4250 - acc: 0.8159 - val_loss: 0.4259 - val_acc: 0.8079\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4242 - acc: 0.8150 - val_loss: 0.4264 - val_acc: 0.8095\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4235 - acc: 0.8128 - val_loss: 0.4213 - val_acc: 0.8095\n"
     ]
    }
   ],
   "source": [
    "mlp_perf_metrics3 = make_MLP_exp(X_train_vect, y_train.values, params3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>0.821018</td>\n",
       "      <td>0.368339</td>\n",
       "      <td>0.404629</td>\n",
       "      <td>41</td>\n",
       "      <td>0.822062</td>\n",
       "      <td>0.852886</td>\n",
       "      <td>0.704615</td>\n",
       "      <td>0.909507</td>\n",
       "      <td>0.771693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.834337</td>\n",
       "      <td>0.817734</td>\n",
       "      <td>0.390761</td>\n",
       "      <td>0.413336</td>\n",
       "      <td>46</td>\n",
       "      <td>0.817466</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.876289</td>\n",
       "      <td>0.775444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.833425</td>\n",
       "      <td>0.819376</td>\n",
       "      <td>0.391539</td>\n",
       "      <td>0.416402</td>\n",
       "      <td>100</td>\n",
       "      <td>0.809586</td>\n",
       "      <td>0.808219</td>\n",
       "      <td>0.726154</td>\n",
       "      <td>0.871707</td>\n",
       "      <td>0.764992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.837803</td>\n",
       "      <td>0.819376</td>\n",
       "      <td>0.380349</td>\n",
       "      <td>0.408097</td>\n",
       "      <td>43</td>\n",
       "      <td>0.818779</td>\n",
       "      <td>0.829225</td>\n",
       "      <td>0.724615</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.773399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.824849</td>\n",
       "      <td>0.821018</td>\n",
       "      <td>0.401368</td>\n",
       "      <td>0.414844</td>\n",
       "      <td>55</td>\n",
       "      <td>0.812213</td>\n",
       "      <td>0.820423</td>\n",
       "      <td>0.716923</td>\n",
       "      <td>0.883162</td>\n",
       "      <td>0.765189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.801314</td>\n",
       "      <td>0.794746</td>\n",
       "      <td>0.457994</td>\n",
       "      <td>0.461529</td>\n",
       "      <td>88</td>\n",
       "      <td>0.787919</td>\n",
       "      <td>0.774790</td>\n",
       "      <td>0.709231</td>\n",
       "      <td>0.846506</td>\n",
       "      <td>0.740562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.812078</td>\n",
       "      <td>0.807882</td>\n",
       "      <td>0.427804</td>\n",
       "      <td>0.429437</td>\n",
       "      <td>63</td>\n",
       "      <td>0.810243</td>\n",
       "      <td>0.821747</td>\n",
       "      <td>0.709231</td>\n",
       "      <td>0.885452</td>\n",
       "      <td>0.761354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.823390</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.415859</td>\n",
       "      <td>0.421892</td>\n",
       "      <td>131</td>\n",
       "      <td>0.806960</td>\n",
       "      <td>0.807958</td>\n",
       "      <td>0.718462</td>\n",
       "      <td>0.872852</td>\n",
       "      <td>0.760586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.815910</td>\n",
       "      <td>0.811166</td>\n",
       "      <td>0.431858</td>\n",
       "      <td>0.435102</td>\n",
       "      <td>105</td>\n",
       "      <td>0.800394</td>\n",
       "      <td>0.801394</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.869416</td>\n",
       "      <td>0.751634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.812808</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.423468</td>\n",
       "      <td>0.421335</td>\n",
       "      <td>87</td>\n",
       "      <td>0.807617</td>\n",
       "      <td>0.810435</td>\n",
       "      <td>0.716923</td>\n",
       "      <td>0.875143</td>\n",
       "      <td>0.760816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_layers  layer_conf      lr  dropout  batch_size  accuracy_train  \\\n",
       "0          2  [512, 256]  0.0001      0.1        1000        0.842000   \n",
       "1          2  [256, 256]  0.0001      0.1        1000        0.834337   \n",
       "2          1       [128]  0.0001      0.1        1000        0.833425   \n",
       "3          2  [512, 128]  0.0001      0.1        1000        0.837803   \n",
       "4          2   [256, 32]  0.0001      0.1        1000        0.824849   \n",
       "..       ...         ...     ...      ...         ...             ...   \n",
       "65         1       [128]  0.0001      0.1        5000        0.801314   \n",
       "66         2  [512, 128]  0.0001      0.1        5000        0.812078   \n",
       "67         2   [256, 32]  0.0001      0.1        5000        0.823390   \n",
       "68         2  [128, 128]  0.0001      0.1        5000        0.815910   \n",
       "69         2  [256, 256]  0.0001      0.1        5000        0.812808   \n",
       "\n",
       "    accuracy_val  loss_train  loss_val  epochs  accuracy  precision    recall  \\\n",
       "0       0.821018    0.368339  0.404629      41  0.822062   0.852886  0.704615   \n",
       "1       0.817734    0.390761  0.413336      46  0.817466   0.816327  0.738462   \n",
       "2       0.819376    0.391539  0.416402     100  0.809586   0.808219  0.726154   \n",
       "3       0.819376    0.380349  0.408097      43  0.818779   0.829225  0.724615   \n",
       "4       0.821018    0.401368  0.414844      55  0.812213   0.820423  0.716923   \n",
       "..           ...         ...       ...     ...       ...        ...       ...   \n",
       "65      0.794746    0.457994  0.461529      88  0.787919   0.774790  0.709231   \n",
       "66      0.807882    0.427804  0.429437      63  0.810243   0.821747  0.709231   \n",
       "67      0.809524    0.415859  0.421892     131  0.806960   0.807958  0.718462   \n",
       "68      0.811166    0.431858  0.435102     105  0.800394   0.801394  0.707692   \n",
       "69      0.809524    0.423468  0.421335      87  0.807617   0.810435  0.716923   \n",
       "\n",
       "    specificity  f1_score  \n",
       "0      0.909507  0.771693  \n",
       "1      0.876289  0.775444  \n",
       "2      0.871707  0.764992  \n",
       "3      0.888889  0.773399  \n",
       "4      0.883162  0.765189  \n",
       "..          ...       ...  \n",
       "65     0.846506  0.740562  \n",
       "66     0.885452  0.761354  \n",
       "67     0.872852  0.760586  \n",
       "68     0.869416  0.751634  \n",
       "69     0.875143  0.760816  \n",
       "\n",
       "[70 rows x 15 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_perf_metrics3.to_csv('mlp_perf_metrics3.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze top `batch_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.842182</td>\n",
       "      <td>0.822660</td>\n",
       "      <td>0.366579</td>\n",
       "      <td>0.415356</td>\n",
       "      <td>49</td>\n",
       "      <td>0.823375</td>\n",
       "      <td>0.834798</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.892325</td>\n",
       "      <td>0.779327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>0.821018</td>\n",
       "      <td>0.368339</td>\n",
       "      <td>0.404629</td>\n",
       "      <td>41</td>\n",
       "      <td>0.822062</td>\n",
       "      <td>0.852886</td>\n",
       "      <td>0.704615</td>\n",
       "      <td>0.909507</td>\n",
       "      <td>0.771693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.839080</td>\n",
       "      <td>0.824302</td>\n",
       "      <td>0.371704</td>\n",
       "      <td>0.408708</td>\n",
       "      <td>126</td>\n",
       "      <td>0.821405</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.733846</td>\n",
       "      <td>0.886598</td>\n",
       "      <td>0.778140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3500</td>\n",
       "      <td>0.840358</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.370699</td>\n",
       "      <td>0.409668</td>\n",
       "      <td>88</td>\n",
       "      <td>0.820749</td>\n",
       "      <td>0.827826</td>\n",
       "      <td>0.732308</td>\n",
       "      <td>0.886598</td>\n",
       "      <td>0.777143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.840540</td>\n",
       "      <td>0.821018</td>\n",
       "      <td>0.378525</td>\n",
       "      <td>0.409411</td>\n",
       "      <td>47</td>\n",
       "      <td>0.820092</td>\n",
       "      <td>0.821918</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.880871</td>\n",
       "      <td>0.777958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_layers  layer_conf      lr  dropout  batch_size  accuracy_train  \\\n",
       "7          2  [512, 256]  0.0001      0.1        1250        0.842182   \n",
       "0          2  [512, 256]  0.0001      0.1        1000        0.842000   \n",
       "41         2  [256, 256]  0.0001      0.1        3000        0.839080   \n",
       "42         2  [512, 256]  0.0001      0.1        3500        0.840358   \n",
       "14         2  [512, 256]  0.0001      0.1        1500        0.840540   \n",
       "\n",
       "    accuracy_val  loss_train  loss_val  epochs  accuracy  precision    recall  \\\n",
       "7       0.822660    0.366579  0.415356      49  0.823375   0.834798  0.730769   \n",
       "0       0.821018    0.368339  0.404629      41  0.822062   0.852886  0.704615   \n",
       "41      0.824302    0.371704  0.408708     126  0.821405   0.828125  0.733846   \n",
       "42      0.816092    0.370699  0.409668      88  0.820749   0.827826  0.732308   \n",
       "14      0.821018    0.378525  0.409411      47  0.820092   0.821918  0.738462   \n",
       "\n",
       "    specificity  f1_score  \n",
       "7      0.892325  0.779327  \n",
       "0      0.909507  0.771693  \n",
       "41     0.886598  0.778140  \n",
       "42     0.886598  0.777143  \n",
       "14     0.880871  0.777958  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics3.nlargest(5, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.842182</td>\n",
       "      <td>0.822660</td>\n",
       "      <td>0.366579</td>\n",
       "      <td>0.415356</td>\n",
       "      <td>49</td>\n",
       "      <td>0.823375</td>\n",
       "      <td>0.834798</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.892325</td>\n",
       "      <td>0.779327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.839080</td>\n",
       "      <td>0.824302</td>\n",
       "      <td>0.371704</td>\n",
       "      <td>0.408708</td>\n",
       "      <td>126</td>\n",
       "      <td>0.821405</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.733846</td>\n",
       "      <td>0.886598</td>\n",
       "      <td>0.778140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.840540</td>\n",
       "      <td>0.821018</td>\n",
       "      <td>0.378525</td>\n",
       "      <td>0.409411</td>\n",
       "      <td>47</td>\n",
       "      <td>0.820092</td>\n",
       "      <td>0.821918</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.880871</td>\n",
       "      <td>0.777958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.829228</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.395538</td>\n",
       "      <td>0.411314</td>\n",
       "      <td>105</td>\n",
       "      <td>0.818779</td>\n",
       "      <td>0.815878</td>\n",
       "      <td>0.743077</td>\n",
       "      <td>0.875143</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.837256</td>\n",
       "      <td>0.821018</td>\n",
       "      <td>0.376550</td>\n",
       "      <td>0.407989</td>\n",
       "      <td>82</td>\n",
       "      <td>0.820092</td>\n",
       "      <td>0.823024</td>\n",
       "      <td>0.736923</td>\n",
       "      <td>0.882016</td>\n",
       "      <td>0.777597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_layers  layer_conf      lr  dropout  batch_size  accuracy_train  \\\n",
       "7          2  [512, 256]  0.0001      0.1        1250        0.842182   \n",
       "41         2  [256, 256]  0.0001      0.1        3000        0.839080   \n",
       "14         2  [512, 256]  0.0001      0.1        1500        0.840540   \n",
       "55         2  [256, 256]  0.0001      0.1        4000        0.829228   \n",
       "35         2  [512, 256]  0.0001      0.1        3000        0.837256   \n",
       "\n",
       "    accuracy_val  loss_train  loss_val  epochs  accuracy  precision    recall  \\\n",
       "7       0.822660    0.366579  0.415356      49  0.823375   0.834798  0.730769   \n",
       "41      0.824302    0.371704  0.408708     126  0.821405   0.828125  0.733846   \n",
       "14      0.821018    0.378525  0.409411      47  0.820092   0.821918  0.738462   \n",
       "55      0.816092    0.395538  0.411314     105  0.818779   0.815878  0.743077   \n",
       "35      0.821018    0.376550  0.407989      82  0.820092   0.823024  0.736923   \n",
       "\n",
       "    specificity  f1_score  \n",
       "7      0.892325  0.779327  \n",
       "41     0.886598  0.778140  \n",
       "14     0.880871  0.777958  \n",
       "55     0.875143  0.777778  \n",
       "35     0.882016  0.777597  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics3.nlargest(5, 'f1_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join and plot both experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both experiment results based on batch size and join\n",
    "mlp_perf_metrics2 = pd.read_csv('mlp_perf_metrics2.csv', sep=';')\n",
    "mlp_perf_metrics3 = pd.read_csv('mlp_perf_metrics3.csv', sep=';')\n",
    "\n",
    "mlp_perf_metrics23 = pd.concat((mlp_perf_metrics2, mlp_perf_metrics3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.857325</td>\n",
       "      <td>0.806240</td>\n",
       "      <td>0.339870</td>\n",
       "      <td>0.431074</td>\n",
       "      <td>11</td>\n",
       "      <td>0.814839</td>\n",
       "      <td>0.808874</td>\n",
       "      <td>0.736025</td>\n",
       "      <td>0.872582</td>\n",
       "      <td>0.770732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.851487</td>\n",
       "      <td>0.819376</td>\n",
       "      <td>0.346702</td>\n",
       "      <td>0.423231</td>\n",
       "      <td>13</td>\n",
       "      <td>0.814183</td>\n",
       "      <td>0.804384</td>\n",
       "      <td>0.740683</td>\n",
       "      <td>0.868032</td>\n",
       "      <td>0.771221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.849115</td>\n",
       "      <td>0.811166</td>\n",
       "      <td>0.348551</td>\n",
       "      <td>0.424856</td>\n",
       "      <td>32</td>\n",
       "      <td>0.812213</td>\n",
       "      <td>0.796358</td>\n",
       "      <td>0.746894</td>\n",
       "      <td>0.860068</td>\n",
       "      <td>0.770833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.866083</td>\n",
       "      <td>0.807882</td>\n",
       "      <td>0.317675</td>\n",
       "      <td>0.425181</td>\n",
       "      <td>14</td>\n",
       "      <td>0.814183</td>\n",
       "      <td>0.799337</td>\n",
       "      <td>0.748447</td>\n",
       "      <td>0.862344</td>\n",
       "      <td>0.773055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.857508</td>\n",
       "      <td>0.821018</td>\n",
       "      <td>0.324476</td>\n",
       "      <td>0.429557</td>\n",
       "      <td>22</td>\n",
       "      <td>0.816152</td>\n",
       "      <td>0.802326</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.864619</td>\n",
       "      <td>0.775281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.801314</td>\n",
       "      <td>0.794746</td>\n",
       "      <td>0.457994</td>\n",
       "      <td>0.461529</td>\n",
       "      <td>88</td>\n",
       "      <td>0.787919</td>\n",
       "      <td>0.774790</td>\n",
       "      <td>0.709231</td>\n",
       "      <td>0.846506</td>\n",
       "      <td>0.740562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.812078</td>\n",
       "      <td>0.807882</td>\n",
       "      <td>0.427804</td>\n",
       "      <td>0.429437</td>\n",
       "      <td>63</td>\n",
       "      <td>0.810243</td>\n",
       "      <td>0.821747</td>\n",
       "      <td>0.709231</td>\n",
       "      <td>0.885452</td>\n",
       "      <td>0.761354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.823390</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.415859</td>\n",
       "      <td>0.421892</td>\n",
       "      <td>131</td>\n",
       "      <td>0.806960</td>\n",
       "      <td>0.807958</td>\n",
       "      <td>0.718462</td>\n",
       "      <td>0.872852</td>\n",
       "      <td>0.760586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.815910</td>\n",
       "      <td>0.811166</td>\n",
       "      <td>0.431858</td>\n",
       "      <td>0.435102</td>\n",
       "      <td>105</td>\n",
       "      <td>0.800394</td>\n",
       "      <td>0.801394</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.869416</td>\n",
       "      <td>0.751634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.812808</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.423468</td>\n",
       "      <td>0.421335</td>\n",
       "      <td>87</td>\n",
       "      <td>0.807617</td>\n",
       "      <td>0.810435</td>\n",
       "      <td>0.716923</td>\n",
       "      <td>0.875143</td>\n",
       "      <td>0.760816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  n_layers  layer_conf      lr  dropout  batch_size  \\\n",
       "0            0         2  [512, 256]  0.0001      0.1         NaN   \n",
       "1            1         2  [256, 256]  0.0001      0.1         NaN   \n",
       "2            2         1       [128]  0.0001      0.1         NaN   \n",
       "3            3         2  [512, 128]  0.0001      0.1         NaN   \n",
       "4            4         2   [256, 32]  0.0001      0.1         NaN   \n",
       "..         ...       ...         ...     ...      ...         ...   \n",
       "65          65         1       [128]  0.0001      0.1      5000.0   \n",
       "66          66         2  [512, 128]  0.0001      0.1      5000.0   \n",
       "67          67         2   [256, 32]  0.0001      0.1      5000.0   \n",
       "68          68         2  [128, 128]  0.0001      0.1      5000.0   \n",
       "69          69         2  [256, 256]  0.0001      0.1      5000.0   \n",
       "\n",
       "    accuracy_train  accuracy_val  loss_train  loss_val  epochs  accuracy  \\\n",
       "0         0.857325      0.806240    0.339870  0.431074      11  0.814839   \n",
       "1         0.851487      0.819376    0.346702  0.423231      13  0.814183   \n",
       "2         0.849115      0.811166    0.348551  0.424856      32  0.812213   \n",
       "3         0.866083      0.807882    0.317675  0.425181      14  0.814183   \n",
       "4         0.857508      0.821018    0.324476  0.429557      22  0.816152   \n",
       "..             ...           ...         ...       ...     ...       ...   \n",
       "65        0.801314      0.794746    0.457994  0.461529      88  0.787919   \n",
       "66        0.812078      0.807882    0.427804  0.429437      63  0.810243   \n",
       "67        0.823390      0.809524    0.415859  0.421892     131  0.806960   \n",
       "68        0.815910      0.811166    0.431858  0.435102     105  0.800394   \n",
       "69        0.812808      0.809524    0.423468  0.421335      87  0.807617   \n",
       "\n",
       "    precision    recall  specificity  f1_score  \n",
       "0    0.808874  0.736025     0.872582  0.770732  \n",
       "1    0.804384  0.740683     0.868032  0.771221  \n",
       "2    0.796358  0.746894     0.860068  0.770833  \n",
       "3    0.799337  0.748447     0.862344  0.773055  \n",
       "4    0.802326  0.750000     0.864619  0.775281  \n",
       "..        ...       ...          ...       ...  \n",
       "65   0.774790  0.709231     0.846506  0.740562  \n",
       "66   0.821747  0.709231     0.885452  0.761354  \n",
       "67   0.807958  0.718462     0.872852  0.760586  \n",
       "68   0.801394  0.707692     0.869416  0.751634  \n",
       "69   0.810435  0.716923     0.875143  0.760816  \n",
       "\n",
       "[140 rows x 16 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mlp_summ_performance(perf_df, x_ax_param: str, group_param: str):\n",
    "\n",
    "    fig, ax = plt.subplots(3,2, figsize=(16,20))\n",
    "    groups = perf_df[group_param].drop_duplicates()\n",
    "\n",
    "    # Plot accuracy\n",
    "    for gr in groups:\n",
    "        ax[0, 0].plot(perf_df.loc[perf_df[group_param] == gr, x_ax_param].values,\n",
    "                      perf_df.loc[perf_df[group_param] == gr, 'accuracy'].values,\n",
    "                      label=gr)\n",
    "    ax[0, 0].set_xlabel(x_ax_param)\n",
    "    ax[0, 0].set_ylabel('Accuracy')\n",
    "    ax[0, 0].legend(title=group_param)\n",
    "    ax[0, 0].set_title('Accuracy evolution against ' + x_ax_param)\n",
    "    \n",
    "    # Plot precision\n",
    "    for gr in groups:\n",
    "        ax[0, 1].plot(perf_df.loc[perf_df[group_param] == gr, x_ax_param].values,\n",
    "                      perf_df.loc[perf_df[group_param] == gr, 'precision'].values,\n",
    "                      label=gr)\n",
    "    ax[0, 1].set_xlabel(x_ax_param)\n",
    "    ax[0, 1].set_ylabel('Precision')\n",
    "    ax[0, 1].legend(title=group_param)\n",
    "    ax[0, 1].set_title('Precision evolution against ' + x_ax_param)\n",
    "    \n",
    "    # Plot recall\n",
    "    for gr in groups:\n",
    "        ax[1, 0].plot(perf_df.loc[perf_df[group_param] == gr, x_ax_param].values,\n",
    "                      perf_df.loc[perf_df[group_param] == gr, 'recall'].values,\n",
    "                      label=gr)\n",
    "    ax[1, 0].set_xlabel(x_ax_param)\n",
    "    ax[1, 0].set_ylabel('Recall')\n",
    "    ax[1, 0].legend(title=group_param)\n",
    "    ax[1, 0].set_title('Recall evolution against ' + x_ax_param)\n",
    "    \n",
    "    # Plot specificity\n",
    "    for gr in groups:\n",
    "        ax[1, 1].plot(perf_df.loc[perf_df[group_param] == gr, x_ax_param].values,\n",
    "                      perf_df.loc[perf_df[group_param] == gr, 'specificity'].values,\n",
    "                      label=gr)\n",
    "    ax[1, 1].set_xlabel(x_ax_param)\n",
    "    ax[1, 1].set_ylabel('Specificity')\n",
    "    ax[1, 1].legend(title=group_param)\n",
    "    ax[1, 1].set_title('Specificity evolution against ' + x_ax_param)\n",
    "\n",
    "    # Plot f1 score\n",
    "    for gr in groups:\n",
    "        ax[2, 0].plot(perf_df.loc[perf_df[group_param] == gr, x_ax_param].values,\n",
    "                      perf_df.loc[perf_df[group_param] == gr, 'f1_score'].values,\n",
    "                      label=gr)\n",
    "    ax[2, 0].set_xlabel(x_ax_param)\n",
    "    ax[2, 0].set_ylabel('F1 Score')\n",
    "    ax[2, 0].legend(title=group_param)\n",
    "    ax[2, 0].set_title('F1 Score evolution against ' + x_ax_param)\n",
    "    \n",
    "    plt.show()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8QAAAR9CAYAAACQzo7EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdd1yVZRvA8d/FHiIbHKi4d5pallbuhg1bamWZ5tuwXbaHWdm2+b5mWy01s7S0bDmyoVZuC/fCgYLgYIPA/f7xPMABAY8KHMb1/XzOx3OeeT2H47nP9dxLjDEopZRSSimllFK1jZurA1BKKaWUUkoppVxBE2KllFJKKaWUUrWSJsRKKaWUUkoppWolTYiVUkoppZRSStVKmhArpZRSSimllKqVNCFWSimllFJKKVUraUKslIuJSG8R2Xsa+z8hIh+VZ0wVTUR+EJGbK+E8S0TkP5VwnhEi8kc5Hm+YiPxcXsdTSqmKJCIxItL7BNs0FpFUEXGvpLBOi5bNFXoeLZtVlaIJsSo39hfcYRHxdnUsNVVJBbQx5kVjTIUXLOXJGHOJMWbq6RyjvAu6Eo4/RUTGV9Txy2KMmW6MudAV51ZK1RwisktEMuxENN7+XqtT3ucxxrQ3xiw5wTa7jTF1jDG55X1+V9OyuZCWzao60oRYlQsRiQbOBwxwRSWf26Myz6eUUkpVI5cbY+oAXYBuwFPFNxCL/iZUStVK+uWnystw4E9gClCkuY2I+IrI6yISKyJHReQPEfG1150nIstE5IiI7BGREfbyIs1pit9xFBEjIneJyFZgq73sbfsYySKySkTOd9je3W6+tF1EUuz1jURkooi8XizeeSLyQEkXKSJtRGSBiBwSkc0iMsRe3l1EDjg2BRORq0Rkvf3cW0TeEpE4+/FWaTXp9rW1cHg9RUTGi4g/8APQwL7bnyoiDURknIhMc9j+Crv52hH7fWzrsG6XiDwkIuvtv8UXIuJTShzNRWSxiCSJSKKITBeRIIf1XURkjf1+fmkfa7y9LlhEvhORg3arge9EJMph34K/b/7fVkQm2NvuFJFLHLYdISI77PPsFKvJUlvgPeBc+304UtI12JqLyN/252KuiIQ4HPtL++92VER+E5H29vLbgGHAI/bxv7WXNxKROfZ1JYnI/4q9ZyVeQ2lKujbH98R+nh9D/uOYiEyx1wWKyMcisl9E9tmfk2rRHFEpVbmMMfuwypAOUPA9/IKILAXSgWYn+k4RkVtFZKP9nbVBRLrYy3eJSH/7+dkistL+zo0XkTfs5dF2+eZhv24gVnl7SES2icitDucZJyKzRORT+1wxItKttGsTLZu1bNayWZ0GTYhVeRkOTLcfF4lIpMO6CUBXoAcQAjwC5IlIE6xC5L9AONAZWHsS57wS6A60s1+vsI8RAswAvnQoUB4ErgcGAnWBW7B+AEwFrhf7zriIhAH97f2LsAu9Bfa6COA64F0RaWeM+QtIA/o67HKDw3GeBM6x4+sEnE0Jd+nLYoxJAy4B4uxmZ3WMMXHFYmwFfA7cj/Wefg98KyJeDpsNAS4GmgJnACNKOaUALwENgLZAI2CcfR4v4GusGyAh9jmvctjXDZgMNAEaAxlAkQKqmO7AZiAMeBX4WCz+wDvAJcaYAKzP0FpjzEbgDmC5/T4ElXZgrM/mLUB9IMc+Xr4fgJZYf8/VWJ9fjDEf2M9ftY9/uV2YfQfEAtFAQ2Dmia6htKBKu7bi2xlj8mOog/V3OAh8Ya+eYl9TC+BM4EKgWjXRU0pVDhFphFUGrnFYfBNwGxCA9d02hVK+U0RkMFYZMByrHL0CSCrhVG8Dbxtj6gLNgVmlhDQT2ItVxlwLvCgijmXoFfY2QcA8SilDtGzWshktm9XpMsboQx+n9QDOA44BYfbrTcAD9nM3rC/cTiXs9zjwdSnHXAL8x+H1COAPh9cG6HuCuA7nnxfry3BQKdttBAbYz+8Gvi9lu6HA78WWvQ88Yz8fD3xiPw/AKoSb2K+3AwMd9rsI2GU/7w3sLXZtLRxeTwHGl7StvWwcMM1+/jQwy2GdG7AP6G2/3gXc6LD+VeA9J//OVwJr7OcX2McVh/V/5MdZwr6dgcMl/X3tv+02h3V+9ntQD/AHjgDXAL7FjlnkM1HG5+hlh9ftgGzAvYRtg+zzBhZ/3+3X52IVeB4l7FvqNZQR20ldG+ALrAIetV9HAlmO+2Ld9PnFmb+nPvShj5r/sL/zU+3vmljg3fzvDPv78TmHbcv8TgF+Au4r4zz97ee/Ac9i/yZw2Cba/l70wEricoEAh/UvAVPs5+OAhQ7r2gEZpZxby2Ytm7Vs1sdpPbSGWJWHm4GfjTGJ9usZFDabDgN8sAqd4hqVstxZexxf2M2NNtpNbI4Agfb5T3SuqcCN9vMbgc9K2a4J0N1u7nTEPscwrMIBrOu+WqzmVlcDq40xsfa6Blg/RvLF2svKW5HzGGPysN6nhg7bHHB4ng6UOMCKiESKyEy7uU8yMI3C97MBsM/Y3/S2PQ77+onI+2I1k0/G+oEUVEaToYKYjDHp9tM6xrrzPhTrjvN+EZkvIm1Ku/hSOH5OYgFPIEysZvQvi9WMPhnrBwkO11hcIyDWGJNzMtdQWlCncG0fA5uNMa/Yr5vY17Lf4fP4PtYddaWUynelMSbIGNPEGHOnMSbDYZ3j9+OJvlOcLbNHAa2ATSKyQkQuK2GbBsAhY0yKw7JYyi6rfKTkMUO0bNayWctmdVo0IVanRay+wEOAXmL19zgAPAB0EpFOQCKQidVsqrg9pSwH6w6un8PreiVsU/CFL1Z/4UfsWIKN1UznKFbTohOdaxowyI63LfBNKdvtAX61f1jkP+oYY0YDGGM2YH2pX0LRJlkAcVhfkvka28tKkk7p124oW5Hz2M2CGmHdMT5ZL9rn62ispm83Uvh+7gcaFmt21Mjh+RigNdDd3veC/JBONghjzE/GmAFYzao2AR/mr3LyEI5xNcZqzZCI9TcahNVEPhCr9sIxxuLH3wM0LuUH2Skp49qKEJHHsH5gjioWTxZWLUz+57GuMaZ9ecWnlKrxiidOZX2nlFWOFh7QmK3GmOuxEoBXgK/sZqiO4oAQEQlwWNaYUyurtGzWslnLZnVaNCFWp+tKrGZP7bCa3nTGSip/B4bbd0E/Ad4Qa5AJdxE5175TOx3oLyJDRMRDREJFpLN93LVYd3T9xBrEYlTxExcTgNVf4yDgISJjsfo45fsIeF5EWtr9X84QkVAAY8xerP7HnwGzi909d/Qd0EpEbhIRT/txljgMjIFV0N6HVch86bD8c+ApEQkXq5/yWKxEvCRrgRvs9+pioJfDunggVEQCS9l3FnCpiPQTEU+swi8LWFbK9mUJwGpqd1REGgIPO6xbjvV3v9v+2w3C6nvluG8GcESsgTKeOYXz598JH2T/mMqy48mzV8cDUcX6YJXkRhFpJyJ+wHPAV8aa9iPAPmYS1o+cF4vtFw80c3j9N9aPjZdFxF9EfESk56lclxPX5rjdJcC9wFWOn01jzH7gZ+B1EakrIm5iDbbSq/gxlFLqRJz4TvkIeEhEutrlaAuxxgIpQkRuFJFwu/zPH1SpyHebMWYPVrn0kv1degZWOV9auVgWLZsLadmsZbM6BZoQq9N1MzDZWPMLHsh/YA3SMMy+Y/cQ8A9W0nkI646xmzFmN9YAH2Ps5WuxBrUAeBOrP0k8VpPm6SeI4yfgR2AL1p3gTIo2x3kDq0D6GUjGauLi67B+KtCR0ptLYzftuhBrwI44rGY4rwCOI1J+jlVILnZoQg5WH6aVwHr7vVhtLyvJfcDlWD8khuFQY22M2WSfY4fdFKdI0y5jzGasu8X/xbrTejnWlBvZpV1XGZ7FmqbjKDAfmONwnmyspmej7DhvxPpRkmVv8hbW+5uINfr4j6dwfrC+ox7Eer8PYb23o+11i4EY4ICIJJa8O2D9Tadg/b18sAowgE+xPiv7gA12nI4+BtrZ7/M3dkF9OdYgGbuxBoMZeorXdaJrczQUaxCWjVI4muV79rrhgJcd/2HgK6w72kopdSpK/U4xxnwJvICVXKZglU0hJRzjYiBGRFKxBti6rpQbzddj1f7FYQ0E9YwxZuHJBqxls5bNaNmsTpMU7WagVO0kIhdg3RVuYvQ/xSkRkb+wBgGZ7OpYlFJKKaVls1LO0BpiVevZzZfuAz7SZNh5ItJLROrZzbJuxpom4lTvNiullFLqNGnZrNTJK7cO6EpVR3Yfo5XAOmCki8OpblpjNUP3B3YA19p9Z5QDu9lgSS4xxvxeqcEopZSq6bRsdoKWzcqRNplWSimllFJKKVUraZNppZRSSimllFK1kibESimllFJKKaVqpVrRhzgsLMxER0e7OgyllFI1xKpVqxKNMeGujqM607JZKaVUeTrVsrlWJMTR0dGsXLnS1WEopZSqIUQk1tUxVHdaNiullCpPp1o2a5NppZRSSimllFK1UoUmxCJysYhsFpFtIvJYCesbi8gvIrJGRNaLyEB7+QARWSUi/9j/9nXYZ4l9zLX2I6Iir0EppZRSSimlVM1UYU2mRcQdmAgMAPYCK0RknjFmg8NmTwGzjDGTRKQd8D0QDSQClxtj4kSkA/AT0NBhv2HGGG1npZRSSimllFLqlFVkDfHZwDZjzA5jTDYwExhUbBsD1LWfBwJxAMaYNcaYOHt5DOArIt4VGKtSSimllFJKqVqmIhPihsAeh9d7KVrLCzAOuFFE9mLVDt9TwnGuAVYbY7Iclk22m0s/LSJS0slF5DYRWSkiKw8ePHjKF6GUUkoppZRSqmZy9aBa1wNTjDFRwEDgMxEpiElE2gOvALc77DPMGNMRON9+3FTSgY0xHxhjuhljuoWH68wYSimllFJKKaWKqsiEeB/QyOF1lL3M0ShgFoAxZjngA4QBiEgU8DUw3BizPX8HY8w++98UYAZW02yllFJKKaWUUuqkVGRCvAJoKSJNRcQLuA6YV2yb3UA/ABFpi5UQHxSRIGA+8JgxZmn+xiLiISL5CbMncBnwbwVeg1JKKaWUUkqpGqrCEmJjTA5wN9YI0RuxRpOOEZHnROQKe7MxwK0isg74HBhhjDH2fi2AscWmV/IGfhKR9cBarBrnDyvqGpRSSimllFJK1VwVNu0SgDHme6zBshyXjXV4vgHoWcJ+44HxpRy2a3nGqJRSSimllFKqdnL1oFpKKaWUUkoppZRLaELspH2bD7P8m+3k5RlXh6KUUkop5bQVuw6xevdhV4ehlFJVkibETtq/4yirf4zFaEKslFJKqWrkuW838NCX61wdhlJKVUmaEDtJxPpXE2KllFJKVScHkjPZcTCNnYlprg5FKaWqHE2InSRuVkZsNB9WSimlVDWRk5tHYmoWAIs3Jbg4GqWUqno0IXaS2FXEWkOslFJKqeoiKS274Gb+oo3xrg1GKaWqIE2IneRWUEOsCXFlyM0z7NKmXUoppdRpiU/OBKBFRB3+3nmI5MxjLo5IKaWqFk2InVXQh9i1YdQGf+88xBX/+4PeE5Ywd+0+V4ejlFJKVVvxyVZz6evPbkxOnuH3LYkujkgppaoWTYidpDXEFW/PoXTumr6aIe8v53BaNm3qBfDUN/+y70iGq0NTSimlqqWEFKuG+OIO9Qjy89Rm00opVYwmxE4qGGVa8+Fyl5aVw2s/baLfG7+yeFMCD/RvxaIxvfngpm7k5RkemrVO539WSimlTkF8chYiEBngTZ/WEfyyOYFcLVOVUqqAJsTO0kG1yl1enuHLlXvoPWEJE3/ZzqUd67P4oV7c178lvl7uNA7145nL27N8RxKfLN3p6nCVUkqpaudgSiah/t54uLvRt00Eh9OPsXbPYVeHpZRSVYaHqwOoLrTJdPlasesQz327gX/2HaVzoyDev6krXRoHH7fd4G5RLNwYz6s/bua8lmG0qVfXBdEqpZRS1VN8chaRdb0BuKBVOB5uwsKNCXRtEuLiyJRSqmrQGmJn2U2mtenu6dl3JIO7Z6xm8HvLOZiSxVtDOzNndI8Sk2Gwprt66eqO1PX15P6Za8nKya3kiJVSSqnqKz45k8i6PgAE+npyVnQIizfqfMRKKZVPE2In5dcQo/nwKUnPzuGNnzfTd8ISFm6M575+LVn8UC+uPLNh4XtbitA63rx6bUc2HUjhjZ+3VFLESimlVPWXkJJFRIB3wet+bSPYHJ/CnkPpLoxKKaWqDk2InSRaQ3xK8vIMc1bvpc+EJbyzeBsXta/H4jG9eWBAK/y8nG+x37dNJMO6N+aD33fw546kCoxYKaWUqhlycvNITM0iwq4hBujXNhKAxZu0llgppUATYqeJ1hCftFWxh7lq0jIenLWOenV9mD36XN65/kwaBPme0vGevLQt0aH+jJm1juTMY+UcrVJKKVWzJKVlYwxFaoibhvnTLMyfRZoQK6UUoAmx08SuItYa4hOLO5LBfTPXcM2kZew/ksHrgzvx9Z09T3sADz8vD94c2pkDyZmMmxtTTtEqpZRSNVN8sjUHcaRDDTFA3zYR/Lk9ibSsHFeEpZRSVYomxE4SHWXaKbNX7aXv60v48d8D3NO3Bb881JtrukadsJ+wszo3CuKevi2Ys2Yf89fvL5djKqWUqp4OH0hjx5qDrg6jyopPzgIoGGU6X7+2kWTn5vH71kRXhKWUUlWKJsROyu9DbPJcG0dVN/GXbTQPr8OiMb0Yc2Fr/L3Lf2avu/q0oFOjIJ74+h8OHM0s9+MrpZSqHrauiOeH9//Rm9WlSEixysiIgKI1xN2igwnw8WDxpnhXhKWUUlWKJsRO0hriE0vLymFnUhoXta9HVLBfhZ3H092Nt4Z2Jjsnj4e/WqfN2JVSqrYSHd+jLPHJWYhAWB2vIss93d3o1SqcxZsOahmqlKr1NCF2UmENsRYcpdl0IBljoH2DuhV+rqZh/jx1WVt+35rIp8t3Vfj5lFJKVT2aD5ftYEomof7eeLgf/3OvX9sIElOzWL/vqAsiU0qpqkMTYicV1hC7OJAqLCYuGYB2lZAQA9xwdmP6tongpR82sTU+pVLOqZRSqgrSwrlE8clZx/Ufzte7VQRuAos3arNppVTtpgmxk/JHmdYm06WL2ZdMiL8X9YqNZllRRISXr+mIv7cHD8xaS3aOdvCucbYvhux0V0ehlKqitIa4bPHJmceNMJ0v2N+Lrk2CdfolpVStpwmxk3RQrROL2X+U9g3qFtw8qAwRAT68dHVH/t2XzNuLtlTaeVUlOBwLn10FM4ZAdpqro1FKVUWaEZcpISWryBzExfVtE0lMXLIOUKmUqtU0IXaSDqpVtmO5eWw5kFppzaUdXdS+HkO6RTFpyXZW7jpU6edXFSQ32/p31+8wY6gmxUqp4xTkw1o2HycnN4/E1Cwiymi11b9tBACLdLRppVQtpgmxk3RQrbJtS0glOzePdvUrPyEGGHt5e6KC/Xhg1lpSMo+5JAZVQdpcBrFL7aRYm08rpUqgRfNxktKyMYYya4hbRNShUYgvizdqs2mlVO2lCbGTdFCtsuUPqNW+QaBLzl/H24M3hnRi3+EMnv9ug0tiUBWk/VVw1ft2UjxEk2KlVIGC8T1cHEdVFJ9sNYMurQ8xWO9fvzaR/LEtkYzs3MoKTSmlqhRNiJ1UUOhqDXGJYuKO4uvpTtMwf5fF0C06hNG9mzNr5V5+ijngsjhUBThjCFz5nsuTYmMMqVk5Ljm3UqoMWjQfJz45C6DUUabz9WsbQVZOHsu2J1ZGWEopVeV4uDqA6iK/hnje22sZOLojTTuFuziiqmVDXDJt6gfg7lZ5A2qV5L5+rfh1y0Een/MPZzYOIiKgcka8VpWg01Dr32/ugM+HwvVfgJdfuZ/GGENCShY7E9PYlZjGrqR0+980YpPSyTiWy3ktwhjRI5o+bSJc/plXqlbTPsSlSkixaohPVA6e3TQEfy93Fm1KoF/byMoITSmlqhRNiJ3kOHDy0YMZrgukCjLGsGF/MoM6N3B1KHh5uPHW0M5c+s4fPPrVej4ZcValjnqtKlh+Uvz17fD5dXD9zFNKivOT3vxEd2diOrFJaexMLEx683m5u9EoxJemYf70bBGGr6c7s1fv5T+frqRxiB/Dz23C4G6NCPT1LK+rVEo5SdDv99LEJ2chAmF1vMrcztvDnfNbhrN4YwLmSqNlplKq1tGE2EniUAuUq/PdFrHnUAYpmTku6z9cXIuIAB6/pA3jvt3A9L92c+M5TVwdkipPnYYCBr6+A2ZeD9d9XmJSbIzhoF3TG5uUzs6kwhrf2KQ00h36y3m6C41C/GgaaiW90aF+RIf5Ex3qT4Mg3+Nqge/v35KfN8QzZekuxs/fyBsLtnB1l4aM6BFNi4iAin4HlFI2ye/4pRXEx0lIziTU3xsP9xP3juvXNoIfYw4QE5dMh4ZVoyxXSqnKUqEJsYhcDLwNuAMfGWNeLra+MTAVCLK3ecwY872IDABeBryAbOBhY8xie5+uwBTAF/geuM9UQlspxxumuTla8jqKiTsKQHsXTLlUmuHnRrNoUwIvzN9Ij+ahNAuv4+qQVHnqdB0Yg/lmNMemDWXd+e+z80guu5KK1vg6Jr0ebkLjECvRPbdZKE3D/GgS6k/TsJKT3rJ4uLsxsGN9Bnasz7/7jjJ12S5mrdzLtD93c37LMEb2jKZ3qwjctDm1UpVCm0wfLyEl64T9h/P1bh2BCCzelKAJsVKq1qmwhFhE3IGJwABgL7BCROYZYxyHAH4KmGWMmSQi7bAS3GggEbjcGBMnIh2An4CG9j6TgFuBv+ztLwZ+qKjrKLgerSEu1Yb9ybi7Ca0iq07NmJubMGFwJy566zce+GItX43ugacTd8lV1WOMITEly05004hNSmNXYjo7E6PolDeaF2InkblzME8fe4hcN28ah/jRJNSPc5qF0DTM30p6Q/1pEOTjVE3JyerQMJDXBnfisUvaMHPFHj5bHsstU1bSJNSPm8+N5tpuUdT10ebUSlWEggEvNR8+TnxyZpkjTDsKD/CmU1QQizYlcG+/lhUcmVJKVS0VWUN8NrDNGLMDQERmAoMAx4TYAPnVioFAHIAxZo3DNjGAr4h4AyFAXWPMn/YxPwWupBIS4q1//UJW8td4BQwjTxPiImLikmkRXgcfT3dXh1JEZF0fXryqI3dOX83g95bTvkFdmtrNYKPD/GgU4oe3R9WKWRU6mJJFOPDI7PV8mVXYJNrDzWreHB3qh3fTG1mWVZ/zYsayNvoTPIfNxMPHNSOdh9bx5q4+Lbjtgmb8FHOAKUt38dx3G3j9581c2zWK4T2iaV5bWirsWQF/vgtXTgJPHdhOKVdISMmi40nU9vZvG8GEn7eQkJKpA1IqpWqVikyIGwJ7HF7vBboX22Yc8LOI3AP4A/1LOM41wGpjTJaINLSP43jMhiXsU+6y0o5icg8CRptMFxMTd5SezcNcHUaJBnasz6MXt+HHmAN8t34/RzOOFaxzE2gQ5FuQIEeH5ifL/jQO8cPLQ2uUXSU7J49nv43hf0D3piG0b9muoE9vw2DfYrX97aFFKL5z74JZw+D6z8HT11Wh4+nuxmVnNOCyMxrwz96jTFm2i8//3sPU5bH0ahXOiJ7R9GoZXrObU+9eDjFzoGFX6HG3q6NRNVn+fyMtlovIyc0jMTWLCCdriAH6tolkws9bWLLpIEPOalSB0SmlVNXi6kG1rgemGGNeF5Fzgc9EpIMxJg9ARNoDrwAXnuyBReQ24DaAxo0bn3aghaMuGm0y7SAxNYv45CzaVaH+w8WN7t2c0b2bA3AkPduaTsdudrvLHmhp3to4kjML55fNT5bza5SbhPpZz8P8aRSsyXJFe/H7jWzYnwzecG3XRtCxadk7nDnM+nfuXTDzBrhuhkuT4nwdowJ5fYjVnPrzv3cz7c9YRk5eQdMwf24+twnXdI0ioCY3p/59AnS5CXy0T6KqGPlFs9GMuIjE1GyMgYgA5/oQA7StH0CDQB8WbYrXhFgpVatUZEK8D3D8Ro2ylzkahdUHGGPMchHxAcKABBGJAr4GhhtjtjscM+oEx8Q+3gfABwDdunU77ZJS3AqHstSEuNCGuGSAKp0QOwry8+LMxl6c2Tj4uHWH07LZmZRmT79jzT0bm5TG3LX7jkuWGwb7FqlRzh+gSZPl0zdvXRxTlu3ioa4NrQ4TzjpzGGBg7t0wc5idFFeNZn/hAd7c268ld/Rqzo8xB5iydCfjvt3AhJ+3cG3XKG7uEU3TMNc09a5QGYdh6dvQb6yrI1E1VkFGrBzkz0HsbB9isG78920bwZzV+8g8llvlukEppVRFqciEeAXQUkSaYiWt1wE3FNtmN9APmCIibQEf4KCIBAHzsUadXpq/sTFmv4gki8g5WINqDQf+W4HXUKAwIUb7EDuIsRPi9vWrfw1QsL8Xwf5edCmWLBtjOJx+rKA2OX/qnl1JaXyzdh8pxZLlqGC/ghplaxRjqzl2lCbLJ7Q1PoXHZq+nW5Ngbr8g9OQSYoAzb7RG15l3j0NNcdVIisGaJ/uKTg24olMD1u05wtRlu5j+VyxTlu2iT+twRvRsyvktwmpOc+rWA2H5u3DWrVC3vqujUTVQQQ2xJsRFxCdnATg9ynS+fm0imfbnbv7aeYhercIrIjSllKpyKiwhNsbkiMjdWCNEuwOfGGNiROQ5YKUxZh4wBvhQRB7Aur87whhj7P1aAGNFJL9q4UJjTAJwJ4XTLv1AJQyoBcWbTGvJmy8m7ihRwb4E+tnNPnf/CXm5EN3TtYGVIxEhxN+LkDKS5Z12ohyblMbOJKt2+evV+0jJKkyW3d2EhkG+Vo1yqB83ntOEllVoZG5XS8k8xu3TVuHn5c7EYV3wzN5z4p1K0uUm698qmhTn69QoiDeGduaxgW34/K89TPsrlps/+Ztm4f6M6BHN1V2iqOPt6l4tp6nPk7B1Afz6Clz+lqujUTWQ1JB7R+Utv4b4ZAfHOrd5KD6ebizeGK8JsVKq1qjQX1vGmO+xpkZyXDbW4fkG4LjMyRgzHhhfyjFXAh3KN9ITE3ftQ1ySDXHJRecfXjwe9q+De1ZDnZpfmDomy12bHJ8sH0rLLtJf2Zo2KJ0VOw8x/58DfH1nDxqF+Mdq1lEAACAASURBVJVy9NrDGMOjs9cTm5TOtFHdrWZ+iadxwIKk+G74YhgMnV4lk2KwfrDe178lo3s35/t/9jN52S7Gzo3htR83M7hbI4af24To6tqcOjgaut0CKz6Cc++CMJ3ORZWzgmmX9Ea1o/jkLEQgrI7XSe3n4+nOeS3CWbgxgXFXGIfKAKWUqrm0/aaT3MR+q4wmxPnSsnLYmZRGO8fm0iYPspJh8XOuC6yKEBFC63jTtUkI13SNYsyFrfnfDV349p7z+PaenmTn5DJi8t8cTT924oPVcB//sZPv/znAIxe15tzmoeVz0C43wRX/hW0L4Ysb4Vhm+Ry3gnh5uHHlmQ2Ze1dPvr6zB/3aRvDZn7vo8/oSRk1Zwe9bD1bPH/0XPGwNcLb4eVdHomqyavhfoyIlJGcS6u99SnOv92sbwb4jGWyJT62AyJRSqurRhNhJ4qY1xMVtOpCMMRStIc63+jOIW3P8cgVAi4gAPhjejT2HMrjts5Vk5eS6OiSX+XvnIV76YRMXtY/ktguale/BuwyHy9+BbQuqRVKc78zGwbx13ZksfbQv9/Ztybq9R7jp478Z8OZvfPZnLGkOTfGrvDrh0OMe2DAX9q5ydTSqhtE+xCVLSMk66f7D+fq2iQBg0ab48gxJKaWqLE2IneQ4qJb2IbYUDKjVsFhCXO8M8AuFHx7TXyllOKdZKK8NPoO/dh7i4S/Xk5dX+96rhORM7pqxmsYhfrw2uFPFNM/renNhUjzrJsjJKv9zVJCIuj48MKAVSx/ryxtDOuHr6c7T3/zLOS8tYvx3G9idlO7qEJ1z7l3gFwYLn9HvBFVB9HPlKD4586RGmHYUWdeHjg0DWbQxoZyjUkqpqqmaj9hSedx02qXjxOxLJsTfi3oOhW5uRi5IHdz7jYVv74V/voIzBrswyqptUOeG7D2cwWs/bSYq2JdHLm7j6pAqzbHcPO6esYbUzBymjepO3Yqcj7frzYCBb++zaoqHTgOPU6s9cQVvD3eu7hLFVWc2ZPXuI0xZtospy3bx8dKd9G8VxogGyXi26urqMAvUO5ROkdnfvQOg16Pww8OwbRG07O+q0FQZRORi4G2sgTA/Msa8XGx9Y2AqEGRv85g9Vojj+g3AOGPMhEqKGdD7LMUlpGTRseGpz/7Qt00E7yzeyqG0bEL8T64fslJKVTeaEDvJcZRpnXbJsmF/Mu3q1y1Sq7f3mwRyM/JoeusNyMqPYcFYaDMQvKrpoECV4M7ezdl7OJ13l2wnKtiPG7o3PvFONcCrP27i712HeGtoZ1rXq4TRtruOsH41f3d/tUyKwfoe6tokmK5NgjkwsC3T/4qlwaQnCdn4L5+ddxEzwi50dYgA3Ooey5OesHbvETo3r2Mt7DoC/pwIC8dB877gpg2UqhIRcQcmAgOAvcAKEZlnD36Z7ylgljFmkoi0wxo0M9ph/RtU0swPBXTMp+Pk5OaRmJpFxCnWEIPVj/jtRVtZsjmBq7tElWN0SilV9WhC7CQ398JSV2uIrdq9zQdSGNkzumBZXmYm6XszIQ9SFi6m7iWvwicXwe9vQL+nXRdsFSciPD+oA3FHMnl67r/UD/KhT+sIV4dVob7/Zz8f/r6T4ec24cozG1beibuNtP797n744iYY+lm1S4rz1Qv04da8XezbGAMCt8Qs4NJPHgFP119P/Zj1sAYen72er+6rh7+3B3h4Qd+nYfYo+PcrOGOIq8NURZ0NbDPG7AAQkZnAIKwa33wGyO8jEwjE5a8QkSuBnUBapURbTLUccK6CJKZmYwxEBJz6d0GHBoFEBHizaKMmxEqpmk8TYidJkSbTWvBuS0glOzePdg4DamX++y/kgbhD4rvvEvDN10jHwbDsv3DmjRDS1IURV20e7m5MHNaFoe8v567pq5l1+7l0OI3mblXZ9oOpPPzlOjo3CuLJS9tWfgDdRgIGvnsAZg2HIZ9Wy6Q4a8dO9j/xBD4towgNX8++ZSG0nf0CIU+96+rQIN5qEbL7cAbj52/kpas7WsvbXw1L37ZGnG43qFq+7zVYQ8Bx8u+9QPdi24wDfhaRewB/oD+AiNQBHsWqXX6orJOIyG3AbQCNG59+a5iCFkpaLBfIn4P4VPsQA7i5CX3bRDB//X6yc/Lw8tAWHUqpmku/4ZzkmBDn5WoNccGAWg0Kk7b0VasBiOgTStaWLaQsWAj9nwU3d1igNcQnUsfbg09GnEWQrye3TFnBviMZrg6p3KVl5XDHZ6vw9nTn3WFd8PZwd00g3W6BS9+ALT9aSXE1GmgLIC8tjb333oN4exP1wFACGmXiF+XGwVmLyYnb6erwCozsGc3nf+9m4QZ7tFo3N+j/DBzZDSsnuzY4dSquB6YYY6KAgcBnIuKGlSi/aYw54Tw9xpgPjDHdjDHdwsNPf656HWX6ePHJ1vfZqY4yna9vmwhSsnJYuetQeYSllFJVlibETnLsQ5x7TBPiDXHJ+Hq60zSssG9w+upVeIV6EtwlEK/oaBInTsQE1IfzH4SN38KOX10YcfUQWdeHySPPJiM7l5GT/+ZoRs2Zo9gYw+Nz/mH7wVT+e/2ZNAjydW1AZ41ySIpvrjZJsTGG/WOfIXvHThq+PgHPsEBEIPKh+8k7BgefvMPVIRa4p28L2tavy2Nz1pOYar+/zftB0wvgt1chM9m1ASpH+4BGDq+j7GWORgGzAIwxywEfIAyrJvlVEdkF3A88ISJ3V3TAgEMfYs2I8+XXEEcEnHoNMcB5LcPw8nBjoY42rZSq4TQhdlLBKNNGp10CiIk7Spv6Abjb8zObvDwy1qzFr6E34iaE3TnaqiVeuBDOvQeCmsCPj0Fu5c6favLymPPyOOa89AyJe2Ir9dwlyTl8mMQPP2T7RRcT/9prJfZ7a10vgPdu6sqOg2mMnraK7BrSZ33qsl3MWxfHmAtb07NFmKvDsZw1Ci59Hbb8YCfF2a6O6IQOT5tO8vz5hN97L/49ehQs9+nWh+CeTTnyZyyZf3zrwggLebu789bQziRn5vDY7H+sz7sI9B8H6Umw/H+uDlEVWgG0FJGmIuIFXAfMK7bNbqAfgIi0xUqIDxpjzjfGRBtjooG3gBeNMZXyxy24Va3FcoH45CxEIKzO6Y0O7eflQY/moSzaFK99tJVSNZomxE4Se1AtY0+7VJsLB2MMG/Yn096h/3DWtm3kJSfjG2Xdka47cKBdS/wuxt0LLnoBEjbAyk8qNdZ/f13IzjUr2bPhXz595B4WfTKJjJTKr5XK3LyZ/U8/zbbefTj4+hvg7s6hjz8h4eWXS/ws9WwRxivXnMGy7Uk8Nmd9tf+8rYo9xPj5G+nfNoLRvZq7OpyizvoPDJxgJcVfVu2kOH3NGuJfeYU6ffoQetutx60PH/8e7l4Q/+xYTF7VuJHSul4Aj1zUmoUb4/lihd1FtWFXaHclLPsfpGrtU1VgjMkB7gZ+AjZijSYdIyLPicgV9mZjgFtFZB3wOTDCuPrLSfsQHychOZNQf2883E//J16/NhHEJqWzI9ElY6UppVSl0ITYSW5S2IcYIC+v9pa+ew5lkJKZU6T/cMbqNQD42QmxeHhYtcSbN1u1xG0ug6a94JcXIL1y+iNlpafxx+ef0qBVW26d+AmdBlzCugU/8PF9t7L6+7nk5lRsbbXJzSV5wQJih9/MzkFXcvTb7wgcNIim8+bSbP53BN90E4emfkrCy6+UmPBe0zWKB/q3Ys7qfby5cGuFxlqRElOzuHP6ahoE+fL6kM64uVXBeVLOvtVKijd/X2WT4pykJPbd/wCe9evT4JWXHcY1KORerwnh1w0gfU8mKR+Pd0GUJbulZ1N6tgjlue82sCv/h3XfpyEnE3591bXBqQLGmO+NMa2MMc2NMS/Yy8YaY+bZzzcYY3oaYzoZYzobY34u4RiVNgexKllCStZp9x/O17dtJACLNsaXy/GUUqoq0oTYSeJeNCGuzf2IN+w/CkC7+oU1xOmrV+EeFoZnUOHA5QW1xO9OshK+S16BrBRYXDk/1JfPnkl68lH6jrwdv7qB9LtlNMNf/S/1mrfil6kfMvXhu9mxZkW5nzf3yBGSPv6Y7QMuZN8995K9dw8RDz9EyyW/UP+5Z/Fp1QoRIfKJx+2keGqpSfG9/VowuGsU7yzayqyVe0o4W9WWk5vHPTPWcCT9GJNu7EKgr6erQypdkaR4RJVKik1ODvseHEPukSNEvfM27nXrlrpt0EOv4x3mTvz7n5N3NKkSoyydm5swYXAnPNyEB2etJSc3D8JaQNebYdVkOLTD1SGqaqpwUK3ae5O6uPjkzNMaYdpRwyBf2tQLYJH2I1ZK1WCaEDupcFAtS14t7kccE5eMu5vQul5AwbKM1Wvw69KlyPskHh6Ejb6DrE2bSFm0CCLaWs1TV02GA/9UaIyH4vay5od5dOg9gMhmLQqWhzVqwjVPPMeVj4wFk8fXLz/L7JeeIWnv6SebmVu2sH/sM2zt3YeE1ybgGRVFw/++Q4uffyZ01Cjcg4KKbF+QFN94o5UUv/LqcT/qRIQXr+7I+S3DeGLOP/y+9eBpx1mZXl+wheU7knjhqo5FWhRUWQVJ8Xz4amSVSYoPvv0O6X/9Rb1nnsGnbdlTVYmnF5GPPEBOKiQ9f1clRXhi9QN9GX9VR1bvPsKkJduthb0eBXevSrtJpmqgKtjgxNUSUrJOaw7i4vq1jWBl7GGOptecQR6VUsqRJsTOchhlGiC3Fk+9FBOXTIvwOvh4WlPmHIuP59jevfh2aAVpiUW2rXvppXg1aWL1Jc7Lgz6Pg08Q/PBYhY6CsmTqh3h4eXP+9cNh9WewcBzsWgq5xxARmnc9m5snTKT38P+wf8smpj58F4snv09GaspJncfk5pKyaBGxI0ay84pBHJ07l8DLL6Pp3G9o8ulU6g4YgHiUMN33H2/Cjl+tpPjJJwgeNoxDU6aQ8OrxA215urvx7rAutIiow+hpq9m4v3qMzPtzzAEmLdnO9Wc35tquUa4Ox3ln3wqXvAabvjs+Kc5Mhm/vgwVjYdVU2PUHJMdV6Gc5ZdEikj78kKAhQwi6+iqn9vG/YhQB7UNI+mEtxzatqrDYTtYVnRpwRacGvL1oK+v3HoGAenDOnfDvbIhb6+rwVDWUfxNWK4gtObl5JKZmEVFONcQA/dpGkptnWLJFa4mVUjVTCb/UVUmkeEJci5tMx8QdpUfzwlGCM/5YCIDfxpcgMBU6XF2wLr8vcdyjj5G6eDEB/ftD36dg/oOw4Rto79wP/JOxY/UKdq5dRa+bRuEXGGQln4e2W/9614VmvaHlhbi36E/XS6+k7fl9WDZrGmt/ms/GP5bQY/ANdBowEDf30ufIzT16lCOz53B4+nSO7duHR/36hI95kKBrr8UjOPjEQf76GmBgxHykYRcin3oSgEOTJ4MIEQ8/VKS2PcDHk8kjz+KqicsYOXkFX9/Vg/qBLp62qAy7EtMYM2sdZ0QF8szl7Vwdzsnrfpv17w8PW0nx4Cng7gn718KqKVjVUg6/wD39IKQZhDSFkOYQ2tx+3dxK+uTUqrGyY2OJe/QxfNq3J/LJJ05q38jxb5F67U3EP3UvUV8tPaXzV4TnB3Vgxa5D3P/FWubfcz6+Pe+1BttbOA6Gf+Pq8FQ1pU2mLYmp2RhDudYQd4oKItTfi8WbEhjUuWG5HVcppaoKTYidJMUG1cqtIVPhnKzE1Czik7OsEaaTtsPSt0ifOg9x98Wn56XQ60EIb11kn7qXXsrBd9/l4MR3qdOvH9J1BKycDD8/DS0vAi+/cosvN+cYSz79kOAGUZx58WWFK1pdAp1vgK0/w7aFsNGeTSSyI34tB9B/wAA69buIJdM+YfHk91m34Ad63zSK6M5dixw/a9s2Dk2bxtG58zAZGfh160bEI48Q0K9vyTXBZTmWDjOGwn8WIsFNrKTYGA59Yo3EXTwprh/oyycjzmLI+8sZOXkFX95xLgE+Va9PbkZ2LndMW4W7u/DusC4FLQmqne63AQZ+eMTqUzx4SuG64XMhONq60XJoByTtsJ4f3Aybf4Q8h6aFBclys6KJcmhzqBNZarKcl5HB3nvvQ9zdiXrnbdy8T+4Hrmfbswi9uBOJ89eTNu9j/K8YdbLvQIUI9PPk9cGduOGjv3jph408N6gDXPAQ/PQEbP8FmvdxdYiqGhEdZbqI/DmIy6sPMYC7m9C7dQQLN8aTk5tXLqNXK6VUVaIJsZMKf7PmJ8S1s/TdEJdMa9nNoO2fw+L54OZJRnpTfDs3Ra59v8R9rL7Eo9n/2OOkLlpk1RJf8gpMGQjL3oHej5VbfKu/n8fh/XFc/fizuHs4JIte/tDuCuthDMT/C1sXWMnx0rfhjzcI9w7k2la92d7qPH79fTOzX3qGZl3O4oIbRuC1dQeHp31G2rLliJcXdS+/jJAbbzxhf84ytbwI9vwJ0wfDqJ8Q32Ain34KsJNigYiHiibF7RrU5d1hXRg5ZQV3Tl/NJyPOwrMK/TgxxvDk1/+wOT6FySPOIiq4/G52uET3263Py4+PWklxt1us5W7uENzEejTvW3SfvFw4usdOlO2E+dAOSNgIm38oliz724lysyKJsgluyoHxb5G1ZQuNPngfz4anVisTOvZdjiw5j/hX36TpJTchnqc3L2l56dEijFHnNeXjP3bSt00EvbuNgj8nWbXETXtBCSNoK6VOLD45C6DcRpnO169tBLNX72VV7GG6Nwst12MrpZSraULspIIaYjsPzquNfYj3rKDxT8/xk/dvmH3+0OMecjuOJPPzywi7okeZuwZedhmJkyYV1hJH97SaS//xFnQeBkGNTju8tCOH+XPOTJp1OYumxWp2ixCBeh2tx/kPQsYR2LEEti1Ati6kRepcokOE1X5d+HPtCqau+psmiUdpk+dB5AMPEDRksHPNok8krCX0vBc+uwpm3gg3zUE8vIl8+mkADn38CSJC+JgxRZLiC1qF89JVHXlk9nqe/PofXrnmjOMGfXOV6X/tZs6afdzfvyW9W0e4Opzycc4d1r8/PgoHN514ezd3q/Y4OPr4ZDk3B5L3Fk2Uk7ZDfAxsmg951lRgR7b5cXRlEGHn+FJn/4ewcHHR2uU6EU41w3YLDCXy9uvY98ZMjrw+huDH/nty116BHr6oNX9sTeThr9bz0/0XENLnSfjmDqsrhUO3C6XKsmnpN2QenocxM10dSpWQX0McEVB+NcQA57cMw9NdWLwpQRNipVSNowmxs9xqaR9iY2Dnr/D767DzN8Ld6vKhx/Xc+uBL4BtM5rJlkJeH75ldyjyMeHgQdsdo9j/+uNWXuF8/GPC81bx0wdNFm6Oeot8/n0pO9jF6D//Pye3oGwTtr7QexpD1948cnvwhdZdu4gLjxvboQHaFB3HAy42eHnGESAZQDgkxQPR5MOhdmPMfmHsXXPUB4uZG5NNPY4wh6aOPAY5Lioec1Yi9h9N5Z/E2GgX7cU+/luUTz2lYu+cIz327gd6tw7m3r+vjKVfn3AEY+PE0WzO4exQmy/Qrui43B47uIWP5IuK/egv/tqGE9Q+HA/8WSZYB8KpT2F/5WHqZpwz4z9P4ffkNBz9fQN2RsbhHNjm9aygnPp7uvDm0M1dOXMoTc/5h0g2DkWXvwOLnoe3lVp9tpZxkTC0pk08gPjkLEQirU76tQQJ8POneNJRFmxJ4fOBptIxSSqkqSBNiJwnFEuKa3oc4Lw+2/GAlwvtWQZ16cOELDF7WnKiIcG71tRLC9NVrwM0N3zM7n/CQgZfn1xJPpE7fvkhQIzjvfljyEpx1K0T3POVwD2zbQsyShXS7/GqC6zs0L03eDyn7wad3mfubvDxSf/uNw59NI23pUsTTk7qXXUmTwYPo7B1Hwt/fs2Tpdhb9vJJ1v/5K77bQ5Oy+0PJCiOpm1QqeqjMGw9HdsOg5CGoM/cYiItSza4qTPvoYRAh/8MEiSfEDA1qx93AGry/YQsNgX67u4rqRnA+lZXPntFWEB3jz1tDOuLlVjRrrcnXOaOsG0YKnwS/sxNufLHcPciSIvRNm4BERSYNPvkLyWyLk5lifkSS7VvnQdqtm+cA/cCTWmrrIN6jEw4qbG5HPPMfOWx/m4FOjqffh9+Uf+ylq16AuYy5sxUs/bGL22kiu7T8OZgyB1VOtKdqUOoGCUabzamc3puISkjMJq+NdIf18+7aJ4LnvNhCblEaTUP9yP75SSrmKJsROkuI1xLk1tPDNzYGYr+GPNyBhg1WTddlb0PkG0nLd2fjtT1zUuXA+2YzVq/Bu1Qr3OnVOeOiCvsSPP07qL78Q0Lcv9LgX1kyDHx6F2389pcTS5OWxeMr7+AUGcc7V1xVduXCcVbPW4+6SLzc1laNzvubQ9Gkci92NR0QE4fffR9DgwXiEFjYLi2h/FYOH57JtwRf8OnsOX63NpPm27+kV+T+C6/pYTWNbXggt+lnNWU/WeQ/C4VjrBkRgI+g2EnFzs5JiY0j68CNACH/wgYIfgCLCy9ecwf6jmTw6ez316vrQo0UFJGonkJtnuG/mGhLTspl9Rw+C/KpGP9UKce6d0G0keJb/CN8mN5e4MQ+Rm5REkxkzijbLd/coHJiruNxjkJMF3qX/H/Q573KCzvkvh//YQfBfP+Hd/aJyj/9U/ef8ZizalMC4eTF0v/c8GjXuAUtegTOuK/OalAKddqm48p6D2FG/tlZCvGhjArec17RCzqGUUq6gI5c4qXCUaUuNazKdk2WN/Py/rlbzXWPg6o/g7lVWAuDhzaYDyRiDNcI0YHJySF+7Dr8uZTeXdhR4+WV4Nm7Mwf/9z5omw8sPLnwe4v+xaoVOwcY/lrB/62bOv2EE3n4OgzjtWQHrZ8K5dx2XSGTt2MmB58ez7YJexL/4Ih4hoTR843VaLFpI2B13FEmG84m7Oy0vvoERE2dw/g0j2J3TkCk7u7MkvQdZ25ZZ/R8ntIT3e8HiF2DP39YAS84QgUvfgBb9Yf4Ya8AvrNq9emPHEjR0KEkffsjBN98qMr2Il4cb793UlehQf26ftoot8Sc3j3J5eHvhFn7fmshzV7SnY1TgiXeo7iogGQZInDiRtGXLiHz6KXw7tHd+R3dPpxLH8Ocn4eYJ8c88Yc0JXkW4uwlvDOmEAA9+uY7cfuMgLcEaZEupEykYZVozYoD45MxyHWHaUZNQf1pE1GHxJp2PWClVs2hC7KTj5iGuKU2ms9Ng+UR4uxN8dz/4hsB1M2D0Mqspr3thI4KYuGTAauYIkLl5MyY9Hd+uzifEVl/iO8jasJHUX5ZYC9tdCU3Og0XPQ8bhkws/I53fZkyhXotWtL/AYQCjvDxrupw69eD8MUBhs+jdt97GjoEDOTJrFgEDBhD95ZdEfz6DugMHIp4n7rfo4eXF2YOuZdTbH9KuVz9WbUnl4x3dWNfhNfJ6PQke3vD7BPh4ALzWHL4aBetmQurBsg/s7mH1pY5sb41ovH+d9Z65uVHvGTsp/uCD45LiQF9rjmIfT3dGTl5BQnLmSb2Hp2PxpnjeWbyNwV2jGHrW6Q+MVlulLFlC4ruTCLz6aoKuvbZCzuER1ZzwIX1I25VO6qevVsg5TlVUsB/PDmrPil2HeX9nKLS5zBr9PS3R1aGpKq6whriGlMmnqSJriAH6tYngr51JJGceO/HGSilVTWhC7KxiCXFeBSXExhjycp2sVTwdGYfh11fhzQ7W/J+hLay5VW9dDG0uLXHakw1xyQT7eVI/0Lr7nLFqNcBJ1RADBF5xOZ6NG5OYX0ssYk3DlHkElrx8Usf66+tZpB0+RN8RtyOOMa+bAXGrYcCz5B5z49Bn09hxyUD23HY7mZs2EnbvPbT4ZTENXnkZ344dTuqc+fyDgrnojvu48cU3CWkQxcIv5zHt+93s7j4BHt4O13wMrS62RrD++nar9viDPvDLi0Wn3nHkHQA3zAKfIJg+BI7sARyS4iFDrKT4rbeLJMVRwX5MHnEWh9OzGTllBWlZOSUfvxztTkrn/plrad+gLs9f2aHKjHRd3WTv2UPcI4/i3bYt9cY+XaHvY/DDb+Id4kb8xKnkpR6tsPOciqvObMilHevz5oItbO3wABxLs7oQKFWG/P8tVajRg8vk5OaRmJpFRAXVEANc3KEex3IN89bGVdg5lFKqsmlC7KTiP1Irog9x2pHDTHvsfr58/skiyU6FmHo5/PICNOoOoxbAiO+gWe8yp3KJiUumfYPAgvcifc1qPBrUx7N+/ZM6tXh4EHb77WRu2FBYS1yvA3QdCX9/aM3X6oTDB+JYNf8b2l3Ql/otWxeuyEyGhc9C1FlkenRkx6WXEv/CC7gHBdFgwgRaLlpE+J134hFWPv1tI5u1YOi4l7ns/sfIykjny+efYO67kzgS3hOueg8e2gq3/gJ9nrD6SP/6KuRmWzXJJalbH4Z9aY0ePH2wNS0UdlI87hmCBg8m6f33Ofh20aS4Q8NAJt7QhU0HUrh7xmpyKnBqsMxjuYyevgqAScO64uN5GoOK1WJ5mZnsve8+AKLeeRs3n4r7IQsg3j5EjrmbYylwaPxdFXqukyUijL+yA8F+Xtz5cxo5Z9wAKz6y+tYrVRptMl0gMTUbY8p/DmJHnRsF0b5BXT5bHlvxv1OUUqqSaELspMI+xBUz7VJKUiJfPPs4CbE72LvxX7atWF6uxz9OWiJ0ugFumAmNzj7h5sdy89h8IKWw/7AxZKxajd8JplsqTeAVl+PZqBGJEycWFqp9n7JqSH98zKkfN79+9jFuHp6cf8OIoit+ew3SEkgJvo5dNw0HEZrMmE70FzMJvOxSxKv8B30SEVqfex4j33iP864bTuz6NUwZM5rfpk8mKzMTGnaBXo/AfxbCIzusZundR5d+wMh2MPQzSNoKs26CnGzrPG5u1Ht2nJUUv3d8UtynTQTPDWrPL5sP8vTcmAr7wTJ27r/ExCXz5tDONA71O/EOqkQHnn+el8EOcAAAIABJREFUrA0bafDKy3g1qpwm5/7XjKZOm0ASv1vJsW3rKuWczgr292LC4E5sTUjlnbxrQdysFhVKlUabTBeoqDmIHYkIw89twub4FP7eeajCzqOUUpVJE2JnFbSYLv8+xEfiDzDzmUdJO5zEkLEvEtwgiqVfTCPP2QGZTpW784OMb0tIJTs3r6D/8LF9ceQkJJxU/2FH4ulJ2B13kBkTQ+qSJdZCvxDo86TVxHjT/DL337VuNdtX/sU5Vw+lTnBI4YrEbfDnJA5l9Gbv02/iHR1N9BdfnHSz7lPl4eVF96uGcMtb79OmZy9WzJvNJ/ffxvpFPxX+Pf1CrGbpdcLLPliz3nDF/2Dnb/DtvQWfvcKk+ForKX7nnSKJ77DuTRjduzmf/72bSb9uL/dr/GLFbmat3Ms9fVvQr21kuR+/tjjy1VccnT2H0NF3ENCnT6WeO3L8m5AHCU/eU6nndcYFrcIZ0SOad1aks6fVcFj/hTUXs1Il0FGmC8Un/5+9846K4u7C8DO7LLD03pGiWLCDFXvvvXdN90ui0SSWRI01iSYxsSdqNNbYe+/YKyoKdikC0ntnd+f7Y1FBQEFpxn3O4QR3fzNzd7PLzDv33vdmACWbIQboXtseY7mMtRc01RsaNGj4b6ARxIUkT4a4mARxTOgTNk+fSGZaKv2m/oije02a9B9KTEgwd896F8sxigP/bEOtZxniNB91uezbCM3nWeLFObLE9T4AK3d1X3NW/uZQSoWCk2tWYGJji0fnHrmeEw9OJuK6MRG772PQsiVO69chs36DMUhviYGZOR3/N44hP/6OiY0dR5cvYv3kcTzxv1W0HdUZBC2/g5v/5uqvVoviGWpRvOxPohctyiWKv21fhW617Zh36B67b4QW18viVkgCU3f70czNgq/aVi62/b5vpN32I3zmLPS9vLD8Iv+RYCWJdo3GmLWrQeLNKFIPri/147+OiR2rUtFSn1EPmqDSMYLjM8o6JA3lFU3J9HNKI0MMINeW0r+eA4f9wokoRRNHDRo0vCXRD+HMfM3fy3zQCOJC8mwOsfhMEBeDaVFUUACbp09CpVTS/4efsKnoBkDlhl5YOVfk/NYNKBXlw8nRLywRuUyKi4V6vEvqNR8kBgbouLm98T7VWeJP1Vli72zxL9WCjj9DfBBcWJzvdjcO7yc29Akth3+EVg5XaNWtfYT8c4XYO9qYjRiOw6KFSPTKtpzXpqIbA2fMpcvYCaQnJ7FlxmT2zP+RhMjwwu+kxQSoMwS8f1bPbM4mpyiOXroslyiWSAR+7VeLBi5mfLvVl0uPY976tcSnZjJ6wzUs9LVZMLAuUonGROtNUMbHEzp2LFJzc+x++xVBWjb91xbTl6KlJxLx01xERcmbsBUFubaUBQPrEpiizR7DAfDgCASeLeuwNJRDXireeq+JSMxAEMDCoORnwQ9t5IRSFNl4KbjEj6VBg4Zi4s5u9Q3mu/vKOpJyR+FrZt9zhOzTblbKQSQGPVCkv12p6NOH99jx4w9o6erSb8oczOzsXxxLIqHpwGHs+Hk6t04cpU77zm91rOLALyyBqraGz0VQmo8P8rp13/pi3rh7d6KX/Un04iUYtGihLn9zbQHVuqkdZj1Hgf6LmcCpiQlc2LYR59oeuHq86H3OCgsh5H8TSI+WY/3dJMyGj3iruIoTQRCo6tWcivUacnXvDi7v3MbDK5exceuIR+ceONUwR/6qCxhBgG4LIDEM9o4FIzuoqB4x9UwUi6JI9NJlgIDlGHUZrI6WlOXDPOm97DyfrLvG9tFeVLIqYF6tSgWbh0J8/hc3IiKJsamsyFTibK6PfG0piDjFfy/zIKpUhE6YQFZkJM7r16FlalpmsUhMrbD6sC9hi7aTsGAiJl+XL0fnGvbGjGtXmYmHM+hgshv50R/UPfgaN3MNOXhueKnpISYyMR0LAx20pCWf63Ay16dFZUv+vRzMF60rISuFY2rQoKGYODEHqnRWG71qAEo4QywIQkdBEO4JgvBQEIRJ+TxfQRCEk4IgXBcEwVcQhM7Zj5tnP54sCMLil7Y5lb3PG9k/pVMP+ywbJqagygxC+RYz+EL8b7Nt9hR0DAwYOH1uLjH8DOc6nthXdefijk1kZZStMBBFEf+nibjbqsullQkJZDx4gJ5H3bfetyCTYTH6M9Jv336RJQaoPUjtspyQW6Cd3bSWrIx0Wo74+PmFUPq9ewT27UVGnBKHKR+XKzGck8w0SEupg1R/OFJtF57e28ehpYv5+5vTbJt7lcv7AogISERU5ZPqkMqg/1qwqAKbh+fqqRQkEmxnzsS4T2+ily4latGLr4yJnjZrRjVAJhUYufoyUUkZBQSXDPf2g0oBJhXy/DzOMuduuikG1q7ILV3yXVPsPxaVoXpvqNC4uP9XlBnRy5aRcvoMNt9NRl67dlmHg9HomcjttIlctx9lVPGV1hcXn7WoSA0na35K7wWhV+HO3rIOSUN5I/vUrBm7VPIziF9meGMnIpMyOOxXhIonDRo0lD1Rd8BvZ1lHUa4osQyxIAhSYAnQDggBrgiCsEcURf8cy6YAW0RRXCYIgjtwAHAG0oGpQI3sn5cZIori1ZKKPT9e9BCDIGaieMOS6cCbPuz+dQ5GFpb0nTobQ7P8R/8IgkDTgcPZPH0SNw7vp373Pm90vOIgJC6NpHQF1e2MAUi7cQMAuYdnsez/eZZ4ydIXWWIh772aiMcPuXXiCJ6du2Nur3bkTT5zhtCvvkKiSsZ5pBu6Q8YXS0zFiSJLie+JEK4eDESZpaJu++p4duzE5V0buLJnO0bmmaiUPbiyP4Ar+wKQG8pwdDfDqbo5FdzN0TXILgvXNVKPY1rZVj2O6ePj6mwx2aJ41iwAopcsAcDyS3VvqqOZHn+PqM+A5Rf4cM0VNn3SCD3tAr76dYeCV+6eVu/7UYxcfZledexp17+2JkP3hiSfOUv04iUY9+iOycCBZR0OoP7cWE+bRuBn3xM9bTTWy/aUdUi5kEoEfu9fh64L4vhUez92x2ciVOlcJENADf9xnv890ijiiMR0rEtwBvHLtKhshaOZnLXng+hay67UjqtBg4a3xNwNTv0E7j0159NsSjJD3AB4KIriY1EUM4FNQI+X1oiAUfbvxkAYgCiKKaIonkUtjMsFuTSAmIEyo+gZ4odXLrJr3kxMbe0YMP3nAsXwMxyq1cCljieXd28jIzWlyMcrLvzCEoAXhlqp13xASwt5rZrFsv/nvcS3bpFy+nS+a0RR5MQ/y5EbGtGozyAA4jZt4slno5EZSXDuEIfuyD+KJZ7iQhRFHl2P5N8Zl7iw8xEOVUwZNK0hTfpUQldfm+ZDRtHuky+ID79Hetwm+k2uSrsP3HF0NyPYL5ajq/xZ9e0Zts29ypX9AUQGJSIa2sGQLZCRBBv6q2cuZ/NMFBv37k30kiVELV7y/LnajiYsGuTB7dAExvx7HWV+Weh8CIlLZeym61SxNmROr5p55nFrKBxZoaGEffMNOm5u2EyfXq7eR3nLPhjXtyf21H0yrp0o63DyUMFcjyndajEjtS9CzAO4Uf5MwDSUHc/amTQV06WfIZZKBIY1cuJyYCx3wxNfv4EGDRrKB60mQ8xD9RQHDUDJCmJ74EmOf4dkP5aT6cBQQRBCUGeHCzsDZHV2ufRUoZSuLHNmiFFlocwsWob4zjlv9sz/ESvnivSf9hN6xiaF2q7JgGGkJydxdd+uIh2vOPELS0QqEahiYwio+4d13d2RyOXFdgzjHj2Q2dsTldNxOgf3zp8m7J4/TQcOR0euR8TceYRPn4F+vZo4eT1E1voTsKhUbPG8LVFPktj9+3UO/XUbLW0p3b+qQ+fRtTCxzm3yVatNR3pPnkFCVCQ7fpyEsUUy7UZVZ9S8pvSdWI96XVwQRbi8L4CtP11l9cSzHDso5UGtNaSHB8PWEaB8cXNGkEiwnZ0tihcvJmrJC1Hczt2a6d2rc+xOJDP2vn5GcYZCyf82+KBUiiwb6olcW9Nr8iaoMjMJGfsVolKJw8IFxfq9KS6s5ixDogUR0yaWdSj50q+eA1Tpgo/KjazjP0JmalmHpKGc8GLs0vvtqqVQqohOzsCqFDPEAP3rOaKjJdGMYNKg4V2iShewraM2a1VklnU05YKydkEYBPwjiqID0BlYJwj51MrmZogoijWBZtk/w/JbJAjCJ4IgXBUE4WpUVNRbB5pLd4uZRRLEu7bs5MCiX7Gv4k7fKbPQNSjA2CgfrF0rUblRU67t30VqYkJRQi42/MISqWipj65Miiozk7Rbt9Cr+/b9wzkRZDLMn2WJz5zJ9VxWejreG1Zj5VIR90ZNCB07ltjVqzEdPBjHJlFITSyg+YRijedNSU3M5OT6u2z58QoxoSm0GFSZAd/Xx7GqWYHbONeqy6CZ85BIpWyaPpFH1y4hkQhYuxjRoKsL/SbV44N5TWk7yh2HqmYE3Y7hyD6BVZGr2X65HVf/+JPIwBe9x89Fca9eRC/KLYqHN3bm42YurL0QxMozAa98LTP2+uMbksBv/WvjYqFfPG9QIRGzski5fJnoFStIu1XEUVXljIg5P5J++zZ2P/+EtrNzWYeTL1oVKmPRuykpj5JJ2jC/rMPJgyAI/NSnFstkw5ClRpB14c+yDklDOeHF1KX3WxBHJ2ciiiU/g/hlTPS06V7bjl3XQ0l8C28VDRo0lCKCAK2nqo1Ur68r62jKBSUpiEMBxxz/dsh+LCcfAlsARFG8AOgCr6wjFkUxNPu/ScBG1KXZ+a1bLopiPVEU61laWr7RC8iJIMn5VmWhzCjAnOglHlw6z8Ptq3iq70jHr6eiLS/6GCCv/kNQZGRwedeWIm9bHPiHJT7vH07380PMyEDu+ebzhwvCpIAs8eU920iOiaZFzwE8GfkBSceOY/3dZKy7uyGEXYG2P6j7a8sQZZYKnyNBrJ92gbvnn1K7tSNDZzWiRgsHJIVw37So4MyQOfMxt6/Arl9m43Mwdy+n3FCbKg1taP9hdUb90pQ+Ez3x7OyCytCeSw+qsfXnq6yedI7j//jz4GoEGWlKtSju2VMtipcufb6vyZ2q0bmmDXMO3GG/79N849l2LYSNl4L5rEVF2le3ebs3p5BkRUQSv20bIWPGcr+xF8HDRxD123wC+/UncMBAEvbtR8x8t+5kxu/cRfzmzZh//BGGbduWdTivxGzSArRNBCIXrkRMK7sWjYIwN9BhUL+BHFfWRXn6N0iNLeuQNJQHNHOIgdKbQZwfwxs7k5qpZPu1kFI/tgYNGt6QSm3AsRGc/gWy0so6mjKnJAXxFcBNEAQXQRC0gYHAy44twUAbAEEQqqEWxAWmcwVB0BIEwSL7dxnQFbhd0PoSQ8xCmfn6O6FP/HzZv3AeUXJrdpm3Z8O1/MXH6zC3d8S9RWtuHDlAYvTbZ7uLQkxyBuGJ6c/7h9N8rgOg51H8gljQ1lZniX19SfG5C0BCbBxX9+ygci0PMr+bSsbDhzgsXoTZgN4Ix6eDnQfUHlzssRQWURR5fCOKjTMvcWHHI+zdTBj0Q0Oa9nNDR0/2+h3kQN/ElAHTf6JSvYac/Gc5J1b/hUqlzLNOIhGwcTGmYTdX+s3pzqgWe2hr/AcOVokE3IrmyEo/Vn1zhp3zbxDa7GPErsOIWriY6GXLnm8/v38dPJ1MGbflBlcDcwsL/7BEvt95i8au5nzTvvKbvzmvQVQoSL16lcjf5vO4Zy8etmjB0ylTSbt5E6NOHbFfuIBKp05i/f33KOPjCfvmGx62aUvU0qUooqNLLK7iIv3uXcKnT0evYUMsx44t63BeiyDXx/qrz8hMEIn98YvXb1AGtK5qjV+1r9BWpBC676eyDkdDOeBZ9ZbqPRfEEYnqm/SlnSEGqOlgTB1HE9ZdCEJVSH8KDRo0lDGCAK2/h6SncHV1WUdT5pSYIBZFUQF8ARwG7qB2k/YTBGGmIAjds5d9DXwsCMJN4F9gpJidGhQEIRCYD4wUBCEk24VaBzgsCIIvcAN1xnlFSb2GnOTMEItiFsqsvEIlJ5GBj9n1y2yMrGzYZdkJlVTGn6cekZD2ZiVFXn0HI6pELu7Y9Ebbvyl+YWqjjGcjl1J9fJA5VUDL4tWGYG+KSY8eyOzsiNp0GFEE792HQBRx3HMYVVYmTuvWYdimjXpGcdJT6DQPJGVT+R8TmsyeBTc4+OctpFoSuo2pTZfPa+fpEy4KMh1duo2fjGfXXlw/tJfdv8wmM/0Vd+4EAb1+v1Klmor2qcP54H8ifSZ44tnJGaVCxeW9gZxMbsT5Vr/hfSgWn5/Wk56Sha5Myorh9bA3kfPx2qsExKgzgmlZSkZvuIaJnoyFg+oW+zzLrMhI4rfvIGTsV9xv7EXQ0GHErF6N1NAQy6/H47J7N5VOncR21iyM2rdHZmOD2bChuB48gONff6JTpQrRCxfxsFVrwiZOIu22X7HGV1woExMJGTMWqbEx9r/9iqD1brg4Ggwcg4GbIdG7LqAI8H/9BmXAR327clTWEgv/1SRGBJZ1OBrKGk2GGFA7TEPZZIgBRng58Tg6hXOPyv/NSg0aNGTj0lz9c3Y+ZJa/yrDSpESv0kRRPIDaLCvnY9Ny/O4PNClgW+cCdls8s36KSG7vLgVKRcGWlvER4ez46Qe09fRo8vl3ZKy8xSgvZ1afC2TF6cd806FKkY9vZGlF7XaduHFkP/W79cbUNu/s4pLguSC2M0IURdJ8fDBo2bLEjvcsSxw+7QfuW1jwIMWfyhFxGFnb4PjnMmR2dhAbABcWQ62B4Fi/xGIpiLSkTC7tDcD/TCjaelo0H1iZ6s3sClUaXRgkEikth32IibUtJ1b9yeYfJtFz4tSCXcm1tGHAeljVAcmWYdh8eBib7tVo2N2V1MRMgv1jCLoVTRASwoNkXPz6NDaVTHCqYc4fHdwZtfMmo9f7cAjYczOM0Dg3Nn/aCMticCsVFQrSbt4k+fQZkk+fJuPOHXXIVlYYdmiPQbPm6Hs1Rmpo+Mr9CBIJBi1aYNCiBRmPHxO3fgPxu3aRsHs38rp1MRs2FMN27RBkRcvKlwSiSkXYxElkhYXhtHZtid08KimsZ/3Ko8GfEDnlc+w2nCzrcPKgp62FQ+/ZsLkF/hsn02jcv2UdkoYyRGOqpSYyKQNBAAsD7TI5fueatszed4e1F4Jo5vb2bWoaNGgoJVpNgVXt4fJyaDqurKMpM8raVOudIeepVhSzBbFKCfFPcq1LTYhn+49TUSoU9P1uJklaagOtllWs6FbbjlXnAohKKlz/8cs07NUfqUzG+a0b3/RlPCc+XYuMrNfPqfB/moi9iRwTPW0yAwJRxsUh9yheQ62XMenZE6mVKd4xrsgzsqjhWhWnjRvUYhjgyBSQyKDt9BKN42WUChU3jgWzftpF/M+GUbOlA0NnNqZmy8L1CReVOu0702viNOLCw9j4/ddEBj4ueLHcRD2jWKarnlGcFA6AnpE2VRvZ0uHjmny4oA3N9S/hFHiI9NAILu56zNklt/ksUYcqYRIepjfmfngyU7pUw9OpYBOw16GIiiJ+x05Cxo3jvlcTgoYMJWblSqT6+liOH4/Lrp1U8j6F3ezZGHVoX6AYFkWRmJDgPGXjOq6u2Eybipv3KawnT0IRHU3o+K952KYt0X/+iSK2bHtLY1asJPnkSawnTECvhL8rJYF2neaYt65KwrVw0o6VjW/B66juXgM/+/7Ujz/IqbP5j2rT8J7wPENctmGUNZGJ6VgY6BR7VU9h0dGSMqC+I8fvRBAar+lH1KDhnaFCQ3BrD2f/gPSyMe8tD2gEcSHJWSEtokSlBO7ugwW1Ieo+AJlpqez4eTrJsbH0mjgNc4cKufp6xrV1I0OhYsnJh28Ug76JKR6dunP3nPerxVEBJMfFcnXfTtZNGsvfvs5sPBRGcmzMK7fxC0t40T983QcAPc+STdIL2toE1qtGkq4O9eztcP7rT6TPnLkfnVS/782/ASPbEo3jGaIoEuAbzb8zL3Fu20NsXI0ZOLUBzQZURle/ZDOSLnXrMXDGXBAENv0wkcfXrxS82KQCDN6iNhva2B8yknM9LdXWosa8CdStLaHuwa/pXiOANiOq4VTFjKqiLofjJ2CnaMqwRk5FilFUKEj18SHyjz8I6N2HB82a8/S770i7eg3Ddm2x/+MPKl84j9P6dVh88jG6Vau+dg5vdHAg2+ZM5Z+v/8eWGd+RGB2ZZ43U0BCzESOoeOggDsuWolOpElF/LOBhy1aETf6OdP/SL/lNuXCBqAULMOrcGdNhQ0v9+MWF+cw/kcpFwufMRlQUbcRcaVFr0EzSJXJUx2YSnlBuRtZrKGWe/SXRZIhLdwZxfgzJPndsuKgZwaRBwztFq+8gPR4uLivrSMoMjSAuJBnKFydbESVKFeo7KaISbv6LIiuL3b/OITLwMd3GTcKucjUgd1+Pq6UB/Twd2HgpmJC4N5ujWb9bH3T09Dm3ZX3h4k5Nxc/7OFtnT2H56JF4r/sbiURCI7tYklIVbJo+kYTIiHy3TclQEBCdgnu2IE695oPUxARtF5c3ir2wXD+8j1uhEdTUf0q9yV+8KINVZsGhSWDqAo0/L9EYnhETmszehTc4sNQXiUSg6xe16fZlbcxsS28MkZWzK0Nm/4apjR275s7ixuH9BS+2qwP9/oHwW7BtFChzixlBKsX2xx8x6t6N5MW/YuG7lw4f1+DTnzxobLCG9ERrLux49NqYFNHRxO/cRej48dxv0pSgwUOIWbESQS7Hctw4XHbuoNJpb+zmzMGoYwekRoVzAU9NTODYyqWsnTCGyMcP8ejcg6igx6z99kvuns8/EyhIpRi2akWFVX/jum8vxn16k3joEAG9+xA4ZCiJhw6ViqjLCg8n9Otv0HZ1wXbWzNeK/vKM1MwGqxE9SH+aRcLSKWUdTr5oGVqS0fBLWnOFv9Zv1Jj5vK9oWogB9bWG9StmECefPUfKxYslGoO9iZw21azZdOUJ6a/xWdGgQUM5wq4uVOsGF5a8txMc3g2nl3JAuuLFH3cRJSoxh8mW71YO3dYl+PZNOv5vHK4eL/paI5MykEkFTLMdh8e0cWPH9VAWHn/AvL61ixyHroEB9bv34eymtYTdv/NceOdEqVAQeNOHO2dO8ujqJRRZmRhb29Cwd3+qNW2JmZ0D/FYV15rV2X42kc3TJ9F3ymzM7HL3Jd8NT0QUeT5yKe3aNeQeHiV6of/o2iVOrl6OaxVn2gpnkOQ0zLryN0TdhYH/glbJ3glPS87k8t4A/E6Hoi3Xoml/N2q0sEdaRuVoBmbmDJjxM/sX/sLxVcuIjwij+dAPkEikeRdXbg9dfoN94+Dgt9Bl/ouyQtQC0u6nn0CEqN9/B0HAYvgAPAx2keLYlZvHwdBcl9qtX0xNE5VK0nx9ST59mpTTZ0j3U5tZSS0tMGzdGoMWzdH38iq08H0ZpSKLG4cPcGH7RjLT0qjToQuN+w5CbmhE3Y7dOLDwF/YvmEfgDR9aj/qkwPFlOpUqYfvDD1iNG0f89h3EbdhA6Ffj0LKxwXTQIEz690PL1PSNYnwVYmYmoWO/QkxPx2HhQiT6pTu3uSQw/mIOcbsOErl6F4ZDv0JqVjrjt4qCWeuxpF3/m04Rf7LmfEtGNXUt65A0lDLPe4hVr28B+i8TkZhBTXvjAp+PnDsXUaGg4sEDBa4pDkY0duaofwQHbj2lt4dDiR5LgwYNxUjL7+DOPji/sNRbEssDGkFcSDIUL2eInxl5wIn72tyLO0PzIaOo3qJNru0iE9OxMtR9ftK2M5EzrJETq88F8GmLilS0NChyLHU7dcPn4B7O/ruWftN+RBAERFHk6YO7+J85xb0LZ0hPSkTX0Ijqrdrh3qwltm55y1RtLXTpP20y2+ZMZfP0ifSbMhuLCs7Pn/fPNtSqbmeEIiaGzKAgTPr3K3K8hSX80QP2LZiHlUtFuvZrhWR7jmHhKdFw6keo2BqqdCqxGJRKFbdPhXJlfwCZ6UpqtHCgQVcXdA3K3qxJW1dOj2++x3vt31zbv5v4iAi6fPkNMt18sgL1PlAPXD/7u7qU+iWjBEEqxe5n9diaqPnzQZmBBdDEM4Jknaqc3foAPVkWVrG3SPY+Tcq5cygTEkAiQV6nDpZfjcWgeXN0qlZ9aUZ30RBFkYDrVzm1diVxT0Nxru1By+EfYe5Q4fkaE2sbBsyYy8Xt/3Jx5xZC7/rRecw32FYq2JxOamSE+aiRmA0fRrK3N7Hr1hH1++9EL1mCUbeumA0dim61vDeT3pSIufNIu3kT+z/+QMf1vyHKBC0tbL7/jsAvZxAzbTRWi3eWdUh50dZHt+13NNg/nlWHN9LUbSxu1q82aNPw3+LFee39TRErlCpiUjKwKiBDLKpUZD55gpieTlZoKDL7kjPlbFLJHFdLfdZeCNIIYg0a3iWs3aFGH7j0FzT6HxhYlXVEpYpGEBeS9Jyu0gIos+uzLsc4ciPODs+qRtTr1jvPdhFJ6XnmAv6vZUU2XQ5m/tH7LBlc9Hm+2rpyGvYawMl//uLWiSMkxURx5+wpEiLC0ZJpU7FeQ6o1a4VzbQ+krxn3YuXsyoAffmbb7O/ZPGMyfb+fhbVrJUDtMG2qJ8PWWJekY2cBkNct/vnDAAmR4eycOwM9IxN6TZyGLOKl0q4Ts9WW8B1/zpXtLE4Cb0VzbttD4iNScXQ3o0nfSpjbFf2GRUkikUhpNfITjK1tObVmBZtnTKLnhGkYmOZjgtV6mloUH5sOxo5Qs2+up5+LYlEkasESqGWAfsVw6qR7E5tpzrG1mdS98SdmOskYtGqFQfNm6iywiUmxvJaYkGBOrllBkO91TO0c6DXpB1zq1Mu3AkGqpUWTAcNwqlmXA4t/Y9O0CXj1G0L9Hn3yz5LneI2GrVtj2Lo1GQ8eELt+Awm7d5OwfQd69ephOmwYhm1av9VYpIS9e4nbsAG21MTAAAAgAElEQVSzkSMx6tjhjfdTHpG3G4hx3T+JPXEHk5tn0K7drKxDyoPgMRzF+cV8E7eJr/5tyo4vmqOtpekGem94niEu4zjKkOjkTESx4BnEiqgoxHR1+1by+fOY9iu5G9uCIDCskRMz9vpzKySBmg4FZ601aNBQzmg5Gfx2qJMpHX8q62hKFc1VQyFJf2nMklIU2X/+AWejnKlmBy10ziAo8hq7RCRm0CfoFke+W4zi9gFQqTA30OHDpi7s933K7dA3c3Sr1bYjhhaWHF2+iEs7tmBsZUPH/43js+Xr6frVRCp6NihQDIsqkT2hX7Lnshch9+Iws3dgwIx5aMv12DLzO0Lvqs2I/MIScbczQhAE0nyuI2hro1uj+hvF+yrSk5PZ8dN0VAoFvSdNR9/kpZLWp75w7R9o8AlYFn1k1euIDUth76Ib7F/iC0CXz2vR7cva5U4M58SjUzd6fDuF2NAQNn7/NVHBgXkXSSTQcxlU8IJdoyHwXJ4lz0SxUacORPkaEThnO/HLl1Ev7ShyOfg1nYDV9kPqNZ07F4sYTktK5PiqP1nz7ReEP7pPy+EfM+KXxbjWrf/acnwH9xoMn7eISg28OLtpLVtnfU9idFShjqvj5obtjOm4eZ/C6ttvyQoLI3TsWB62a0/0ihUo4uKK/FrS79/n6bQfkNfzxOrr8UXe/m1JT07m5JoVLPtkKGsnjmHv/J848+8abp88Ssid26TEx7212ZDlj0sQJBAx9ZtiirqYkcrQajuNSjyhatQBfj92v6wj0lCKvBi79P4q4sikV88gzgx6YXKVcv58icfTx9MBPW0pay8ElvixNJQBGUnqaRaPT5V1JBqKG4tKUHuwukUxMaysoylVNBniQpKzZBpAJagIC1dfQLfo0xPh2Bm4dxBq5M4Sx8Wnk5BQkXikpC73pbPbPLRbfsFHTbuw5kIQvx25x+pRDYocj5ZMRtexEwl/9IDKDb0wMDMv9LaBt6J5kloNWUYWu3+/jrWLEZ6dnBkw/We2zZ7Cth+n0nX899wLT2JkE2cAUn2uoVuzJhLt4p1xqDYjm01CZDh9v5+NuYNj7gWiCIcmgp45tJhYrMdOT8ni8r4AbnuHItOR0qRvJWq2dED6jmSXKno2YMCMueyaO4NN076l21eTcK7zkgO4lg4M3AB/t4dNg+HDo2BZOdcSQUsLu1nT0IvajMSzPwajZiA1McE6IpVt866yf+kt+nzr+dZl40qFgptHD3Bh60YyUlOp1a4TXv0Go2dUtAyCroEBXcdOwK+2BydW/8W6CV/S7tMvqdww35HmeZAaG2P+4QeYjRxB8smTxK5bT9Rv84levATj7t0wHToU3Sqvv/GiTE4mdMxYJAb62M+fX6ozkFVKJb7HDnFu6wbSk5Nwq98YRWYGUcEBPLx6EZXyheeBTFeOiY0tpjZ2mNraYWJti4mtHaY2dugZm7z2JoTMpTrmPRoStf0yyVsWYdD/y5J+eUXHvSfY1WVK1E4aejemdVUr6ju/+egwDe8OQox6asP77DKdc5pFfmQFBwOg16ABqecvICqVCNKCK2veFiNdGT3r2rP9Wgjfda6GqX7ZzEbWUELEP4EHR9SCuM9KcO9RpuEkZyjYevUJ5x5GM6B+Bdq5W5dpPO88LSaA72Y4/St0nV/W0ZQaGkFcSNJfFsSIKLONtrQrNYdLtuoPUA5BnJapxDJZRERKLdvr3Aqvw94AI7pGfYmR+WwWVB7KJzcrcSUw9o0u3uwqV8WuctUib+dzOBhDrRgGtj7HfYtv8TkcxIGlvpjbG1C/59f47FvInl9mYWfelup2tVClpZHufwfzkSOLfKxXIapUHF72ByF3btN5zLc4uNfIu8hvJwSfh24L1LN2iwGlUoXf6VAu7w0gM01B9Wb2NOjugtzg3TtpW7tUZPCc+eycO4Mdc2fQ5oPR1G73Uo+1nhkM3QYr28KGPvDR8Ty9IYKWFqaVUqGhG2RngU2s9eg8uhZ7/rjBgWW+dP+qDlqyN7uICrhxjVNrVxIb+oQKNevQavhHufrVi4ogCNRo1Q77qu7sX/gre+f/RI1W7Wk98pP8e6rz24dUimHbthi2bUv6vfvErV9Pwt69xG/dhl6DBpgOG4ph69b5XjiKosjTyd+R+eQJTmv+QWZVer02gb7XObVmBTEhwTi616TliI+xcn7Rt6xUKEiMjiQ+/ClxT8OID1f/RAY+4sHl87nMh7Tlckys7bIFsi0mNmqhbGJjm0ssm32/mPhjDYn4/U/0u3+IoJu/qVmZIQjQdgYma7szxuAU4zYbc3BsMwx1y773X0MJk5pdIfIeu4znnGaRH5lBwSCTYdK3D2ETJpLu74+8Zs0SjWl4Yyc2Xgpmy9UnfNqiYokeS0MZIZPD1pHQ9XfwHFnqhw+LT2PN+UA2Xg4mKV2BiZ6MY3ciaVPViundq+NoVs7OU+8Kpk7gMRx81kKTMWDqXKKHy1SoCIlLxfUNPJWKE40gLiRpeUYIKFAp1ReWWjq6ULMfXFyqNn/StwDUZUwVsyToSJNpUvESdt2GceRvCbsN1tJN6zda3pvFGV0zDm7vR70vpiHolPyHIexhPOGPE2hmeRxtLRk1mttTrYktD69EcO1QEKc3hWBo1huJwRY6RxzGJNyNtEwTyMpC7lm8/cNnN6/j7jlvmg4aQbUmLfJfdHEZ2NSCusOK5ZhBfjGc2/qAuPBUHKqa0rSfG+b25bc0ujAYmlswcMZc9i2Yx7GVS4iPeErzwSNzm12ZOsPgzbC6C2wcACP3gfbrnZDtKpnQdpQ7h1fc5vg/d2j/YXUESeF7uGNCn+C9diUBN65hamtHzwlTcfVoUGxO5aa29gyaNY/zWzZwec92Qu/60WXMt8/74AuLbpXK2M6aieX4cSRs307sxo2EfjkGmZ0dpkMGY9KnT65y8dhVq0k6ehSriRPRq1evWF7L64h7GsqpdX/z+NpljK1t6P71d1Sq3zjPeynV0lJng23scHmpYuC5WH4aRlx4mFo0h4cR+fghDy6dyyuWbeyei2R5j86kbzuO7pwvsZu5svyNlXJtARVb82nILpbHN2HGXn9+7Vd0J38N7xYaSy31NAtBAIsCbupmBgejbW+PftOmAKScO1figriqjRENXMxYfymIj5q5Ii3CeUPDO0LHn+H2Dtg7FtLioMlXJebxkhPfkHhWnglg/62niKJIp5q2fNjUhZr2xqw+F8Afxx7Qdr43n7eqxCfNXdF9wxv57zXNv4Hr68H7F+i5pFh3nalQ4RsSz8XHMVx8HMvVoFj0tbW4OqVtmV5XaARxIcl4qYcYUYFKoUJLUKqFR+2Baqvy29uh4acAPI1LxzVLioPJfSSCSEUPKzrJJBxafptd0hl075mM6P0jI+P+Imv+FmRen0ODj4stE5of1w8Hoasvo5rReUAtQqVSCVUa2VK5gQ0BN6O5ejAQaUwPJFo7ufrPIrTcamMG6NWpU2xx+B47xOVdW6nVpiMNevQteKEqCzrNhVcYJxWGuPAUzm17SNDtGIwt5XQeXRPnWhbl76L+DdGW69Hz26mc+Gc5V/fuICEinE5fjEemkyNjYO8JfVfB5iGw/SMYsL5Q72slTyuSYipxfsdDDM108erzerGZlpzEhW0buXnkADIdXVoM+5C6Hbsi1Sr+jJ1US0azwSNxquXBwSW/sXHKNzQZMJT63XoX2QFby9QU848+wmzkSJJOnCBu3Xoif/mVqEWLMe7eHdOhQ1DGxxM5fz6GHTpgNnJEsb+el0lPSebi9k1cP7QPqUz9Wj06dUfrDdoXconll55TKhQkRkW8EMrZ2eVcYtnNnov3I9Ae2Q8TW/sXZdjPhLOtHXJDI8rsW9V2Olp/Necv13MMvKZP22pWdKxhW1bRaCgFngvi97hkOjIxHQsDHbQKGAuYGRyMzKkCWmZm6Lq7k3L2HBaffVbicQ1v7MQXG6/jfT+S1lU1Zaz/OWR6MOhftUfJsenqhFD72SUiipUqkWN3Ivj7TACXA2Mx0NFilJczI7ycc2WCP2lekW617Zi97w7zj95nh08I07tXp2WV98sx+a0xsoP6H8GlZeopJRZFSzLkJEOhxDckgYuPYrgYEMO1oDjSs9SaqqqNIQPrV6CRqxlKlYiWVCOIyz13Eq+Q8+skkoUkPROJSlT341hXB+uacHPTc0H85H4sugg4G70weXGuaUHXL2qzf6kvO7fr0fnLvYxeuZoPVDuof3I2nFsA7WdBvVGvjenc1aX4R/kyov1CtKWvvziOCU0m8FYM9bu6ILuXmed5QSLgWtcSlzoWjP71HPZhfTGI3M3F+zewq9MGV219iuM+W8D1qxz7eykudevR5sPRrxalNfqAk9dbHe/qgQCu7AtES1uCV59K1Gr17vQJFwWJVEqbDz7D1MaOU+tWkjTzO3p+OzW3SVnVztBxrno+8aFJ0GleoU5eddo5khiTxvWjwRhZ6FKjRf7jNFRKJTePHeT8lg1kpKRQs017mgwYVuQ+4TehQo1aDP9lMUf/WsSZjf8Q5Hudjp+Pw9DMosj7ErS0MGrfHqP27Um/e5fY9etJ2L2bsD0nCHbpiHWVVlSaNatEb6ioVEpuHT/Cuc3rSEtOokbLdjQdOCyv6VwxIdXSwtTWHlPbvCNZlIosEiIjiTp7kEcLV6BwNCXLyJjwxw+4f/FcLkMjHT19TAykVFA600ylKl1xbFsbavSl4d3NtLBtzeQdt/CoYFrgOBoN/wFeKOIyDaMsiUzKwMow//5hURTJCgp6Xsmi38SLmH/WoExOQWpQsvPSO1S3wcpQh7UXgjSC+L+KVAa9loPcFC4sVmeKuy0EafHIi5QMBduuhbDqXABBManYm8iZ0qUaA+o7FtgSY2ssZ8kQDwbcj2L6Hj9Grr5Cx+o2TO3mjr2JvFjiei9oOg6urYZTP0Hfvwu9WYZCyc0nCdkZYLUAfpZUrGZrlC2AzWnoYlau/AU0griQKITE3A+IWWgnJpOZJZLxKBBdMyeoPQCOTIHoB2DhRsz9BBSIOBk/Bl5cxDpUMaXH2DrsXXSTvX9cp2XbTvQ74sz6LnKa+oyHGxsKJYjn3/mH+6o0Du8bwNwWv1LR5NV9OtePBKOlLaFWSwe49+p9n0tJoXtzO0ZWmcaeGZMJEwNZMeYPPLv2pnZrR/SM3uxDHPH4IXt//xnLCi50/WoikoKMPayrQ6V20G7WGx3nGeGPE7i0J4CKHpY0H1jljeN+VxAEAc8uPTC2smb/ol/YOOVrek+anmuuLw0/gfgg9cnLxAm8vijUfpv1dyM5Np3Tm+5jYKqLc63cQjPwpg+n1q5U97ZWr0WrER9j6fRyHrJkkRsY0m38ZG6dOMLJNctZ++2XtP9sDG71G7/xPnWrVsVu9mxUfT7l7N93yVBICQMezr1FzZYOuDexRUeveDPfwbdvcnLNCqKDA7GvWp1WIz4uchl4cSLVkmFmZ49Z/48wOb6LWO+HOH/6KfKWM5+L5fjwMHVWOSKMSN9zXIl0xOX+XRxrF9008K1oPQXBfzcLbQ/TMLobE7b7snrk6x3MNbybaDLE6h5i6wJu+ihjYlClpqJdQX0O0G/ShJgVK0m9chnDVq1KNC6ZVMKgBhVYcPwBgdEpOFuUrADXUEZIJOqb63rmavGUFq+uRpO9+Y3IpwlprDkfxMZLQSSmK6hbwYQJHarSobp1gZUQL9O8siUHv2rGyjMBLDrxAO/foviyTSU+auqqGc1XGAws1Qm+s39As6/Vc4rzIT1Lyc0n8Vx8HMvFxzH4BKsFsCBANRsjBjdUC+AGzuVLAL+M5hNRSNpXfcl1VsxCyFIgVYlkBoeoH6vZDwQJ3NyEKIpkBqcQIlOhK1Pk2Z+NqzE9x9VFkaki9VAYdY31+eGyBNG0cALiafJT7qvSaJeSSmRSKAP2DWDz3c0FXhQkxabz4EoE7k3tXusWHBKXRlK6gup2xlhKE2h58wIOdhXJSPLm0s5/WfPdOU5vvk9SbN4xU68iMTqSnXNnoGtoSK+J09DWfcWdOpMKaiMo47zZqsIiqkTObL6PvrE2rYdX+8+L4ZxUqt+IAT/8jDIri3+nfkuQ743cC9rNUjtDHvke/HYVap8SqYT2H9XAwtGQwytvExmkvkkUGxbKzrkz2P7jNJRZWXT/5nv6TZ1T6mL4GYIgUKtNB4b9vAAjSyv2/DqHoysWk5VRtM9rTh5ciWDf34/QNTVg0A8N6fhJDQzNdDm//SH/TDqH98Z7xIalvHXs8eFP2f3rbLbO+p7MtDS6jZvEgOk/l6kYfhmLWcuQ6kLErBmIKtVzsezqUR/PLj1o88Fo+vb0RCZR4HfmdOkHaOYC9UZhfGcTP7eQc+peFOsvBr1+Ow3vJpomYiISC84QZ2Y7TGsnXoETs5F7eCDo6pJyruTHLwEMblgBLYmg+Q6+ASqlivjIVJ74x6LIfNnHppwhCNByEnT+Fe4dgPV9IL3oY0VvhSTw1abrNJt7kuWnH9HMzZLto73Y+b8mdKllW2gx/AwdLSmft6rE0XEtaOZmwbxD9+i04DTnH0YXObb3Eq8xoGMIp358/lB6lpKLj2P449h9Bi6/QK0ZRxiw/CJ/HL9PYnoWQxo6sXyYJ9entuPA2Gb80K06HarblGsxDJoMcaGxNbDjdo5/iygQlEqkKpGMoCfqBw1twLUl+G4hrupXSFOVRFlICywZtKxgSK/xHuxecJ32EQKrtVKI0E3HJv/zWi68Q7wB+CIuHiP9Kkyxq8DsS7M5G3aWmV4zMdXNXVZ545j6pFinbYU8+3oZvzD1HzF3OyPSzp5DAvT4ajynDuzE79Qx9G21uX1KxM87lCqNbPDo4ISJ9avd/NJT1LOGFZmZDJwyu0hjot6UOxeeEhmURNtR7mjrvn8fdZuKbgye/ZvagfrnH2j70efUbN1e/aREAr3+gqRw2PEJDNxYqH3KdKR0+bwW2+deY8+iyzi6PcDf+xBa2to0HzKKup26o1WK44dehZmdA4Nn/8rZTeu4uncHIf5qN3Nrl8I7noqiyLWDQVza8xjbSsZ0/qwWugYyzGz1qehhRdSTJG6dDOHO+afcPh2KQ1VTarV2xKmGOZIimMhkpKZycccmrh/cg0SqRdOBw/Hs0vON+oRLGqmVI5ZDOxO+8iCJf03HePTMPGtkMimVDaO5f+UibTLSc/eylwbNJ8CNjfSI+Zsdlb9kzoE7eFWyoGIZu1hqKH6eJf5VqvdzDrFCqSImJaPAtoDMoGxBnOYLp32QWLmj16A+KefyzqUvCayNdOlQw4YtV5/wdfsqyLU1Bkcvk5mmIC4ilfjwFOLCU9W/R6QSH5mKKnvCSc2WDjQfWPk1eyoHNPhYXT6981P4pysM3aHONL4ClUrk+N1IVp55zKUAdX/wCC9nRr7UH/w2OJrpsXx4PU7cjWD6Hn8Gr7xEt9p2TOlSrcDqCg2AnhlZDUYjOzOXDTt3syfSiutP4snMzgBXtzNieCMnGmZngI2LuVquNHn/VMIbYmvwkjGLmIWoFJGKqhcZYoBaA2HnJwSe8QEgy/LV6tbMTp9eX3uw+4/rDI7Xxe+pGVZO4a9N3XuHeFNB0MElS4EQcJ6lvVayIeQ4v1/7nT57+jCn6Rwa26nLRNOTs/A/G4ZbfWsMzV7/xfcLS0QqEahqY0iMz3W0LC3RqVCBDp+OQaajy43D+6jWTBd9i/bcORfOnQtPqeRhhUdHJywdDfPsT6nIYs9vPxL3NIw+383AwtHptTG8LRlpCi7ueoSNqzGVG7y/vUtGllYMnDmPvb//zJG/FhIfHkbTgcPVZlMyOQz8F/5upx6dUEjkBlpU8ojk4vYN3H6ShnvzdjQfMrzEelvfBqmWjBZDP8C5lgcHl85n4/df02zwCDw793it4ZZSoeLUhrvcvRBO5QbWtB5WDaks9zaWjoa0Hl6Nxr0r4n82jFunQjmw1BcjC11qtnSgmtery6lVKiW3Tx7j3OZ1pCYmUL1FG5oOHI6BafmeoWsy9mfi9xwlcuVWDAeNQWKSt0+7unEkfsE2PLxykWpNW5ZugAaW0PgLBO+f+X3QZ7QJkTJu8w22j/ZCVsQMg4Z3g703w6jTrmSdk8sj0cmZiGLBM4gzg4NAKkVmpAXJwN6vMKg7nojTZ8gKC0NmZ1fiMQ5v5MR+36fsuRnKgPqvvyn/X0RUiSTHZxCXLXrjnwnf8BRSEl54uggSAWNLOaY2ejjVMMfURo8nd+K4fTqUGi3sMbN9B8rOa/YFXWPYPAxWdYBhO9WjfF4iNVPB9msh/H02gMAc/cH96ztiVEIj81pXtcarogXLTj1imfcjTtyJYFy7yozwctacG7JJz1LiExz3vAT6YXAVjmsZYOczn1SrOYxo7ERDF3Pqu5hhLH93BfDLaARxIZEFPs39gJiFShSR5SyZBqjWFfbpE3gzkjgdc0ws5PCaSkoTKz16f+PJpnlXuR89FBv5Rqq/Yn1qViqXn16mv9QUQUsOijQk/rsZ1mg09W3qM/H0RD45+gkj3EcwxmMMt7xDUWSqqNu+cCci/7BEKlrqoyuTknbtGnJPT3X/nSDQetSnaOvqcnn3NtybZTF05v+4dSqMW94hPLwWiVMNcxr1dMXCQS2MRVHkyJ8LeeLnS6fPx1OhRumMQbl6IJC05Cy6fuH23vcO6ujp02viD5xY9SeXd28jPiKcjp+PQ6atA/rmMGSrWhTn9VnLQ9CtG5xas4LoJ0FYOlclKaE+6elu6OqXvGnW2+BUqw7D5y3iyF+L8F73N4E3fej4v3EFCs+M1CwO/nWb0Htx1O/iTP2uLq/8HMkNtPHs6EyddhUIuBGN78knnNv2kEt7A6jayIZarRwwtcl9IfPE/xYn16wgKvAxdlXc6T1perkqjX4Vgkwb68kTCBr3I9EzRmP1+9Y8axz0EjCysMD/9InSF8Sg7o2/shLzCz/xU8/ljN54nUXHHzC+fZXXb6vhneHZ19InOK5sAykjIpNePYM4KygYmZ0dghSwqAxJ4egnqNtkUs6fx6TvK6Y8FBMNXMyoYm3I2gtB9K/n+J8+JysylcRHpqozveHqTG9ceArxEakoMnOOtdPC1EYPx2pmmNjoYWqjj6mNHkYW8jymn841LQi6Fc35HQ/p+vk7MkrOrR0M3w0b+8GqjjBsB1hVAyA8IZ01FwLZeCmYhLQsajuasLhDFTpWtylySfSboCuTMq5dZXp72DN9jx+z999h69UQZvWsQQOX8n0zuiRIy1RyPTju+RikG0/iyVSqkAhQw96Yvk3ciUn/jFa+v9Kqhwwq5N9L/K6jEcSFJPV87n4bUcxCKQhIVSoyg54giqL6j7y2PmkV+/L0lCkB+plUMNJ5rSAGMDTTZfCkemyaupeLob2pnJSIzNAo37UXn14kU5VJS20TdY+tTA63tkGj0VQ1q8qmrpv47epvrPFfw9VQH1qf+QSnmuaFnrfrF5ZI44rmZIWHkxUWlmu0jCAINBs8EpmunHOb16HIzKTzmG/w6FCBW6dCuXn8Cbv/uMGgaQ3RM9Lm/NaN+J85iVf/Ibg3b12o478t8RGp+J54QjUvW6yc8n8P3zekWlq0/fhzTGztOL1+FUkxUfT8dip6xiZgXhEGbYa1PcAg/2x63NNQvNev4tHVSxhZWtNt/GTcGnjx4EoER1f5c3ztHdp94F6uL3T0jIzp8c33+B47yKm1f7P22y/oMHosFT0b5lqXGJ3GvsU3SYhKo83IalRtVPixPVKphEqeVlTytCIqOAnfk0/wPxfGbe9QKribUbOVA8YWWZzZuJoHl89jaGFJl7ETqNK4Wbl+7/JDr9MwjP5ZTuzhW5j4XUS7eqNczwsCuHs149Le3STHxpRKm0QudAyhxQQ4OIFOTfzp7WHPmgtBfNjM9T91V/t9R8huShLe0ybiiMQM4FUZ4uBsQ61g9SiVFhPR3vYhWsYVST53rlQEsSAIDGvsxJRdt/EJjsfTqfxVExUFURRJTczMkeVNJS5CnflNik1/0c8uqK/tTG30sHczzRa+avErN5QV+m++3FAbz87OXNjxiCf+sTi6vyOirUJDGHUQ1vWGVR151P4fFj8wZe/NMFSiSIfqNnzUzAWPCqZlcv5zMtdn1cj6HPGPYOZef/r/dYHeHvZM7lQNywJ68v8rnH8UzYWgZC4+juHGk3iylCISAWraGzOqiTMNXc2o52z2IlOfOR4erYGTs2HE3rINvoTQCOLCovVy30sWKomAFBWqlFSU0dFoWar7JILkPQEwkvpiZegGLyWXC0LfRBd5XS3iLhvjs3Y3DT8flu867xBvDGQGeEiyBW7NfnB0GsQ+BjNX5FpypjSagpedF+u37CcrRUVitQBEsdZr/+jEJGcQnpiOu60RaT7qsm+5h2eedY16D0Cmo8uptSvI+jWDbuMnU6+zM651LNny0xVOrLuDU7UILm7/lxqt2tGo98DCvQnFwNmtD9CSSWjUo/C9ou8DgiBQv1tvjK2sObjoNzZO+ZpeE6dj7uAIjvVhwuM8rpAZqSlc3LEZnwN7kMpkNB2kLjd+1ttauYENSbHpXNz1GCNzXRr1LN/vuSAI1G7XGYdqNdm/cB675s2iTocuNB/6ATJtHcIfJ3BgmS8qpUj3sXWwr/zmF26WFQxpM8Kdxr0qqcupTz5i16+7UWb4IJFKadR7MA169VFn6t9RrGYtIqnXQCKnjsNhx4U8z1dr0oyLe3Zy55w39bv1Lv0APUfBhSVwbDozRh5nXNvKGjH8H+Mdu49U7EQkFpwhFkWRzKAgjLt1BdS9xNTsi/DoJPqX95F89ox6bGRB0x6KkV517Zl78C5rLwS+M4JYqVCREJX2IsubLYDjwlPJTHthlqqlLcHEWg8bV2OqedliYq0WvSZWcrSKqWe6ditH/E6HcnbbAwZMaVAkj4qyRGXpzoXm66l4eBi2u8Nae0wAACAASURBVAeQIo5nWONujPJyoYJ58fQHvw2CINChug3N3CxYcvIhy08/5qh/BN+0r8LQRk5I35H3+XWExadx7E4EOleeMAAYufoKSok2NeyN+aCpC41czKnnbFrgKCu09dVO04cmwWNvcG1RqvGXBhpBXEgkOWaqiQIgKlAKoKOtdv7LDAx8LogDnlqgJ31IJ+1DJBsNLtJxXFs25+6NrfjetqdObDQ6L81QVYkqvJ9408S+CbKoWPWD1XurBfHt7dD82+drW9i3JChGRqRZGH8Gz+byqVNMbzwdE12TAo/vF6Z2Dq5uZ0Tqvz4IenrovuywnY1nlx7IdHQ4unIJO3+eQc8JUzGz06dxr4p4bzjK/bO7cKpVl7YffV5qd/8Cb0UTdDuGJn0rvVeu0kWhcsMmGJpZsOuXWfw77Ru6j/+eCjVq5RLDKpWS2yeOcnbzOtKSEqnRsi1NBgzLt8TYo4MTidHpXDsUhKG5LtWbvbkzeGlh7uDI4DnzOfvvP1zbv5snfreo2fZDrhxIQt9Ym65f1M5T4vymyA200JHfJz1hLcr0ePTNaqNQNsDvvAlKMYiaLe2L7ViljaxyHSy6ehK124eUncvR7/VJrufN/s/eeYdHVW19+D1T03vvlfRAQkgg1NCRIlWaiIoidj8bKHr12vu1IVbELtKUIr0mlNBDGoSS3hPSe2bO98cJIUgIgZCC5H0engxnztl7z8yZ2XvttdZv2dph6+lFwp4dhIyb1PFeAIUKhr4Max7A8MxfGAbe1bH9d9P+NNxSwu3pICavrAZBAAuDK+c7TXEx2rIylE5O0FSkeMy76O+IoiS5gupjB9Ht07/dx6mvVjCltwO/RKfy0ljfLumBKy+qIXF/FnmpZRTnVlKSX4WovXRj6RurMLHRp0eoteTptdbHxEYPAxM1QjsbTnKljPDJHmz+Oo7EfVldfp6tqtWw6lgG30clc76gAj+jN/hB912+qvwAwdUTzFtKDOx49FQKnhvlzeRgB175K55X1sXzx5F0Xp/oT7DTrbGB0xRRFDmVU8a2hFy2JuQQlymt7RcbSRs5X9/Tm95uNlc3gJuj932w71PY9Sa4DvrX7UZ2G8StRFA0MYjlckSxDq0MdHTrAKhJTkavTx/q6zSkJRbhYF1KhOYEx1SV19VPTwcTPtVV0r9Wn+M/baTvk3Mvez6hMIHC6kIGOwyG/LXSQRNHcOonhU0PfLbxJj17JI/KojqmLojAWlHFJ8c/Ycr6Kbw14C3C/tnxxfazpS+Nr50RF44fQ7dn4GWv/Z8EDh+NQq1m8xf/Y9WbLzN50avYutahqdqIIDNn0Ownkbdw/c1EU69l36qzmFjrETDEoUP6vFWx9fRqVKBe/dbLjJj/OP5DhgOQHn9Sym1NTcbe25eIuf9tMbdVEAQGz+xBeVE1e36TahQ7+3dweOwNoFAqGXLPgzgHBrPh4w/Yvfw1zBxHMuX5B9C7Svjh9ZKRGMeuH74hL/kctj28mfj8y9h6eJGXWsrJXRnER2USuzsDJz8zAoc64uRj1u4Lq5uN2UufU7wznNwPPsH1jnsQ/qEo7TtoGDu++4L81GSsXNw6foD+U2D/J7CzodSYoustxLu5cYTGv7enRZxXWo2FgbrZ3Mu6iyWXnJwhuckTagP0H/4Mtj1Exfcvoxuyo0MWt3P6ObN8fworDqfx2FDPdu+vNYiiSG5yKTE70zl/LB+tKGJmq4+5nT7uwZaYWuthYqOPqbUeKt3OXTK7BVli62FM9LrzeIZYd/p4miO3tJofD6TwS3QaxZV1BDoY8+nMIMb426CsHQ6/zYRV86CqCPo80NnDvQJ3SwN+mhfKxthsXt+QwOQv9jOjjyPPj/bGrIuXDarXaDmcUsS2hFy2JeaQfqEKQYAgRxMWjfFmhK817qdOwQ4Y0sMKFNcZLaXUgUHPwsan4ex2KU/8X0TX+zZ1UYQmIdOiQg71dWgF0NGpR1ApqU1OASAzqZj6Gg0aTyeUpzW45W65rn50VXIqnbwxOXeSmFPOBGZnoWd7SQVyd/puZIKMgfYD4fjaSxcGTIWNz0BuPNj4I4oix7emYWqrj1ugJe6yewm1DWXh3oU8uPVB7tOT8Zio5Z9fh/isUuxNdDHU1pF96jQWCxZcc8y+AyNQqtRs+OQ9Vr62mMrSYnQN9ZHpTGXP7ylMec4MWQcIJZzcmUFxbiXjHut5hShFN1dibGUtKVB/9DZbln5MQXoqJbk5nD18ACNLK8Y9tYgeffu3yqsnk8sY9aA/az88xpZv4pj0bHCziuNdDY1GS0qcPoJqNob6u7mQvonNS/IY9fBTbVLNLsnLZe8v35N0MAoDcwvueOI5vMMHNb6XVs5GDL/Xl/DJHsRHZhK3J5MNn8U0buZ497O5ZUqFyQxNsXp4Dpnv/UTRh89g9uKSy573Ch/IruVfk7B3R+cYxDIZDH9Vqot55Hvoe+3ftG5uHS7+PN1a20g3j7yyVtQgdna63CAGFL6DUDtZUhGbgsWR7zrEOHG3NGCAhwW/RKexYLB7hwgoXQ1NnZazR3M5uSuDvNQyVLoKAoc64D/YAWNL3U4bV0sIgsCAaZ6sfPsIRzen0m9S10lRis8q4buoZNbHZFGvFRnpa80DA90IcW6SH6xrIolrrbxXWq9WFkkGVhfzNAqCwLhAO4Z4WfHpjjMsi0pmc3wOz4/yZkYfxy4Vrl5ZW8/epAK2JuSw81QexZV1qBQyBnhY8MgQD4b5WF2eTnGqjR0GzYF9H8PON8BjeJf77NpCt9XQSgT5JdNRVCoQxRpEAdSqelSODtQmS7NNyskCFCoZOTZOJGqdMD239mpNXpUQZ1NWqfXQiEqO/nS5Qb03Yy+9LHtdGfbsOxEEOcStAiAt/gKFmeUEjXBq9Dj5mvuyYtwKJntOZpmenLvLjpFSknJZM/FZJVL94ZgToNWi2zu4VWP2DAtn4nMvcSEzndqqSqa8+F8i5oSQl1LKkU2p1/0eXC8VJTUc/jsZ5wDzW8I72VXQ0Tdg8guv4h8xgqMb1pJ68jj9p8/h3o+W4tVvwHWFuKp0FIx7tCdqPQUbPo+RxEW6MDVV9Wz8PIaEqCxC7vDhgU/fY+j9C0iPj+XH5x/n/PHD191mbXUVUb//xPdPL+D8scP0mzqL+//3JT79Bzf7XuoZqegz1pV73gpnxDxf1HoKIlcksXzRPiL/SKI47/oiTDoLw3sXoeesS/7vO6jPunzlrWtgiHvvUBKj9qDVaK7SQjvjPgxcBsL+zzqn/27ajcZv1e3pICa3tPqqdVRrU9NAEFA6NB8xZTByApWFOmg2LIbchPYcZiNz+jmTXVLN9sQ8RK0WUezYD66ipIbo9ef54cV9bF+eSF2NhsGzvJj7djj9p3p2GWNYW11NbUoKFQejKd28GU1ZGSBtpnqF2RCzI53SgqrOHaMIO0/lMuubg4z9NIrNcTnMDnNm97ND+GpOCH1czK6c95S6MP1nqUTprjdg8wvQRWuIG6gVvHiHD38/ORAva0NeXBvLpKX7ic0o6dRxFZTXsOJwGvOWHybotW0s+PkoOxLzGOplxdLZwRx/eQTL7u3DzFCnq6rP3zAKFQxeBNkn4NTGm9t2J3NruCC6ADLlJQ+xVqkCUfohUoXORBWbQ01SEqIoknKyAEcfM45V1LKegTyf9QvomoF+y4XJmxLiYsa3URY8aJ1C3HkneqWcx9DFjZyKHBIvJPJU8FNXXqRvAe4RELsahr3CsS2pGJiqr6jBq6fU49XwVxl4dAWvGFdz14a7eCH0BSZ6TKSyVkNyQQUTetpRdXQLyGTo9uzV6nG7BoUw+62PQBCwcHTG0lnK6T3ydwpOfmbYuLZfaZ6Df51HU6dlwNSuEYZ1KyFXKBn50BN4hoZj5eLWJjVgfRM14x7ryZr3j7Lh8xgmP9cbdRcM6yotrGLjkpMU51QSMccb3/5SFEbQqHE4+viz8dP3WfvOfwkaM55Bs+5rFBG7GqJWS0LkLiJ/+4GKogv4DBjCgJlzMbJo3fderpDRo48NPfrYkJNcQuyuDOL2ZHJyVwbO/uYEDnHAsQuHUwsyGdavvkXy/U9R8NICbOaNvex530FDOXNoPyknj+EW1KcTBiiATSBknej4vrvpEG7XkOnc0hoC7JufW2vTUlHY2iBTN+9B1h8wgMJvv6PygjGGq+6H+bskg6UdGeZthZ2xDr/sSSDru1epr6nBM7QfnqHh2Hn7IJO1j8BXTnIJJ3dmcO5oHlpRxMXfnMChjjh4d7zCsVhbS11eHvXZ2VI1j+wc6nNypMc52dRn56ApuryMmNrVAaeff0dhLpW2PHcsjwN/nmPUA/7tNs46jZaKmnoqajXS35p6Kmo0CPkF9Ade2xDP8mIlNkY6LBrjzcw+ThjrtSIMV66EiUtBzwwOfgFVF+DOJdLxLkgPa0N+n9+Xv05k8cbGRCYsiWJ2mBPPjfRu3eu9CSQXVLA1PodtCbkcTStCFMHeRJeZoU6M9LOmj4tZx9VRDpwOUR9JucRed0hRWP8Cut5KtYvS1EOsVasRRWmHSOncE1W5LmU7d1KQUkx5UQ19xrmyKS2XTP1hUP2b9GW/DkJcpFDNwl7esKWSwz/vZuhLbuzN2AvAEMchzV8YMA3WPkTOwQNknamm/1SPq4YOD6sV8TMKYbGulv/s/w+RmZFMdHgKUQQ/O2Mqfz6G2tsLucH1if1YOLlc9v9B03uQlVTM9u8TmL44FKX65k92uSmlnNqfTdAIJ0ysO1+18FZEEATcgm+OoWJub8DoBQFs+DSGzV/FdrkQ9tyUUjZ+cRJNnZZxT/TE0ftyoTALJxdmv/U/9v76Pcc3rSc9Ppaxjz97xb19kcxTCez64Rtyz5/BxqMHE55+Ebse3jc8PhtXY2xcjQmf4kH83kziIrNY/1kMpjZSOLVX364ZTq3TbzSm/V0p2peMSe9omu5Luwb1RsfQiIQ9OzvHIO7m30vX3CPqEOo1WgorarC6ioe4Li1dyh++CrrBwQg6OlTohGGY/xtsWQzjPmqv4QKgkMuYFWJH8k8fUlqXg7M5xGz7m2Ob1qFnbIJHSF88Q/vh6B+I/HpzHP+Bpl7L2aN5Ulh0SikqHTkBQxzwH2KPiZW0VtBoRYoraiiqrKO4spbiyjq0oohSLkMpl6GQCyjlAgqZrOGYgKLhr1IuQyETUCpkKGXSuXKtBk1+PnU5udTnZFOXLRm6jY9zc9AUFMI/POMyIyOUNjYobG3Q9Q9AaWuDwsYWpak+mh/vJitaJHXaBJx+W4uBtRVBI504vDGFwIgSbN2NEUWR6jot5TX1VNbWN/zVSH9rJIP20nOaS+fUaKiovWTsXjynokZDraZ5z20PIZ2talArZHwyoxd3BNhevzEmk8GotySjeOcbUFUM05aDqmuu4QRBYGKQPUN9rPhoaxI/HkhhU2wOi8Z4MyXY4aaHUWu1IjEZxWxNyGVbQi5n88oBSfD2yWGejPC1xtfWqHPKNcoVMOQFWD0P4tdIKZv/ArreqqqLIlM2EdVSqkGUwkGVajUqFxeor+fcvmQQpCLqeXGpKIztwHYwnN91XX1ZGKhxtdBnb5ku97idJfa8I0GJCezJ2oODgQNuxlfJw/MeCwodjm9JQa3ngO8Au+bPa8BGpsPXIz5mefxyPj/+OQczjyPXnYyP1UDKT57EZHLby6So9ZQMv9eXPz8+zr7VZxkyq3nF6htF1IpErkhC10hFyB0uN7Xtbm4cR28zIu7xZsfyRHb9fIphc326RJ3d88fz2bYsHl0jFROfCsLMrvkNH4VKxdB7H8KlZzBbln7CLy8+zaA599Nr5NjG11FakMfeX5Zzev9eDMzMGfPYM1Jo9E3aLdU3VhM63o3eo104eyyPkzvT2ft7Egf/PIdPfzsChthjbNm1Fg+Wry+ldPRoclccwCnikq0iVyjxDh9E7M4tVFeUo6Pfupro3XRzLS7lEN9+HuKC8lpEseUaxIbDh1/1eplKhV5oHyri0+GpJ2D/p1Kkmc/49hoyoihiH7uOmpocBjlnE6x3llozOckWEzhT60rivj2c3LEZtZ4+br1D8QwLxyUwCKW65dBPURSpqtNQVFlHbm4F5w/mkh9TgLZSg2iooMLPkFwzBfsqiihamUdxZS1FFbWUVte32G5TZKIWk+oyLKuKsawqxqKqBIuGx5ZVJVhWFWNaXYr8H/dilVKHIn0TivVNKTHzosTZjHIjM8qMzKkwMqPK1AJRRxeFXIaqwQhXyGSoRAHDgnJedKqmXq+K/N0FnBo/ki+mvUqO2pz+cvjqkyOsNqmnoq4ebSu/Aiq5DD21HH2VAn21HH21An2VAktDdcMxBXpqOQYNj5ueo69WYFpuCath0RgfBL82qF0LglQZRddMyin+eTLM/F3KNe6iGOkoeXWCH9NCHHj5zzieW3WSFYfTee1Of3ztjNrUdk29hv3nCtmWkMv2hFzyymqQywT6uplxd5gTw32tcTDtInO+32SI/BB2vy2lbMpvfXPy1n8FHUTTOn1ik/I0Sh0d1K5SiGlKfDHWLkboGanIK6vBz84I/GZct0EMUh7x9sRcPn5gJAmvx3DgtwNE94hmao+pVzcs1IYU2d/F+WgrQkbbtcqLJJfJmRcwjzDbMOb9/RR6zt+wbXcOfSsr0QsOuu5xN4e9lym9hjtxYlsaLgHmuARYXPuiVpJ0KIfc5FKG3uPTJRUXb2e8+9pSVljNofXJGJnrEDq+EwSVGhBFkZgd6exbfRZrFyPueDiwVWW53IL6cM97n7Fl6cfsXPYlKSeOMvS+BcTt3saRdWsA6DtlJqETpqDUucm5Og3IlTK8wmzoEWpNbrKkTh27K4OYnem4BFgQGOHQKWF/zY7V1gXLGcPJ+WEHZRk6NF0e+A0ayoktG0g6uI/AYaM6bYzd/LsQGrZdOv/u73jyyq5eg1hTVobmwgVJUKsF9MPDyXvnXep8vkCZEgl/PQZ2QWDcPpUa9q/8lfPRkZT4jcBK8xn1nmNQWXriFf0lXjIl9fc8SbJ+OImHD3P26CESI3chU6pQu/qBcwDlVj0oqpdLBm2DR7eospaiyjrMqkWCaxR418mRI3BOoeGYfj0psioMCqowqVRiqqfCRE+Js5kepnpKTPRUmOopMdVXYaKnwqQwG3laCtrcHMS8XMjLhfw8ZAV5yC8UIPxDB0GjUlNrakG1tQVVJh4km5hTbmxOhaFk8JYYmlGl1KVOq6VeI1Kn0VKnEanXai891mipK6+lTqOlXiudc/Fc3foSXgSWW03EdXgWvjtO8vjvi/l15otccHfHPqmS2XbmKN0M0VMpMFDL0WvWkJVjoFagp1KgamvEVq60oXnTvnN95oGuKayZD8vHwt1rwND62td1In52xqxaEM6qYxm8s+kU4z+P4p5+zjw9osd1lTIqqaxj1+k8tiXksvt0HhW1GvRVcgZ7WTLS14YIL6sOC8u+LmQyiFgMK2bDyRUQNLuzR9Rmui2IViI0Cd3RKi5NPgqlCpWLCzUqYwqLIGyQhSTjX1rNUG8r8B4Hyuvf0QlxMWXl0QyyFcb09M7naKIrBjYWUrmlFjheNg45dQS4pgJXL5XzT/wt/LEoWUSZwSoSd/9JX6DI04a27Xddou8EN9ITCtn50ylmvhyKrmHb5etrq+vZv/YcVi5GePe1uQmj7OZmE3KHC6UFVRzemIKhuQ4+4S1HLbQHWo2WyBVniNubiXuwJcPv9UWhan3ovr6JKZMWvcrxzevZ+/Myvn18HgDe/QczcNZcjCys2mvolyEIAjZuxti4GVMxxYO4vZnER2ay7mQBZnb6RNztjY1b++XptxaTZz+iaGMwOYeNKVu4GJWbOypnZwwdHTG1sSNhz45ug7ibm4bYUID4djSIc0trgOY9xBcVppVOLRvEBv37kwdURB/GZMp38NUgyTCZux5ucj5v/J4dHFz9G/4RI7AYMwfxu8/Yn1HH97XjURv3ZGbJNwyOfAs90YJtdTPYaDUTu+oc3CvO43buNAZJxwEZNQZOVFp6ga03jiYm9FXoYFpWg6K8DkEpYBRoimOoFSMcDTHRU2Kiq2rRCNRWVFC6aRNFK1dSHXMSADkgqFQobGxQ2tig7BEmhTDbWEvHbG1R2tggMzZu383IygvwHjw90gf6fkLVX5+S/vIS5v76Bo5ffMS6agsMUmuYdX8QyuuY17oc/pNBxxhW3A3LRsGctWDmen1tVJdIbXQQMpnAXSGOjPS15oOtp1m+P4UNJ7N5aawPE3raXfW+yCyuYntDfeDo8xeo14pYGKiZ0Muekb7W9HM3R0d5C3yW3mPBthfseUdK2VTc2Lpe1GjQFBejMO9cQdxug7iVXFZ2SX7JIJap1MhNTLjgGAqAa6BFY+6GtZEa1AZwxwfXHQIS4iLlNR5NvcDEOWM5/PJB+qWNJcQ65KrXlBfVcPq0Cl+DbeglF0PPYa3ur06j5UxuHfe6P8mo6mLyTWJ57tDDLJYtZrzb+Db/4MuVMkbc78cfbx9m18+nGLMgoM1tHt2USmVJrdRWFxUbut0RBIEhd3tTUVzD7p9PY2Cqg6OP2bUvvEnUVtWz5ds40uIvEDzKib53ut/QvSIIAsFjJuDoG8CxTevwHzICe2/fdhhx69A3URM2wY2QMS6cOZrLoXXJrPngGKHjXAge7dKpZSEEpQq7d94m76OPqIpLoHTr9kYVUUsrE5JszYmdOgUzZ1dUzs6onJ1QOTujdHZGbmLSJTzd3dxCiFc8uG3ILb26h/iyGsQtoPLwQGFlRfm+fZhMnQpjP4S1D0nhkIOfv2ljTU+IZetXn+HoF8jwBx5BJldQqJJTUVNPXlkNpnqOrLR4m7OaOMbnfMZnFZ/zqmkvUkIWI3eajomOnNrsFLJORHPm0AFKk7dAylYUakcQ3DGx9Sdoui/efW1bFS0miiLVsbEUr1xF6caNaCsrUXm4Y7VoIXp9+qC0sUFu1oxKcieje+cTOJm7kPb4QtIWPEWfpxfx935nYran3/ppYx7D4J518Os0ySi+ew3YtFI0LCcOvuwvhe+Oea9DPcwmeiremBjAXSGOvPxnHE/+foLfDqXx+p3+eFobIooiidllbGswguOzSgFwt9TnwUFujPC1ppeDSZcq59QqBAGGvgS/TIUTP0PI/dfdhKjRkL34JSqPHMF17Rrkhp1XrrPbIG4lguLSW6WV6zU5Lu2IFNoEo6stw8xOn3P5FUCTSeoGQgncLPQx01dxOKWIaSEOxDvsISB1DAUHD2Hbv3+z18TsTEfUivQKroPE9TD2I6mQdis4l19Obb0WXxtDjE5nYdxvKD7mZSyOWkxURhQv9XsJI1Xb/MXm9gb0vdOd/avPkrg/u1HZ90YozqvkxI40vPratKt6dTdtRy6XMXp+AGs+OMqmr2KZ/GxvLBzaP4e07EI1G5ec5EJ2BUNme+E3sA25Tg1YOrsyasGTN2F0Nwe5UoZ3X1tcAy3Y8+tpotclk5ZwgRH3+2Fo1j4h3K1BZ8B4nAZIeYja2lrqMjKpTU1B71QiSbv/JkNHgW5MDKWbNl1WckNmZNRgJDujcnJC5SI9Vjo5oTC98brQ3fyLafQQ32KLyZtAXlkNggAWBld6ZmpTGwxix5ZDnwVBQL9/f8p37kTUaBB6zoBzO6XcQNdB4NS3zeO8kJXJug/exMTahglPvyiJZdWUYSGvYkyAI2PuHNjk7GDQzoYTv2K+4zXMt02TVG2HvQKBAZiauFCv6cfp6DjqqpKQCcnUVu2i4Pwu4nZ4UVMWjmdoOCY2ts2ORVNSQsm69RSvWkXN6dMIuroYjRmDybSp6Pbq1eUM4ObQGTAB55/sSLv3Xuo+eAfnsf/h6BYZPv1t0TduPp/8lsGxD9y3GX6aBMvvgFl/tO4erMiT/ib8KaUpjngNgu7pUAXkQAcT1jzSnxWH03l38ynGfBLJKD8bYjKKySiqQhAg2MmURWO8GeFrjbvlv0BLw2M4OIbBnveh56xW2xxwyRgu+fNPLB5/rFONYeg2iFtPk4RxjXDpAxeUKupqNRQo7bEvOIwg3Hkpr+cqQhetQRAEejubcjS1iMQLiURb76RnZjgH/yxlYr9+Vwj31FTWER+ZiUdvK4zDxkDS13BmK/hOaFV/8ZnSjpWvUE59fj42Yf35buQ0lsUtY8mJJcTkx/D2wLcJtm5dXeKr0WuYI6lxBUT9cQb7HqY3XPNv36qzyOWyLlWYvpuro9JVMO6xnqx6VyrHNHVhCAam7Tdx56eVsWFJDPU1GsY/1hNH347zSncGaj0lI+b54eRvzt7fkljxxiEGz/LCM6Tz87BkKhVqN1fUbq4YRkTgVJBBZn4eo3/6HbGujrqMDGpTUxv/1aWmUnX8OKUbN16mxCozNr5kKDs7S8Zyw2O5SdcVYemmvbmYQ3z7eYjzSquxMFCjaEbhtzYtDYWVFTK9a6ds6ffvT8natVQnJKIb4C9FtaVHw+oHYEFUm0SOKktLWPvuqwhyOZMWvYqOQYMRsPMNqCmFoDlXXiSTQ/Ac8JsIUf9Ds28p54/kEiu7n+wCQxRqOQGDexEQMQ5TG30KM9M5e+gAZw7tZ+8v37P3l++xdHLBIzScHmHhmDk4UXXkCMWrVlG2ZStiTQ06fn7YvPoqRuPGIje49QwTtX8Iziv/Im3WVOy3LyW99ytErznN0PsCO3tobcfKG+ZtkYziHyfCXT9Cj5Gtu/bOJRDzO6x/Ek7+AeM/AYuOK8cplwnMCnNitL8N7246xaa4bPq4mPFYhAfDfKyxNLzFNyz+yUUv8Q/j4ej30PfhVl0majRkv7iYkr/+wuLxx7B89NF2Hui16TaIW0nTkOk6Lt3QMqWajMQLaJFjnnkITVkZeY15PW3z0PRxMWVbQi5/nz2JRl5HUHA5Rw85kr5zF07DLw+HjtubSV21hqCRzmDvLdU9wDhQKAAAIABJREFUjlvVeoM4qxQdpQzLlFPkALpBwchlch4MfJAw2zAW7l3IfVvuY37gfB4KfAiF7MZuHUEmMGyuL7+/fojt3ycw6ZkgZNcp15+WUEjKyQL6TXK/9XdDbyMMTHUY91ggaz44xoYlMUx+JrhdhNCSTxaw9ds4dAyUTH6uN+b2t95i50YQBAHvvrbYuhuzbVkCW7+NJy3hAgPv8uxSZZp8Bw1j8xf/I+t0Ivbevqjd3FC7XSm4JnmWM6hNaTCW0xqM5WPHrjCW5cbGKC96lpuEYaucnZEbd0eQtBZBEEYDnyClUH4riuI7/3jeCfgBMGk4Z5Eoin8LgjACeAdQAbXAc6Io7uyYUd9+hvBF8spqsLrKArs2LRXVNfKHL6LfT/LAVezbJxnEOkYwZRksGykZFtOWX5Lzvg7q6+pY9+GblBUWcNd/3sLEukHrI+MIRH8FfR4Ap7CrXl9Vqya+9h7iygdTUarBSJ7DAIsteE8YgjpkYKP3z9zeEfNJjoRNuovS/DzOHDrAmUP7OLD6Nw6s+hV9jYh1YTG2dSLOkydhOm0aOr6dl/Jys1C5uuO8agOps6bikLGTRIYR0CcRS3+fzh5a2zFxkjzFv0yB32fCxC8hcNq1rzP3kPLfj/8EW1+Cpf0lJev+T95wjuuNYKav4t2pgbw79V+wQXEtXAdJ/yI/hOB7QNVyuVbJGH6Rkr/WYfHE41g+8kgHDbRl2jWWQBCE0YIgnBYE4awgCIuaed5JEIRdgiAcFwThpCAIdzQcN284Xi4Iwuf/uKa3IAixDW1+KnRQfEtTlenqJvsIokJJ8skClAoRk+Kz1CYnN+b1tNUg7u0sebV2pu2mp2VP+syaiqGykIMbsxGbhBnW12mI2ZmBk68Zlk6GkjfbbzKc3gzVpa3qKyG7BG8bI6qPH0dmaIja85IgV6BlIKsmrGKc2zi+jPmSezffS0ZZxg2/LkMzHQbN6EHO+RKObUm7rms1Gi1Rf5zB2FKXnkMdWzxXFEXOFZ/j54SfWXNmDbkVuTc85m5uDhYOhox+0J8LWRVs/iYOzVXqHN4oMTvT+XvpScxs9Zm6MOS2MYabYmypx6Rng+k9xplTB7L5483D5Ka07negI/AMC0ehVpOwt2V7SfIsu2E4NALz++7F9pVXcFq2DI+dO/CKOYHbxg04fLEEq4ULMRwzGpm+HlVHj1KwZAlZzz1Pyl3TSQrrS1JYX5KnTyfz5yPkx6gu++3s5hKCIMiBJcAYwBeYKQjCP62Gl4A/RFEMAmYAXzQcLwDGi6IYAMwFfuqYUV/aFzHVSHVnbydyS6uvus6oS01D2URhOrfchvRiJ0Txyg0Ehbk5al8fKvbtu3TQoTcMfVkKQT3243WPTRRFtiz9mMxTCYx+5P+w69FgpNXXwrrHwdAWhv2n2Wvz08vY8WMiP7ywn+i/zmNmb8TYRwKZ/awLPZ0SUP/9IHw7DNKir7jW0MwcLyMzBlyoZtipNPzT89FXKEm2NmOfvRnrcpPZfyiS9IRYtFpNM73fWijt7XFe+ReedcdR1lWyd8lOxLMdtBfV3hhYwtwN4NgX1jwgbaK0BkGQDLNHD0vCT7vekMTi0g+173hvZyJegop8OPR1i6c1NYYtn3yiyxjD0I4e4iaT6wggAzgsCMI6URQTmpx2cXJd2jDx/g24ANXAy4B/w7+mLAUeBKIbzh8NbGqv19FIk5DparGJ+ptMQUpsIQ7u+si2aySDWOaJvkqSuG8L/vZGqNVlZFSeYYr3k8h1dAkdIGfHLjvOb9jExWDhUwdyqCqtJWhUE/GMgKlw6Cs4tRF6zWyxH1EUScgqZVxPOyo3HUU3qNcVIdn6Sn3eHPAmA+wH8NqB15i2fhov9X2JsW5jb+i19Qi1JiW2gMMbknHyM8PKuXX5yXG7MynKqeSORwKRK6/cz6msqyQ6O5qozCiiMqPIqsi6vF/THgywH8BA+4H0tOqJUtYF5ez/5Tj5mTNkthe7fjrFnl9PE3G3d5vztrRakaiVZ4jdlYFrTwtGzPO7tRU324hcLqPvne44+ZqxbVkCa947SugEV4JGOne6cIdKR5ceoeGcPhBJxL3zUaiuf9deplKhdndH7X5lyoS2poa69HRq09Iu8y5XnjpDRZUSyw7MKbvFCAXOiqJ4HkAQhN+BO4Gmc7YIjcUHjIEsAFEUjzc5Jx7QFQRBLYpiTXsP+qLKtFedgrNH8/AKu30qDuSW1hBgf2UEhLaykvr8/EZBLY1Gy99np1BZZ4jVu0cJHeeKk9/lglEG/ftTuPwHNOUVyA0aPDzhT0j5mJsXSXmcll6tHtuBVb9yat8e+k+fg3f4oEtP7P8E8hJgxm+SJ/rimDVakmMKOLkrg6wzxShUMrzDbQkc4tCkXrwFPLBTKvOy47+SB9t/Cgx/lboqJcWr11C8ZjX1WdnITU2xnTkbn2lTUbu5UVVexvmjhzhz6ACx27dwfNN6dI2M8ejTF8/QcJz8A6Xc5lsQpZUVHr/+QvajH5GgHEbCex/id38ShD10Q579LoWOEdy9GlbPg03PQ2UhDHmhda/L0BqmfQ89Z8CGp+G7kVJUwrD/XHbvtRdVsXEU/fYblk88jtLmX/675BQGniMh6mNJXKsZtW9RoyHrhRcoXbcey6eexGLBgk4Y6NVpzzi6tkyuFUCUIAiX1Q0SBMEWMBJF8WDD/38EJtIBBnHTkOkq8dKCKuN8EVWltbjd6YL4rZyalBRyLR2xaqN3GECtkOPkkEYONJZb6jFlAsf2ryR6B7iGa0EUOL4tDStnQ+x7NMnzcegjhZzErbqmQZxRVEVpdT2BRgK1Z89hPG78Vc8d4zqGQMtAXoh8gUWRi4jKjGJx2GIMVNfniRMEgcEzvcg+W8L27xOY9mKfaxowVWW1HNqQjJOvGS4Bkjy7KIoklyYTlRFFZGYkR3OPUqetQ0+hR5htGA8EPsAAuwGU15UTlSmd82P8jyyLW4aB0oB+dv0YaD+Q/vb9sdLrmPI53YBvfzvKCqs58ncKRua6bVLHrK2uZ9t38aTEFtJruCP9Jnt0utHXVbDzNGX6S6Hs/uU0B/88T3rCBYbf54uBaecJboEUNp0QuYtzRw/h1W/ATW1bplaj9vBA7fGPsnObX0Q8cv2ertsIeyC9yf8zgH/Gs74KbBUE4XFAHxjeTDtTgGMdYQwDl0VMV5fXdUiXXYF6jZbCippm1xq16dLHeLEGcWpsIZV1hvhZnyStNJQNn8dg7WpE6HhXHH3MGoW1Cr/5lsrDhzCMiJAakslg0lewNBxWzYMHtrdKNCchchcHVv2G3+DhhE2669ITBWck8R3fieB9ByB9Zgn7sojdnUF5UQ2G5jqET/HAJ9wWHf1mDFSZTFrT+E5A3Ps/yv74iuIv9lCRrQIE9MPDsX7+eQyHDkVostmma2CI3+Bh+A0eRm11FcnHj3Lm0H5O7dtL7I4tqPX0cesdimdoP1yD+qBQ3lrGscLcnAFLniVt4Q6ia+/G7utnMM2NlcRVFbd4aplSB6b9IIXv73lXKkU15r3WC2b1GAWPHpTy1qMbHEVjP2y8B9uD6tNJpD3wANqSEiqionD8cum/Iky/RSJehK+HwMGlMOTyoGBRoyFr0QuUrl+P5VNPYbHgoc4ZYwu0p0F8sybXf7bZNFY3o+HYFQiCMB+YD+DUyjyaFmkSMl2hvfhYzslNyQgyOS69rMlwsKc2OYU8dZ+r5vVcLwqDBLTlptjrSfXYZAoFYcOM2Py3Lkkx1cj1AijNryJ8vv/lXjZBAP+psO8TqCgAfYur9hGfVQKA74UUAPR6tyycZW9gz7JRy/gm9hu+ivmK43nHeWfgO/Sy6nVdr01HX8mwuT6s++QEB9acY9CMHi2ef3DdeeprNPSe5MjejL1EZkYSlRlFZnkmAO7G7szynsVAh4EEWwWjlF8+oXmaenKf/32U15ZzMPtgo4G8LXUbAF6mXgx0GMgA+wH0tOx5w3nS3bSO0PGulBVWE73uPIbmOjfk2SkvqmHjFzEUZpQzeGYP/Ae3rKh6O6Kjr2TUg36cOmDG3hVn+P31Q0Tc7Y17cOdtADn6B2BgZk7C3h033SBuCaHbOdxWZgLLRVH8UBCEfsBPgiD4i6KoBRAEwQ94F7iqAs7NnpvFJo9udgpGV6agvBZRvEoN4tRUgMYc4oSoLPSVZQxy2404+wkS92dzdFMK6z+NwcbNmNDxrtgFBSHo6FCx/8AlgxjA0EbK3/x1Gmx/Bca82+K4MhLi2PrlJzj6BTJi/qOX1iVarWTQKHXQjnqXzFMXSIrO4cyRPDR1Wuy9TBk4vQcugRbX3NCsTU2leNUqitduQVNggMJQhblPISYBeqgmDoGeI1s0llQ6unj1G4BXvwHU19aSFhdDUvQ+zh2JJjFyF+aWpox5+hWs3Tyu2kZXRGluysCH+rLpuyQOn51I2NrVGOcnwfSfO7QMUbsgV8Cdn4OeKez/DKqKYOLS1ucFqw2lezdgGqx7QspL9pkAd7wv3eM3kdqUFNLmzUOmVmP3xRfkvPE6KbPvxv7DDzAcOvSm9tWlsAsC73FwYAmEzgc9Ke1T1GjIWriI0g0bsPy//8PiofmdPNDm6ewVf4uTa1sQRfFr4GuAkJCQNqtuNC27VKmRXTxIfb4MO09jdPSVqFxcqE1OJs++hp4ObVc9ra6vJrc+jvryYGIySujnLnlF3caNwXLXrxwqnoi6shZjK11ce1le2UDAVIj6COLXQuiDV+0nIasUmQAWKacoUSrRCQi45tgUMgUP93yYfrb9WBS5iHs338uCngt4MOBB5LLWh6o6+pgRONSBkzszcAkwx8nvysLcoigSk3iahMhM8j2SGL/7WWq1tegqdAmzDeN+//sZYD8AO4PWlXEyUBkw3Hk4w52HI4oiSUVJjSHW38d9z7ex32KoNKSfXT8G2A9ggP0ALPWaeX+7aROCIBAxx5vy4mp2/piIvokaB6/Wl9YpyChjw+cnqa2qZ+yjPXH279yi7l0ZQRDwCbfD1t2Ebcvi2fx1HL79bRlwVw+U6o4PLZfJ5PgMjODI+jVUlhSjZ9ytEt0FyASaCjM4NBxryjykNCVEUTwgCIIOYAHkCYLgAKwF7hFF8dzVOrnZc3NTk1hbf/sIbDVWs2ihBrHSyYmyC9WkxRcSbHMSmSCCQob/IHt8+tmSuD+LI5tSWffJCWw9jHHpMxZl0zzii/QYCWEPQ/RScIsAr9HNjqkoO5O/PnwTI6sm5ZUaEI/9SMHZbE7bfcbZt85SUVKLUkeOV5gNgREO19R70NbUULZ1G8UrV1J56BDI5RgMGYLJ1CkYDByIkHNCCu3+61Epj3HU2+DSfInKpihUKtyC++AW3AetRsO5Rb7syHLi1xefpO/wQYTd9ywy+a2TfuMaYo9jZC4p4nhs9h/CkSRMSyMko9i+bVVCOh1BgJFvgJ6FtDlTXSwpUF9DxOkyHELgoT2w/1PY/S6c3wMj/gvBc29Kiaa6rCxS778fNBqcfliO2t0d3QB/0h9+hIxHH8Nq4fOYzZ17S5T3uiEiXpQ88Ps/g+GvINbXS57hLm4MQ/saxG2aXFtos6kLqLk22wWhMr/xcYVW+tJoGr48LoGS91Xt4kpl9CHyAiqx9m37btyhnEPUaWuoL/fhaOqFRoNYkMnoe4cN61fLKKuGIVOcmt9RtfYDSx+IW92iQRyfVYq7pQG1R0+g6+uLTKf14ZS9rHqxcvxK3ox+kyUnlnAg6wALei64rtxcdT8R3VgZm78/ScAj+ij1pPe1tLaUA1kHiMqIonf0ZEwUVsQ672SGy4xGL7BK3jbVQEEQ8DLzwsvMi3kB8yirLbvkPc6IZGvqVgB8zHwajeNAy8Bu7/FNQq6QMeahAFa/f4xNX8Yy+blgzO2khZEoiqSWpmJvYH+Ftz8ltoCt38aj1lMw+blgLBw6t37drYKJtR6Tn+vNofXJHNuaStbZEkbc79vqHP6bie/ACA7/tYpT+/YQfMedHd5/N1dwGPAUBMEVaV6dAcz6xzlpwDBguSAIPoAOkC8IggmwEUl1uhmLqiMQbytRrdzGahbNeYjTkJubIzcwIHFDMqIIvpYnLztHrpThP9gB73BbEqKyObY5hQPyoZgYOaDYfxbn8H94R0f8F1Kj4K9HYME+MLq8zm9VWSlr3nkVQRCYvPCVxvJKJflVnIk6S9IOBUX1HyIrFnDyM6J/qDWugRYorpEqVZ2URPHKVZSsW4e2pASlgwOWTz2F8aRJKK2bRLk4hMC8bdJ6Z9srUg1bnwlSPVoz11a9pzK5HE+jCzi4ObIztpz92yI5fziK0Q8/hXmvYdduoAsgCAL9p3ux4o0SMsLnodzzKaJcwOz7MTDh89YpNXd1BjwFuqaw4SmpLNOsFdd3vVwJA5+RQvfXPym1c7FEk2XLkYotUV9QQNp996MtK8e5wRgGUFha4vzTj2Q9v5C8d96lNjUVm8WLL3O0/Wuw9pPy+qO/RAyZT9brH1K6cSOWTz+Nxfyr2yFdgfb8NG54cr1ag6IoZguCUCoIQl8kUa17gM/aY/BXcHZz48Oyeslgq5HJ0EWL1lkKOVa5uiJWV2NQWoS1Udtl73en70ZPoYetQSCHU4oue85haATmP39HudwCd48WjM+AqbDzdShOB5PmVZnjs0oJdzKkOjYW09mzr3uchipD3hn4Dv3t+vNm9JvM33b9O0DmtvZMjn2ald9Gsq3H8oulJdFV6DK8djK2Ze70mmbNs8NWXnfb14OhypARziMY4Tyi0XscmRlJZEYky+KkMHFDlSHhduGNBrKF7tXD0bu5Nmo9JeMeC2R1kxrFmdpU3jv8HtHZ0VjoWnBXj7uY5jUNC10LYndnELkiCXMHA8Y92hN9k1s8P6qDkSuk+t2OvmZs/z6B1e8dJexON4KGOyF0YO61haMz1m4exO/d2W0QdwFEUawXBOExYAtSSaVloijGC4LwGnBEFMV1wDPAN4Ig/B+Sa/ZeURTFhus8gP8IgnBROnikKIpX29y+eeNu6iG+jUKmW6pmUZuWhsrJCa1WJHFfFo4+phipSwDdK85VKOUERjjgO8CWmDUxHNtszYYf07CPLpNCqT0aojcUaqkU09eDYe18mPNXo0etvq6Ovz54k7KCfKa9/BZqfXNid2eQdCiHnPOSwr2tqoTBd/rjMdAPHYOWN8y1FRWUbtpE0cqVVMecRFAqMRwxApNpU9ELC7tC9LMRQZDWPF53SGGbUR9B0mapNurAZ1stpKTrOYCxDyzE85fX2bb1GD+/8wEDen5H8Pw3EcxbZ1x3Jub2BvgOsCNxn4DrsDvJ3fEXotId8zUPQG6cJCp1HZF8XZLec6X62KsfgOVjIfzx62/D3L2hRNPPUommLy+WaHrquks0aYqLSbt/HnV5eTh9990V+cIyXV3sP/mY/I8+ovDb76jLyMT+fx/dkjWwr8mQFxBj15D18D2UHs/A8pmnsXiwaxvD0I4GcVsmVwBBEFKQBLdUgiBMRJpcE4BHgOVIv+yb6AiFabhsoXgxh7hCULOr94dk5faid4+3Ubm4AOBQnt9mUS1RFNmTsYdwu3B09axYfyILjVZE3jCOyoMH8T36JVqZivxXt+Lw5dLmQzD8p0gGcdxqaVftHxSW15BTWk1oXRVibe0184dbYrz7ePrZ9eNc8VWj5Vok27IW+Y4ghtmEYdFTiVquxsOgB6tfP46uk4p+ER0rSNDUe/xAwAOU1pZyMOtgY+7ylpQtwCXv8SCHQQRYBFxXyHg3Ekbmuox7rCdrPjjKt+9u4UfP19HRUfFIr0c4mX+SL2K+4JuT33DXhccxPOWMS6AFI+737VL1dW81HLxMmfFSKLt+OcWBNedIT7jAsLm+GJh23AaD76Ch7Fr+NQVpKVg4uXRYv900jyiKfyNVb2h67D9NHicAV8ShiqL4BvBGuw+wRUQ0t1XIdA2CAOb6Vy7ca9PS0A8NJS2+kPKiGvpP9YQTLbenUMoJvisI/S+eIzdwIueyerH2g2M4+pgSOt4NGzdjyXsWsRi2LoaCJLDyRhRFtn71KZmn4gm+40GOb68jLWEfolbEzE6fvv3r8Ex6FKMRC2DQ1XVGRFGkOjaW4pWrKN24EW1lJSp3d6wWLcT4zjtRmLY+nQaVHgx+DoLuhh2vSVoqJ36FoS9B0JzWGYNKXXrc+xb2o5LZ+tFL7D5Rwtln5zJ6TAjGY19sUZelKxA63o2kw7mc95hBL3UdeX//jXbkICyiPkbIS4Ap3zarBHxL4Xsn6JjA77Ng/ZXr21YhCBA8RxLe2rwIdr0prZfHf9pijeymaMorSHvoIWqTk3H86kv0goOa70omw+rZZ1E6OZHz2uukzpyF45dLUdo3K4V0yyKauJB1KpDSmAysHnsQ81vAGIZ2ziG+0cm14TmXqxw/wpWlmNqdpjuSdYKi4a+KILd+bE5ey5PBT2LuKu0cOpTntVlU69SFU+RV5jHYcTCiiSm/RqeRlFuGj60RoiiS/7+P0bMyxXT2LPLe/4DiFX9gOmP6lQ2ZuYJ9CMSuatYgTsiWdm89cyUjVjeo+S9ya7HQtbhhj6l2isifqcfI3FzOwJDeGFnoEr3uPOVFNYyc59fpysFGKiNGuoxkpMtIRFHk1IVTjbnHF73HRiojwu3CGegwkHC78G7vcSup09SxqWwt23rsZHDc3czJXMTcp4dhqid5J87mn2fDN8dQpplx0mY3+53PI2TNYoTziCvCqbtpPToGSkbP9ydxXzaRfySx4o1DRMzxxq05TYJ2wDt8EHt++o6EyF0Mmn1fh/TZzb+Lpibw7RQynVdajYWBGoX8cm+ptrqa+uxslM5OnIjKQtdQiWtPi2saxCBtAhuFhyHb+Qv9dj9EfFQOx7emsvq9ozj5mtFnvCs2Jg1CaNp6NBotW75cRmLkLlT6/Uk4YIiBaTm9hjvSI9QGCwstLAkDWyvo/2SzfWpKSihZt57iVauoOX0aQVcXozFjMJk6VSoB2ZZcSyNbmLRUShnb8qIUHnvoGxj1FrgNblUT+rauTHzvZ+I3rWTXLz/xw9pEhkQPJWD83Qjhj15f/moHomekImSMCwfWniPw8RcwVqspWLsWcdxELM9uQPhmGMz8HSxuLeGwK3AbLHl5f5kKlW0QtjewgqnLIHA6bHwGlo2CPvNg2CstRhZoq6vJePRRquPicfj0E/TDw6/Zleldd6FycCDjyadInj4Dx6VfoNsK7Z5bAbG+nqznn6c0Jg+rXuWYu2Rc+6IuQrfmZmtpOukIAhqZgnpBwVDbyYiI/JL4CworSzQ6utiXFzQbxnQ97M7YjYDAQPuBhDhLSm1HUi4AULZ9O9VxcVg89hhm992Hfng/ct99l9qUlOYbC5gGubGQf/qKp+KzJIPYNPkUKhcXFOadJ0wkkwkMv1fyAm9fnkBJfhXHt6Xh2ccaW4+uJbojCAI+5j48GPggP4z5gT3T9/D+4PeJcIzgcM5hFkctJuKPCKZvmM7nxz/nRN4JNFpNZw+7yyGKInvS9zB53WTeP/I+Fl46+E+yQJlhSsyfuYiiSEVJDce+KUSVbkboFCcGTfeiuKaIhZELGbV6FF/GfElBVUFnv5RbFkEQ8B1gx10v9sHATM2mL2PZ/csp6mrb/37VMzbBpVdvEiN3oe3+fnRzQzQV1bqNDOKymmY33usypAVovZUzKbGFePe1Ra5o/VJPPzwcTUkJmrNJBI1wYs4b4fSb5E5eapmU1rJOh3PVfdm7sYyvH/+axL1/otTzx2/oRCY9E8Q9b4YTPtkDCwcDqVZwWTZM+EzK2/zna/jgA84MGkzum28iKBTYvPoKnnv3YPfWm+gFB9084SH7YLhvk1S6p6YUfpwAv82CwtZFswmCgP8ddzH34++w8fBiW4YTa3/8k/L3e8Phb0HTNct9BQ51wNBch31rzmP9+uuYzJxB4YZD5FbORKy8AN8MhTPbO3uYbcc+GO7bLAljWbUxkrDHKHjkIIQtgMPfSRs6iRuaPVWsqyPzyaeoPHQIu3fexnBY6/PM9cPDcfn9N2Q6OqTOuYfSLVvbNu4ugFhfT+Zzz1H69yasnnsO87unwbEfoSi1s4fWKroN4lbyz9w6Ua6kXqbAVGXDSOeRrExaSXldOZVW9jfFQ7w3fS8BlgGY65rjYKqLtZGaI6lFiBoN+Z98gsrNDeMJ4xFkMmzffhtBpSJz4ULE+vorG/ObJNUbiV11xVPxWaXYG+tQH3MC3TaES98sjCx0GXhXD7LPlrDmg6MIAoRPdu/sYV0TY7Uxo11G88aAN9h5105WjFvB40GPo5Kp+Cb2G+ZsmsPgPwbz/N7nWX9uPYVVhZ095E7nbNFZFmxfwGM7HwNgybAlLB2+lIhRvQge5UT83kwi/zjDqneOUJRbyR0PB9JnhAezfWazftJ6lgxbQg/THiw5sYSRq0ayOGox8YXxnfyqbl1MbfSZ+nwIvUY4ER+Zxcq3DpOfXtbu/foNGkp50QXSYmPava9u/n2Ijfbw7RUynVtafdX8YYDUcgtErYjvgNZVX7iIfng/ACoa1KaVajnBo5yZ82Y/+k50IydHzubihcRGn6HqwiYsnLxY8MV/GXq3D3aeppfWSmkHJWMxbAE49L6in9KtWyn89jsMhw3Ddc1qXFevwnTGDOSG7SSQKAjgNxEePSx5/ZL3SMbOlsVQVdyqJowsrZj234+IuPch0mut+CHBnVO/vgNLQqUwW23X2pBRKOWET/agMLOc0wdzsfnPfzCbO5eiv3aQc2EiorGDVE5r/2dNv0i3JpY9YMKnrc4TbxG1AYx5Bx7YIZUOWjEbVtwNpdmNp0ilhBZSvmcPNq+8gvH48dffjbs7Lit+R8fbm8wnn6Tw228Rb9HPQayvJ/PZ5yjbtFkyhufdD4OelWyPve919vBaRbdB3EqEf4QliQo1tYKSOq0iPAiFAAAgAElEQVTIXP+5VNRVsDppNRfMbHAsz0dffePR6PmV+cQVxjHYQQrpEQSBEGczjqQUUbphA7Vnz2H5xBONCnVKa2tsX32F6piTFHz51ZUNGlqDy0CIXXnFj158Vgn91RVoiovRC+58gxjAu58NbkGWVJbU0nu0CwambfO2dzQyQYavuS/zA+fz0x0/sXf6Xt4f9D6DHQYTnR3Ni1EvEvFHBPO3zmdP+h60ba8ydktRXF3MmwffZOr6qcQWxLKwz0LW3LmGQQ6DGj0Cfe90xzPEithdGYhakcnPBDequYP0Hg9yGMSXI77kr4l/McVzCttStzFjwwzm/D2HzcmbqdN2zV37/2fvrOOiSP84/p4turtDQQW7A1Hsbs/O+9ntecadp97ZcWee3llnd9fZ3WCfYiKigICINCzszu+PxUBQUQlF3q/XvsCdmef5Du7OzOf51peMVC6hWuvCNBtSmuTEVLZM9+Pq4SBEdc7dpF3LVkRLT49bJ4/m2BwF5F/EN9aqv6U+xGExye+sMC0icPduCnbuxhhb6X7UuDIzM7Q8ir0SxC9RaMso18CZrt3i8dH/DVL2YGxtzXfjJ6DQecuO1GRNr1cjB03e7luoYmJ4OnEiWh7FsJ0xPUMBohxFrg3Vh8Ogy1C6g6b41rwymlBqVSYOhbcQJBLKNmxKlxl/YuJclL0hRdl915zEDb1hiQ8EHM/5c/gICpW1wKaQEed3BZCSrMJy9CjM+vThxc5/CQ2ogujeRFNQantfSEnKa3O/LOzLQe/jmgWUe4c0Cyh+yxFTUwkdN+6VJzTTdMUsIjMzw3HFPxg2akj4rN8J/eUXxJSv69lFTEnRiOH9+7EcOVIjhgEMbTVh51fXw7P7eWtkFigQxFnlLQ9xdNnmXDYug0qtxtPMk4rWFVntv5pQQ3PME16gTvr0C8up4FMArwQxQHlnE8KexxI6dz7aHh4Y1Kub7hjDhg0xbNaUZ4sWkXgtE09LibYQ9RBCLr96K1Ut8vBZPBXiNCvKOl+IIBYEgVpdilKjYxHK1HXMa3M+GyMtIxq4NGCy12SOfXeMDU020KdUHx68eMDAowNpsr0Jq2+tJlaZ8964vCRFncKaW2totL0Rm+5uoo17G/a23Etnj84Z2nQJEoHa3TzwautGm9HlsXB8t9fA1ciVnyv/zJG2RxhZYSTPEp/x48kfabC1AYuvL+Z50vOcPrV8h0MxU9r/UhFHDzPObLnP7gXXiI/+jPys9yBTKChSpTr3Lp5DmZiQI3MUkI95uVYjit9MH+JUlZrI+GQsMulBrAx6RLR9GWKfKz/aO/wS/WrVSLh6FXV8fIZtyclxnA+TgAAtR49HRz+Ta/Pp2fDsDjSZrfG2vUX4zFmoIp9j89vEvGs9Y2ClCeXuc1LTKmbfCE2VYfWHRTGAqa0d7X+bgVf7rtx/rsfKEB8CghNgVXNNK6CQLCRt5wKCIFCtrRuJMUou73+EIAhYDhuKxdAhRO/ZR/AFM0TvMXB9g6ZV1Rte0HcRHfmc61HWxMd9A9drqVyzgNLvLNiWQtw9jLBuVYneug3z/v1ei7/PQKKtje2sWZj160v0lq0E9e6NKiYmG4zPedKJ4VGjMOv5Vi0Qr2GaCvUnpuWNgR9BgSDOIm+X+Zc7FSNSYUaKSnMD7ubZjfCEcG7rxyBBRPko6JPnOv74ODZ6NribvO6HVsHZlPqPLiKGBGMxbGimbQesx45FZmlJyMhRqBPeulAVawpSBdzY+uqtqIQURBFcQu4jNTV9VSX7S0BLV05xbzuk8vz1EZUIEjzNPBlQegD72+xnpvdMzLTNmOE7gzqb6zDlwhQeRj/MazOznVNPTtF6V2um+06nuFlxtjTdwtjKYzHRfnflUKlcQqnaDlmOEDBQGNDFowt7Wu5hQa0FFDIqxPwr86m7uS5jT4/FP9I/u07nm0BHX0GjfiWo0bEIofdesGHiRQKv50yutod3bVKVydy9cDZHxi8g//Jm26VvpajWszgloph5D+KUR0GEOtRES0+Ga5lPK46nV60apKQQ7+v76j1RrebqwX2s+HMzsSlaNP++KybWmQju8NtwchYUbwNudTNsjr9wkRebN2PavTs6xT0/yb5sxaakpihTu7Uaz7aoBknWRLpEKqVSy+/oNOUPdM2s2X7HigOSDiQ/ua5pT7WlJzwPyOET+DBWzoa4V7Li6uHHxEQmAmDety+Wo0YRu/8ATzYHoW65QlNnZnFNeOKXYQxRrSbw2mV2zJzI0t+mc+ipG6v+3s6jG1+G8M9xzApB1108S/mOqEuxmBRJxLxEvOYz8w4S42K553uO5LefxzNBkEiwHDIEm6lTSfC7RGD7DigfP87OM8h2xJQUgn8YQeyBA5rIgx7dM+6kbwmV+mhSNsNu5bqNH0P+Uhs5iCBN7yF+GRKtSgsl9LLzopBRIW6baR66lQ8/TdQkq5I5H3o+XfgogLuRjI53DhPuUgw9L69Mj5UaGmI7bRrKoCDCZrwVs69jDG710vJcNMVrXsQrATC4fwud7CxgUUCWkEvkNHBpwOpGq9nQeAN1nOqw5e4Wmu1oRt/DfTn55ORXH04d8CKAfof70f9If9Simvm15vN33b9xM3HLsTmlEik1HGqwuN5idjTfQUu3lhx8dJDv9nxHt3+7cSDwAKlZ9AJ86wiCQHFvO9r+VAE9Yy32LrzOyfV3SM3mglu27kUxtrIpCJsu4BP49gTxyx7Elpl4iOOeRPBUy4UilayRyT+tBaBO2bII2trEn9EsUEUGP2bjr6M5smwh1naWdHO9jH0h54wHqtWwe7DGK9wgo0dInZTE03HjkDs4YDFo4CfZliMIAhRrAgMuQKulUO7jKt5bOrvSccpsKrZoy81bIawK9iaoUC+4vQ8WVIC9IyAux1tyv5fKzQshCHB+x2uBbtajO1bjfiHu2DGezNuLutMejTfvn0aaMFcgOSGey//u4p/h/dg6ZRwhd29Tqa4PbRxvoK2jxZbJv3Bm05pvoihi5PJ/eLbtNEbNGmHVqRbCiWnwV3VNvvxbBP13jVU/DmTXrMn83a8bR//5m+chwR+cw7hlCxyXLSU1MpLAdu1JuHwlJ07ls3klhg8e1Ijh7t3fvXPVwaBlAMen5Jp9n8IHBbEgCIMEQfiIBnD5lLc8snoKzY0mJS1nSSJI6OrRlWBLzUVPGfhpgvhi6EUSUxOp6VAz3fuxGzdgmhTDplJN3itc9SpVxLRHD15s2Ejs8ePpNxZvDXFP4ZEmNygqQYmTkIj6yWN0y2YselFA7uFp7slkr8kcbHOQAaUHcPf5XQYcGUCzHc1Y67+WOGVcXpv4UUQnRzPt4jRa7WrFtfBrjCg/gu3NtlPToWauLrwUMi7E2MpjOdz2MCPKjyAsIYwRJ0bQYGsDlt5YSlRSVK7Z8jVjaqNH21HlKVXbgRsngtk8zY9nT7LvMykIAh7etXh88zoxEXn74FjA18WbRbXUqm8jZDo8VuOVettDLCqVPBadEJGkD5dWxsOze6BjmqXxJQoFuhUqEHvmDOe2rmf1yEFEPg6ifr+htOnSCGPFO1LCLi2Hxxc0bY30M3qnny36C+WjR9j8OgGJjk7WTjY3kWlBybZg9PF9YWVyOdU7dKP9bzOQyhVs3nObY1ajSSnZGfyWw9zScGwKJOVNKKyBqTal6zpyzzeMpwHRr9437dgRm8mTiD97lse/zEHVaS84VuLZhuEcHt+Nv/t249iKxWjr69No4A/0XrgCryYNcNJ7Qef/NcfTuxbnt25g88SfiXuef4uFRm3cRPjMmRg0aIDN1BkI3y2HjpshJUHTomnPMEiKRpWawsl1K9g8aSxyLW2aDB2NW4XKXDv0L/8M68O2qeN5ePUS4nsKsOlVrKipQG2gT1D37kTv3ZuLZ/phxJQUgof/QOzBg1iNGf1+MQyawmRVBoD/bgj5MgU+ZM1DbAX4CoKwSRCEBsI36kZ8u8r02x5iAC+beiRKDHlhKEf5MPCT5tkfuB8dmQ4VrCu8ek8VG0vk4iWEe5TjX8GK2KT3J9xbDB2Clrs7oWN/IfX5G/mT7g1Aof+q2nRUQgq1VU8B3tlIvIDcxVzHnL6l+nKg9QGmV5+OkZYR0y5Oo86WOky7OI1HMV92+fpUdSrr/NfReHtj1t9eTyu3VuxptYdunt3ytF+wocKQbp7d2NtyL/N85uFs5Mzcy3Opu6Uu48+O587zjC3JCkiPVC7Bq60bTQeVIjEuhS3T/Lh29HG2VcX08PYBwP/08WwZrwAQBMFOEISqgiB4v3zltU3ZjfjGz2/NQ/x2lenkJ08IsamChbEKM9s3cnd9l0Lic6jcL8tzxHsU4agshbOb1lK4QhW6/7GI4jXrvHtBMyYEDk0A15pQqkOGzUm3bxO5bBlGLVtmqVfr14qte1G6TJ9HmQZNuXz4MKtPJBPaaJMmfPzEdJhXGs7/9d5Q25yiTD1HdI0UnN58L91127h1a2xnziTu0iXODRzGhvtFWPmwHP/dicDdMpnOE36j46TfKVbdB5n89X1crpDToP8wGvQfxtMH91g1ajCBVy/l+nnlNNG79/B0wgT0anhjN2M6gjQt8sK9nqZFU+X+cGkFUb97sf7H3vju3EIJn7p0GdabItfG0LCSEb3nL6Fq206EBwawbep4/vmhP1cO7Hln3QwtFxecN2xAu2QJQn4YwbNFi76ICtQaMTyc2EOHsBozGtNu3bJ2YOV+oGOiWRT6QvmgIBZFcSzgBiwDugP3BEGYIgjCl98LJwd5KYhT31iRjooXSXlelccmqUTf/7h8RVEU+fPqn+x6sIvmhZqjJX298vv8n39QRUej6N0ftQhXgt7fIkCiUGA7cybq6GhCx417/SVS6ELRxnBrJ6JKSXRiCqVfPELQ0srdKo8FfBC5VE4j10asbbSWdY3W4ePgw8Y7G2myvQn9D/fnTPCZLy6c+mzwWdrsasPUi1MpalKUTU02Ma7KOEy1s+aVyA2kEik+jj4srbeUbc220axQM/YF7KPN7jZ039+dQ48OFYRTfwBHTzPaj62IfTETTm+6x54F10mIUX72uEaW1tgXK87Nk0e/iBv/144gCNOBM8BY4Me014g8NSpHeC2JUxNzX2TkBeGxyQgCmOkp0r3/5FIQCbrWFCn1RqErZTycmQeFaoFDxQ+OrUxK5OiKv9l34TipUgl1vWrTZOgo9IzfEygoipqwYHWqppDWW6JZVKkIHfsLUiMjLEf++FHn+jUi19KmVo8+tBk7idTkZNb/Po/T6jqoehzU9MndP0oTSn19U662alJoy6jc3JWwhzHc93sdiRP/IopbSTGcrOjJBZKIunMHr1bt6TOwBQ0Mz2F16H+a/OJ34FmjNp2nzEbX0IitU8dzav1K1Kr8EUIde+QIIaNHo1uhAvZz5yIo0n/n0NJHrD+F/0rOZPV/jkQ/DaFpRV3qdWiLPOoOYugN2PsDeusbUaW4Eb0WLKPRwB/Q0tHh6PK/+Ltfd46tXMKLpxmLmclMTHBcvhzDZk2JmDuP0NFjUCs//177qYhKZZoYPozVT2OyLoYBtI2g2hC4dxAeX8w5Iz+DLOUQi5qnk6dpr1TABNgiCMLX0VwqB9B/KYjf8BCHxSShjKpEuJmM5IcPs/xQJ4ois/xm8de1v2hRuAWjK45+tS01MpLIFSsxaNiAEj4VkQjgF/jhqrnaRdyxGDaMuMNHiN627fWG4m0g6QVCQiQqtYh98H10SpTI+CUv4IuhhEUJplafyqE2h+hfqj+3Im/R93Bfmu9ozvrb64lPyVgJNDd5GP2QgUcG0udwH5RqJXN95rKk3hKKmBbJU7s+hJuJG+OqjONw28P8UO4HQuNCGX58OA23NWTZjWW8SMpab8pvEV1DBY37l8S7vTvBd6LYMPECgTc+v+CWh3ctokKe8PTB3Wyw8punBVBEFMVGoig2TXs1y2ujsps377IpMV9XasmnEh6ThLm+FrK32kHeuR6HNDUR95qur9/0XQYJz6DGaD7Ewyt+rPihP1f276F0vcb4PE/E7GEWCvv474I7e6HmaDB1zbD5+erVJP33H1Y/jUFm8u1k4DmVKE23WQvw8K7Fhe0bWbtwLRE+86HzVk2/3G294G9vuHc41/oAF61sg7mDPme23SPov//YO28mi/v34OzmtVi6FaFBo1bUvB2ExdrNyIu2gu57IDkGltSGuwfeOa6ZvSOdpvxBcZ96XNyxmU2/jSE2MmeKMOYW8WfPEjx0GNqentgvXIhEO2POflJcHHvmzuDAhh1YuhenXuNGxIfrs3/CSlb9o8fi8LWElZ+r+f/d0AHpmhYUczWk05TZdJz0O65lK3D1wB6WDe3N9um/Enj9SjrtIFEosJ0+HfPBg4jeuZPHPb8nNSr3U71EpZInr8TwT5h27frxg1TsDXoWcHRS9huYDWQlh3iIIAiXgBloVptLiKLYDygHtM5h+75YdNNyiFVvrO6FxSSBWhfDQiXQSkgh9MntD46jFtVMPD+RVbdW0aFoB36t+itSyetCGJGLlyAmJWExaDD6WjI8bA3xDczal8G0ezd0K1UibPKU19XqCvm8yiOSpKrRCbyHTrmC/OGvAXMdc/qV7sehNoeYWn0q+nJ9plyYQp3NdZh+cTpBMZ9e2fxTiE6OZobvDFrtbIVfmB/Dyw1nR/Md1HKs9VUVaDPSMqJ78e7sa7WPOT5zcDRwZM7lOdTZUocJZydwN6pAnGWGIAiUqGlP2zHl0TFQsPfP65zaeJfUlE/3DLhXroZMrigorpU9BAB5l6eQS7z58JiLzrY8JSwmCUuD9PnDSfEpBEVoY/P8CtrWafm7yng4MxdcfcCx0jvHS4iJZu+8mWybNgG5ljbtf51B7Z59Ma5SlfgzZxHf5+1LfAH7fgTrklAlY6Es5ZMnRMydh36NGhg2avRJ5/s1o6WrR4P+Q2n+4y/ERz1nzZhhXPgvCvX/jmkKeCXHwNrWsLIpPMn5cOPUlGRsXIJ5/mgZmyeO5uEVP0rXb0yP2X/T+qff8OzWE8e//0L5+DGPunQlRcsFeh0DM1dY1w5O/UH6ZajXyLW0qd93MI0G/kD4wwBWjRpMwBXfTPf90km4fIXHAwaicHHBcfHfSPX10m1Xq0VunfJl2ZB+3D1/BkOrWkSG1+Pg6RKcje5MuKoIVsI1dIRY/j3pQnynE9D4d3h2F5bUgi09sTHXovHgH+m1YDmVW7Uj9P5dtk7+hRU/9OfaoX2kpLVvFQQBi/79sZ01i8Rr13jUvgPKwMBc+1uISiVPhg0n7vARrH7+GdOuXT5tIIUeeA2Hhyfg4cnsNTIbyEpteVOglSiK6ZIXRVFUC4LQJGfM+vJ56SFOeSNk+mWhi8qVW6HafJkDp/6hR8d3O9FT1amMOzOO3QG76Vm8J0PLDk0nJFJCQ4lavx6jli3QcnUBoLyTKRt9H5OiUiOXvn89Q5BIsJ06hYDmLQgZOQqnNasRpHLwbAF+y9F/kQQqVUH+8FeGXCqniWsTmrg24XrEddb6r2XD7Q2s9V+Lt703HYt1pIpNlRwTpanqVLbe3cqCqwuITo6mlVsrBpYZiLmOeY7Ml1tIJVJqO9amtmNt7kbdZZ3/OvYE7GHrva1UtK5Ix2IdqWlfM92CVQFgZqdP2zHlObftAdePPSH4bhR1e3piZpex/+iH0NLVo1CFytw+c5KaXf+HVJbv9VxOkgBcFQThCPAqllgUxcF5Z1IO8Oo6J34zgjg8NjlD/vCdC09RI8FJFvT62u+3XOMdrpm5d1gURfxPHePYqqUoExKo0qYDFVt89ypPVK9aNaJ37CDplj86JYpnbszh8RAfAR03gjT9I6UoijwdPwFBELAeP+6rWijNbgqXr4Ste1GOLF3I6fUreXDpAg37D8NkoB9c+gdOzICltcCjOdQaB+aFs3X+F09DuXpoHzePHSIpPg4tPWuQ1qPz5G4YWxql21evShUclyzmcZ++POrcBacV/yDvsR92DYQjv8Ld/e+dq1h1H6wKubFn9jS2T/uVCs1aU61dF6R51XP6I0ny9+dxnz7ILS1xXLYUiZERcVFJhD2MISwwhqcPonjif5CU+PMIEkP0zDthVbgIVs6GWDkbYulsiK6BHK4m8Oz6ZbZeasT+pf60GNYTacl2mkWqsws0RaYq9kbfewTVvutMpRbfcefcKS7/u4vDSxdyav1KivvUo0z9JhhZWmHUpDFyWxueDBhIYLv22C+Yj26FCh8+oc/glRg+cgSrsWMx7dzp8wYs3xPOzoejk6Fn9QzpFXlJVj6d/wKvYnQFQTAEiomieEEUxW+2sWdmRbXCY5Iw1JbhXLIiDwD/K4eJaxOHviLjg2GKKoVRp0Zx6NEhBpUZRK8SvTLcLJ4tXASiiEX//q/eK+9swoqzgdwKiaGUg/EH7ZTb2mI97hdCfhxJ5JKlmPftowmb9luOWbQSBAGd0qU/8a9QQF5T0qIkJS1K8kP5H9h8dzOb7myiz6E+uBq50rFoR5oWaoquXDfb5jsXco4ZvjO4/+I+FawrMLLCSIqaFs228b8U3E3cmVB1AkPLDmXb/W1suL2BoceGYqtnS8diHelYrCNySYFYe4lMLqV6O3ccPEw5usqfzdP8qNDYGVs3E4wsdNAxkGf5YdjTuxZ3zp4k4IofbhWq5LDl+Zpdaa98TXoP8ZfzcJWThMUkU9L+tYgRRZFbp0MwTA7F3DbNk6VMSPMO1wTHyhnGiA5/yqElf/Lo+hVs3ItSr/cgzB2c0u2jV1Xz/Ys/cyZzQRx4Gi6t0HiGbTMurMfs2kX8mTNYjR2L3DaTnsXfGLqGRjQZNprbZ05wZPkiVo0ajHenHpSu2wuhdEeNSDo7H/z3QNkumjB3WcZe01nlZe/gKwf28PDqJSQSCYUrVqVM/cbombiwYeJFrhwIwaeLUYZjdcuXx3H5MoJ69SYwTRQrWi8DK084MvGDc5va2tNh8u8cX7kE311bCb59i8ZDfsTQ3PKTzyc3SA4IIKDXAGIsiyPpNow7W0IJC7xDQnRa7q4QjTrpACkJT7D3qIpP9z5YOJpmfn8r0wnzMp2oXTycA0v+48SGO/h0LopQa6xGGB6dDOf+hCtrwPtHZBV74VmjNh7etQi548/l/bu5vG8nl/fupFD5ipRp0AyHMmVw3riBx3378ajn99hM/A3jFi1y5G8hKpU8GTqMuKNHsfplLKadPlMMA8i1wXsE7B0O94+AW53PHzObyIogXgSUfePfcZm8l/8RAVHEIEnzpdBTpHmI04VMa1Zt5ba2iHIZpuFJbLu3ja6e6WPtk1KTGH58OKeCTzGywki6eGQMP1AGBvJi2zZMOnZEbve6BUB5J024s9+jqCwJYgDDJk2IO3aMiAULkFWuwswAfbqp7dB5rkLLzQ2pUcaLYQFfF5a6lgwoPYBeJXpxIPAAa/zXMOnCJOZenktLt5a0L9oeBwOHTx7/UcwjZvnN4vjj49jp2zG75mxqO9bO9yv+xtrG9Czek64eXTn++Dhr/dcyy28WAdEBTKgyId+f/8fiXMKc9r9U4shK/3T9LhXaUowsdTGy1MH4zZ8WOmjrpxfLTiXLoGtkzK0TRwsE8WcgiuJKQRAUgHvaW3dEUXx/i4KvEPGN31TfgIc4VaUmMj4Zizd6EIc9jOF5SDxFgk6gaFxM86bfco3n9q3cYbVaxeV9uzizaQ2CIKFWz76UrtsIQZIx4kxmZoaWRzHiz57VLKanMyQZdg8BYyfw+Smjnc+fEzZ1GjqlS2PSof3nn3g+QRAEinnVxN6jOAf/ns/R5X9x3/c89fsOwdBnDFT4Hk7O1Pz/XduoEcYfSVJcHP8dP8S1g/t4ERaKnrEJVVq3p2TtBuibmr3ar0RNe64dfUwJH3vM7Q0yjKNTqhROK1cQ1PN7HnXuguOKf9Cq/gNYesKBnzLNF38TuUKLur0G4uBRgkNLFrB61BAa9B9KoXLvDt/PbVQqNc+D4wl7GE3ozacE+z4g3vMnECRwNAJjK13si5pg5WxIfNQN/HauQ5BIaDz4R4pWq5GlOQqXsyQy2Bm/fYFYOBhQoqY9GNpCiz811ZcPjYODP8PFxVB7HELx1tgV9cCuqAexkc+4dmgf1w7v577vecwdnCjTsCnuK/4hbOQoQkePISUoCPNBg7L1eURUKnkyZChxx45lnxh+SZkucGYOHJ0IhWt/MV7irAhiQXxjCTYtVPrriHvITkQVDa+nPeB5gJ5WWg7xGyHTYbFJWBlqI0ilaDk54RH/nPn+q+lQrMMrb1J8SjyDjw7G96kv46qMo61720yni5i/AEGhwLxP73TvWxtpY2+ig1/gc773csmS6ZpwpfFEX/TjSt8hbPEajKH3Olrs64VOk4Jw6fyEQqqgaaGmNHFtwrWIa6zzX8c6/3WsvrWaGg416FSsE5WsK2X5whmrjGXx9cWs8V+DQqJgaNmhdPbonK4K+reATCKjjlMd6jjVYf6V+Sy+vhhbPVv6lOrz4YO/MXQNFTQZWJLo8ERehCcQHZ5IdHgCLyISCQ+M4cGl8HT1YxQ6MowtdTCy0MHIUhdjSx0cS1Th7rlDJMREo2uYTQt2NX7U9EL8RhAEoSawEggEBMBBEIRuoih+eclbn8GbnyW1+GU8WOUkz+KUiGL6HsS3TocgkwtYPb2Iwql+mnd4DrjUAKfXi0rhgQEcWjyfpw/u4Vq2ArW/74+hecZ+wW+iX60akStWoo6PR6L3Rh7lqVkQeR+6bNfkBr5F2NRpqOLjsZn42+s2NQW8wsDUnFajJ3DjyAGOr1rKyhEDqNWjDx7etRAazdQIpaOTNSIpi4QHBnD1wB78T58gVZmMXVEPqrXvglvFKpmmn5Rv5Mzt86Gc2XKfZkNKZ/pcoF2sGE6rVvKoZ08edemK4/JlaBdtAEUaZNmuotVqYOVamD1zZrBjxkTKNXj8HWMAACAASURBVG5B9Y7dcj0lRhRFYp4lER4Y8yr8OeJxLKoUzUqaQhWPYdIzitR0xa6CK5ZOhmjryUlOiOfIskX4nz6ObREPGg8agaHFx3m6KzZx4dmTOE5tuoepjR52RdKKy1kXhy7b4MFRODgOtn6v8RrXmwTO1TAwM8erfVcqtWrHnTMnufzvLg4tXsApfQNK1K6DjaU5zxYuQvkoCJspk5Foff6zmVqpJPilGB73C6YdO372mOmQKTQLdTv7w+29UOzLyL7NirANEARhMBqvMEB/NMU6vi3UKt68VLwMmU5JFzKdTCUXzY1By8UFl9vxPI1/ysHAgzR2bUyMMoZ+h/tx89lNJntNpmmhpplOlXT7NjF792LWpw8y84x5mRWcTTl9/xmiKGZJ2KjVIv9cj2R/0VZMPP0367mEh3sxHsbFoVtQUCtfIggCpS1LU9qyNGHxYWy6u4ktd7dw/PFxChsXpkPRDjRxbfLOcGqVWsW2+9tYcGUBUUlRtCjcgsFlB3/1ecLZwcDSAwmNC2XB1QXY6tu+83v8LSMIAsZWuhhbZfx8qVLVxDxLJDoi8bVojkgkLDCG+2liWZ1qjlqVyj8j/sHSpdpr7/Ir0ayLlp7s41bEdUw0r2+H34F6oijeARAEwR1Yj6YgZr5BfMNH/C0I4pc9iC3TPMTKxFTu+YXh4iggUyWjcHTU5KTGR6TLHfbdvY3T61eipadP4yEjKVKlepa+P3pVqxK5ZCnxvr4Y1Kz5esOdfVCyvaad01vEnTxJzO7dmA8YgJab2+edcD5GEARK1mmAY4nS7F84m/0LZ3Pf9xx1ew1E19QV2iyDqoM0/59udTMdQ5Wawr0LZ7lyYC8hd24hU2hRrHpNStdrjKXz+z242npyKjZx4dTGewTeiMSlZOb3dy03N5xWrSKoR08edeuO49Il6JQo8VHnamJjR4eJMzmxZhmX9u4g5I4/jYeMxMjS6qPG+RiS4lMIC4xJJ4CT4jRBMlK5BEtHA4rXsMPcQkrKH2ORP76D0z/L0SlV6tUYIXf92TtvFrGREVRt24lKLb9D8gkLPIJEoG4PD7ZM92P/4v9oO6Y8huY6r3coVAv61IDrGzUh6SsaQZFGUOdXsHBHrtCiuE9dPGvWIdj/Jpf378Jv704AHOt6YXP8CMpuwTgs/BOZ6ae3ulQrlQQPHkLc8eNYjx+HSYeMPcWzhZLt4PQfmr7ERRpBJhEquU1WBHFfYB6aXoYicATo/d4j8iNi+iqLWjIJMonwqsq0KIqExyZhmVboQuHsguz4cQrpO7Py5kqq2Fahz6E+3H9xn1k1ZlHH6d1x8xFz5yExNMSsZ49Mt5dzMmH7lWCCnifgZJZxZfZNwmOS+GHzNU7de0Zdby90HeJg/VqeJUcDoFPm24p8/xax0rNiUJlB9C7ZWxNOfWsNE89PZM7lObR2a037ou2x038dln8x9CLTfadzN+ouZS3LsqjOIjzMCvpUv0QQBH6t+ithCWGMOzsOS11LKtl8OSFgXzpSmQQTaz1MrDNeu1QpamIiNUL5wKKjiOJdtHRr8DQgmvt+Yem8gVq6slde5XSh2Ba6aOsX5HcD8pdiGEAUxbuCIOS7P8zrNWkREQmiWkSQ5F9h/LJ450sP8V3fMFKValwMIkkF5DaWsH4OuHiDU1UA7pw7xck1y3GrWJW6vQeiY2CY5fl0ypZF0NYm/szZ9IJY1wzqT8mwvzo+ntAJE1AUKoRZn2/vUfFTMLay5rvxU7i8dyenN65mxQ/9qdtrIG6VqoJtabCdm+GYuOeRXDu8nxtH9hP/IgpjKxtqdv0fnjXqoK2f9YKGnt523DgezNmt93H0NEX6jmKtWi4uOK1ZTVD3HgT16InD4r/RLftxz48yhYLaPfvh4FGCA3/NY/XowdTvNzRbUmNEUST8USxPA6JfCeDoiETNRgFMbfRwKWmOZVrhK1M7PaRSCarYWIK694DA+zgsXvxKDKvVKi5s38S5LesxMLOg3YTp2BUp9lk2KnRkNOpXki3T/dj31w1a/1gOudYb4loihdIdwaMFnF8Ip+fAwspQrrtmcUvfEkEQsPcojr1HcWIiwrl6aB83Du/nUWE7DBMjce3Sgcq/z0Wv6MfXdlErlQQPGkzciRNYTxiPSfscTHWQyqDmGI1H/NZ2KJ73TYs+KIhFUQwHChJAxNfJSQqZBEEQkEoEUtNCpqMSUkhRia9uUgpnZ0hJ5XuzJvz0aAFtd7UlWhnNPJ95VLev/s5pEq5cIe7YMSyGDXtnbm8FZ83qj29g1HsF8RH/MH7ccp0EZSqTWxanY0VHxOTiPPQ9T+yhw8isrJDbFRS6+FbQkmrRrFAzmro25WrEVdb6r2X1rdWsurWKmvY1aVaoGbsDdnMk6Ai2erbMqjGLek71CvJkM0EulTPbZzZd93Vl2LFhrGq4isIm2VsV9FtEKn8tlss1qs+JNcup1sYcMzuHV2L5RVoI9kvv8tOAaO75haXrBKKlK9MIZQsdTTj2G6JZWy/facJ34ScIwlJgTdq/OwF+eWhPzvBW/1a1SkSajwXxSw/xyyrTt06HYGanj8Hzi7zQ1kYWtAfiw6HGCkATRrt/0RxNqOeQHz86TFWipYVuhQrEnz2reUORFvXRYBromWXYP3zuXFJDQnFatxaJQvFpJ/kNIpFIKd+0FS5lyvPvn3+w648pFKvuQ63ufV4JXFEUCfa/yZUDe7jvew61Wo1rmfKUrt8E55JlMs0D/xBSqYRqrQuzd+F1/jsRTKla7641orC3fy2K/9cL065dMG7ZEoWT0zuPyQz3yl5YOhdiz9zp7Jo1mbINm+Hduccnh1BHPI7l9KZ7hNx7AYCesRZWzoZ4eNli6WyIpZMBCu2MckedmMjjvv1IunMH+wXz0atUEYCYiHD2LZhF8O1bFK1Wgzr/64+W7vudT1nF2EqXut97snfBNY6s9Kd+L8+Mz1gKXU3hqbLd4MR0TYTA9Y1Qbagm7SftO2hoYYl3x+5Uad0e/9PHubR9M1cjwrg1djielb2o0KM3BmZZi+pLL4YnYNK+Xbac73vxbAWnfodjU6FY8wxV6nObD84uCII28D3gCbyq4iCKYs8ctOvLQ/3aQ6wl01x05FIJqWnL02/fpBQumvxeL7UrZtpmxKbEsqjOIipYv7tEupiaSvi06UjNzDDt0vmd+7lZ6mOoLePSo+e0KWefYXtSioop+/xZde4RxWwMmd+hNIUtNQUTBG1t7GbO5OF37dAtV65A7HyDCIJAGcsylLEsw9P4p2y6s4nNdzdz9PFRdGQ6DC4zmK6eXb+5POGPxVBhyMI6C+m0rxP9j/RnbaO1WOi+Px+vgKxT1KsmJ9euwP/UMbzad00nlt9GlaIm+lUYdsIr0fz0QXqxLJVJ6DOvRr72IL5BP2AA8LLN0ilgYd6ZkzO8Liyt+U9WpaqRyvM+/C6nCI9NRhDATE9BRFAsEUGxVG/nTsqmIBQO9ghn54JzdXCuRmJsDLt+n4y2rh7Nho/5ZMGhV60q4dOmkxIaitylJvQ5CTalMuyXeO0aUavXYNKxw0d7DwvQYGbvSIeJs7iwfRMXtm/k8c3r1P6+P/FRz7l6cC/PggLR1tOnbKPmlKrbCGMr68+e06mEGfZFTfDd85Ailazfu2got7bGafUqQseNJ3LxEiL/+hvdChUwat0Kw3r1kOhmrauFsbUN7X+byck1y7n87y6C7/jTZOiojzqfhBglF3YFcOtMCNq6cqq3c8e1tAX6Jh9+dlErlTwZOIjEK1ew+33Wq+iH22dPcnjJn4iimoYDf8Cjuk+W7ckqTp5mVG5ZiHPbHnD5gD7lGjhnvqO+BTSeBZX6atqbHZsEfsvA52eNJzmtBaRcS5uStRtQolZ9Hp44xvn5f3Dl4hmu+p3DvbIXZRo2w9a96Duf99XJyTwZPJj4EydzTwyDJkza5yfY2BlubNKcUx6SFTm+GrgN1Ad+Q7PK/O21WxLVuDYM50+aoSXTfAg1HmKN5/i1IE7zELs4A6B+FMzylsuRSqQ4Gb5/FS1yyRISr13Ddtas915UJBKBck4m+AZGZdjmHxrDkA1XuBsWx/deLoxsUOSVvS/RLlYM5zWrkVnlXO5GAV8H1nrWDC47mD6l+nAh9ALFTIsViLqPwFbflj9r/0n3/d0ZcGQAKxqsyNY2V98y+iamOJUqw62Tx6j2Xef3ej+kcgmmNnqY2mQUy6kpKmKeJREdnkBiXMq3IoYRRTEZ+CPtlX95q7K0Kp+Xmg6PScJcXwuZVMLN0yFI5RLcK1oRPCsILWMgLgzaLEetUrFn7gzinkfSbsJ09Iw/PX9ev1o1woH4s2cxbt06UzEsKpWEjv0FmaUlFsOHf/oJFoBUJqNq244UKleRf//8g50zNW2OLJ0LUa/vYIpW9Uaupf2BUbKOIAh4tXVj46SL+O0NxOu79+d9yywscFi0kJSwMKJ37OTFtq2Ejh5D2MRJGDZqiFGrVuiUzrxIV7px5HJq9eiTFkI9l9WjBlO/3xDcK1V773GqVDXXjz7Bb99DUpVqSvk4UL6xc5ajf8TUVEJ+GEH8mTPYTJ6EYcOGKBMTOPrPYm6eOIxN4SI0Gvxjtiw2vIsydR159jiO8zsDMLPVx/kd+duApid1+7Xw6BwcHKvpB31+EdT7DQq/Tr8UBAHXmrVwKleBu0MG4//wHgEXz3Hn3CmsXAtTpkFTiojqdMJPnZzMk0GDiD95Cutff8Wk3Xc5ds6ZUrQJ2JTWhId/BYK4sCiKbQVBaJ7WxmEdmpXmbwu1Ci2jVOJTdNGWv/QQC688xOExmryel4UuZCYmSI2MUD58iKtx9w8On3jjPyL+XIhh48YYNWn8wf3LO5ty7M4douKVmOgpEEWRFWcDmfrvbQy15azqWRFv93cLmzeLBhRQgJZUC29777w246vEw8yDWTVmMejoIEacGMG8WvOQSb69Qvw5gYd3LfbNm8njW//hWLzkJ40hk0vfKZbzI4IgbBJF8TtBEG6QLpBcgyiKn/aH/EJ5JX/TzlSVkuGU8xVhMUlYGmihTErl7sWnFC5niZa2hJSgIPR1k9O8w16cXL2MoBtXqdd3MDZuRT5rTkXhwsgsLYk/c0YjiDMhcvlyku/dw37hn0g/Ioe1gHdj5VqYzlPn4H/6OGb2Dti4vdvL97mY2elTrJotN44/oXgNu0wLIr6N3MoK8z69Mevdi8RLl3ixdRvRe/byYvMWFK6uGLduhVGzZsgs3r/I7lapKpYuruyZM53df0yldP3G1Oj8PbK3Qu5FUSTwRiRnNt8jOiIRp+JmVGtTONOooXchqtWE/jyW2EOHsPppDMatWxN6/w775s3iRfhTKrdqR+XWHZDKcvYeLggCPl2K8iIsgYPLb9J2dPkPn4dTFfjfYbi5HY78Cmtag6sP1JsI1q+LnEkNDCi6eAnGkyfzbONGIqtW5GFSIvsXzuakjpwy+g5UUqsR81oMg6blkkMlTUh4HpOVuKKXfQtfCIJQHDACvuzO2jlBWlEtFZJXIdNv5hC/9BBbGLwO1VC4uKB8+PCDQ6sTEwkZORKZuTnW437JkjnlnTSrvZceRfEsLpmeK3z5dfctqhc258DQ6u8VwwUUUED24m3vzc+VfuZU8CmmXJiCKObvh/LconD5Sih0dLh18mhem/I1MSTtZxOgaSavfIWrxcuHSM13Tp3fPcSxyVgZanP/UjgpSSo8vGxJDQtDTElBoRUDNUdr8gn3bKd0/caU8Kn32XMKgoBe1arEnz2HqM74900OCODZnwsxaNgAg1oZq04X8OnIFApK1KqHrXuxHE9xq9TMFalcwtlt9z/qOEEQ0C1fHtupU3A7dQqbSRORGhkRPnMW92r68Lj/AGKPHEFMeXcbdCNLa9r/NoNyjZtz9cBe1v/yI1FPQ15tjwyJY/e8q+xbeB2JVKDJoFI0GVjq48SwKBI2aTLRO3diMWQwxp07cWHHZjaMG4kqNZV246ZSrV2XHBfDL5ErpDTsWwKZXMK+RTdITshCm3hBgOKtYMBFqD8VQq/CX9Vhez+IDn69m0yG9bhx2I4cidWZi9QIjqLlwBFYWRpwJsKZfxfN4/GAgRox/FseieEvjKwI4sWCIJigqTK9C7gFTM9Rq75E1G8KYk0IskzyOoc4PDYZY1052vLX4ckKFxeUgYEfHDp85iyUDx9iO23qOwtpvU0pB2PkUoGV5wJpMOckZx9E8ltzT5Z2K4+ZfkHuZwEF5DbfFfmO74t/z+a7m1n+3/K8NidfINfSxr1yde5eOENKUlJem/NVIIpiaNqvz4DHoig+ArSAUkDIOw/8SjHQTh8mqUrN34I4LCYZK0Mtbp0OwcRaF5tCRigDNAJGUbgYYWprDv41D/tixanZtVe2zatXrRqqFy9IupU+Y05UqwkdNw5BVxfrn3/OtvkKyH10DRWUa+jEw2vPeHInY0peVpDq62Hcpg3O69fhum8vZj26k3jjOk8GDOReTR/CZswk+X7mglsqk1Ozay+a//gLMRFhrBk9hBtHj3Fy/R02TvIl/FEsXt+50e6Xijh5Zizo9iEiZs8hat06TL/viaJtG7ZM+oXT61dSuEIVus6Yj71H8U8658/BwFSbBr1LEBORyMFlt1Crs7iYLtOCKv1h8BVNa67/tsD8snD4V0iKATQLFWbdu2O/YD7Ke/dRjf+NRmUdqGIWiP/ZU5wNuovlrxMw+a5ADMMHBLEgCBIgRhTFKFEUT4qi6CqKoqUoin/nkn1fDmkeYjUStNJCpmVSgVT16xxiK4P0OR0KFxdSIyJQxcW9c9i4U6c0X9Bu3dCrXDnL5mjLpRS3M+LUvWeY6Wmxa6AXXas4FxTJKqCAPGRw2cE0dG7InMtz+Pfhv3ltTr7A07sWKUmJ3Pc9l9emfG2cBLQFQbADDgJdgBV5alGO8rKoVtajM6ITU4hPTs0pg7KdVJWayPhkLNVSwh7G4OFliyAIKE9v0myv0Yudv09G29CQpsNGZ6unS6+qpjVO/Jkz6d5/sWkziX6XsBr5IzLzgj71XzulajtgYKrNmS33si7O3oGWqyuWI0bgduwY9osWolu2DM9XrSKgSVMC27UnatOmTJ+PC5evRMcpc9AxsObg379z+d+VeFS1oPNvVShVy+GdraHex7O/FxO5eDHG7dsR7V2V1SMH8fT+Xer3HUKToaM+qlVVdmPrZkz19u4E3Yzkws6AjztYx0QTMj3QD4o11fT2nVcaLi4BlcbjbFC7Nk5rVkOqikfTdmN7PZEioZGEmBhwNjQQVerXcw3MSd77qRJFUQ2MzCVbvmze8BBrv/IQv84hDotNxtIwvWdW4awpoqV8GJjpkKlRUYT89BNabm5YDB/20SYNqe3GD3Xd2TmwGkWsDT76+AIKKCB7kQgSJnlNoqxlWX4+/TN+T/Nfl5vcxq6oB4YWVtwsCJv+WARRFBOAVsBCURTboukWka/5mJDp3qv8GLn1eg5ak708i1MiiqAfnIREJlCksjWkJKG8dBhRKnDg5HUSol/Q/Ief0TUyzta5ZWZmaHkUSyeIU8LCCJ81C93KlTFq1Spb5ysgb5DJpVRpVYhnj+O4fS70wwdkAUEmw8DHB/v583E7cRzLUaNQxcfxdNx47nlVJ2TUaOIvXnyVavToZiT//hVIkrIZxrZeqJKvE3T9bxJiwj9p/udr1xIxeza6jRtzw9aM3X9MxcjKhi7T51Lcp+4X4Ugq7m2HZ3VbLh94xF3fpx8/gIkTtF4KvY6BpQfsG6HpYey/G0QRHU9PnDdtRG5uQOIzOV4Dh1Kjy/fcPXeKPXOmo0rNQrh2PicryyyHBUEYIQiCgyAIpi9fOW7Zl0ZaH2LVmx5iiQSV6mVRraRXLZdeopXWeimzsGlRFHk6bjyqF9HYzpyBROvjw5xrFrFkUG23dGHaBRRQQN6ikCqYV2sedvp2DDk2hIDoj1zxLSAdgkSCh7cPQTeuEfv8WV6b8zUhCIJQBU1niL1p7+W7m4WQoe1S1r1aT6ISOfcg8qvJ+Q+LSUImQmpALIVKW6Cjr4DLK0mJSuJOYQce37pB3V4DsS70/irBn4p+1aokXLmCOj5e8wzz20TElBRsfp3wRYiKArKHwuUssXY15MLOAJRJ2es9lJmZYdajO667d+O8aSNGzZsTe+QIQV27cb1JR7b9uIs986+hVok07l+ann+MouWo8cQ+j2TNmKH4nz7+UfO92L6DsImTSKnhxRFVLDeOHqRC8zZ0mDgDExu7bD23z6V6O3dsChtxbNVtIoJiP20Qu7LQbTd02AiCVNPS6J+G8NgXubU1zqOb4dowAuNWLSjfpCU+3Xtz3/ccu/6YSup7cry/BbIiiNuh6WV4EriU9vr23B5pHmK1+Lqo1suQabVaJDw2GUuD9KJW7uQEEkmmhbWid+wk9tAhLIcMRrto0Zy3v4ACCsg1jLSMWFRnETKJjP6H+/MssUDIfQ4e1X0QRTW3T5/Ia1O+JoYCY4DtoijeFATBFTiWxzblOB+TQxyTmMLzeCWPIhNy0KLsIzw2GfcUKepkNR5etpCSBKdnc19iQ4C2nLINm+FZo3aOza9XrRqkpBDv60vswUPEHTmCxaCBKJze31KygK8LQRCo1taNhBglVw4G5dgcOiVLYvPrBBwPHiWk+2zO2H9PeJSUwg+2Uy10BWYhfogpKbiWrUDX6fOwcHJh3/xZHPx7HinK5A/OEXPgICE//8yTCqU4EhtBSlIibcdOwrtj90/uyZ2TSGUSGvQugba+nH1/XSchRvlpAwkCFGkA/c5Ck9kQ+QCW1YFN3ZAkBKNl9HqRo2zDZtT+vj8Bly6y6/fJpCo/cc58wAcFsSiKLpm8XHPDuC8KMbOiWpqQ6ch4JSq1mMFDLFEokNvZoQxML4iVT54QNmkSuuXLY9qjR+7YX0ABBeQq9gb2/Fn7TyITIxl0ZBAJKV/HQ/eXiImNHTbuRbl54shX483La0RRPCGKYjNRFKen/TtAFMXBeW1XTqPOoiBWqUVi0/KHLwd9WgGh3CYsJomSyVL0zLSxczeBy6sIDYvlipEpVvpGeHfumaPz65Qti6CtTez+AzydNBEtj2KYdu+eo3MWkDdYuxjhVsGKK4eCiH2eMwUN1So1/50MZt2Uq9x+pKBYdXs6jPCgbFM3Uh/cI3jYcO5X9+bppMnIwyNoN34qFVu05cbRg6z7aTiRwY/fOXbcqVM8GD2SSyUKc10Zh2vZ8nSduQDH4l92y1FdQwUN+5YgMTaFA0v++7y+6lIZlO8Jgy9DjVFw7yBcXplht9L1GlG390AeXr3EjpkTs7TYkK1UHQhdd+bunJnwQUEsCELXzF65YdwXhfp1US3tV0W1JKSqRMJjNRcLK8OMYc8KZ2eS38ghFlUqQkaNBsB2+jQEab6LYCuggALSKG5enBneM7j1/BajTo5ClXYdye88ePGAPQF7snVMT+9aRD4JIvzhg2wdN78hCMKctJ+7BUHY9fYrr+3Lbl6H6r4Mmc7aA2RM4uvwwCtBL7LbrBwhIiQOB5UUTy8bBFUy8cfmsSu0NIrUVGp51c7xdjESLS10K1QgescOVM+jsJk4ESGXWtQUkPtUaVkIgPM7sv+a++T2czZN8eXEujuY2ujx3U8V8OlcFKOiLlgMHkzhw4dwWLoUvWpVebFxIw9btiKwbVs8VTJaDB5J/Iso1owZys0TRzKMneDry6XRIzntZs9zhZS6vQbS7Ief0TEwzPbzyAksnQyp1aUoIfdecHrTvc8fUMsAfH6CQZehbFdN72JJeg95ydoNqN93CI9uXGXH9F9zt6uDsSPYls69+d5BVq5kFd74XRuoDVwGVuWIRV8qmXiIpRJNyHR4jGY1xfItDzGAwsWZBD8/RFFEEAQily8n8dIlbKZNRW73ZeUvFFBAAdmPj6MPoyuOZsqFKUz3nc6YimPybb6dKIqsv72e3/1+R6lWUsK8BE6G2RNO6V6lOsdWLObWyaNYuRbOljHzKavTfs7KUyvyiKzmEMckvRbEX4uHOOlODDJEPKraovJbwe7bJiSpZFQKfIKhW5FcsUGvWlXiT53CtHs3dDzzfY22bxoDU21K13Hg0r+PKOnjgJXL5wvK6IgEzmy5z8NrzzAw06ZB7+K4lrHIcE8UpFL0vaqh76Vp9xW9dy/RW7cRNmkSglxO3Vo18DVIZf/C2Ty+dYPaPfoi19Ym9vIVDowdySN7MyzsHWk8bAxm9g6fbXdu417RmmeP47hyKAhze308q2eDXjC0gWbz37m5eM06SKRS9v85m23TJtBy1DgUOrqfP+9XwgcFsSiKg978tyAIxsCGHLPoSyWtqJYa4XUOsURAmaomLOalhzijINZycUFMTCQ1LAzV8+dEzJuPQf36GDVvnnu2F1BAAXlKh6IdCI4NZuWtldjp29HNs1tem5TtRCZGMu7sOE4+OUkZyzJcCb/ChdAL2SaIdfQNcC1XEf8zJ/Du3DPHvWFfK6IoXkr71Q9ITOsWgSAIUjT9iPMVby8tZbXKdHSah7iYjSG3n8aSoExFV/HlfqZUKWp0ghMJN5CgpwdH1q0hONGImpWqoXv5Lgonx1yxw7hFC9Tx8Zj1zNnw7AK+DMrWd8L/TCinN9+j1Y9lP3kxV5mYit+/gVw7+hiJVEKl5q6UruOALAtFYaXGxph26oRpp04k+fvzYtt2YnbtolR0NAaFHbl5/DCh/jep6FOfM6uWEmusR5madfH+X39k8i8vVzirVG5ZiMiQOE5uuIupjR42hbO3cnxmeFT3QSKVsm/+LLZOGU+rMb+ipfttiOKPb+YF8YBLVnYUBKGBIAh3BEG4LwjC6Ey2OwqCcEwQhCuCIFwXBKHRG9vGpB13RxCE+m+8HygIwg1BEK4KgpB7xb3ULwXxm32IJaSqRcLSPMQW+pmHTAMk3b5N8MiRyIyNsZ4wYBklqwAAIABJREFUPt96iAoooIDMGV5+OHWd6jLLbxYHAw/mtTnZypngM7Te1ZrzIecZXXE0KxusxFLXkotPL2brPB7etUmMiSbw2uVsHTefcgR480lGBzicR7bkHBmqTH+cIK5ZxAKVWuT6k+gcMC77CLgWgTwVYu20ubFyIlfDjSjnVQ4HQQEyGXIbm1yxQ2psjMWAAUh0dHJlvgLyFoW2jErNXXkaEM39Sx/f9kitFrl1JoQ1489z5WAQ7uWt6PxrZco3dM6SGH4b7WLFsP75JwqfOonDnNmUsnGiYkAocU8es3/DCpRSCc3/N5Ba/YZ81WIYQCIRqNvTEwMzbf5d/F+O5XK/TdGq3jQZOoqnD+6ydfIvJMVn7BWdH/ngcqggCLt5eafRCGgPYFMWjpMCfwJ1gSeAryAIu0RRvPXGbmOBTaIoLhIEwQPYBzin/d4eTc9EWzStn9xFUXyZgOcjimLulm0V39WHWE1YbBKmegoUsozrC4q01ktPf/uN1JBQHJYsQWZiknt2F1BAAV8EEkHCFK8pRCREMObUGCx1LSltmfd5M5+DUqVk9qXZrPFfQ2Hjwiyu93/2zjs8yirtw/eZlt47KZDQUugloYmCUkU6i35Ywa5rWwuKVEHRXbfgrq5YWUUFASlKVUFEgdAUSEIN6T0hlSTTzvfHTEKQQAJkMgm893XlYjLvOed9JiQz53eetpROXp0AiAuM45esXzBLMypxNWevFxPeoxdObu4k7vyR9r1jm2TN6xhHKWXtTkZKWS6EuO6O+sUffMSNzyG2FNS6uZMf7+04zcG0s/SL8Gly+5qKxF1ZlKslHo45/LDtEGHeksGPzSbrhRfQBQfbPZdXb9JTqi+lXF9Omb6MMkOZ5V99GeX6css1Q/lF30d6R3Jv9L109m6ekG+FKyeyfxCHt2ew+5vThHf3bbSQzTpZzK6vT5KfVkZghAe3P96NgHZNk8er0ulwHzkS95EjCcrOpu3KFRzbt4e+M1/Eq2evJrlHS8DRRcvox7qx6s39bPrvESY+3wuNzva1hzrFDUT17Mts+MdiVi2czaRZC3BydbP5fe1JY95B6+YhGYFUKWVGI+bFAqeklMkAQoivgHFAXUEsgZq/Dg8gy/p4HPCVlLIaOCOEOGVdb3cj7msbzHVyiLXnQ6aNJkle6cUtl2rQBAQgnJ0xZmXjNW0arjcNajaTFRQUWhaOGkeWDF3CPZvu4c8//pnPR3/eZCHFzc3p4tO8tPMljp89zl2Rd/Fc7+dw1JxPG4kNimVD8gZOFZ+qFcnXilqjJXLgzRz+YTNV5eU4uro2ybrXKRVCiF5SyoMAQojeQKWdbWp6/ughrm5c39QaD3E7HxfCfV1adGGtkvxKMo6dJUFbQu9fP8dFU8WYx59DpdFgSE1D20C4tNFs5IMjH/B7/u9NYo/ZbK4VtzVfevPl27WohApXrStuOrfar0CXQLalbmP96fXEBcVxb/S9DAoe1GQHaApNg0olGDS5A+v++RuHf8yg14jLf2aVFlaye81pTh3Iw9XLgWEzounYJ8BmkZHaoCDCnn6G5kkaaH68g1wYNj2Gje8dZvvnx7jtgehmiTLt0LcfY//yChv+/jpfvzaLKa8utFlhMrNZolLZN3K2MYI4DciWUlYBCCGchBDtpJQpDcwLBurWRM8A4v4wZh6wVQjxZ8AFuK3O3D1/mFuTUS6tcyTwvpRyaX03F0I8DDwMEBbWBH8mVg+xWq3Gzyp+LX2ILVWm68sfttqBY+fOmEpK8H/+L9duh4KCQqvGy9GLd299l7s33s1j3z/G56M/x9vR295mNRopJSuPr+Sv+/+Ki9aF/9z6HwaHDL5oXFyg5e1+b/beJhPEANGDh3Jo8wZO7NlFt9tGNtm61yHPAF8LIbKwyMZAYKp9TbI9VyqIPZy09AzzZOeJ/Nrily2NxF+ykJhoU/IdGKoYG6fBqctopJTo09Lw6HVpj9jZqrO8sPMF9mbvJdI7Eq3q2sNIhRC469xp49rGIm6150Wuq84Vd537ReLXWeNc78+2VF/K6hOrWZ60nCd+eIIIjwjuib6HMRFjLjhgU7AvIZHetOvmy/5NKUT2D8LZXXfRGEO1iYNbUjm0LQ0B9B0TTs/hYWibwaN5vRPezZe4O8LZu/4MvqFu9BzWPPK/fe9Yxr0wm3V/W8jKBa8w5dWFOHs0XS5zVbmB/RtTyE0pZeLzvRB2FMWNEcRfAwPqfG+yPte3/uFXxF3Ap1LKt4UQ/YHPhBBdGpgzSEqZKYTwB7YJIY5JKXf+cZBVKC8F6NOnz7U3rrR6iBdP7olnlzYAaFQqTGZJbmkVkYGXDiUI+fc7CI1GyblRUFAAIMw9jHdufYcZW2bw5x//zEfDP2oVm7+zVWeZ8+scdqTvYGCbgSwctBBfJ996xwa5BhHmFkZ8djz3RN/TZDYERHTAOziUhJ0/KoL4Mkgp9wkhIoGaWNTjUkrD5ea0Rs5vnywf80Z9415iaZUBrVrgqFXRM8yLNQczSS+qJMynZUWVm0xmjv2ajVqzC7/qXEYGH8d/zFIQAlNREebycnSXOPRPKkzime3PUFBZwIIBC5jQcUIzW98w7jp3HujyAHdH383WlK0sS1jG/N3zeefQO0ztPJWpnafi49RyQ9lvJAZO6sCX8/eyd0MyQ6ZF1j4vzZIT8Tns/uY0FSV6OvYNoP+E9rh5t/zPtNZE71HtKMgoZ/eaU/i0cSEspnn+LsJ79GbCi3NZ+9fXLKJ49iJcPK8t9dNoMHF4ewYHNqViqDISNbANRoMZrYP9Dk8aE5eikVLWxsJYH198NHQxmUDdWuch1ufqMgNrPrKUcjeWtk6+l5srpaz5Nw/4Bksote2xVpn2dXNCoz4fMl1tMJFfVn1JDzGAxscHtYdHs5ipoKDQOuju153FNy3mSP4RXv755Rbfo3h31m4mrZ/EL5m/8GLfF3n3tncvKYZriA2KZX/ufozmxnntGoMQgujBQ8k6nkhxTnaTrXu9Yc0Xfgl4Wkp5FEt9jjF2NqvpqfE6Wo+9TfrGe4i9HVUIs4leYRaPx6H0ltd+KfVIIaX5+zmXf4Ao7wJCOoRB+6EA6FNTAeqtMP1t8rfcs+keTNLEslHLWqQYrotWpeX2iNtZMWYFH4/4mG6+3Xjv9/cYvmo4836dx+lipf+4vfEMcKbLLcEk7cqiMNNSniAnuYTVfz3A958m4eLpwMQXejN8Rowihm2AEIKh90bh3caVrR8lUJx3rtnu3bZbDybOnEtpfh4r5r9MeVHhVa0jzZLje3NYPncPu9ecJqiDB1NnxzLk7ki7imFonCDOF0KMrflGCDEOaExBq31ARyFEuBBCh6VI1vo/jEnD0tcYIUQUFkGcbx13pxDCQQgRDnQE4oUQLkIIN+t4F2A4cLQRtlw7NZtV1fn/MI1akF9ejVlyyRxiBQUFhUtxW9vbeKHvC3yf9j1vH3jb3ubUi8Fk4O39b/Pwtodx07nx5e1fck/0PY3K84sLjKPcUE5SYVKT2hR90xAQgsSff2zSda8zPgH0QH/r95nAQvuZYxvqBtgJsxGzvnEHSyWVBpbwFqy6n84Bbjjr1BxMbXmC+MDG3RjP/Yinvxcj/ZPQD3qx9hDAkJYGgLaOh9hoNvJm/Ju8/PPLdPHtwooxK+ji21DgXctBCEHfwL68c+s7rB+/nvEdxvNt8reMXzeex75/jD3Ze5Dy2oP+FK6OvreHo3PSsPOrE2z7JIHVbx2grKiKW++LYvJLfQhqrzh/bInOUcPox7oihGDju4fRVzbdYXNDhMZ0Y+Ir8ykvKmTlgpcpK7yy2sbpx4pY+cY+vv8kESdXHeOe7cmYJ7rj06Zl1AJpjCB+FHhFCJEmhEjDcuL8SEOTpJRG4ElgC5CEpZp0ghBiQR2B/RfgISHE78CXwP3SQgIWz3EisBl4wlphOgDYZR0fD3wnpdx8JS/4qqkpcC3OC2K1SoXBZHlj9r+Mh1hBQUHhUtwTfQ/ToqbxWeJnLE9abm9zLuBMyRmmbZzGpwmfMrXzVL4a89UVVYPtG2jJrNmbs7dJ7XLz8SWsS3cSd/6obI4vTXsp5VuAAUBKeY6L2/ZeR0hUZiNGQ+M2iKWVBvw5C0kb0GQfoluIB4fSW1ZhrZzkTNIOL8fR1Zux/vv5TXbALXp47XV9ahqoVOiCLSVWiqqKeHjbw3ye9DnToqbxwfAPWnW4cbhHOLP7z2bb5G082eNJkgqTeGjrQ0zZMIX1p9djMF13GQAtHkcXLX1vDyfrZDGnD+TTe2Rbps3vR2T/ILvmf95IuPs6MeKhGIrzKtn2SSLS3HyfgSGRMUyetYCK4mJWzJ9JaX7DrbgKM8vZ8M7vrP/nb1RXGBk2PZopM/sQ0rllddxpUBBLKU9LKfthabcULaUcIKU81ZjFpZQbpZSdpJTtpZSLrM/NkVKutz5OlFIOlFJ2l1L2kFJurTN3kXVeZynlJutzydax3aWUMTVrNgv1eIi16vN//JcLmVZQUFC4HC/0eYGhoUN5M/5Nfkj7wd7mIKVk1YlVTP12KtkV2SwZsoRX+72Kk+bK6iD4OPnQ0asje7ObVhADxAweSkleLpnHExsefGOiF0I4YQ0mFkK0B6rta5INqLMHF9KIqZEe4tJKA6qaHdBPb9IrzIvErFKqDC0jdcFoMLD+7ddB6rl9dG/8TFl8qr0Tjeb8HkSfloa2TRuETkdCYQJTv53K4fzDLBq0iJmxM5ukgFZLwMvRi0e6P8LWyVtZMGABJmli1q5ZjFg9gg+PfEhJdcvuIX290eWWYG7+v87837w4+o1vj87Rvi2/bkRCIr0ZOLkDKYcLiP/uTLPeu02nKCa/+hpV5WWsmD+TkryceseVn63mx/8lsWJhPLlnShgwqQP/Nz+OTrGBLfLwpEFBLIR4XQjhKaUst/Yx9BJCXHdhVw1Sr4e4riBWQqYVFBSuDrVKzeLBi+nq25WXdr7E4fzDdrOluKqYZ3c8y/zd8+nu153VY1czJGzIVa8XFxjHobxD6E2Xb8typXSI7Y/WwZHEnUrY9CWYiyXCKlQIsRz4AXjRviY1PXW3VSqzCZOhcX2ISyoN5z/DT27hZtd0jGbJ4Qz7iyspJd9/+C5lBakEdZpEu7SlnNZFctr9wkYd+rQ0dGFhrD+9nns33otAsGzUMsa2H3uJlVs3OrWOCR0nsGbsGt6/7X06enXkXwf/xbBVw1i0ZxFppWn2NvGGQK1W0WVwMO6+SqFYe9JtSAiRA4LY/10Kpw817KltSoI6dGbKq4vQnzvHV/NmcjYnq/aavtLInnWnWT5nN8fjc+h2ayh3v9afnsPCGt3D2h40JmR6lJSyNo5ISnkWGG07k1ootR7i8z8yrbW4lhDg66oIYgUFhavHSePEkqFL8HPy488//pn00vSGJzUx8dnxTNowiZ8yfuL5Ps/z/rD38Xf2v6Y1YwNjqTZVN1kP1Bp0jk50jBvAid27MOivP8fntSAs/W2OAROB+7GkJPWRUu6wo1m2obaVj0QljZiNjfQQVxlRCwHtbgInL3omWzo4Hkq79jzizPJMfsv77arn/751Iwk7tqF2jCO2mxZK0limu5MAjwsFiD41lSOOBczaNYvu/t35asxXxPjEXKv5LR4hBAOCB/D+sPdZM3YNI9uNZPXJ1Yz5ZgxP//g0B3IPKKkUCtc9QghuuaszAeHufP9pUm2hs+YiIKIDU+a8jkmvZ+W8meSnp3FkRwafz9nNgU2phPfwY9q8fgya3BFHl5YfrdIYQawWQtSqPWsI1o2n/qxVpuvzEPu4ONSKYwUFBYWrxcfJh3dvexeTNPH4D49TXNU8OY0Gk4F/HPgHD259EGeNM8tHL+e+mPsaVTirIXoH9kYlVMTnxDeBpRcSPXgo1ecqOL2/6UOyWzPSogY2SikLpZTfSSm/lVJeWQWUVsJ5OWwpqmUymEFKuEx+qZTyvIfYwR36P4EueSvDPLI42ASCeOGehdyz6R5e/OlFCiqv7MeekXiU7cuW4urdGTffwYRnvAHBvdlY2eWCSLS8nGTMJSX8LE5xT/Q9LB22tFX1M28qOnp1ZMHABWydvJWHuz3MwbyD3L/5fu767i42ndnUpBXuFRRaGmqtilGPdEXnqGbje4epKm/evHr/dhFMmfM6hmojn7/0IjuW78Yr0IXJM/swfEZMq4oiaMxuZznwgxBihhDiQWAbsMy2ZrVAagRx3RxiqyBWKkwrKCg0FeEe4SwZsoSs8iye2v4U1Sbbej9TS1O5Z9M9fHz0YyZ1msSKMSuI9olusvXdde5Ee0cTn930gjg0piuuPr5K2HT9HBRC9LW3Ec2HpaiWyWiGE5vhrQiorP9AqUJvwmSWqGpCpmMfAUdPntau4WBa8TV5F6WUJBQkEOYWxvdp3zNu7Ti+OflNo9YsSEth/d9fx803AKO8jaj2Z1GXnMF004sUntPj52apVXK04Cgzv7wfgOED7+XFvi+iUd3YeZy+Tr482fNJtk7eyux+s6kwVPDizhcZvWY0yxKWUaYvs7eJCgo2wcXTgVGPdqW8uJrNHxzFbGpc2khTkJNcws8rCpEaS1s3YVzDoCneBLRzbzYbmorGFNV6E0urhiigM5aq0W1tbFfLoyZkuo7HRG0Nn1byhxUUFJqSXgG9WHTTIg7lHWLWrlmYZdN/wEkp+ebkN0zZMIWM8gz+ecs/mdt/Ls5a5ya/V1xQHIfzD3PO0LR9E1UqNdGDbiHl94NUFLe8ljl2Jg7YI4Q4LYQ4LIQ4IoSwX3K6jRB1kohV0mQJmS7NgupSKKq/d21ppcWLoq6Z7OgO/Z+kS/mvBJQnkVlcedX25FTkcLb6LPdE38OqO1bRwbMDc36dw4ytM0gpSbnkvMLMdL5e+CoqjYbOAx8CHIguXwJtepEfeDNSWvYaa0+t5b5N9xFQZHlPGBg76aptvR5x0jjxp85/Yt34dbwz9B1C3EL42/6/MWzVMN7a9xZZ5VkNL6Kg0MoIDPfglv+LJPP4WX5dbfue3cV559i89Air3zpASX4lQ+8dwD1vvoXWQcvKBa+Ql5JscxuamsbGw+ViiUiaAgzF0kbpxqKeoloaa5VppcK0goJCUzOy3Uie6/0cW1K28M8D/2zStUuqS3j+p+eZ8+scuvp2ZdUdq7i17a1Neo+6xAbFYpRGDuYdbPK1owcPRZrNHPvlpyZfu5UzAojA8pl9BzDG+u91irSGTNcJkS2uPw+/xCqIk3Kd2RB/jh3/+4ADZ0NJqgzjIcM64hNSMJuvrtp0YqGl6nm0TzQRnhF8MvIT5vSfw7HCY0xaP4mlh5de1C7obHYmX782C4ApsxaScsRAcFAVnucOwC0zyS2rBoz8XPQBs3+ZTc+AnjzuNxGEQBsaelV2Xu+ohIpbQm/h4xEf89WYr7gl9Ba+TPqSUWtG8fxPz3Mk/4i9TVRQaFKiBgTRbWgIv/+YTtKv2Ta5R2W5np0rTvDlvL2kJhTRd0w40xb0I+amYHxDw/jTvMVodQ58veAVcpMb1ZCoxXDJGBshRCfgLutXAbACEFLKqy832pqpp+2SpiZkWhHECgoKNuD+mPvJLM/kk4RPCHYNZmrk1Gtec3/Ofl7e9TIF5wp4ptcz3B9zP2qVbSs/9vTviUalIT47nkHBg5p0bZ+QMAIiOpKw80d63z6+SddujQghHIFHgQ7AEeAjKeX1m0hZx0Wskn8UxPVXHS6pNCCkmQMZzghhxLRtM0Z9NbXBb++8yL/eVePq7YObjy9uPn64+fgS0TuWkMjLF61KKExALdR08upksUmomNJpCreE3MLi+MW8c+gdNp3ZxNz+c+nh34Pi3BxWvjYLs9HIn+a+wblyV0oLqogLWgltekLH4Zw6fAynth/yS14K98fcz9O9nibvh1fRBAaiclAi1BoixieGxTct5plez/DFsS9YdXwVW1K20NO/J/dG38uQ0CE2fw9UUGgOBk7qQGFmBTu+OIZXkDOB4R5Nsq5Rb+L3H9M5uDkVQ7WJqEFtiB0TjovHhe8/XoFtmDrvDVYueIWvX5vFpFkLCOrQuUlssDWX8xAfw3KyPEZKOUhK+Q7QMhr02QNZX8h0jYdY+UBSUFBoeoQQzIydyc0hN/N6/Ov8lH71XlCD2cCSg0uYvmU6DmoHPhv9GTO6zmiWjaCTxonuft3Zm2Ob4lfRg4eSn5JMfmrz9mNsoSwD+mARw6OAt+1rjm05L4clwmy6sMp0Sf0e4tJKAz76QvQmFbd2d+Kp/63i8Q+/4J4FixgRepqIYBN9xkwguHM0QqjIPnWcgxvX8fWCWSQf2ndZe5KKkojwjMBRc+FBuZ+zH2/f8jbvDH2HMn0Z9266l0Vb57Bi/kyM1dVMfnUhvqFtSdyVhYODmQjzt3DzTA4XHOFvRx9F7ZjJrL6L+Eufv6BRaWpbLik0nkCXQJ7r/RzbpmxjZuxM8s7l8eyOZxnzzRiWJy1v8pQOBYXmRqVWMfKhLrh6OrDpv0eoKL62GiRms+TY7myWz93DnrXJtOnkxZ1z4hgyLfIiMVyDh38gU+cuxtHNjVULZ5N1onUEFV9OEE8EsoHtQogPhBC3cmHLvxsLcz1FtayVpf3dFA+xgoKCbdCoNLw1+C0ivSN5YecLJBQkXPEa6aXp3L/pfj448gHjO4xn5ZiVdPHtYgNrL01cYBxJhUmUVDd9n9fIgYNRqdUk/ry9ydduhURLKe+WUr4PTAZusrdBzYFDdYnFQ1y3oMxlQqaDqywhhcE+aoQQOLm549+5O0Wdb2eC+6/E3tSd0X9+nqnzFvPgkg95dOnn+Ia1Zf3br5N6uP6WSlJKEgsTifa+dFG6W0JvYd34dfxf8CQMX8RztjSftjPG4t8uAqPexJnDBXRy2oWmTQxrRAX3b74fpIbK1MeY3HlM7ToWQayES18NLloXpkVN47sJ3/H3W/6Oj5MPi+MXc9uq2/jHgX9QWFlobxMVFK4aR1ctox/rhr7KxKb3jzS6N/sfSU8sYuXr+/hhWRLO7jrGP9eT2x/vhneQS4Nz3f38mTp3MS6enqxaNIeMpKNXZUNzcklBLKVcK6W8E4gEtgPPAP5CiPeEEMOby8AWQz05xIqHWEFBoTlw1jrzn1v/g5eDF0/88ASZ5ZmNmielZP3p9UzeMJkzpWf4281/Y8HABTYpnNUQcUFxSCT7c/c3+drO7h6E9+xD0s/bMZtu3EAmK7UJqtd1qLSVmojpdqmbUJmNmE11qjlfJmS6TVUW7o4m3J0v3AYZ+jxMiXTm3LbXL3je0cWVybNewysomLV/fY2MxIs3eLnncimqKmqwSrssq8JvQwZe0oXEW7W8fPJ1nt3+LEkJqZgMZkLEDl4L68Tc3fPoE9CH/o4L8NaGo7EewpvKyzEVFqJVPMTXhFqlZljbYXw++nM+G/UZ/YL68WnCp4xbN45vk79VehkrtFp8gl257b4ocs+UsuPL41f0u1yQUcb6Jb+xfslv6CuNDJ8Rw+SX+hDcyeuKbHDz8eVPc97AzduH1W/MJT2hZdd0bEyV6Qop5RdSyjuAEOAQ8JLNLWtp1JNDHOrtjKuDhrbeDZ+WKCgoKFwLvk6+vHfbe+jNeh7//vEGPa1l+jJe+vklZu2aRZRPFGvGrmFEuxHNZO3FdPXtipPGib3Ztgubrig+S9qR+r13NxDdhRCl1q8yoFvNYyFEqb2NsxVCWkOmzXUC2UrSLT2J/0CNhzjE4+Lzgq4dwvjYOAqvtK2QfeEGzsnNnSmvLsTd1481b86/KBSwbkGtS1FRfJaVr82ioriYKbMW8tF9X/N0r6f5OfNn/vvdZyCMzG9fwcq8PTzQ5QHeve1dzpZpLzh4N6RZhL4u7MZr+GErevj34O+3/J01Y9fQ1r0tL//8Mk/++CQ5FTn2Nk1B4apo38ufPqPbcezXbI7syGhwfPnZKn74NJEVi/aRl1LKwMkdmDavHx37BiBUVxcg7Ortw5/mvoGHXwBrFs+/ZHRNS6CxVaYBkFKelVIulVLarhxpS6UeD/HNnfz4fe5wPJy1djJKQUHhRiLCM4J/DfkXaWVpPLP9GfQmfb3jDuUdYvL6yWxN2cpTPZ/io+EfEegS2MzWXohWraWXfy+b9CMGiOgVi4OLCwk3eE9iKaVaSulu/XKTUmrqPG59zSEbxLJRk8JaVKtudGB1KVRd3Iu4IjcTJ3MVIZ4XRxP4uzmyxW0C51Qu8NObF1139vBkyuxFuHh4suaNeRdUUk0qSkIlVHT2rr+IzLnSEr5+bRZlhflMnDmXNp2i0Kq0PNj1QdaMXUN4aRcy3E+TpNXz15v/ynO9n0Oj0pBXVn1Bapa+RhC3VTzETU17z/b8b+T/eKHPC8RnxzNh3QRWnVileIsVWiWxY8Jp182XXV+fIuNYUb1jqiuN7F57ms/n7OHE/lx63BbG3a/1p8dtYai1VyQT68XF04s/zX0Dz8Ag1r61gDO/HbjmNW3Btb/SG4V6PMRwPmxaQUFBoTnoG9iXhQMXsj93P7N/mX1Bj2Kj2ci7v73L/ZvvRyVU/G/U/3io20MtpoJqbFAsp0tOU1BZ0ORra7RaIgcM5tS+PVSfU4rj3Chc0IfYbMQs/7CtqSePWJ9hEbGhnvVHlHdsG8IX3A7HvoWci9vzuHr7MGXOIhxcXFi1aHZtMbfEwkQiPCJw0jhdNKeyrJRVr82iJC+XCS/OJSTqwhx+HxmAU6kXvRwS+MojjpHtRtZeyy2tvsBDrE+1CmKl5ZJNUKvU3BtzL2vGriHKJ4r5u+fz0LaHyChr2MumoNCSECrBsAei8fR3YssHCZQWnO+xbjKaObw9nc9n7+bg5lTa9/Rj2rx+DJzUAUeXpnX0Obt7MGX2IryCQ1j319c4fcA2B+PXgiKIG0vNplMoPzIFBQX7cnvE7TzV8yk2ntnIvw/9G4CMsgwe2PwA7/3+HmMixrDeWSB1AAAgAElEQVRq7Cq6+XWzs6UXEhcYB2AzL3H04Fsx6qs5ufcXm6yv0PIo/e18nrCQposFcT2VplW5yVTrXPFwrL/YTK8wT5acG4bZwb1eLzGAu68/U2a/jkan4+uFr1KYkW4pqFVPuHRVeTmrFs6mKDuTcS+8SliXi/8uM5Is3ptbdEeJ0JxPwzKazBRWVON3gYc4FbWfLyoXJV3LloS6h/Lh8A+Z3W82RwuOMnH9RJYnLb/gEFJBoaWjc9Iw+rFuSCnZ+N4R9FVGTh3I48v5e/l5xUl8gl2Y8nIfhk2Pwd334sO8pqJGFPuGhbP+7dc5uW+3ze51NSjqrrFcwkOsoKCgYA8e7PogkzpO4oMjHzB/93ymbJjC6eLTvDX4LRYNWoSLtuVtliO9I3HTuRGfYxtBHNSxM15BbUi8wcOmbyQM+Za0aInVQ8wfPqP/UFhLSoljYSoVXmEXeJfr0ivMi1JcOBV+NyRtgJz6K6R6BgQyZfbrCCFY8dpMqguKifKOumBM9bkKVr8+m8KMVMb9ZRbtuvWsd620xCKc3LT46i4smFdQrkfKC4t3GlLTlPzhZkIlVPyp859YO24tvQN6szh+Mfdtuo/kkmR7m2Y3iqqKWHViFamlqfY2RaGReAY4M3xGDEVZ5Xw2azdbPjiKWqvi9ie6Me6Znvi3bZ5sGidXNya/+hoB4e359h+LObFnV7PctzEogrixtLF+iKl19rVDQUFBAUuP4lf7vcrA4IGsOrGKTl6dWDV2FaPCR9nbtEuiVqnpG9CXPdl7bLK+EILom4aSnniE0vw8m9xDoWWhqpO2pDIbMV0kiC/0EJfk5eKgL8PoH37JNaOC3HHQqFjnOA4u4yUG8G4TzJTZizAY9IzYG0A4QbXX9JXnWP3GXPJSkrnjuZcJ79mn3jWkWZKeVERolDdCXJirmltaBUDAH3KIlR7EzUugSyDv3vourw96neSSZKasn8KHRz7EaL7uC7nXUlBZwN/2/Y2Rq0cyf/d87vjmDp7b8RxHC1p+Sx0FCIvxYdCfOuLgrGHI3ZFMndWXdl19EZc6GbQRji6uTJr1GoEdOvPtv97i2K87m/X+l0IRxI3lri/gwR9Ao7RYUlBQaBloVBr+fvPf+ffQf/PRiI9o49rG3iY1SGxQLJnlmTbLx4u6aQiA4iW+UajZzAkQ0ogUmvOFpR09oeRCD3FNP0x1UIdLLqnTqOga7MHuLBPEPQpJ6y/pJQbwDW2LZmpvtEYVCe8up6ywAENVFWsWzyPn1AnGPP0S7XvHXXJ+QWY5lWUGQqO9L7qWV1YNgL/VQ2w+dw5jXp5SUMsOCCG4o/0drBu/jsEhg/nXwX8xbeM0jhcdt7dpNqWgsoC39r3FqNWj+CzpM24Nu5XPRn3Gg10fZE/2Hu767i5mbJnBL5m/KMXHWjjdhoRy92v9iR7UBpXafhLQwdmZSa/MJ7hzNBuX/I3En7fbzZYaFEHcWBw9IKT+010FBQUFe+Gsdebm0JvRqDT2NqVR1OQR78vZZ5P1PfwDCInuQuLPPyqbsxuBWg+xQAqLt85cU2raM/SikOmMpKNUqRxxDbz84VHPME+OZpZS3fcRi5d451uXHX9Mk0nCUA3V5WV8/dorrHlzHlnHjzH6z8/TMW7AZeemJ1ryh0OjLhbEtR5id4uHWJ9uOUhSPMT2w9fJl38M+Qdv3/w2ORU53Pntnfz70L8vWfW/tZJ3Lo83499k5OqRfJH0BcPbDWfduHW8cdMb9PDvwVO9nmLb5G083+d5UkpTePT7R5myYQrfJX93Q3nOFa4OnaMTE2fOIzSmCzuWfUD1uQq72qMIYgUFBQWFZqO9Z3t8HH3Ym2ObfsRg6Ul8NjuL7JPXt+dGAVRWD7Hl6MNS68NUYW237BF2Uch0RtJRMh2D8HC+fLRXrzAv9CYziWfVEPcIJK6D3IRLjk8sTCSsUwwTZ86nvKiIjKQERj3xLJEDBjf4GtKTivAJdsHF42Kb8kqrEAJ8XCzpWvo0S96mVskhtjs1AnFk+EjeP/w+U7+dypH8i6uStzZyK3J5Y+8bjFo9ii+Pfcmo8FGsH7+eRYMW0c6j3QVjXbQu3BdzH5snbmbBgAXozXpm/jyTMd+M4YukL6g0VtZ/EwUFQOvoyPgX5zB13mIcnO1b90QRxAoKCgoKzYYQgtigWOKz423mwe0UNwiNVqeETV8hQoiRQojjQohTQoiZ9VwPE0JsF0IcEkIcFkKMrnPtZeu840KIEc1mdJ0cYiksgth8rsTyhGcYVBZBdTkAZUUFlOTmkOUYhLvj5SMqerX1AuBgWjH0exx0bvDLv+odW1BZQF5lHlHeUQRHRnPngreY8urC2vD9y2HQm8g6VVyvdxgsIdO+rg5orOGNhpoexGFKy6WWgKejJ2/c9Ab/ufU/lOpLuXvT3by9/22qjFX2Nu2KyanIYdGeRYxaM4qVx1cypv0YNkzYwGsDXyPM/fIRCVq1lgkdJ7B23FqWDFmCn5Mfb8S/wYhVI3jv9/corqcfuIICgNbBEZ8Q+0e8KIJYQUFBQaFZiQuMI78ynzMlZ2yyvoOzMx1i+3P8150YDQab3ON6QwihBv4DjAKigbuEEH/sIfQqsFJK2RO4E3jXOjfa+n0MMBJ417pec9hd+9hsDZk2VVg3355W0WhtvZSRZPHwZjq2wd3p8n02A9wdaePhyKG0s+DszRHVfRw9Ub9oTSxMBKhtueTfLoKwLt0bZX/WyWLMRllv/jBYQqb/2INY7eWF2r15qsIqNI7BIYNZO24tEztO5NOET5m8YTIHcg/Y26xGkV2ezcI9Cxm9ZjSrTqxibPuxbJiwgfkD5hPqdmUHLyqhYkjYED4b/RnLRi6jm1833v3tXYavHs7i+MVklWfZ6FUoKFwbiiBWUFBQUGhWYoNiAWweNl1VUc6Zg7bJVb4OiQVOSSmTpZR64Ctg3B/GSKBGiXkANbvbccBXUspqKeUZ4JR1PdtTx0NsUNUIYmvItKfV62ANm85MOorawZECnQ8eDQhigJ5tvTiUVkxpQSW7Um8hPmMg0nxxVENiYSICQZRPVD2rXJ70xCLUGhVtOnjWez2vrBp/pcJ0q8BN58bc/nP5cLil+vT9m+9n0Z5FVBjsmxt5KbLKs5i/ez6jvxnN6pOrGd9hPN9N/I55A+YR4hZyzev3CujFv2/9N2vGrmFY22GsOLaC0WtG8/LPL3Pi7IkmeAUKCk2HIogVFBQUFJqVENcQ2ri0IT7bNv2IAdp27YGLpxcJSth0YwkG6ibcZlifq8s84G4hRAawEfjzFcwFQAjxsBBivxBif35+/jUbLerkEJtqimqdK7Nc9LB6t4otebcZSQm4hHVEClWDHmKAnqGeZBZX8vPa05ilmkqjCwWZ5ReNSyxMpK1726vq/Z2eVESbjh5odPU71HNLqy/0EKelolUqTLdo4oLiWDN2DXdH3c2K4yuYsG4Cv2b+am+zaskoy2Der/O4fc3trDu1jkkdJ7Fxwkbm9J9jk04FHb06smjQIjZO3MhdkXfxQ9oPTFo/ice/f5z9OfuV4ocKLQJFECsoKCgoNCu1ecQ58Zil2Sb3UKnVRA66hTOH9nGutMQm97gBuQv4VEoZAowGPhNCXNE+Qkq5VErZR0rZx8/P75oNEqrzbZdM1hxiU6VVELsGgFoHJemcKy2hMCMNbbCl3VJjPMS92nrhZRKkHMijg88xADKSzl40LrEw8aq8w+VnqynKqiA0yqfe60aTmcKKavysHmJzdTXG7Bx0SkGtFo+z1pmXYl/if6P+h6PGkUe+f4TZv8ympNp+70XpZenM+WUOd3xzBxtOb2BK5ylsnLiRV/u9SpBrUMMLXCNBrkG8FPsS2yZv48keT5JQmMADWx7g7k1380PqDzb7LFBQaAyKIFZQUFBQaHbiguIo1ZfatIdnzOChmE0mjv+602b3uI7IBOomDIZYn6vLDGAlgJRyN+AI+DZyrk2om0NsVNUI4nPWiyrwCIHiNDKPWfKHTf7hQOMEcUwbdwZXa5EqwU3ttuPlVED6saILxhRWFpJ7LpcYn5grtj09ydpu6RL5wwXleqSk1kNsyMgAKZUexK2IHv49+PqOr3mw64NsOL2BCesm8GNa80atpJWm8equV7njmzvYeGYjUyOnsnHiRl6Je4VAl8BmtQXAw8GDR7o/wpZJW5gVN4vCykKe2fEM49aOY83JNddd+yqF1oEiiBUUFBQUmp3YQGsecbbt8oj92obj1zZcqTbdOPYBHYUQ4UIIHZYiWev/MCYNuBVACBGFRRDnW8fdKYRwEEKEAx0B28XD10HU6UNssuYQm/V1NtQeoVCcTkZSAhqtjnPuFk+Yu2PDgrg0+xyd9GrSfFU4aysJdU8h62QxRoOpdkxSURJwvqDWlZCeVISTuw6f4PpDrWt7EFs9xPrUmgrTiiBuTTioHXi619N8cfsXeDt68/T2p3nxpxcpqipqePI1kFKSwqxdsxi7diybUzZzV+RdbJq4iZmxMwlwCbDpvRuDo8aROyPv5NsJ3/LW4Ldw0jgx99e5jFo9ik+OfkK5/uL0BAUFW6EIYgUFBQWFZsff2Z9wj3CbFtYCS3GtnNMnKcxIb3jwDYyU0gg8CWwBkrBUk04QQiwQQoy1DvsL8JAQ4nfgS+B+aSEBi+c4EdgMPCGlNF18FxtQJ4fYqLYW1ZJ1Wip5hkFJOhmJRwnqFEmpXuKkVaPTNLz9iV+fjFkr2GQ4h1lCqHsqJoOZnNPnw15rKkxHekdekdnSLElPKiIsyvsCL3dd8sqqAfC3eojP9yBWBHFrJNonmi/HfMmTPZ5kW9o2xq8dz6Yzm5o8hza5JJmZP89k3LpxbE3ZyrSoaWyetJmXYl/Cz/na0xSaGo1Kw6jwUawYs4L3h71PuGc4fz/wd4avGs4/D/yT/HPXXmtAQaEhFEGsoKCgoGAXYgNjOZB7AIPZdq2RogbdglCpSPxZ8RI3hJRyo5Syk5SyvZRykfW5OVLK9dbHiVLKgVLK7lLKHlLKrXXmLrLO6yyl3NRcNqvU57cx5pocYi4UxNUlBeSlJhMSFUNJpaFR4dI5ySWkHCnEu5cPZWYzFXojbdzTUakE6XXyiBMLEwlzC8NN53ZFdhdklFNVbrhkuDTU8RC7WzzEhrQ0VO7uqD3rr0it0PLRqrQ80v0Rvh7zNSFuIby480We2v4Ueefyrnnt5OJkXtz5IuPXjufHtB+5L/o+Nk/azAt9X8DXybcJrLctQggGtBnAh8M/5Kvbv6J/m/58kvAJI1aPYN6v80gtTbW3iQrXMYogVlBQUFCwC3FBcVQaK0koSLDZPVw8vWjXvReJP29HmpWiLdcb5vMR06g0FkFslnUEr0comZXuICUhUV0orTTi7qS5eKE/sGfdaZzctNw0OgKA8iojOrWBgAj32txfsAjiqwmXTkssBCAk0uuSY/JKqxACfFx0gCVkWhcWdkmPskLroYNXBz4b9RnP93me3Vm7Gb92PN+c/OaqvMWnzp7ihZ9eYPy68exI38EDXR5g86TNPNfnOXyc6i/Y1tKJ8Y3h7VveZsP4DYzvMJ4Npzdwxzd38NyO5zhacNTe5ilchyiCWEFBQUHBLvQN6ItA2DSPGCxh0+WFBaQlHLbpfRSanxpBLAGNyiImqqt05B9xsxyAeIaRcc4DlVpFUMfOjfIQpx8rIvN4Mb1HtiPU34UAdwfKqi3h2KFR3uSnl1FVbuBs1VmyK7KvOn/YJ8QVFw+HS47JK6vG19UBjdULrvQgvr5Qq9TcF3Mfa8auobN3Z+b8OodHtj1CZnnj6tGdOHuCv+z4CxPXT2Rnxk5mdJ3BlklbeLb3s3g7XjryoDUR5h7GnP5z2DJ5Cw92fZA92Xu467u7mL5lOrsydyktmxSaDEUQKygoKCjYBU9HTyK9I22eR9y+Txw6J2eluNZ1iLnOLkantkQAVBQ5U5Dghv5MKniGknHOg8BAL7QOjg0KYikle9cl4+rlQMzgNggh6BXmRVmVJaw/NMobJGQcP0tS4dUV1DJUm8g+VUJY1OVFS25pVW2FaWkwYMjKUnoQX4eEuYfx0YiPeDXuVX7P/50J6ybw5bEvL9mG6HjRcZ7d/iyT1k/il6xfeLDrg2yZtIWnez2Nl+OlIw5aM75OvjzV6ym2Td7G832eJ7U0lce+f4wpG6bwXfJ3GM1Ge5uo0MqxqSAWQowUQhwXQpwSQsys53qYEGK7EOKQEOKwEGJ0nWsvW+cdF0KMaOyaCgoKCgqth9jAWH7L+40qY5XN7qHVOdC5/yBO7v0VQ5Xt7qPQ/Jg47yHSqSwCwmS2hERXn0nFoPMht9KVED9L2HFpleGyFaZTjhSSe6aUPqPbodGqAegV5kW1wYzeZMa/rRs6RzXpSUUkFl1dQa3ME2cxm+Rl84fB4iH2t1aYNmRlgcmk9CC+TlEJFVMjp7J23Fp6+ffi9b2v88DmB0gpSakdk1SYxNM/Ps3kDZPZk72HR7s/ypZJW3iq11N4Ot4YeeUuWhfui7mPzRM3s2DAAvRmPTN/nsmYb8awPGk5WeVZ9jZRoZXScCLNVSKEUAP/AYYBGcA+IcR6KWVinWGvYqlk+Z4QIhrYCLSzPr4TiAHaAN8LITpZ5zS0poKCgoJCKyE2KJZlicv4Lf83+gX1s9l9+o6dRI8RY9A4XDpEVaH1YaqTTutkFcTVRh2uWARxcfIpzKgIcbMchJRUGnC/hIdYmi3eYQ8/JyIHBNU+3zPMEz1QXm3EW60iuLOXRRAHJxLiGoKHg8cV2ZyeVIRaqyKow+Xn5ZZW0y3EMkafZm25pHiIr2uCXIN477b3WH96PW/ue5PJGyYzvct0koqS2JG+AzedG493f5xp0dNw17nb21y7oVVrmdBxAuM6jOOn9J/4+OjHLI5fzOL4xQS7BhMbGEvfwL7EBsa2iBZTCi0fmwliIBY4JaVMBhBCfAWMw9KWoQYJ1PxFewA1RzvjgK+klNXAGSHEKet6NGLNRmEwGMjIyKBK8RY0OY6OjoSEhKDVNlzJU0FB4camd0BvNEJDfHa8TQWxV1CwzdZWsB9mcT6s1MkaMl1ltnz26FNSyUg6ikDSRpuPySwpqzJeMmT61IE8CjPLGTY9GnWd6tVdgj04JKCi2og3lrDpM78XkJKRRXToVeQPJxYR3NGz1gNdH0aTmcKKavyUHsQ2oaXvATvRiaXdllKiL6HKUEWUexT39LgHF60LKqEi83QmmTQu17i5ac49oEqoGBI2hCFhQzh59iTxOfHEZ8fzfdr3fHPqGwDauretFcd9A/u2iorbCs2PLQVxMFC38WMGEPeHMfOArUKIPwMuwG115u75w9ya3UxDazaKjIwM3NzcaNeunVKxsQmRUlJYWEhGRgbh4eH2NkdBQaGF46J1oYtvF5vnEStcn5isurVao8bXGj5tMFvCo6vPpJLh446flw6HihMUW/OA6/MQm01m4r89g3cbFzr2udCj5KhV4+KgobxOYS0AbZYn0T2uTBCXFVVxNucc0YPaXHZcQbkeKanNIdanpaJydkbt0zqrBrc0WsseUEpJtakarUqLWnXpA5SWgj33gB29OtLRqyPToqZhMps4fvY4+3L2EZ8Tz6Yzm1h1YhUAER4RFwjk6zXvWuHKsKUgbgx3AZ9KKd8WQvQHPhNCdGmKhYUQDwMPA4TVc6JaVVXV4t8IWyNCCHx8fMjPVxqpKygoNI7YoFg+OvIR5fpyXHWu9jZHoRVhEhYRnOrniaehAnRgNFoEb2VaGlkeOrpH+kLZDkrLKwHq9RAf25NDce45Rj3aFaG6eF/gotNQXFGJlBIPfye07oKQks5E+URdkb01LZtCG1FQCyCgJoc4NQ1t27bKnqWJaC17QCEEjhpHe5vRaFrKHlCtUhPtE020TzT3xdyH0WwkqTCJ+Jx49uXsY/3p9aw4vgKwCOkacdwnoM8Vp0C0VMzSTE5FDmdKzpBckmz5Kk5Gq9Lyn9v+g4NaSR+qiy0FcSYQWuf7EOtzdZkBjASQUu4WQjgCvg3MbWhNrOstBZYC9OnTp9667C39jbC1ovxcFRQUroS4wDiWHl7KgdwD3Bx6s73NUWhF1AhiAJO0fPZIa8h0sVqNyaAnpEMEHDNzrtASdvxHQWwymNn33Rn827oR3r3+cEpnBzUFZZKc0iqCPJwwB5cSfKIjkZ5XLoidPXR4t3G57Li8smoA/Gs9xGk4dOp0uSkKV4iyV7ENLfHnqlFp6OrXla5+XZnRdQYGs4GEggRLiHVOPKtOrGJ50nIEgkjvyFoPcu+A3i3+kNZgMpBWlnaR8E0pTaHSWFk7zl3nTqhbKAfzDrLqxCqmRU2zo9UtD1sK4n1ARyFEOBbReifwf38YkwbcCnwqhIgCHIF8YD3whRDi71iKanUE4gHRiDUVFBQUFFoR3f27o1Pp2JuzVxHECleEsc7e24RAmE1g0qB2NFHkavGsBcd0h2NgKEwFBO6OF259EnZlUl5UzdC7oy65mXfRWeYcyy4jyMOJbK9T+Jt6YshVQyMjQ81mSXpSEeFdfRsUDbUeYndHpMmEPiMDt9tuu+wcBQWFxqFVaenh34Me/j14uNvD6E16Ducfrg2x/vLYl/wv8X+ohIpo72j6BlkEci//Xjhrne1ic4WhgpSSlAtEb3JJMhllGRjl+bZTgS6BRHhE0DugN+Ee4YR7hBPhEYG3ozdCCGZsmcEHhz9gYseJOGmc7PJaWiI2E8RSSqMQ4klgC6AGPpZSJgghFgD7pZTrgb8AHwghnsVSYOt+aemynSCEWImlWJYReEJKaQKob01bvYbG4urqSnl5ub3NuGqWLFnCe++9R69evVi+fLm9zVFQULjBcFA70NO/J/HZ8fY2RaGVYarTPLLaLBDSiEHrRXbETeTrs/B0ccO5TWcAzGdTgXZ4OJ/3EBtMGvZvSqVNR09Coi6dS+iss+RvJuWUMiTSn9+1uxlGT9KTiggIb1y134L0MqorjA22WwLIK61CCPBx0WHIzgaDQakw3UJR9oCtH51aR5/APvQJ7MNjPEaVsYrf83+vDbH+LOEzPjn6CRqhIcY3pjbEuod/jyYVlVJKiqqKSC5JPu/xtQrf3HO5teM0QkOoeyjtPdszrO2wWtEb7hHeoGB/oscT3Lf5PlYeX8l9Mfc1me2tHZvmEEspN2JppVT3uTl1HicCAy8xdxGwqDFrXs9IKZFSolLZrmX0u+++y/fff09ISIjN7qGgoKBwOeKC4lhyaAlnq84qRU4UGo2xTsh0lVShMhvJCYwjW/aluvhdHIyh/LrdSIS+M6IkA2h3QR/iw5nRVJbqGfVwl8t6bTUqgYNaxbHsMkr1pZzWH2e4bzUZx4roM7pdo2xNS7TkD4dENkIQl1Xj6+qARq2iIi0VAK1SYfqGQ9kD2gdHjSNxQXHEBVnq9p4znOO3vN9qBfLHRz/mgyMfoFVp6ebXrVYgd/frjk6ta3B9szSTVZ5Vr/At1ZfWjnPSOBHuEU7fwL61gjfCI4JQ91C0qqur4t0roBcD2gzg46MfM6XTFLt5vFsa9i6qdV1RXl7OuHHjOHv2LAaDgYULFzJu3DjmzJmDt7c3zzzzDACzZs3C39+fp59+mr/+9a+sXLmS6upqJkyYwPz580lJSWHEiBHExcVx4MABNm7cSNu2bS+63+bNm3nllVcwmUz4+vryww8/UFRUxPTp00lOTsbZ2ZmlS5fSrVs35s2bR1paGsnJyaSlpfHMM8/w1FNP8eijj5KcnMyoUaOYPn06zz77bHP/2BQUFBSIDYqFQ7AvZx/D2w23tzkKrYS6fYirzQKVJZiMiPIvSEKPC278/mMWh8yL0e05xzCVlrKUMkxdHDCaHDmU3o22XXwI6uDZ4L2cdWqO5ZSSVJgEgF9HZ7LjSzBUm9A6NFwBOD2xCN9QV5zdG94w55ZW1akwXdOD+OJ9gELLQdkDXr84a50ZEDyAAcEDACjXl3Mw72BtiPX7h9/nvd/fw0HtQA+/HpYc5KBYOnt1JrM8szbM+UyxRfymlKZQbaquXd/b0Ztwj3BGtBtBhEdErfgNcAlAJZr+MOTxHo9z98a7+fLYl8zoOqPJ12+NKIK4CXF0dOSbb77B3d2dgoIC+vXrx9ixY5k+fToTJ07kmWeewWw289VXXxEfH8/WrVs5efIk8fHxSCkZO3YsO3fuJCwsjJMnT7Js2TL69au/L2d+fj4PPfQQO3fuJDw8nKIiy8nz3Llz6dmzJ2vXruXHH3/k3nvv5bfffgPg2LFjbN++nbKyMjp37sxjjz3Gf//7XzZv3sz27dvx9VV6sykoKNiHGJ8YXLQu7M3eqwhihUZT10NcqXLBWV+Ka3kGLuIgEELPYxuI3P4Saf95kaN5HYguj+L7/x5lp6Madx6g2uhI3NiIRt3LSacmOb+CI/mnAYjp3o7tu0+TdbKYtl0u3w5JX2UkJ7mEHreFXnZcDXll1QS4n+9BLBwd0fj5NWqugn1Q9oA3Dq46VwaHDGZwyGAASvWlHMg5UOtB/vdv/4bfLpwjELRxbUO4RzhxQXEW4esZQbh7OJ6ODR/INSXd/bpzU/BNfJLwCVM7T23xhcOaA0UQNyFSSl555RV27tyJSqUiMzOT3Nxc2rVrh4+PD4cOHSI3N5eePXvi4+PD1q1b2bp1Kz179gQsp4snT54kLCyMtm3bXvKNEGDPnj0MHjy4ts+bt7clBGvXrl2sXr0agKFDh1JYWEhpqSX84vbbb8fBwQEHBwf8/f3Jzc1VQmQUFBRaBBqVht4BvYnPUfKIFRpPXUFcbZIM/P2fqE16Ert74ubojENpGerys3SKKMPH+B9Gui/h6wk9Sf4tn5S9JUQGnMAvbGij7uWsU2M0S/ZlHSHIJYhO0SHs1Jwh/VhRg4I460QxZpNssN1SDbml1XQLsbR/0aeloQsNRdgwbFbh2rGqklsAACAASURBVFH2gDcu7jp3hoQNYUjYEACKq4rZn7uf5JJkQlxDiPCMoK172xZVxOqJnk9w57d3sjxpOY90f8Te5tgdRRA3IcuXLyc/P58DBw6g1Wpp164dVVWWSpEPPvggn376KTk5OUyfPh2wvHm+/PLLPPLIhb+IKSkpuLhcviXD1eDgcL7nmFqtxmg0Xma0goKCQvMSGxjLzoyd5FTkEOgSaG9zFFoBBmGufWwyGlCZqhBSkitdaRfWFvYcQX/6NFrPUDz1ubi7qGjX1Zd2XX2h/CHwDAMebdS9nK2Vpo+fTaJ7QBQanZqgDh5kWHsLX460pCI0WhVB7Rv2BJmlpLCiGr+aHsRpqWiVcOkWj7IHVKjB09GT29q27KrwMT4xDAkdwrLEZdwVdRfuusYVB7xeUY4bm5CSkhL8/f3RarVs376d1NTU2msTJkxg8+bN7Nu3jxEjRgAwYsQIPv7449rqhJmZmeTl5TXqXv369WPnzp2cOXMGoDZc5qabbqqtErhjxw58fX1xd7+xf8kVFBRaB/2CLB6RfTn77GyJQmuhrocYQK9RU+6ooxoNYV27AlB9Ohk8w1Bjoq3DVVYDVutwrMzBWXuOQn0m0T7RAIREelGYWUFFSfVlp6cnFtGmkxdqbcPbriqDGSkhwN0BaTajT0tHF6YI4paOsgdUaG083uNxyvRlfJb4mb1NsTuKh7gJmTZtGnfccQddu3alT58+REZG1l7T6XQMGTIET09P1GpL8Y3hw4eTlJRE//79AUvp/s8//7z2+uXw8/Nj6dKlTJw4EbPZjL+/P9u2bWPevHlMnz6dbt264ezszLJly2zzYhUUFBSamI5eHfF08GRv9l7uaH+Hvc1RaAUY+KMgVlHqZPGEhfTsTr67O9WnT0G/QQC00xRe3Y36Poj46i7u9N7A11AriEOjvNmzNpmMY2fpHFcnqkFKsFatLi2spDj3HF3+n707D4+yOhs//j2zZCUBkpA9ARLAJJAQCCQIiq8LOy4IVi281qXFjf58taC2lYq8pVSsUlusaBWpSF2A+qLUKi5UiwoIGkJYjRASIGHPQtZJ5vz+mGGYQJYJZDKT5P5cFxeT82z3HMg8555znnNGx7h0qcpa28RgEUF+1B07hq6pwUdmmPZ60gYUHU1SSBJjeo9hxa4VzEieQXff7p4OyWMkIW4DZ7/dCwsL4+uvv250H6vVyqZNm1i1alWD8oceeoiHHnrogv1zc3NbvO6ECROYMGFCg7KQkBD+7//+74J9582b1+T58/PzW7yWEEK4m0EZGB45nM3Fm9FaN7sMjhDQSA+x0Ui1yda0CQ4JpSwxkdof9kOP2wGINxy/uAslTYSkycQWfQEEkRyaDECvuCB8A00c2n3qXEL8yVNQlA3//S4Ah3afBnD5+eEqiy0hDg/2pfbgPgBZg9iLSRtQdGT3D76fTw5+wt92/o3/N/T/eTocj5Eh0+1g165d9OvXj2uvvZb+/ft7OhwhhPBaWZFZFFcUU1he6OlQRAdgaWTIdK3JgNlQh8lsxicxgZr9+6G7bXbnaE5c/MUmPsNuXx961Cmos83KqgyK2MtCKNx9Cq3tsZzMgx8+g+N7Adv6w4E9fOkZ5dp6n1W1tmc7I4L9qD27BnGcJMQdlbQBhTfr37M/4/uM543db3C6+rSnw/EY6SFuBykpKezfv/+ij8/KyqKmpuHzSStWrCDV/nyUEEJ0FplRmQBsLt5MfLAkAaJ5jT1DXGsyYjbakkrfhERKV6+h7kwNpTqYcOvRi79YcDQ7gnqRXl7Myc1vEXbdnQDEJffkh2+Pcbq4kpAop8mQct7BevUTHNpzir7pvVwe8VBVW49SEBrow6mCAjCbMUfJJHMdlbQBhbe7L/0+Pjr4Ea/lvsYjwx7xdDgeIQlxB7B582ZPhyCEEO2iT3AfwgPC2VK0hVsG3OLpcISXs2Bt+LPJQK3JiMlsAcA30bbGcOne7ynUYYRaLj4hrrBUcMRazoiabsRtmQ+jbgT/no6h0IW7TzVMiHe8w/G+P6emso54F4dLg23IdFg3X0xGA7UHC/CJjUW58Fyp6JykDSjcLaF7AhP7TuTNPW9yx8A7CPPvemtSy5BpIYQQXkMpRVZkFluKt5wbgipEEyLMDZenqTUaqTEbMZhtPcQ+if0AKN/3PYd1GN1riy/6WntO7UGj2VY3Ed/aEtuzwkBwmD/BvfwvXH6ppIDCTTmgIDa5p8vXqaytJyLYNjFYbUGBTKglhHC7+wbfh8VqYVnuMk+H4hGSEAshhPAqmVGZnKo+xfcl33s6FOHlruvmlCwqRa3JSI3JCGbbxFTm6CiUvz9VeT9wWPcisKrINgP0Rdh1chcAgSFX857fDbDtNSiw9d7FJYdweF8J9fX2Huvu8WAOoCD3KL3igvDv5uPydaos9YQH+aG1pragALNMqCWEcLPewb25PvF63tn7DscqXVv+qzORhFgIIYRXyYy0PUe8pWiLhyMRXs9w7rlcn27B1JiMWIxGrPYeYmUw4NO3D3X7f+CQDsNorYGKi5tpetfJXfTy70VqZCzzz9yI7h4L7z8EdbXEJffEUlPP0QNl9mACqe13I0dPdScuqXVLmVTZe4jrT5xAV1bKGsRCiHYxM20m9dZ6XtnxiqdDaXeSEAshhPAq0d2iiQuKY3OxPDsnWuA0UZVPUHcqfc2gFPX2HmIA38R+cPAgh7X9ubiSi5vBfNfJXaSEpnBZZDCn63woGvVbOL4bvv4zMQN6opTtOeKzDgdPxYqJ+KA8l6+hgeq6enoF+VFbUGB7X9JDLIRoB3FBcdzY70ZW71tNccXFP17SEUlC7CH5+fn4+/uTnp7uKOvTpw+pqamkp6czbNgwR/mqVasYOHAgBoOBrVu3Oso//vhjMjIySE1NJSMjg88++6zF686ZM4ekpCTS0tKYMmUKJSUlF8STnp7Offfd5zimtraWmTNnMmDAAJKSklizZg0AixcvJj4+nlmzZl1yfQghhLOsqCy2Fm+lzlrn6VCEF1POPcRBPagx2+YKrfVxTogTMJ04yjGLvae25GCrr1NpqeRA6QFSQlNIigwC4Fu/LEi+AT5fhF9NIb16BzvWHAYoPB2HSVUTeeJtl69j1Ro0RAT7UnvQnhDLM8SdjrQBhbe6N+1eNJq/5vzV06G0K5ll2oMSExPJzs5uULZhwwbCwhrO7jZo0CD+8Y9/cO+99zYoDwsL4/333yc6Oprc3FzGjRvH4cOHm73mmDFjWLhwISaTiccee4yFCxfy9NNPNxkPwIIFCwgPD2ffvn1YrVZOnbJ9A/7www/Ts2fPBh/QQgjRFrIis1i9bzV7Tu1hUNggT4cjvJVTD7EKCHa8rvapd2zzSbDNNK3KNAQCpa3vId57ei8aTUpoCv3Cu2E0KPYUlTN5wiL4YQP88xfEJf2Bb9cXcHjfcXRRBQUZp4kJL8OY9y+oKgH/Hi1ex2p/BDkiyL4GsdGIOTq61fEK7ydtQOGNorpFMbX/VNZ8v4a7U+8mpluMp0NqF5IQA0+9v5NdR8ra9Jwp0cE8ef3ANjlXcnJyo+VDhgxxvB44cCBVVVXU1NTg6+vb5LnGjh3reD1ixAhWr17d4vWXLVvGnj17ADAYDBd8WAshRFsbFmnrIdlUtEkSYtEk5x5i7R/keF3lawWzPwC+iYkAhJWXoft2R5UUtPo6ZyfUSg5Jxs9sJCEskD3FZRB8GVz3JHwwm7isHLZZu/PD7u50qzhDaVwVqePiYXsN7H4Pht7R4nWs9gm/woN9sRQUYI6JQZnNrY5XuE7agM2TNmDX87PUn/Hu9+/ycs7LPDXyKU+H0y5kyLQXUUoxduxYMjIyePnll1t17Jo1axg6dGizH4TnW7ZsGRMmTHD8fODAAYYMGcJVV13Ff/7zHwDHcJq5c+cydOhQbrnlFo4evfh1HIUQwhVh/mH069FPJtYSzXNKiOv9ujlenzHXgzkAsA05thqM9Ks6juoR3+pniGvqa/jqyFeE+oUSHhAOQFJUMLuLym07DLsbYjII/+6XGK21lEalcKZPCgDRyb0htB/kvOPStc4mxBHBfrY1iGW4dJchbUDhLSICI/jRZT9ibd5aCssubs6FjkZ6iKHNvsW7VBs3biQmJoZjx44xZswYkpKSGD16dIvH7dy5k8cee4z169e7fK0FCxZgMpmYPn06AFFRURQUFBAaGsq2bdu46aab2LlzJ3V1dRw6dIiRI0fy3HPP8dxzzzF79mxWrFhx0e9TCCFckRWVxZp9a6itr8XH6PqyNaILcRoyXeu0JvEZPwMYjLZdzGZKQyPpXXHcthzS6QMundpSb+HdPFsvydHKo8xInoGyXy8pMoj3tx+hrNpCsJ8Zrn+eklkT6aG+pyRyIAZTD3xLTlP72hKYeCtsWGBLxHvENXtNqwYUhASYKSkooPvgwa2sENFa0gaUNqC40D2p97B632qW5ixlwRULPB2O20kPsReJibGN0w8PD2fKlCls2dJyz8ihQ4eYMmUKr7/+Oon2YWEtWb58OevWrWPlypWOm7uvry+hoaEAZGRkkJiYyL59+wgNDSUgIICbb74ZgFtuuYVvv/32Yt6eEEK0SlZkFtX11eQcz/F0KMJLOQ+ZrrH3CNcb6yn3adi8OR4SRWxpMfSIh5KCZtcitlgtrNm3hsnvTuZ/N/0vUYFR/HXsX3l0+KOOfZKjbMOz9xbbeolrKgI4sTuIKHIoqwvhUM1lRIdbKVv7f1RYLrMdlNvy8FSr1viZjajyMqzl5TLDdBcibUDhTcL8w7j1sltZt38dB0pd+xKxI5OE2EtUVFRQXl7ueL1+/XoGDWr+ubmSkhImTZrE73//e0aNGtVg2x133NHoh+mHH37IokWLeO+99wgICHCUHz9+nPp626yc+/fv5/vvvychIQGlFNdffz3//ve/Afj0009JSUm5lLcqhBAuyYjMwKAMbCmWYdOiCU49xBUG2z1Nm+upMBob7HakeyRhZcfR3aKh9gxUneZ8ddY61uat5YZ3b2De1/MI9Q9l6XVLeX3C64yIGuFIHgCSIm0TeO0pLkdbrRQ9+SSGwCCSBubbzqV96HfTCMwxMRQ/9zI6OhO2v91sIg5gtWoCzEYsB20zYZtlyHSXIG1A4Y3uGnQXvkZflm5f6ulQ3E4SYi9x9OhRrrjiCgYPHkxmZiaTJk1i/PjxALz77rvExsby9ddfM2nSJMaNGwfAkiVLyMvLY/78+Y6p8o8dOwZATk4O0Y3MTDlr1izKy8sZM2ZMg6n1v/jiC9LS0khPT2fatGksXbqUkJAQAJ5++mnmzZtHWloaK1as4Nlnn22PKhFCdHHBPsGkhKSwuUjWIxaNc+4hPmNPiDHXUeGUvAIUdAvHoK3UVvjZCpxmmq631rNu/zpuWnsTT3z5BEE+Qbxw7QusnLiSUTGjGiTCZ0V19yPYz8SeojJK1qyhaus2Ih6dQ/jtcwgwnAKsxKdFEvmbudTu38/Jgt62NYuP5jb7fqwa/H2MTmsQ976IWhEdjbQBhTcK9Q/lx0k/5l8H/sUPJT94Ohy3kmeIvURCQgLbt29vdNuUKVOYMmXKBeVPPPEETzzxxAXlZWVl9O/fn9jY2Au25eXlNXqNqVOnMnXq1Ea39e7dmy+++KK58IUQwi0yozJ5fdfrVFoqCTAHtHyA6FqcktXyeiORyorVVMeZ83LYH/x7AVBzWuMLUFKAFc36+hL+8t4UDpQeYEDPATx/9fNcHXd1o0lww8sqkqKCKfzhEMfe+QMBw4bRfepUlFIM6L2UkvIz+HUzw1VXETR2LCfe/zfB43zwyXkbIlMbPWe1pR6r1gT4mGxrECuFuZH7uOh8pA0ovNWdA+/kzT1v8uL2F/nDVX/wdDhuIz3EHmI0GiktLW2wKHtbCQ4OZtWqVW1+3vMtXryYhQsXEhwc3PLOQghxEbIis6iz1vHdse88HYrwRk55a3ltPd18ajH51VChNNppePJe3yC0Umzf+x1/COnBvbkvco1/OXPqCzEqI8/913Osun4V18Rf02IyfFZSZBBXrn8DXVVF5PynHMeNevReJv32Tsd+Eb/6JcpoonhnX3TOGrDWN3q+97cfwao18SEB1BYUYI6KwuAjk8l1RtIGFB1FD78ezEiZwUf5H7H31F5Ph+M20kPsIXFxcRQWduypzB9++GEefvhhT4chhOjEhkQMwWQwsbl4M6NiRrV8gOhSnFPXM9UWrok/yIae1VjpwZLsJRwoPcC+U/uoH1DAse6avO1f8NaNQSTUljKq3sSVvpGMvWENBtX6/oHhx/bSr+BbTHfPxDchwSmohgm1OTKSXg/9P44u/D3lkacIzt8ICVc12EdrzbIv87nOYCAs2I/8goOYZUKtTkvagKIjuSPlDt7cbesl/uPVf/R0OG4hPcRCCCG8lr/Jn8G9Bst6xKJx2up4eaamDh+jlUhrHQB/zfkre0/tJT4ogdoT11AX3Y8r6vqyuSKQd4x9WFAbwHhjj4tKhq1VVSSs/AuF3Xqxf2zjQ02d9Zw+Hd+kyzj6bQ/qv/n7Bds37T/F7qIy/M0GlALLwQJ84uX5YSGE53X37c5/D/xvPi34lF0nd3k6HLeQhFgIIYRXy4rMYvep3ZTWlHo6FOFtnBLi8mpbIjy+opK1wcPZ9ONN/PPmf/KL9N9Re2IM/n2HwMFDGILjbEsvXYITL7yAofgIf06fxp6TNS3ur0wmop56irpqAyfe+gQsVQ22L/vyACGBPviajdRXWqgvKcFHZpgWQniJGckzCPYJ5i/Zf/F0KG4hCbEQQgivlhmViVVb2Xp0q6dDEd7GuYfYnhAbgQT/CMckbKVVFlt5377o2los1rBLSoir9+zh5GvL6T71ZsouS2VPcZlLx/kPHkyPiaM5tcdM9b9edZQfPFnBJ7uPMj0rHgXUnqwEkDWIhRBeI8gniDsH3snnhz5nx/Edng6nzUlCLIQQwqulhaXhb/KXYdPiQs4JcW0dCvtEWk4zkpfZE2LfxEQAas74Q3UJ1LiWyDa4XH09Rb95EmP37kTMmUNSZBB7ispdPj587u8x+imK/rgMbV/3dflX+ZgMihkjbEOka0/YEmJZg1gI4U1+nPxjevr25IXtL3g6lDYnCbGH5Ofn4+/v75hhsLCwkKuvvpqUlBQGDhzI888/79h33rx5xMTEONaZ++CDDxzbcnJyuPzyyxk4cCCpqalUV1c3e905c+aQlJREWloaU6ZMoaSk5IJ4nNemA6itrWXmzJkMGDCApKQk1qxZA9hmGIyPj2fWrFltVi9CCHE+s9HM0PChbCmWhFicxykh1hp8sCW/+JxLiM/2EHcb0A+A2hL7hrLDrb7c6Tffojonh4hfPo6xRw+SIoM5cLKCqtrGZ44+n7FHTyJuGU71kSpKVrxGebWFVVsPMSk1iohg2xrJlrM9xHFxrY5PdAzSBhQdUaA5kLsG3cWXh78k+1i2p8NpUzLLtAclJiaSnW37D2UymXj22WcZOnQo5eXlZGRkMGbMGFJSUgDbbH6zZ89ucHxdXR0zZsxgxYoVDB48mJMnT2I2m5u95pgxY1i4cCEmk4nHHnuMhQsX8vTTT18Qj7MFCxYQHh7Ovn37sFqtnDp1yhFTz5492bpVhjEKIdwrMyqTxdsWc6LqBGH+YZ4OR3gLp4QYIBD787w+3RxlZ3uIu4eHUNIrjJpjFdATsE++5SrL0aMcX7yYwFGjCJ48GYDkqCC0hu+PlZMW28Ol8wTfNZuSj27i2J+WsKXXEM7U1HH3FX0d22tPVGCKiMDg79+q+ETHIm1A0RHdetmtLN+5nCXZS3hl7CueDqfNuDUhVkqNB57H9kjPK1rr35+3fTFwtf3HACBca93Dvu1pYJJ92/9qrd+2ly8HrgLOzq5yp9b60r6m+NfjUNzG4+EjU2HC71vezy4qKoqoqCgAgoKCSE5O5vDhw44Pw8asX7+etLQ0Bg8eDEBoaGiL1xk7dqzj9YgRI1i9enWLxyxbtow9e/YAYDAYCAuTxqgQon1lRWYB8E3xN0zoO8HD0Qiv4bTWMECAsifEzkOm7c8WB/uZqUrsR83hE7aEuJWO/va36Lo6Ip/8jWPN4aRI2xqse4pcT4hVZCqRY8M48Pdy6l94nmHX39fg2NoTlfjE9299gOLiSBuwWdIGFM4CzAHcM+gentn6DN8Uf8PwyOGeDqlNuG3ItFLKCLwATABSgNuVUg1+s7XWD2ut07XW6cCfgX/Yj50EDAXSgSxgtlLKeeXvOWePu+Rk2Avl5+fz3XffkZWV5ShbsmQJaWlp3H333Zw+fRqAffv2oZRi3LhxDB06lEWLFrXqOsuWLWPChHMNywMHDjBkyBCuuuoq/vOf/wA4htPMnTuXoUOHcsstt3D06NFLfYtCCNEqSSFJBJmD2Fy02dOhCG9yXg+xo9gn0PG6tMqCr8mAn9mIb0ICtfmFaKNfqy5T/umnlH/8CWEPPthg9uf4kAD8zUZ2uzixFgBK4XvV7YQmlZO1/xseCD7VYHPtyUpZg7gLkzag8HY/uuxH9PLvxQvZL6DP+1Kyo3JnD3EmkKe13g+glHoLuBFoagGr24En7a9TgC+01nVAnVIqBxgPvOOWSFvxLZ67nTlzhqlTp/LHP/6R4GDbdwD3338/c+fORSnF3Llz+cUvfsGyZcuoq6tj48aNfPPNNwQEBHDttdeSkZHBtdde2+J1FixYgMlkYvr06YDt28mCggJCQ0PZtm0bN910Ezt37qSuro5Dhw4xcuRInnvuOZ577jlmz57NihUr3FoPQgjhzGgwMixymCTEoqEmEmKryR+j/XVppYXu/rahpD6JCVjPnKHOFI25fr9Ll6gvKaH4qfn4DhhA6F13NthmMCgGtHJiLQBSbyE0ZR6FB3sR//oSrNPGYPD1xWqB+vJaWYO4PUkbUNqAolX8TH78NPWnLNyykC3FW8iKymr5IC/nzkm1YoBCp58P2csuoJTqDfQFPrMXbQfGK6UClFJh2IZVO88usUAplaOUWqyU8m370D3DYrEwdepUpk+fzs033+woj4iIwGg0YjAY+NnPfsaWLbaJZWJjYxk9ejRhYWEEBAQwceJEvv322xavs3z5ctatW8fKlSsdw758fX0dw20yMjJITExk3759hIaGEhAQ4IjnlltucekaQgjR1rKisjh05hCHz7R+MiTRSTWRENebnIdMn0uIfRPtE2vVuj7ss/h/f0vdqVNELfwdqpFnNJMjg9hTXNaqnpKdFUFsJoWYURYsBw9y8q+2Z/Fq7R3NsgZx1yNtQNGRTB0wlYiACJZ8t6RT9BJ7yyzTtwGrtdb1AFrr9cAHwFfAm8DXwNkpHH8JJAHDgRDgscZOqJSaqZTaqpTaevz4cTeHf+m01txzzz0kJyfzyCOPNNhWVFTkeP3uu+8yaNAgAMaNG8eOHTuorKykrq6Ozz//3PG8yR133OH40HT24YcfsmjRIt577z0CAs41GI4fP069fQmI/fv38/3335OQkIBSiuuvv55///vfAHz66afNPtMihBDucvY5Yll+STg0kRBbjOcmpCqtshDsSIgTAKipDGz0uPOV/etflP3zn4Q9cD/+Awc2uk9SZBCnKy0cK69xOezXvsznn+pKInoWEnz1CE6+9BK1+fnUltkSFFmDuGuRNqDoaHyNvsxMm0n28Wy+OvKVp8O5ZO5MiA/TsFc31l7WmNuwJb4OWusF9meExwAK2GcvL9I2NcBr2IZmX0Br/bLWepjWelivXr0u8a2435dffsmKFSv47LPPLpha/9FHHyU1NZW0tDQ2bNjA4sWLAejZsyePPPIIw4cPJz09naFDhzJpkm0espycHKKjoy+4zqxZsygvL2fMmDENptb/4osvSEtLIz09nWnTprF06VJCQkIAePrpp5k3bx5paWmsWLGCZ599tj2qRAghGkjskUiIX4gsvyTOcaGHuLTqXA+xMSwMQ3AwNSWqxVNbjh2jeN5T+KWmEjZzZpP7JUXZhrbuLnLtOeLj5TW8l32EgMFTwOhL+FXBKF9fiufPdyTE5jhJiLsSaQOKjmhKvylEB0Z3imeJ3fkM8TdAf6VUX2yJ8G3Aj8/fSSmVhG2+x6+dyoxAD631SaVUGpAGrLdvi9JaFynbOI+bgFw3vod2c8UVVzT5n6m5ZzVmzJjBjBkzGpSVlZXRv39/YmNjL9g/Ly+v0fNMnTqVqVOnNrqtd+/efPHFF03GIIQQ7UEpRVZkFpuLNqO1dgz3E11YUz3EhoZDpgdEBAG2/0O+CQnUHi+BqGZOqzXFc3+Dtbqa6Kd/jzI13VxKirSde09xOf91WXiLIa/cfJDaeiu3jU6FunGYD66j10PzOLrgd9QEGjAG+WDs5loPtugcpA0oOiKz0cy9g+/lya+e5ItDX3BV3FWeDumiua2H2D4h1izgI2A38I7WeqdSar5S6ganXW8D3tINPwnMwH+UUruAl4EZ9vMBrFRK7QB2AGHAb931HtzJaDRSWlrqWJS9LQUHB7Nq1ao2P+/5Fi9ezMKFCx0TPwghhLtlRmVyvOo4B8oOeDoU4Q2aSohN52aRLq20EOx3LqH16ZdIzZFTjR127pg1azjz+eeE/+IRfBMSmt23R4APUd392ONCD3FNXT1vbDrINUnhJPTqBmm3QuUJeg4Px2/QIOoqFD6hAS2eR3Rs0gYUncX1idcT2y22w/cSu3UdYq31B9ieBXYu+815P89r5LhqbDNNN3bOa9owRI+Ji4ujsLCw5R292MMPP8zDDz/s6TCEEF3I2dkstxRtIaF784mK6AKaaICd7SG2WjXlNXWOIdMAvgmJlJasoa5GNdoIqj10iKO/W0hAVhY9z+t9a0pSZBB7ilueafr97UWcOFPL3aP62gr6jwG/Hqidq4icN4/8W6biEya9w52dtAFFZ2E2mLlv8H088eUTfFb4KXPufAAAIABJREFUGdfGtzzLuTfylkm1hBBCiBbFdoslOjBaniMWNk30ENcpWwJcXlOH1jgm1YJzE2vVll+4FrG2Win65a9AKaJ/twBlcK2ZlBQVzA/Hz1Bb13g8YBuGvWzjAQZEdGNUP9uMvph8YeAU2PNP/Pv3Jm5sPWHXJbp0TSGE8AaTEibRJ7gPf8n+C9YmPpO9nSTEQgghOgylFJlRmWwp3tJhb7yiDTWVENfbysuqLEDDhNjHvvRSTSNLL516/XUqv/mGiF/9CnNMoytFNiopMghLvWb/iTNN7rP5wCl2FZVx96i+DZ9/T7sVLJWw5590i9X4hMmQaSFEx2EymLhv8H3sO72Pjw9+7OlwLookxEIIITqUzMhMSmtK2Xtqr6dDEZ7W1DPE9bah1KX2hNh5yLQ5Ogrl709t8AjIutdRXpOXx/HnFtPt6qvpfvOUVoWRFGl7jnJPUdPDppdtPEDPADM3DTkv0Y7Lgh7xkPN2q64phBDeYnyf8SR2T+TF7Bept9a3fICXkYRYCCFEh+J4jliGTYumll2y2hLiskYSYmUw4NO3DzWlBkiwzYqqLRaOPPY4hoAAouY/1eoZzBN6BWI2KnYXNz6xVsHJSj7efZTpWb3xMxsbbjQYIPVHsH8D1LT8HLIQQngbo8HI/en380PpD3yU/5Gnw2k1SYg9KD8/H39/f8csg3fffTfh4eGORdfPmjNnDklJSaSlpTFlyhRKSkoAsFgs/OQnPyE1NZXk5GQWLlwIQFVVFenp6fj4+HDixIn2fVNCCOFm4QHh9O3el81Fmz0divA0p4TYbDyXxFqstvKzPcTBfuYGh/kmJFLzw7klaE689DLVO3cSOW8epl69Wh2G2WigX3hQkz3Ef/s6H6NS/PflvRs/QdqPbO+lrrrV1xYdj7T/RGc0pvcY+vfsz4vbX6TOWtfyAV5EEmIPS0xMJDs7G4A777yTDz/88IJ9xowZQ25uLjk5OQwYMMDxwbdq1SpqamrYsWMH27Zt46WXXnJ8yGZnZze6KLsQQnQGmZGZbDu6DYvV4ulQhCdpK7f23k5hXBKBvufmjK47f8h0wHkJcb9E6o4UYa2ooGpHLidefJHg668nePy4iw4lOTKIPY30EJdXW3j7m0Imp0UREXzhRF4A9LoMogZf9LVFxyPtP9HZGJSBBwc/SH5ZPh8c+KDlA7yIW5dd6iie3vI0e07tadNzJoUk8VjmY606ZvTo0eTn519QPnbsWMfrESNGsHr1asA2uUxFRQV1dXVUVVXh4+Mj68EJIbqErKgs3t77NjtP7CQ9vO3X8hQdhLYSG1BGTW0AgT4msHew1tl7iMuqLxwyDeBjX1u4es8ein7zJKawMCKf+PUlhZIUFcQ/vjvMqYpaQgJ9HOWrtx3iTE0dd1/Rt/kTpN0KRdsvKQbRet7QBpT2n+gsrom/huSQZJZuX8qEvhMwG8wtH+QFpIe4g1m2bBkTJkwAYNq0aQQGBhIVFUV8fDyzZ88mJCTEwxEKIYT7DY8YDiDDptuQUmq8UmqvUipPKfV4I9sXK6Wy7X/2KaVKnLYtUkrtVErtVkr9SbX2IdyLZR8ybUXRrYkeYqNBEejT8Lld30Tb0kZFv36C2h9+IOq3v8XYvfslheKYWMupl7jeqln+VT4ZvXuSFtuj+RMMmgrKALRP1YmORdp/oiNQSvFg+oMUlhey7od1ng7HZdJDDK3uyfWUBQsWYDKZmD59OgBbtmzBaDRy5MgRTp8+zZVXXsl1111Hgv2bbyGE6Kx6+PUgKSSJLcVbuHfwvS0fIJqllDICLwBjgEPAN0qp97TWu87uo7V+2Gn/nwND7K9HAqOANPvmjcBVwL/dHri2Jb4aRaCvLem1auXoIS6tshDsZ7pgkiyf+HgwmajNz6fH7bfR7corLjmUpKggwDbT9MhE25JOn+05xsGTlTw6LqnlEwRFwk1LIWLgJcciXNcR2oDS/hMdyejY0QwKHcTS7UuZnDAZs9H7e4mlh7iDWL58OevWrWPlypWOG/vf//53xo8fj9lsJjw8nFGjRrF161YPRyqEEO0jKzKL7GPZVMtERG0hE8jTWu/XWtcCbwE3NrP/7cCb9tca8AN8AF/ADBx1Y6znaNvyHlYMBPqa+H7qerJqljiWXSqrqrtguDSAMpvxTUzEHB9PxJw5bRJKr26+hAb6NOghXrbxANHd/Rg3MMK1kwy+FSIHtbyf6DKk/Sc6GqUUDw55kCMVR3g3711Ph+MSSYg7gA8//JBFixbx3nvvERAQ4CiPj4/ns88+A6CiooJNmzaRlOTCt9BCCNEJZEZlUmutZftxee6yDcQAhU4/H7KXXUAp1RvoC3wGoLX+GtgAFNn/fKS13u3WaM86b8h0fa9kjtPTsexSaZWF4EYSYoDYP/+J3itWYHC6r14KpRRJUUHsLbbNNL3rSBlf7z/JT0b2wWSU5pZoPWn/iY5qVPQoBvcazMs5L1NbX+vpcFokn9Be5Pbbb+fyyy9n7969xMbG8uqrrwIwa9YsysvLGTNmDOnp6dx3330APPjgg5w5c4aBAwcyfPhw7rrrLtLS0pq7hBBCdBoZERkYlVGeI25/twGrtbZ1zyql+gHJQCy2JPoapdSVjR2olJqplNqqlNp6/PjxS4/EKSEO9DVhMtiaNZb6c0OmG+shBtuwaXNE+KXH4CQpMpi9R8upt2pe+/IA/mYjtw2Pb9NriM5H2n+is1FKMWvILI5WHmXN92s8HU6L5BliL/Lmm282Wp6Xl9doebdu3Vi1apU7QxJCCK8VaA5kUNggthRv8XQoncFhIM7p51h7WWNuAx50+nkKsElrfQZAKfUv4HLgP+cfqLV+GXgZYNiwYfqSo7YnxBoD3XxNjrWIz06qVVZtIaan/yVfxlWXRQZRbbHybcFp1mYf4dbhcRcs+STE+aT9JzqjrMgsMiIy+GvOX5nSbwp+piaWnfMC0kPsQUajkdLSUsfC7G3l7MLsFosFg0H+iYUQnVdmZCa5J3I5U3vG06F0dN8A/ZVSfZVSPtiS3vfO30kplQT0BL52Ki4ArlJKmZRSZmwTarXrkOl6DAT6GjEabAnx2SHTZVUWgv3aLyFNts80/eTandTWW7lzVJ92u7boOKT9J7qCszNOH686zqp93v0Fjvy2eFBcXByFhYWOhdnbytmF2Q8fPizT8AshOrURUSOo1/V8e+xbT4fSoWmt64BZwEfYktl3tNY7lVLzlVI3OO16G/CW1tq5d3c18AOwA9gObNdav98+gTccMm22P6trsVrRWjc7ZNod+kd0w6BgV1EZV1/Wi8Re3drt2qLjkPaf6CqGRw4nKyqLV3a8QqWl0tPhNEkSYiGEEB3W4PDB+Bh85DniNqC1/kBrPUBrnai1XmAv+43W+j2nfeZprR8/77h6rfW9WutkrXWK1vqRdgza9heKYD8zJsO5IdPVFiuWet2uCbGf2UjfsEAA7r6ib7tdVwghvNWD6Q9yqvoUb+9929OhNEkSYiGEEB2Wr9GXIeFD5DnirsreQ/w/113GjenRjtmcLfVWSqssAAT7t+90KVkJoaTFdueKfmHtel0hhPBGQ8KHMCp6FMtyl1FhqfB0OI2ShFgIIUSHlhmVyZ5TeyipLvF0KKK92RPirMReBDn1ENdbtSMhbs8eYoDf3jiINfePdKwZK4QQXd2D6Q9SUlPCm3san0DO0yQhFkII0aFlRmYCSC9xV2RPiFG25ozp7CzTVk1Ztb2HuB0n1QIwGJTjWWYhhBCQ2iuVq2Kv4rXc17xyEkz5xPaQ/Px8/P39G8ww2KdPH1JTU0lPT2fYsGGO8lWrVjFw4EAMBgNbt251lH/88cdkZGSQmppKRkaGY5H25lzMud58801SU1NJS0tj/PjxnDhxAoA5c+YQGRnJH/7wh0uqCyGEuBSDwgYRaA6UhLgrOi8hNjutQ1xa6ZkeYiFaIm1A0RU9kP4AZbVlrNi9wtOhXEDWIfagxMTEC2YY3LBhA2FhDZ87GjRoEP/4xz+49957G5SHhYXx/vvvEx0dTW5uLuPGjePw4aaWjby4c9XV1fHQQw+xa9cuwsLCePTRR1myZAnz5s3jmWeeITAw8BJqQAghLp3JYCIjIkMm1uqKzkuIDQaFQXl2yLQQrpA2oOhqUkJTuCbuGlbsXMGPk35Md9/ung7JQRJioPh3v6Nm9542PadvchKRv/pVm5wrOTm50fIhQ4Y4Xg8cOJCqqipqamrw9fVts3MZDAa01lRUVBAaGkpZWRn9+vW7yHcihBDukRmZyReHvuBoxVEiAiM8HY5oL46E+NzzuiaDAUu905BpSYhFM6QNKG1A0X4eSH+Aae9PY8WuFcwaMsvT4TjIkGkvopRi7NixZGRk8PLLL7fq2DVr1jB06NBmPwgv5lxms5kXX3yR1NRUoqOj2bVrF/fcc88lX0MIIdpSVlQWIM8Rdznn9RCD7TniOudZpv3ku3/h/aQNKLqCy0IuY2zvsbyx+w2vmghT7hLQZt/iXaqNGzcSExPDsWPHGDNmDElJSYwePbrF43bu3Mljjz3G+vXrLzmG889lsVh48cUX+e6770hISODnP/85Cxcu5IknnrjkawkhRFsZ0HMAPXx7sLloM9cnXu/pcER7aSwhNijq7EOmu/maHEsxCdEYaQM2fS5pAwp3uH/w/Xx88GOW71zO/2T8j6fDAaSH2KvExMQAEB4ezpQpU9iypeWejkOHDjFlyhRef/11EhMTL+n6jZ3r7PMtiYmJKKX40Y9+xFdffXVJ1xFCiLZmUAaGRw5nS/EWtNaeDke0l7P/1g16iA3UWa2UVdVJ77DoMKQNKLqKfj37Mb7veP6+5++crDrp6XAASYi9RkVFBeXl5Y7X69evZ9CgQc0eU1JSwqRJk/j973/PqFGjGmy74447XPowbelcMTEx7Nq1i+PHjwO2mQibegZFCCE8KSsyi6KKIg6VH/J0KKK9NNVDXG/rIZbnh0VHIG1A0dXcP/h+auprWL5zuadDASQh9hpHjx7liiuuYPDgwWRmZjJp0iTGjx8PwLvvvktsbCxff/01kyZNYty4cQAsWbKEvLw85s+fT3p6Ounp6Rw7dgyAnJwcoqOjL7hOa88VHR3Nk08+yejRo0lLSyM7O5tfecnwIiGEcJYZZVuPeHOxzDbdZTSSEJuN9km1qiwyw7ToEKQNKLqavt37MjlhMm/teYsTVSc8HQ6qKwwtGzZsmHZebw1g9+7dHv2WKz8/n8mTJ5Obm9vm5y4rK+Oee+5h1apVbX7u882bN49u3boxe/bsBuWerl8hRNejtea6VdeREZHBoqsWufVaSqltWuthLe8pmtLYvbnVlgyHE/vgwS3Q6zIArnpmA0PierCnuJy4kAD+eof8M4mGPN1GkTagEFBQVsAN/3cDtyfdzmOZj7XJOS/23iw9xB5iNBopLS1tsCh7WwkODm6XD8I5c+bwxhtvyDp0QgivoJQiMyqTzcWb5TnirqKRHmKjQWGxT6olPcTCG0kbUAiID47nhsQbWJu3lkpLpUdjkdkmPCQuLo7CwkJPh3FJnnnmGZ555hlPhyGEEA5ZUVms27+OvJI8+vfs7+lwhLs1NmTaYHAsuyQJsfBG0gYUwubnQ37OA+kPEGAO8Ggc0kMshBCi08iMtD1HLOsRdxGOhFg5ikxGRbXFSmVtPcF+khALIYS36hXQi8jASE+HIQmxEEKIziO6WzRxQXFsLpKJtbqExmaZNho4VVELQHd/GQgnhBCieW5NiJVS45VSe5VSeUqpxxvZvlgplW3/s08pVeK07WmlVK79z61O5X2VUpvt53xbKeXjzvcghBCiY8mMzGRr8VbqrfWeDkW4W2PrEBvUuYQ4QHqIhRBCNM9tCbFSygi8AEwAUoDblVIpzvtorR/WWqdrrdOBPwP/sB87CRgKpANZwGylVLD9sKeBxVrrfsBp4B53vQchhBAdT1ZUFuWWcnaf2u3pUIS7NbEO8YkzNQAyZFoIIUSL3NlDnAnkaa33a61rgbeAG5vZ/3bgTfvrFOALrXWd1roCyAHGK6UUcA2w2r7f34Cb3BK9m+Xn5+Pv7++YYbCwsJCrr76alJQUBg4cyPPPP+/Yd968ecTExDjWhvvggw8c23Jycrj88ssZOHAgqampVFdXN3vduXPnkpaWRnp6OmPHjuXIkSMArFy5krS0NFJTUxk5ciTbt28HoKqqivT0dHx8fDhxwvPrhAkhREuGRw4HkGHTXUET6xDX1NnKZVIt4Y2kDSiEd3FnQhwDOE+hd8hedgGlVG+gL/CZvWg7tgQ4QCkVBlwNxAGhQInWus6Fc85USm1VSm09fvz4Jb8Zd0hMTCQ7OxsAk8nEs88+y65du9i0aRMvvPACu3btcuz78MMPk52dTXZ2NhMnTgSgrq6OGTNmsHTpUnbu3Mm///1vzObmb/5z5swhJyeH7OxsJk+ezPz58wHo27cvn3/+OTt27GDu3LnMnDkTAH9/f7Kzsxtd4F0IIbxRmH8Y/Xr0k4m1uoImll06SxJi4a2kDSiE9/CW2SZuA1ZrresBtNbrlVLDga+A48DXQKseBtNavwy8DDBs2LBmF6T8zzv7OFF45mLiblJYXDeu/NEAl/ePiooiKioKgKCgIJKTkzl8+DApKSlNHrN+/XrS0tIYPHgwAKGhoS1eJzg42PG6oqICZZ+Zc+TIkY7yESNGcOjQIZdjF0IIb5MVlcWafWuw1FswGyUp6rQa7SE+lxAHS0IsWiBtQGkDCuHOHuLD2Hp1z4q1lzXmNs4NlwZAa73A/nzxGEAB+4CTQA+l1NlEvrlzdlj5+fl89913ZGVlOcqWLFlCWload999N6dPnwZg3759KKUYN24cQ4cOZdGiRS6d/9e//jVxcXGsXLnS8e2gs1dffZUJEya0zZsRQggPyIzMpLq+mpwTOZ4ORbhTo88Qn3stPcSio5E2oBDtz509xN8A/ZVSfbElrbcBPz5/J6VUEtATWy/w2TIj0ENrfVIplQakAeu11loptQGYhu2Z5J8Aay810NZ8i+duZ86cYerUqfzxj390fJN3//33M3fuXJRSzJ07l1/84hcsW7aMuro6Nm7cyDfffENAQADXXnstGRkZXHvttc1eY8GCBSxYsICFCxeyZMkSnnrqKce2DRs28Oqrr7Jx40a3vk8hhHCnYZHDMCgDW4q2kBGR4elwhLs0uuySrdfLx2TAz2z0RFSiA5E2oLQBhXBbD7H9Od9ZwEfAbuAdrfVOpdR8pdQNTrveBryltXYe1mwG/qOU2oVt2PMMp+eGHwMeUUrlYXum+FV3vYf2ZrFYmDp1KtOnT+fmm292lEdERGA0GjEYDPzsZz9jyxbbc3GxsbGMHj2asLAwAgICmDhxIt9++63L15s+fTpr1qxx/JyTk8NPf/pT1q5d69LQGyGE8FbBPsGkhKSwqWiTp0MR7uRIiM8NkzbZnyGWGaZFRyJtQCE8x63rEGutP9BaD9BaJ2qtF9jLfqO1fs9pn3la68fPO65aa51i/zNCa53ttG2/1jpTa91Pa32L1rrGne+hvWitueeee0hOTuaRRx5psK2oqMjx+t1332XQoEEAjBs3jh07dlBZWUldXR2ff/6543mTO+64w/Gh6ez77793vF67di1JSUkAFBQUcPPNN7NixQoGDPCeb0uFEOJiZUZlknMih0pLpadDEe7S2DrERtvr7v7eMk2KEM2TNqAQniV3Cy/x5ZdfsmLFClJTUx3T8P/ud79j4sSJPProo2RnZ6OUok+fPrz00ksA9OzZk0ceeYThw4ejlGLixIlMmjQJsH3T19isgI8//jh79+7FYDDQu3dvli5dCsD8+fM5efIkDzzwAGCb8XDr1q3t8daFEMItsiKzWJa7jOxj2YyMGdnyAaLjaWZSLXl+WHQU0gYUwrMkIfYSV1xxBQ1HjZ+zYsWKJo+bMWMGM2bMaFBWVlZG//79iY2NvWB/5+Exzl555RVeeeWVVkQshBDeLT08HZPBxObizZIQd1bNTKolM0yLjkLagEJ4lluHTIumGY1GSktLHd8EtqXg4GBWrVrVJuc6uyi7xWLBYJD/LkKIjiPAHEBaWBpbimQ94k6rmXWIpYdYeCtpAwrhXaSH2EPi4uIoLCz0dBgtOrsouxBCdEQjokawNGcpZbVlBPsEt3yA6FhkyLTogKQNKIR3ka97hBBCdFqZUZlYtZWtxfI8XKfU6LJL9iHTMsu0EEIIF0hCLIQQotNKC0vDz+jHlmIZNt0pNfoMsfQQCyGEcJ0kxEIIITots9HM0IihbC7a7OlQhDsMsU8o1MikWpIQCyGEcIUkxEIIITq1zMhM8kryOFF1wtOhiLY2+Y/wqyOglKPIZH+GOFjWIRZCCOECSYg9JD8/H39//wYzDN59992Eh4c7Fl0/a86cOSQlJZGWlsaUKVMoKSkBwGKx8JOf/ITU1FSSk5NZuHBhi9ddsmQJ/fr1QynFiRPnGocrV64kLS2N1NRURo4cyfbt2x3bFi9ezMCBAxk0aBC333471dXVAEyfPp2QkBBWr159SXUhhBDuNCJqBADfFH/j4UhEmzMYwSewQZHZkRBLD7HwTtIGFMK7SELsQYmJiQ1m77vzzjv58MMPL9hvzJgx5ObmkpOTw4ABAxwfeqtWraKmpoYdO3awbds2XnrpJfLz85u95qhRo/jkk0/o3bt3g/K+ffvy+eefs2PHDubOncvMmTMBOHz4MH/605/YunUrubm51NfX89ZbbwG2D9AbbrjhUqpACCHcLikkiSBzkAyb7iKMMmRadADSBhTCe8h4ImDD8pc5dnB/m54zvHcCV985s1XHjB49utEPs7FjxzpejxgxwvFtnFKKiooK6urqqKqqwsfHh+Dg5pcVGTJkSKPlI0eObHCNQ4cOOX4+e36z2UxlZSXR0dGteVtCCOFRRoORYZHDZGKtLsLRQyyzTAsXSBtQ2oBCSA9xB7Ns2TImTJgAwLRp0wgMDCQqKor4+Hhmz55NSEjIJV/j1VdfdVwjJiaG2bNnEx8fT1RUFN27d2/w4SyEEB3BmN5jyIjIoLa+1tOhCDe7NjmCn1/Tj5ge/p4ORYg2JW1AIdxDeoih1d/iecqCBQswmUxMnz4dgC1btmA0Gjly5AinT5/myiuv5LrrriMhIeGir7FhwwZeffVVNm7cCMDp06dZu3YtBw4coEePHtxyyy288cYbzJgxo03ekxBCtIfrE6/n+sTrPR2GaAcxPfz5xdjLPB2G6CCkDXiOtAFFVyU9xB3E8uXLWbduHStXrkTZZ9P8+9//zvjx4zGbzYSHhzNq1Ci2bt160dfIycnhpz/9KWvXriU0NBSATz75hL59+9KrVy/MZjM333wzX331VZu8JyGEEEII0TxpAwrhXpIQdwAffvghixYt4r333iMgIMBRHh8fz2effQZARUUFmzZtIikpCYBrr72Ww4cPu3yNgoICbr75ZlasWMGAAQMaXGPTpk1UVlaitebTTz8lOTm5jd6ZEEIIIYRoirQBhXA/SYi9yO23387ll1/O3r17iY2N5dVXXwVg1qxZlJeXM2bMGNLT07nvvvsAePDBBzlz5gwDBw5k+PDh3HXXXaSlpWG1WsnLy2v0WZI//elPxMbGcujQIdLS0vjpT38KwPz58zl58iQPPPAA6enpDBs2DICsrCymTZvG0KFDSU1NxWq1OmYfFEIIIYQQl07agEJ4jtJaezoGtxs2bJg+fxjJ7t27PfotV35+PpMnTyY3N7fNz52bm8uyZct47rnn2vzc57vzzjuZPHky06ZNa1Du6foVQgh3Ukpt01oP83QcHVlj92Yh2oOn2yjSBhTCPS723iw9xB5iNBopLS1tsCh7Wxk0aFC7fBBOnz6dzz//HD8/P7dfSwghhBCiM5A2oBDepUvPMq21dkxO0N7i4uIoLCz0yLXbysqVKxst7wqjDoQQQgjRcUkb8NJIG1B0Jl22h9jPz4+TJ0/KL24b01pz8uRJ+cZQCCGEEF5J2oDuIW1A0VF12R7is5MKHD9+3NOhdDp+fn7ExsZ6OgwhhBBCiAtIG9B9pA0oOqIumxCbzWb69u3r6TCEEEIIIUQ7kjagEMJZlx0yLYQQQgghhBCia5OEWAghhBBCCCFElyQJsRBCCCGEEEKILkl1hRn2lFLHgYNtcKow4EQbnKezk3pyjdSTa6SeWiZ15Jq2rKfeWutebXSuLknuze1O6sk1Uk+ukXpqmdSRazx+b+4SCXFbUUpt1VoP83Qc3k7qyTVST66RemqZ1JFrpJ46J/l3dY3Uk2uknlwj9dQyqSPXeEM9yZBpIYQQQgghhBBdkiTEQgghhBBCCCG6JEmIW+dlTwfQQUg9uUbqyTVSTy2TOnKN1FPnJP+urpF6co3Uk2uknlomdeQaj9eTPEMshBBCCCGEEKJLkh5iIYQQQgghhBBdkiTEQgghhBBCCCG6JEmIXaSUGq+U2quUylNKPe7peNqbUmqZUuqYUirXqSxEKfWxUup7+9897eVKKfUne13lKKWGOh3zE/v+3yulfuKJ9+IuSqk4pdQGpdQupdROpdRD9nKpJydKKT+l1Bal1HZ7PT1lL++rlNpsr4+3lVI+9nJf+8959u19nM71S3v5XqXUOM+8I/dRShmVUt8ppdbZf5Y6Oo9SKl8ptUMpla2U2movk9+5LkLuzXJvboncm10j92bXyb25ZR3u3qy1lj8t/AGMwA9AAuADbAdSPB1XO9fBaGAokOtUtgh43P76ceBp++uJwL8ABYwANtvLQ4D99r972l/39PR7a8M6igKG2l8HAfuAFKmnC+pJAd3sr83AZvv7fwe4zV6+FLjf/voBYKn99W3A2/bXKfbfRV+gr/131Ojp99fGdfUI8Hdgnf1nqaML6ygfCDuvTH7nusAfuTfLvdnFOpJ7s2v1JPdm1+tK7s32SSTGAAAgAElEQVQt11GHujdLD7FrMoE8rfV+rXUt8BZwo4djalda6y+AU+cV3wj8zf76b8BNTuWva5tNQA+lVBQwDvhYa31Ka30a+BgY7/7o24fWukhr/a39dTmwG4hB6qkB+/s9Y//RbP+jgWuA1fby8+vpbP2tBq5VSil7+Vta6xqt9QEgD9vvaqeglIoFJgGv2H9WSB25Sn7nuga5N8u9uUVyb3aN3JtdI/fmS+K1v3OSELsmBih0+vmQvayri9BaF9lfFwMR9tdN1VeXqUf7sJgh2L5hlXo6j324UTZwDNsH3A9Aida6zr6L83t21Id9eykQSuevpz8CjwJW+8+hSB01RgPrlVLblFIz7WXyO9c1yL9b4+T/fxPk3tw8uTe7RO7NrulQ92aTO04quh6ttVZKyRpegFKqG7AG+B+tdZnty0AbqScbrXU9kK6U6gG8CyR5OCSvopSaDBzTWm9TSv2Xp+PxcldorQ8rpcKBj5VSe5w3yu+c6Mrk//85cm9umdybmyf35lbpUPdm6SF2zWEgzunnWHtZV3fUPqQB+9/H7OVN1Venr0ellBnbDXel1vof9mKppyZorUuADcDl2IbInP2Szvk9O+rDvr07cJLOXU+jgBuUUvnYhoFeAzyP1NEFtNaH7X8fw9aAy0R+57oK+XdrnPz/P4/cm1tH7s1NknuzizravVkSYtd8A/S3zyLng+3B+Pc8HJM3eA84O+PbT4C1TuV32GeNGwGU2odIfASMVUr1tM8sN9Ze1inYnwt5FdittX7OaZPUkxOlVC/7t88opfyBMdie6doATLPvdn49na2/acBnWmttL7/NPotjX6A/sKV93oV7aa1/qbWO1Vr3wfZ585nWejpSRw0opQKVUkFnX2P7XclFfue6Crk3N07+/zuRe7Nr5N7cMrk3u6ZD3pu1F8xE1hH+YJsBbR+25yl+7el4PPD+3wSKAAu2Mfz3YHsO4lPge+ATIMS+rwJesNfVDmCY03nuxjZ5QB5wl6ffVxvX0RXYnpnIAbLtfyZKPV1QT2nAd/Z6ygV+Yy9PwHZDyANWAb72cj/7z3n27QlO5/q1vf72AhM8/d7cVF//xbmZLKWOGtZNAraZOrcDO89+NsvvXNf5I/dmuTe7UEdyb3atnuTe3Lr6kntz03XT4e7Nyn4xIYQQQgghhBCiS5Eh00IIIYQQQgghuiRJiIUQQgghhBBCdEmSEAshhBBCCCGE6JIkIRZCCCGEEEII0SVJQiyEEEIIIYQQokuShFgIIYQQQgghRJckCbEQHYBSqo9SKrcV+9+plIp2YZ8llxjXfKXUdZdyDiGEEKIjknuzEJ2DydMBCCHc4k4gFzjizotorX/jzvMLIYQQncidyL1ZCK8jPcRCdBwmpdRKpdRupdRqpVSAUuo3SqlvlFK5SqmXlc00YBiwUimVrZTyV0oNV0p9pZTarpTaopQKsp8zWin1oVLqe6XUoqYurJQyKqWW26+zQyn1sL18uVJqmlJqmP1a2fbt2r490X7+bUqp/yilktxeS0IIIUT7kXuzEB2cJMRCdByXAX/RWicDZcADwBKt9XCt9SDAH5istV4NbAWma63TgXrgbeAhrfVg4Dqgyn7OdOBWIBW4VSkV18S104EYrfUgrXUq8JrzRq31Vq11uv16HwJ/sG96Gfi51joDmA385dKrQQgh/j97dx4fVXU+fvzzTDYSICwJYQv7FpaEsIi4oYBsUhdE/WpRa2ndfqXf1lZrW0Wp3yIudWuxRVqpilRbXFq1lkIliLix1ACCbEKUIFkgZCXbJOf3x7kzTMIkZJ1JyPN+vfJi5m7n3DvDPfPcsynVYmjZrFQrp02mlWo9DhtjPnRevwz8L3BIRH4GRAFdgV3A29X2GwYcNcZsATDG5AOICMB7xpg85/1uoB9w2E/aB4GBIvI74J/AWn8ZFJH/AcYC00WkA3A+sNpJCyCinueslFJKtWRaNivVymlArFTrYfy8/z0w3hhzWEQWAe3qecxSn9cV1HBPMMacEJHRwAzgDuA6YL7vNiIyClgETDLGVIiIC8h1nkwrpZRSZyMtm5Vq5bTJtFKtR18ROc95/W1gk/P6mPPE9xqfbQsAT1+kvUBPETkHQEQ6iki9HoaJSCzgMsa8DtyPfdLsu74z8ApwszEmG7xPuw+JyLXONuIU3EoppdTZQstmpVo5rSFWqvXYC/xARFYAu4E/AF2wI1ZmAFt8tn0BWCYixcB52L5IvxORSGwfpfpOx9Ab+LPzZBngF9XWX4lt0vVHTxMs5+nzPOAPInI/EAa8CmyvZ9pKKaVUS6Vls1KtnBhTvaWHUkoppZRSSil19tMm00oppZRSSiml2iRtMq2UqkJEPuX0ESdvMsbsDEZ+lFJKqbZOy2almo82mVZKKaWUUkop1SZpk2mllFJKKaWUUm2SBsRKKaWUUkoppdokDYiVUkoppZRSSrVJGhArpZRSSimllGqTNCBWSimllFJKKdUmaUCslFJKKaWUUqpN0oBYKaWUUkoppVSbpAGxUkoppZRSSqk2SQNipZRSSimllFJtkgbESimllFJKKaXaJA2IlVJKKaWUUkq1SRoQK6WUUkoppZRqkzQgVkoppZRSSinVJmlArJRSSimllFKqTdKAWJ3VROQSEUn3eZ8mIpcGOA8NTlNE+opIoYiENHW+mouIzBORtQFI5xYR2dTc6ThpGREZ3ITHKxSRgU11PKWUChYR2SUilzivRUT+LCInRGSziFwkInvrcIyAlBuNISKLROTlRuz/LxH5TlPmqbkFqqxq6jK2lnReEJFfN+HxlonIwqY6ngoeDYhVwDiBYbFzg81wbkwdgp2vlqR68GyM+doY08EYUxHMfNWHMWaVMWZ6Y4/TnAVk9QclgeZ8pgeDlb5S6uwkIheKyEcikiciOSLyoYic05xpGmNGGmM2OG8vBKYB8caYCcaYD4wxw+pwjCrlRqACpObiL3g2xswyxrwYrDw1RFOUVU0dhPo5fsArOjyMMXcYY/4vGGmrpqUBsQq0y40xHYBkYAzwiyDnRymllGr1RCQaeAf4HdAV6A38CigNYDb6AWnGmKIApqmUUo2iAbEKCmNMBvBvbGAMgIhEiMhvRORrEcl0mqJE+qy/UkRSRSRfRL4UkZnO8u+KyBciUiAiB0Xk9obkqbb0neN/y2fbUBHJFpGxzvsrnGZjuSKyQUSG15BGlSelvjWVIrIS6Au87dSi/0xE+jtPykOdbXqJyFvOk/8DInKrz7EWicjfROQl51rsEpHxtZzvMyJy2Lme20TkIp91kSLyotPs7QsnL75Nz3/ufAYFIrJbROb4rKvSlNnJ/x0ist+5Ps+KiDjrBovI+05txjER+auzfKOz+3bnWvxPzachS53994jIVJ8Vfr8XItIe+BfQyzl2oXNdQ0Tklz7ntU1E+vikdam/c6jl+vo9N59rMthJt9Dn76SIGJ/t5jvncEJE/i0i/WpLUynVpg0FMMa8YoypMMYUG2PWGmN2gPfe/GEt98xOIvK8iBwVkSMi8mvx6a4jIrf63FN3+5R/aSJyqYh8D/gTcJ5zP/uVnN5tqY+IvCG2/DwuIkt98rbJeX3a/V9EPheRy32OE+bcV8f4uxAi8i2xvxdyxdaYJznL7xWR16pt+4yI/NZ5XWMZW22f01oZ+VyHmcAvgf9x8r/dWb9BRL7vvHaJyP0i8pWIZIkttzs56zzl/nfE/h45JiL3+f3E7fazReQzsWX5YRFZVG39zU46x0VkofjUqIrIBBH52LlOR53vRrjPvt6aerG/X54VkX8634FPRWSQs05E5CnnXPJFZKeIjBKR24B5wM+ca/F2TecBXCa2rD4mIo+LiMs59iARWe/k/5iIrBKRzs660343Ocs9LSVynWtyi086XfydQy3X1++5+VyTXzuvPXnw/FV60hWRBBFZ53yv9orIdbWlqYLAGKN/+heQPyANuNR5HQ/sBJ7xWf8U8Bb2yXZH4G1gibNuApCHbYrlwj75TnDWzQYGAQJcDJwExjrrLgHS/eXBT/5qS/8BYJXPtrOBL5zXQ4EiJ29hwM+AA0C4n/N+Afi1z3FqzR/QHzBAqPN+I/B7oB32YUI2MMVZtwgoAS4DQoAlwCe1fB43AjFAKPBTIANo56x7BHgf6OJ8Vjuq5fNaoJfzWfyPc/49nXW3AJt8tjXYWovO2IIrG5jprHsFuM85Tjvgwmr7Da4l/7cAbuAu57r/j/Md6Vrf74Wz7B7sd3KYs89oIOZM51BL/up9bsAq4BXn9ZXO92i48xndD3wU7P/H+qd/+tcy/4Bo4DjwIjAL6FJt/ZnumW8CzwHtgThgM3C7s+5a4AhwjnN/HAz0c9alcaqMq37/995rseXSdmxZ2973vlhDuTHY5/3PgL/6vL8S2FnDdRgDZAHnOml+x8ljBLYG+yTQ0SdPR4GJzvszlbEvVz8vn3R9r4N3W5/1G4DvO6/nO/f3gUAH4A1gpbOuv3P+fwQisWVRKTC8hvO9BEjEljVJQCZwlbNuBFCIbcoeDvwGKPfJ5zhgIraM6Q98AfzY3+eA/f1yHPt7LBRbXr3qrJsBbMOWkYItt3r67Pdrf3mvlk4K9vdXX2Cfz7UajP19FQF0cz6jp/1dd+d9P6AAuAH7PY8Bks90DrXkrd7nhv3/9w3QB/tdPwx810lzDHAMGBHse4b+nfrTGmIVaH8XkQLszSELeBDsEzjgNuAuY0yOMaYAeBi43tnve8AKY8w6Y0ylMeaIMWYPgDHmn8aYL431PrAWuIh6qEP6fwGuEJEo5/23sQEP2B8V/3TyVo4tcCKB8+uThzrksQ9wAXCvMabEGJOKfRp/s89mm4wx7xrb53gltiD1yxjzsjHmuDHGbYx5AlvYePp6XQc8bIw5YYxJB35bbd/VxphvnM/ir8B+bAFTk0eMMbnGmK+xhZ6nZUA5tvDq5ZxTfQfJysIWjOVOPvZiA+GGfC++D9xvjNnr7LPdGHO8DudQk3qdm4jcCyRgfygB3IF9IPOFMcaN/T4mi9YSK6X8MMbkYwMfTzCV7dR2dvfZzO8909nmMmwwVGSMycIGrp4y8PvAY8aYLc798YAx5qt6ZnEC9kHqPU4a9bnnv4ytQYx23t+ELeP8uQ14zhjzqbE15S9iA8qJTp7/C3haNU0BThpjPqljGdtU5gFPGmMOGmMKsd3HrhenNZjjV8bW8m/HPkjwW54bYzYYY3Y65fEO7G+Ti53V1wBvG2M2GWPKsA/3jc++24wxnzi/A9KwD0QupmZvGmM2O2XSKqqW5R2xZZg45dbR+lwQ4FHn99fXwNPYgBbnu7bOGFNqjMkGnjxDHr8N/MfYlhLlzu+c1DqcQ03qdW4iMhT7UOo6Y8xh4FvYbgR/dq7zZ8Dr2IdMqoXQgFgF2lXGmI7YJ5oJQKyzvBsQBWxzmrjkAmuc5WCfsn3p74AiMktEPnGaouRiC/VYf9vWotb0jTEHsE9OL3eC4iuwQTLYAt77w8AYU4kN+HvXMw9n0gvwBOseX1VLJ8Pn9UmgXbUC1ktE7hbb/C3POd9OnLpuvbDn4HG42r43y6nmaLnAKGq/5tXz5RlM7WfYJ66bxTbxnn/anrU7YowxPu+/cvLekO9Fjd+xM5xDTep8biIyC/gR9v9HsbO4H/CMzzXOcY7X1N8rpdRZwvmxfosxJh57X+6FDS48arpn9sPWph31uec8h60phjPfH+uiD/CVE4jUizHmG+BDYK7TXHYWNpjxpx/wU895OOfSB6dswJbdNzivv03VsvxMZWxTqfK7wXkdCvg+vKhTmSMi54pIithm6HnYh6l+y3JjzElsDaln36Ei8o7YgU7zsQ9e612WG2PWA0uBZ4EsEVnu8/Cirnx/Z/iW5d1F5FWxzfjzsQ9HAlaW1+fcxDZ7/wf24brnYU8/4Nxq38d5QI/a0lWBpQGxCgqnxu4FbG0q2OYjxcBIY0xn56+TsQNwgb1RntbPQ0QisE/afgN0N8Z0Bt7FBg71cab0wT51vQHbVGu3EySDbRbjrbVzapv7YJuXVVeEDbw9qt8QDTX7BugqIh19lvWtIZ1aie0v/DNsTXAX57rlceq6HcU2lfbo47NvP2ztwwJsk+LOwOfU/5pjjMkwxtxqjOkF3A78Xuo3smhv53p79AW+qcP3wt919vsda6i6npuIDKPq02Tf/Nzu833sbIyJNMZ81FR5VEqdvYxtRfUCNjD28HvPxN5vSoFYn/tNtDFmpLNdU9wfDwN9a3pIWwcvYrv6XAt8bIypqew7DCyudu+MMsZ4WnWtBi4RkXhsTbEnIK5PGVulLBfb17qbz/raynJPWr6tffpim7NnnmE/f/6C7e7VxxjTCVhGDWW52HFRYnz2/QOwBxhijInG9n2ud1kOYIz5rTFmHLaZ9lBsNyQ487Xw8B2zw/O9BBukGyDRyeON1fJY/fhNWpZDrefmJbbP81+AFGPM8mr5eb/a97GDMebOpsyjahwNiFUwPQ1ME5HRTq3qH4GnRCQOQER6i8gMZ9vnge+KyFSxg1H0FpEEbJ+YCGw/H7dT01bvKX/qkD7Aq86x7+RUAQrwN2yTs6kiEobtj1sK+AtcUrHNvrqKSA/gx9XWZ2L7FPnL42HnmEtEpJ3YQUK+h31aWl8dsYVvNhAqIg9g+5/5ntMvRKSLiPTGBr8e7bEFUDbYwauo+oOrzkTkWudHCcAJ57iVzvsar4WPOOB/xQ6wci22b8+7nPl7kQnEOE9zPf4E/J+IDBErSUR8fzg05bl5tonGPk2+z0/TwWXYz2Cks20n5xyVUuo0Ygfu+annvuM0Ab4B+MRnM7/3TKcJ6FrgCRGJdsrZQSLiaZr6J+BuERnn3B8HS/27b2zGBmiPiEh7pxy7oIZt/d3//w6MxbameamWdP4I3OHUnIqT1mxPoOs0u90A/Bk4ZIz5wllenzJ2H7YF1myn3L8fW+b45r+/EyT58wpwl4gMEDv95MPYPtL1rj3Hluc5xpgSEZmArfX2eA3bsu18sYNlLaJqMNkRyAcKnd9UDQrSROQc53qHYR8WlFC/shzgHuc3Rx/sZ+wZiLIjth90nvN7pHowWv34q7CDYF4ndgDUGBE5U7PoGp3h3Hwtxv4++lG15e8AQ0XkJuf/XZhzTL+Dr6rg0IBYBY1TKL2E7dMCcC92kIlPnGYx/8Hp02qM2YwdkOApbE3m+9gBPQqA/8UGcCewBcFbDcxSjek7eTgKfIztG/xXn+V7sU8sf4etab4cO71UmZ80VmL7AqVhf3z8tdr6JcD9TrOau/3sfwN24ItvsAOgPGiM+U99TxQ7wvcabKH+FfYG71s7+RCQDhzCXofXcKbuMMbsBp7AXotM7GAeHzYgD2AHaPlURAqxn9uPzKk5DxcBLzrXoqYRGT8FhmCv+2LgGqe/UK3fC6fm5BXgoHP8Xth+SX/Dfi752IcwkTRcbefmMRb7HXtKfEandPL4JvAo8Krzffwc20xQKaX8KcAOJPWpiBRhA+HPsQ9pPfzeM511N2MfJu7G3jdfA3qCHTfC2f4vTjp/xw6AVGfGjm1xOXaQpK+xZUxNMwgsotr93+lO8jowADsIVU3pbAVuxTZzPYEt12+pttlfgEup+nAb6ljGGmPygP+HfVBwBBso+Y46vdr597iI/NdPNldgfw9sxJazJcAPazqnM/h/wENix2d5AFuOefK5yznuq9iHEYXYfuSeqbjuxpaPBdgHCdV/k9RVtLP/CexviuPA486654ERzmf591qO8Q/s4FWpwD+d/cBOHTYW+9vvn5z+2Vf53eT0Qb4M+73PcY5X43gqjTw3XzdgByg74VOez3N+j0zH9sf/Bttk+1GqPkBRQSZVu5IopdTpRORO4HpjTG0DWSillGqhxE4B831jzIXBzktDOa2Zhhpjbgx2XlojpzY6F9tE+lCw86NUS6E1xEqp04hITxG5wGk2Nwz7pPXNYOdLKaVU2yQiXbFNmJefaVt1iohcLiJRItIeO67GTmwrNaWUQwNipZQ/4dgRRguA9dimTL8Pao5aIBFZ5tvU2edvWbDzppRSZwsRuRXbredfxpiNwc5PK3MltqnuN9jm8tcbbR5ahYhcVENZXhjsvKnA0CbTSimllFJKKaXaJK0hVkoppZRSSinVJjV0LrhWJTY21vTv3z/Y2VBKKXWW2LZt2zFjTLczb6lqomWzUkqpptTQsrlNBMT9+/dn69atwc6GUkqps4SIfBXsPLR2WjYrpZRqSg0tm7XJtFJKKaWUUkqpNkkDYqWUUkoppZRSbZIGxEoppZRSSiml2qQ20YdYKaXOVuXl5aSnp1NSUhLsrJyV2rVrR3x8PGFhYcHOilJKKaWagQbESinViqWnp9OxY0f69++PiAQ7O2cVYwzHjx8nPT2dAQMGBDs7SimllGoG2mRaKaVasZKSEmJiYjQYbgYiQkxMjNa+K6WUUmcxDYiVUqqV02C4+ei1VUoppc5uGhArpZRSSimllGqTNCBWSimlFCIyU0T2isgBEfm5n/X9ROQ9EdkhIhtEJN5n3RoRyRWRdwKba6WUUqpxNCBWSinl1aFDh2BnoVF++9vfMnz4cObNmxfsrLQqIhICPAvMAkYAN4jIiGqb/QZ4yRiTBDwELPFZ9zhwUyDyqpRSSjUlHWVaKaVUQBhjMMbgcjXfs9jf//73/Oc//yE+Pv7MGytfE4ADxpiDACLyKnAlsNtnmxHAT5zXKcDfPSuMMe+JyCWByapSSinVdLSGWCml1GkKCwuZOnUqY8eOJTExkX/84x8APPDAAzz99NPe7e677z6eeeYZAB5//HHOOecckpKSePDBBwFIS0tj2LBh3HzzzYwaNYrDhw/7TW/NmjWMHTuW0aNHM3XqVABycnK46qqrSEpKYuLEiezYsQOARYsWMX/+fC655BIGDhzIb3/7WwDuuOMODh48yKxZs3jqqaea58KcvXoDvh9OurPM13bgauf1HKCjiMTUJxERuU1EtorI1uzs7AZnVimllGoqWkPcUP9ZBN0SYPT1wc6JUko1uXbt2vHmm28SHR3NsWPHmDhxIldccQXz58/n6quv5sc//jGVlZW8+uqrbN68mbVr17J//342b96MMYYrrriCjRs30rdvX/bv38+LL77IxIkT/aaVnZ3NrbfeysaNGxkwYAA5OTkAPPjgg4wZM4a///3vrF+/nptvvpnU1FQA9uzZQ0pKCgUFBQwbNow777yTZcuWsWbNGlJSUoiNjQ3YtWpD7gaWisgtwEbgCFBRnwMYY5YDywHGjx9vmjqDyr8/fXCQUJdwywU6n7ZSSlWnAXFDbXJqHzQgVkqdhYwx/PKXv2Tjxo24XC6OHDlCZmYm/fv3JyYmhs8++4zMzEzGjBlDTEwMa9euZe3atYwZMwawNcz79++nb9++9OvXr8ZgGOCTTz5h0qRJDBhgf6x37doVgE2bNvH6668DMGXKFI4fP05+fj4As2fPJiIigoiICOLi4sjMzNRm0o1zBOjj8z7eWeZljPkGp4ZYRDoAc40xuQHLoWqwV7cc5kRRGTef1x+XS6cSU0opXxoQK6WUOs2qVavIzs5m27ZthIWF0b9/f0pKSgD4/ve/zwsvvEBGRgbz588HbAD9i1/8gttvv73KcdLS0mjfvn2T5y8iIsL7OiQkBLfb3eRptDFbgCEiMgAbCF8PfNt3AxGJBXKMMZXAL4AVAc+lapCMvBIKS91sT89lTN8uwc6OUkq1KNqHWCml1Gny8vKIi4sjLCyMlJQUvvrqK++6OXPmsGbNGrZs2cKMGTMAmDFjBitWrKCwsBCAI0eOkJWVVae0Jk6cyMaNGzl06BCAt8n0RRddxKpVqwDYsGEDsbGxREdHN9k5qlOMMW5gAfBv4Avgb8aYXSLykIhc4Wx2CbBXRPYB3YHFnv1F5ANgNTBVRNJFZEZAT0DVqLDUTWGpfWCUsqdu/yeVUqot0RpipZRSp5k3bx6XX345iYmJjB8/noSEBO+68PBwJk+eTOfOnQkJCQFg+vTpfPHFF5x33nmAnb7p5Zdf9q6vTbdu3Vi+fDlXX301lZWVxMXFsW7dOu/gWUlJSURFRfHiiy82z8kqAIwx7wLvVlv2gM/r14DXatj3oubNnWqojDzbskMEUvZm85Ppw4KcI6WUalnEmLN/TIvx48ebrVu3Nu1BF3Vy/s1r2uMqpVQ9fPHFFwwfPjygaVZWVjJ27FhWr17NkCFDApp2MPi7xiKyzRgzPkhZOis0S9msTrNp/zFufP5TLhoSywf7j7H5l1OJi24X7GwppVSTa2jZrE2mlVJK1dnu3bsZPHgwU6dObRPBsFKtXUa+rSH+9oS+AGzYq9NdKaWUL20yrZRSqs5GjBjBwYMHG7z/ueeeS2lpaZVlK1euJDExsbFZU0r5kekExJMT4ugR3Y6UvVlcd06fM+yllFJthwbESimlAubTTz8NdhaUalOO5hXTJSqMdmEhTE7oxtvbj1LmriQ8VBsJKqUUaJNppZRSSqmzVkZeKd2dPsOTh8VRWOpma1pOkHOllFItR1ACYhGZKSJ7ReSAiPzcz/qnRCTV+dsnIrnO8sk+y1NFpERErgr8GSillFIq2A7vyeHjv38Z7Gy0aBn5xfToZAPiCwbHEh7iYr1Ov6SUUl4BD4hFJAR4FpgFjABuEJERvtsYY+4yxiQbY5KB3wFvOMtTfJZPAU4CawN6AkoppZRqEY7uz+W/a76iLcyY0VAZeaX0dALi9hGhnDuwKyl7NSBWSimPYNQQTwAOGGMOGmPKgFeBK2vZ/gbgFT/LrwH+ZYw52Qx5VEoppVRLJ2L/1XjYrzJ3JceLTjWZBtts+svsIr4+rj+flFIKghMQ9wYO+7xPd5adRkT6AQOA9X5WX4//QNmz720islVEtmZn6xQDSinVHNLS0oiMjCQ5Odm7rH///iQmJpKcnJtBaMQAACAASURBVMz48aemA1y9ejUjR47E5XLhO//sunXrGDduHImJiYwbN4716/3d8qu65557SEhIICkpiTlz5pCbm3tafpKTk7njjju8+5SVlXHbbbcxdOhQEhISeP311wF46qmn6Nu3LwsWLGj09VCBpfFw7bIKSjAGevgExFMS4gBYvyczWNlSSqkWpaWPMn098JoxpsJ3oYj0BBKBf9e0ozFmObAcYPz48VpWKqVUMxk0aBCpqalVlqWkpBAbG1tl2ahRo3jjjTe4/fbbqyyPjY3l7bffplevXnz++efMmDGDI0eO1JrmtGnTWLJkCaGhodx7770sWbKERx99tMb8ACxevJi4uDj27dtHZWUlOTl2YKG77rqLLl26VAnSVevgDYiNASSoeWmJPFMuefoQA/SPbc+A2Pas35vNLRcMCFbWlFKqxQhGQHwE8J0AL95Z5s/1wA/8LL8OeNMYU97EeVNKqVbrV2/vYvc3+U16zBG9onnw8pFNcqzhw4f7XT5mzBjv65EjR1JcXExpaSkRERE1Hmv69One1xMnTuS11147Y/orVqxgz549ALhcrtMCdtUaaRVxbY7mnR4Qg202/fKnX3GyzE1UeEuvG1FKqeYVjCbTW4AhIjJARMKxQe9b1TcSkQSgC/Cxn2PU1K9YKaVUkIkI06dPZ9y4cSxfvrxe+77++uuMHTu21mC4uhUrVjBr1izv+0OHDjFmzBguvvhiPvjgAwBvk+qFCxcyduxYrr32WjIztcloq+epFNaA2K8MJyDuGR1ZZfmUhDjK3JV8dOB4MLKllFItSsAfCxpj3CKyANvcOQRYYYzZJSIPAVuNMZ7g+HrgVVNt6EgR6Y+tYX4/cLmuhTGn2mwppVQQNVVNbmNt2rSJ3r17k5WVxbRp00hISGDSpEln3G/Xrl3ce++9rF1b98kDFi9eTGhoKPPmzQOgZ8+efP3118TExLBt2zauuuoqdu3ahdvtJj09nfPPP58nn3ySJ598krvvvpuVK1c2+DxV8J3qQ6wRsT+Z+SW0C3MRHVn15945A7rQPjyE9XuzuHRE9yDlTimlWoagtJMxxrwLvFtt2QPV3i+qYd80ahiEKygqKyBEmxsppZRH7972Fh0XF8ecOXPYvHnzGQPi9PR05syZw0svvcSgQYPqlM4LL7zAO++8w3vvvYc4kVFERIS3dnncuHEMGjSIffv2MW7cOKKiorj66qsBuPbaa3n++ecbeoqqpdF42K+jeSX0iG7n/f/hEREawgWDY9mwJwtjzGnrlVKqLQlGk+mzS6V2Y1ZKKY+ioiIKCgq8r9euXcuoUaNq3Sc3N5fZs2fzyCOPcMEFF1RZd/PNN7N58+bT9lmzZg2PPfYYb731FlFRUd7l2dnZVFTYcRgPHjzI/v37GThwICLC5ZdfzoYNGwB47733GDFiRGNOVbUAnkBO42H/MvNLTus/7DElIY5v8krYm1kQ4FwppVTLogFxY1WUBTsHSinVYmRmZnLhhRcyevRoJkyYwOzZs5k5cyYAb775JvHx8Xz88cfMnj2bGTNmALB06VIOHDjAQw895J0uKSsrC4AdO3bQq1ev09JZsGABBQUFTJs2rcr0Shs3biQpKYnk5GSuueYali1bRteuXQF49NFHWbRoEUlJSaxcuZInnngiEJdENSdPk+lKDYn9ycgvqTLlkq/J3umXsgKZJaWUanG0rW9jVbiDnQOllGoxBg4cyPbt2/2umzNnDnPmzDlt+f3338/9999/2vL8/HyGDBlCfHz8aesOHDjgN425c+cyd+5cv+v69evHxo0ba8u+amVEp1qqkTGGzLxSenSK9Lu+e3Q7RvSMJmVPFv/vksEBzp1SSrUcWkPcWFpDrJRqw0JCQsjLyyM5ObnJjx0dHc3q1aub/LjVPfXUUyxZsoTo6OhmT0s1MR1lukY5RWWUVVTSI7rmEdunJMSx7asT5J3U7l9KqbZLA+LG0j7ESqk2rE+fPhw+fJjU1NRgZ6XB7rrrLvbu3cvDDz8c7KyoehKdhrhGNc1B7GtyQhyVBt7fnx2obCmlVIujAXFjVWhArJRSSgWV0ZC4usx8T0Dsv8k0QHKfznSJCiNF+xErpdowDYgbS5tMK6WUUkHhHWVa4+HTZHgC4hoG1QIIcQkXD+3Ghr1ZVOjAZEqpNkoD4sbSGmKllFIqOLQPcY0y8kpwCcR2CK91u8kJcZw4Wc729NwA5UwppVoWDYgbSwNipZRSKihO9SHWiLi6jLwS4jq2IzSk9p96Fw/thkvQZtNKqTZLA+JG+vpY3pk3yj8KRceaPzNKKRVgaWlpREZGekeZPnz4MJMnT2bEiBGMHDmSZ555xrvtokWL6N27t3eu4Xfffde7bseOHZx33nmMHDmSxMRESkpKak33nnvuISEhgaSkJObMmUNubu5p+fGdnxigrKyM2267jaFDh5KQkMDrr78O2FGm+/bty4IFC5rsuqhA0VG1apKRX0L3WgbU8ugcFc64fl10PmKlVJul8xA30i9Wb+OpQecT17GWQue170J0L7hmReAyppRSATJo0CDvKNOhoaE88cQTjB07loKCAsaNG8e0adMYMWIEYEd0vvvuu6vs73a7ufHGG1m5ciWjR4/m+PHjhIWF1ZrmtGnTWLJkCaGhodx7770sWbKERx999LT8+Fq8eDFxcXHs27ePyspKcnJyvHnq0qULW7dubfS1UIHlrSHWgPg0GXklDOrWoU7bXjIsjsf/vZfM/BK619LnWCmlzkYaEDeSy7jJzCutPSDOOQgRHQOXKaVU2/Svn0PGzqY9Zo9EmPVInTfv2bMnPXv2BKBjx44MHz6cI0eOeANif9auXUtSUhKjR48GICYm5ozpTJ8+3ft64sSJvPbaa2fcZ8WKFezZswcAl8tFbGzsGfdRqrXKyC/hgsF1+45PSbAB8Ya9WfzPOX2bOWdKKdWyaJPpRgrDTc7JWkaarnBDoTZDUkq1PWlpaXz22Wece+653mVLly4lKSmJ+fPnc+LECQD27duHiDBjxgzGjh3LY489Vq90VqxYwaxZs7zvDx06xJgxY7j44ov54IMPALxNqhcuXMjYsWO59tpryczMbOwpqiA7VUOsVcS+ikrdFJS461zbm9CjIz07tdNm00qpNklriBspjApyikpr3qAoG+3cpJQKiHrU5Da3wsJC5s6dy9NPP010dDQAd955JwsXLkREWLhwIT/96U9ZsWIFbrebTZs2sWXLFqKiopg6dSrjxo1j6tSpZ0xn8eLFhIaGMm/ePMDWUH/99dfExMSwbds2rrrqKnbt2oXb7SY9PZ3zzz+fJ598kieffJK7776blStXNut1aE1EZCbwDBAC/MkY80i19f2AFUA3IAe40RiT7qz7DnC/s+mvjTEvBijTAUmmtfFMudSzDn2IwU5fNTkhjn98doRSdwURoSHNmT2llGpRtIa4kcJwk1NUy0jThRmBy4xSSrUA5eXlzJ07l3nz5nH11Vd7l3fv3p2QkBBcLhe33normzdvBiA+Pp5JkyYRGxtLVFQUl112Gf/973/PmM4LL7zAO++8w6pVq7zz0UZERHibXI8bN45Bgwaxb98+YmJiiIqK8ubn2muvrVMabYWIhADPArOAEcANIlK9nftvgJeMMUnAQ8ASZ9+uwIPAucAE4EER6RKYfNt/TWUgUms9MvNsQFyf/sCTh8VRVFbB1rQTzZUtpZRqkTQgbqTQM9UQF2hArJRqO4wxfO9732P48OH85Cc/qbLu6NGj3tdvvvkmo0aNAmDGjBns3LmTkydP4na7ef/99719jm+++WZv4OxrzZo1PPbYY7z11ltERUV5l2dnZ1NRUQHAwYMH2b9/PwMHDkREuPzyy9mwYQMA7733Xq39mtugCcABY8xBY0wZ8CpwZbVtRgDrndcpPutnAOuMMTnGmBPAOmBmAPLsQ1ti+TrqBMQ96lhDDHDB4BjCQ13abFop1eZok+mGkhAwFYTJGWqINSBWSrUhH374IStXriQxMdE7FdPDDz/MZZddxs9+9jNSU1MREfr3789zzz0HQJcuXfjJT37COeecg4hw2WWXMXv2bMBOx9SrV6/T0lmwYAGlpaVMmzYNsANrLVu2jI0bN/LAAw8QFhaGy+Vi2bJldO3aFYBHH32Um266iR//+Md069aNP//5z4G4JK1Fb+Cwz/t0bI2vr+3A1dhm1XOAjiISU8O+vZsvq6d4WgZoF+KqPE2me9SjhjgqPJSJA2NI2ZPFwm/pwyKlVNuhAXFDhYSDu5gwKjiuNcRKKQXAhRdeWOMAR7X1173xxhu58cYbqyzLz89nyJAhxMfHn7b9gQMH/B5n7ty5zJ071++6fv36sXHjxhrzoM7obmCpiNwCbASOABX1OYCI3AbcBtC3bxOMZqxdiP3KyCuhU2QYkeH16ws8eVg3fvX2btKOFdE/tn0z5U4ppVoWbTLdUCF2jsww3JzQPsRKqTYqJCSEvLw8b21wU4qOjmb16tVNftzqnnrqKZYsWeId/KuNOgL08Xkf7yzzMsZ8Y4y52hgzBrjPWZZbl319jrHcGDPeGDO+W7duTZZ5HWW6qoz8kjoPqOVrSkIcACl7tdm0Uqrt0IC4gYwTEIfiPkMNsU7roZQ6e/Xp04fDhw+Tmpoa7Kw02F133cXevXt5+OGHg52VYNoCDBGRASISDlwPvOW7gYjEiojnd8MvsCNOA/wbmC4iXZzBtKY7y5qdeEfVCkRqrUdmfkm9BtTy6BfTnoHd2ms/YqVUm6IBcQMZl21tHk4FJ07W1of4aM3rlFJKqRbAGOMGFmAD2S+AvxljdonIQyJyhbPZJcBeEdkHdAcWO/vmAP+HDaq3AA85y5rdqXmIA5Fa63E0r6Re/Yd9TRkWx6cHcygqdTdxrpRSqmXSPsQNZFzhAHSKgBMny6ioNIS4/HRmKtQaYqWUUi2fMeZd4N1qyx7wef0a8FoN+67gVI1x4HiLXY2IPcorKjlWWFqvEaZ9TU6I40+bDvHRl8eZNqJ7E+dOKaVaHq0hbqAK59J1ixSMgbxiP7XElRVQqM2OlFJKqebgiYe1hviUrIJSjKnflEu+zunflQ4RodpsWinVZmhA3ECVlbb07Rppi2O/cxEXHQNTrwE4lVJKKVVX2of4NBl59Z9yyVd4qIsLB8eyYW+WDlamlGoTNCBuoEqnkOjSzhMQ+6kh1hGmlVJtQFpaGpGRkd6RpufPn09cXByjRo2qst0999xDQkICSUlJzJkzh9zcXADKy8v5zne+Q2JiIsOHD2fJkiUAFBcXk5ycTHh4OMeOHQvsSalWobLCjTG1jOPRBmV65iBuYA0x2NGmj+aV8MXRgqbKllJKtVgaEDeQU0FM5wj7r98aYh1hWinVRgwaNMg70vQtt9zCmjVrTttm2rRpfP755+zYsYOhQ4d6A9/Vq1dTWlrKzp072bZtG88995w3yE5NTaVXr14BPRfVehzY/E9Kc3+nNZk+jjayhhjgkmF2Siydfkkp1RbooFp1VeGG0nyI6gqcqiHuZMfW8l9D7BlhOio2EDlUSrVxj25+lD05e5r0mAldE7h3wr312mfSpEmkpaWdtnz69One1xMnTuS11+z4TCJCUVERbreb4uJiwsPD2/qcwKrObCstTzcmZWuII0JddI4Ka/Ax4qLbMap3NCl7svjB5MFNmDullGp5glJDLCIzRWSviBwQkZ/7Wf+UiKQ6f/tEJNdnXV8RWSsiX4jIbhHpH5BMv/MjeGwAVNjA1xMQdwyrBGqoIfaMMN1BR2lUSilfK1asYNasWQBcc801tG/fnp49e9K3b1/uvvtuunbtGuQcqtbA04UYDYi9MvJK6NGp3ak5mhtoyrA4/vv1CU4UlTVRzpRSqmUKeA2xiIQAzwLTgHRgi4i8ZYzZ7dnGGHOXz/Y/BMb4HOIlYLExZp2IdAAqA5Lxz9+0/7pLISSMSifVUFNBh4jQGmqIMyCyK4SGBySLSqm2rb41ucGyePFiQkNDmTdvHgCbN28mJCSEb775hhMnTnDRRRdx6aWXMnDgwCDnVLV4TtBXqaNqeWXkldC9Ec2lPSYnxPHb9QfYuD+bK5N7N0HOlFKqZQpGDfEE4IAx5qAxpgx4Fbiylu1vAF4BEJERQKgxZh2AMabQGHOyuTMMgDiXyhk12lNDTEUZXdqH1dCHOAM69ghI9pRSqjV44YUXeOedd1i1apW3Busvf/kLM2fOJCwsjLi4OC644AK2bt0a5Jyq1sBbC6o1xF4Z+SX0bMSAWh5J8Z3p2j6cFJ1+SSl1lgtGQNwbOOzzPt1ZdhoR6QcMANY7i4YCuSLyhoh8JiKPOzXOzc/lCYht1bA3IK5007V9BDknaxhlWgNipZQCYM2aNTz22GO89dZbREVFeZf37duX9evtbb6oqIhPPvmEhISEYGVTtSremYiDmouWwhhDRn5JowbU8ghxCZcM7cb7+7Kp0AcOSqmzWEsfZfp64DVjvJP5hgIXAXcD5wADgVv87Sgit4nIVhHZmp2d3ficeGqIKz0BsbO8ooyuUTXVEGdCBw2IlVJtyw033MB5553H3r17iY+P5/nnnwdgwYIFFBQUMG3aNJKTk7njjjsA+MEPfkBhYSEjR47knHPO4bvf/S5JSUnBPAXVSlQrmtu8EyfLKXNXNmrKJV+TE+I4cbKc1MMnmuR4SinVEgVjlOkjQB+f9/HOMn+uB37g8z4dSDXGHAQQkb8DE4Hnq+9ojFkOLAcYP3584x9teiqiq9cQV5TTtX0EezOqzdVXWenUEHeH7C8anbxSSrUWr7zyit/lBw4c8Lu8Q4cOrF69ujmzpM5S4jzXN0YjYrD9h6FxUy75mjSkGyEuIWVPNuP66UB3SqmzUzBqiLcAQ0RkgIiEY4Pet6pvJCIJQBfg42r7dhaRbs77KcDu6vs2i+p9iCt9A+Iwck5WG4WxOAcq3dCxZ0Cyp5RSwRISEkJeXh7JyclNetzi4mKSk5MpLy/H5WrpDZpUUOgo01Vk5BcD0L2Jaog7RYUxrm8X1ms/YqXUWSzgNcTGGLeILAD+DYQAK4wxu0TkIWCrMcYTHF8PvGqMMT77VojI3cB7YkfS2Ab8MSAZd52qIa6oNFQYYwviSltDXFJeyckyN1HhziUtyLD/6pRLSqmzXJ8+fTh8+PCZN6ynyMhIUlNTm/y4Sp2tMvJs962mGFTLY3JCHI+u2eOdzkkppc42wWgyjTHmXeDdasseqPZ+UQ37rgMC37nM21GpgpyiMjt+h2D7ELcPAyCnqOz0gFgH1VJKKaWahWeU6UqjNcQAGXnFuAS6dYhosmNOcQLilL1Z3DChb5MdVymlWgptg1ZXPn2Iswt8BtCqsKNMgw2IvQo1IFZKKaWalWfaJQ2IATvlUmyHCEJDmu7n3dDuHejVqZ1Ov6SUOmtpQFxX3kK3kqyCklPLK8ro2j4cqBYQe5tMa0CslFJKNQcpskGa0WGmAcjIL23S5tJga+EnJ8Sx6cAxSt0VZ95BKaVaGQ2I60pcVBjhnyteIN13pNTK8poD4nadIKz5+tus3nqYx9bsqf+ORcfh9Vvh8zeaPlNKKaVUoBRmAj4DXbZxGXnFdG+iEaZ9TUmI42RZBZsP5TT5sZVSKtg0IK4rVwjHSqLYs2UbWf984dTyihoC4sKMZh9hetn7X7Ls/S85XuhnDuSapG+D5ybBzr/BwZTmy5xSqk1IS0sjMjKyygjT/fv3JzExkeTkZMaPH+9dvnr1akaOHInL5WLr1q3e5evWrWPcuHEkJiYybtw41q9ff8Z0G3KsV155hcTERJKSkpg5cybHjh0D4J577qFHjx785je/adS1UIHnabylTaatjLySJq8hBjh/UCzhoS5S9mQ3+bGVUirYNCCuK3FRVGED33IJxeVySuGKcqLbhRLqktNriJtxhOkjucV8mV1EpYF1uzPPvIMxsPmPsGIGuFwQ3qHZ8qaUalsGDRp02mjQKSkppKamVglWR40axRtvvMGkSZOqbBsbG8vbb7/Nzp07efHFF7npppvOmGZ9j+V2u/nRj35ESkoKO3bsICkpiaVLlwLw+OOPc8cddzTo3FVwCdqH2ONkmZv8EneTTbnkKzI8hPMGxpCyV/sRK6XOPkEZZbpVkhDyymwhUyrhhIjYkaYryhARurQP54TvXMQFmdDvvGbLzsZ99iltx4hQ/vV5BtfXNvJjWRG8/WNbKzxkOsx5Dv5wfrPlTSkVHBkPP0zpFw3oRlGLiOEJ9PjlL5vkWMOHD/e7fMyYMd7XI0eOpLi4mNLSUiIiah4pt77HcrlcGGMoKioiJiaG/Px8Bg8e3MAzUS2Fp4ZYW0zb2mGAHs3QZBpss+kH39rFoWNFDIht3yxpKKVUMGgNcV2Ji7xyJyCugBBPDXGlG4CuUeEcL3QCYmOcJtPNN6DWxn3Z9Ihuxw3n9uWjL4+RV1zuf8Nj++GPU2HnaphyP9zwV4jq2mz5UkopEWH69OmMGzeO5cuX12vf119/nbFjx9YaDDfkWGFhYfzhD38gMTGRXr16sXv3br73ve81Og3VUmhEnJHvBMTNNFfwlIQ4ANbraNNKqbOM1hDXUZFLyHJH2jelRTYgroBKdxkuoKtvDXHxCagoa7YRpt0VlWw6cIxZo3owc1QPlm88yPo9mcwZE191w11/h38sgNBwuOkNGDSlWfKjlGoZmqomt7E2bdpE7969ycrKYtq0aSQkJJzWtNmfXbt2ce+997J27dpG56H6scrLy/nDH/7AZ599xsCBA/nhD3/IkiVLuP/++xudlgoi7wQQGhA3dw1xn65RDI7rwIa9WXzvwgHNkoZSSgWD1hDX0cuhZXyG7XcbUnaSEJdwqLALz+4ew8n8PLq2D+e4pw+xZ8qljs3Th3h7ei4FJW4mDe1GcnxnukdH8K+dGac2qCiHf98Hq78D3YbB7Rs1GFZKBUzv3r0BiIuLY86cOWzevPmM+6SnpzNnzhxeeuklBg0a1Kj0/R3L08d50KBBiAjXXXcdH330UaPSUcHnGVNLK4ibv4YYYPKwbnx6MIfCUnezpaGUaiY5h+Dj3+uYC35oQFxHIeIipNIWvREVxYSIcLw0irLKUDIP7LU1xJ6AuNATEDfPKNPv7zuGS+DCwbG4XMLMkT14f182RaVuyD8KL14OHy+FCbfDd/8FneLPfFCllGoCRUVFFBQUeF+vXbuWUaNG1bpPbm4us2fP5pFHHuGCCy6osu7mm2+uU0B9pmP17t2b3bt3k51tx19Yt25djf2QVevh7UOs8xCTmVdCdLtQosKbr/HfjJE9KKuo5K3Ub5otDaVUM9n1Jvz7F/D568HOSYujAXEduU49h6ZdZRmIUFphC52sQwfo0j6c3OJyKirNqRriZhpleuO+bJLiO9M5yo56PXNUT0rdlezY9A48dxEc3Q5zn4fLHrPNpZVSKkAyMzO58MILGT16NBMmTGD27NnMnDkTgDfffJP4+Hg+/vhjZs+ezYwZMwBYunQpBw4c4KGHHiI5OZnk5GSysmw/xR07dtCrV6/T0qnvsXr16sWDDz7IpEmTSEpKIjU1lV+2kCbmqilojcfRvJJmrR0GGNevCyN7RbPiw0MYrWVSqpVx/s+uXWgH3FVe2oe4jkKk6rMDd2UIJZUhAGSlHSTmvPMxBnJPlhHjbTLd9H2Ic0+WsSM9lwVThniXTejfhbsi3+XcD/4CMYPgO29DnNZ8KKUCb+DAgWzfvt3vujlz5jBnzpzTlt9///1++/Lm5+czZMgQ4uNPb+VS32MB3HHHHTq90llGvH2Ig5uPliAzv4QenSKbNQ0RYf4FA/jp6u18sP8Yk4Z2a9b0lFLNoOAb+OBJmLow2DlpMbSGuI5c4ltHDGUVod4a4uyv0+jS3tbE5hSVQWEmRERDeNNPS7DpwDEqDVw8NNYuKM4l5G838iPzMmvNBEq++x8NhpVSARMSEkJeXh7JyclNfuzo6GhWr17d5Met7p577uHll1+mfXudSqb1sSWz1lY6NcTRjR+d/Uy+Nbon3TpGsOLDQ82ellKqGQy/Aj76ne1TrAANiOvMhQuM4Aq3gW9pRQglTkB8IjOTzmG2MM4pKoOCo1WaSx8v7kZucXST5GPjvmw6tgtldHxnyN4Hyy+B/f/mwNj7uKP0h3zwdWmTpKOUUnXRp08fDh8+7B20qjV6/PHHOXDgAHfeeWews6LqyVtD3MYDYndFJccKS5tthGlfEaEh3DSxHxv2ZnMgq7DZ01NKNbFpD4ErFNbqLAseGhDXUYiIfQ4dHgZASYWL0soQBAPGEJJrm0nbgDizSnPptYfmkrL/kkbnwRjDxn3HuGhILKEhLtj0FBRlwy3/pO9ldxPdLow1n2ec+UBKKaVUNSIyU0T2isgBEfm5n/V9RSRFRD4TkR0icpmzPFxE/iwiO0Vku4hcErg8Oy/aeECcXVhKpaHZm0x7fPvcvoSHunjhI61hUqrVie4FF/0E9rwDX6YEOzctggbEdeRy+hBXhNqAuLTcRUlFGN3b2aejlcfSAcg5WWZHmXYC4vKyCk6UxJJVEEdlReM6Oe3PKiQjv4RJQ5w+OxWlNp2+EwkPdXHpiO7854tMyhuZjlJKqbZFREKAZ4FZwAjgBhEZUW2z+4G/GWPGANcDv3eW3wpgjEkEpgFPiEhAfl9oPGwd9cxB3Kn5m0wDxHaI4KrkXry+7Qi5J8sCkqZSqgmdtwC69Ic1P7fTtbZxGhDXUYgIYqA8xA6kVey2NcSxEUVEREVRdPRrAHIKSu0o006T6ZwjRRhcuCvDOH6kcSO6bdxnpwupaRCLmSN7kFdczsdfHm9UOkoppdqcCcABY8xBY0wZ8CpwZbVtDODp/9MJ8My9MwJYD2CMyQJygfHNnmPwmYi4bT8IznQC4u4BaDLt8d0LBlBcXsGrWw4HLE2lVBMJawczHobsPbDlT0HLRkl5kqWIcwAAIABJREFUBTvT84KWvocGxHXkwgbC5cZQGRJOcXkIpRWhtAtxExcfT87XaXSMCKWoIAfcJd45iI+lF3iPkZmW36g8vL8vm8FxHejV2X+TqElDuxEVHsK/tNm0Ukqp+ukN+EY26c4yX4uAG0UkHXgX+KGzfDtwhYiEisgAYBzQp3mzW1Vbn4Y4I98GxD0D1GQaYHjPaM4fFMOLH6VpyzSlWqNhl8GgKZCyBIqOBTRpYwz/2nmUaU+9z00rPqWw1B3Q9KvTgLiOQpyOSu7KCmjXnoKyENwmhHYhbrr1ibcjTUeFUplfdcqlY4cLCXeVEBlWTOahhj8BKSmvYPOhnFPNpf1oFxbC5IQ41u3OsPMhK6VUM0tLSyMyMtI7yvThw4eZPHkyI0aMYOTIkTzzzDPebRctWkTv3r298wO/++673nU7duzgvPPOY+TIkSQmJlJSUlJrugsXLiQpKYnk5GSmT5/ON9/YyspVq1aRlJREYmIi559/vncKqOLiYpKTkwkPD+fYscAW/GeRG4AXjDHxwGXASqdp9ApsAL0VeBr4CKjwdwARuU1EtorI1uzs7EZnyFNBvDM9t9HHas0y8koID3XRJSosoOnOv2AAR/NKdPwSpVojEZj5CJQXwXsPBSzZz4/kcf3yT7hz1X+JCgtl6Q1j6RAR3JmANSCuI5eEIAYqKg0hUR04fsLWGL/fMYyY+F64y0rp7SpCCp1CwWkyfSy9gJioTLp3zCTzUMNriD89lEOpu5JJnumWajBrVA+OFZaxJS2nwWkppVR9DBo0yDvKdGhoKE888QS7d+/mk08+4dlnn2X37t3ebe+66y5SU1NJTU3lsssuA8DtdnPjjTeybNkydu3axYYNGwgLq/2H/T333MOOHTtITU3lW9/6Fg89ZAvzAQMG8P7777Nz504WLlzIbbfdBkBkZCSpqan06tWrOS7B2eAIVWt1451lvr4H/A3AGPMx0A6INca4jTF3GWOSjTFXAp2Bff4SMcYsN8aMN8aM79atCeawdSLiFR8ebPyxWrGM/BJ6RLdDRM68cROakhBHv5gonYJJqdaq2zCYcDv89yX4pnlni8gqKOHe13Zw+dJN7M8q5NdXjeKf/3shFw6pPbYJhOCG461IiLgAA2IIjepIXmYYuGBTdDhjOhQD0L38OGEns+wOHXtSWWk4dqSI4dEZREVB2lf9KT1ZTkQDnuBu3JdNeKiLcwfE1Lrd5GFxRIS6WPN5BhMH1r6tUurs8sHf9nHscNNOgxLbpwMXXTe0ztv37NmTnj1tl5GOHTsyfPhwjhw5wogR1cdnOmXt2rUkJSUxevRoAGJiznzvio4+NZVdUVGRNxA4//zzvcsnTpxIenp6nfPexm0BhjhNno9gB836drVtvgamAi+IyHBsQJwtIlGAGGOKRGQa4DbG7CYAPOGftPFGUXYO4sD1H/ZwuYTvnt+fRW/v5r9fn2Bs3y4Bz4NSqpEu/hns+Cv8616Yv8Zn+P6mUVJewYoPD/Hs+gOUVVTy/QsHsGDKEDpFBrZFS220hriOXOLyFryuyPa4XfbSlYVVkmoO4goJpUtJNu1KnCZgHbuTn12Mu7SC2MhMune0gXJWWoGfo5/Zxn3ZnDugK5HhIbVu1z4ilElDu7Hm8wwqtdm0UiqI0tLS+Oyzzzj33HO9y5YuXUpSUhLz58/nxIkTAOzbtw8RYcaMGYwdO5bHHnusTse/77776NOnD6tWrfLWEPt6/vnnmTVrVtOczFnOGOMGFgD/Br7Ajia9S0QeEpErnM1+CtwqItuBV4BbjJ0AOA74r4h8AdwL3BSofHsehAS2XrTlycwvoXunmgPijIcfJvOxx5sl7WvG96FjRCh//jCtWY6vlGpmkZ3h0gfh8Cew87UmO6xvP+HH1uzl/MGxrL3rYu6bPaJFBcOgNcR15hKxFcQYXJEdvMvLQiv56PhnzOkzEHdhJh06ZGMi2iMRHTmWboPgblEZRHcMAYHMtDz6jOhar7S/yS1mf1Yh142v2xgls0b1YN3uTLan5zJGn9Yq1WbUpya3uRUWFjJ37lyefvppb23unXfeycKFCxERFi5cyE9/+lNWrFiB2+1m06ZNbNmyhaioKKZOncq4ceOYOnVqrWksXryYxYsXs2TJEpYuXcqvfvUr77qUlBSef/55Nm3a1KzneTYxxryLHSzLd9kDPq93Axf42S8NGNbc+fOrrVcNY390ZuSVMGNkzQFxwbr/8P/ZO+/wKKr9D7+zLbvpvXdIKAkhEHooKkVAUUEsCHZFxV6vv3ut2LvXi17ggg0FUSwUKUqREhAIEHpJSC+bnt2U3Wyb3x8TAiEJBEwIyLzPw8PulHPOTmZ3zud8m6O2Fv+nnkRQte/Uz9VJxS39w/hiazb/HN/9gib2kpGRaScSp8HO+fD7S9BtHDi5nv2cM3CgwMDMFYfYkVVBtwA3vr1vIMldO981ujVkC3EbUQgnLbMKnUvj63q1g4zqHNxCAlFWFuEjViK6nkioVY1CIeClLcVJZcErwPm84og3p5+53NLpjOwRgEohyEkuZGRkOgWr1cqNN97I1KlTmTRpUuP2gIAAlEolCoWC+++/nx07dgAQGhrK8OHD8fX1xdnZmfHjx7N79+429zd16lR+/PHHxvf79u3jvvvuY+nSpW1yv5a5dBE4YSG+fIVxVZ2Vepuj1ZJLDrMZW1ERDqMR84EDHTKGO4dEIooiX2/L6ZD2ZWRkOhiFAsa9C9WFsOXD826mpNrMc0v2MmHWFjJKanhjohQnfDGLYZAFcZtRnnCZFkSU2pOC2KKWSg1UutsQ64x42g1YdJJwLcuvwSvIGZVCSrYZEOVOcbYRycPsDIhik3+bjpYS6OZErL9Ls30t4aFTM6SrL6sO6M/el4yMjEw7Iooi9957Lz169OCpp55qsq+oqKjx9c8//0x8fDwAV199Nfv376eurg6bzcbGjRsbY47vuOOORuF8Kunp6Y2vly5dSvfu3QHIzc1l0qRJLFiwgNjYi8diLtNBnPCVvowfdSdLLrUsiC05uY2va7akdMgYwrydGdMzkIXbczFZWkwwLiMjc7ETPhASboGt/4GKc0tUaLba+XRDBle+9wc/7yng/mHR/PHsFUwdGIFKefHLTdlluo0oBCUetaCrqkSlOfnQcRMseOgCOWYtxA9Q15sxOXVFi2QhDu3uDVbp2IAoD45s02MsM+Ph14pL0arnYft/m2z69MSL00Lk/qyeSqlwLxNaaGZcfCD/99N+DhUZiQv2aLLPfOgQhT+Cx6BsfK5r4WQZGRmZ8yQlJYUFCxbQq1evxlJMb775JuPHj+e5554jLS0NQRCIjIxkzpw5AHh5efHUU0/Rv39/BEFg/PjxXHPNNYBk7W0pM/Tzzz/P0aNHUSgUREREMHv2bABmzpxJeXk5M2bMAKSs16mpqRfio8t0Ao25PTp1FJ2L3iAJ4tYsxJacbAAUrq7UpqTg98jDHTKOe4dFsfqgnp/25DN1YESH9HG5YDHbKM42oj9uoCyvhr5jIwiIdD/7iTIyf5VRr8LhFbDmBZiy8KyHi6LIqgN63lx5mPxKE2N6BvDP8T2I9HU567kXE7IgbiNKQYHSAZ61ZupVGgAUDgduFujj348N2Vu5Di9s9SI1Gl+cjBZqDRZ8w1yhYZElIEr6MSvONrQuiMuOglswJN0JQKHBxOIdeYzvFUS3QLfGw0QRjixLpNakwVBqatbemJ4B/Ovn/aw+oG8iiKuWLEE/8zVEi4BxXwmyM6GMjEx7MnTo0FY9UxYsWNDqedOmTWPatGlNthmNRmJiYggNDW12/Kku0qcyb9485s2bdw4jlrmUucBVhi5KTliIA1u1EEtuzB4TJ1K5cCF2oxGle/uLq34RXvQK8eDzLVlM6R+OQiH/cdqCKIpUV5jRZxrQZxgoyjRQnl8jOQEKoFQqMJTWcfM/+6O4BCxtMpc47kEw/BlY9ypkrIOurefyOFBgYObyQ+zIrqB7oBsL7xvIkIvcNbo1ZEHcRhTCyR8htVoSxGq7Aw+zSLJ/Ej/mrEbtEUFVvRMeCm/Il0qf+ISeFMQ+wS6oNAqKs4zE9g9svTOPELjieQC+X3uMTxzp3DVhNLhoGg+pKKih1iS5EWbuKaXPmPAmTfi4OjEgyptVB/Q8PaYbDrMZ/WuvYfjxJ1yGDEZdsZWqY9U4TCYUOjkBhoyMzPmhVCoxGAwkJiY21iJuL9zd3fnhhx/apS2TycTgwYOxWq0oFPKk8u9CY9mly9hnWm8wIwjg7+bU4n5LTg5KX1/cx4ymcsECardvx3306HYfhyAI3DM0kicX72VzRhkj2pj35HLDbndQlleD/riBouMG9JkGaqvqAVA5KQmMcidpXCRBXTwIiHIn/0glq+ce4ODmQnpd0XxxUEam3Rn8sFSXePX/wUMpoGyaEbrEaOa9NUdZsjsfb2cNb07sxS39w1BewotgnSKIBUEYC/wbUALzRFF8+7T9HwFXNrx1BvxFUfRs2GcH9jfsyxVF8YI4/SoFRcPjVkTVcGOobQ48TSID/RJRCkqs3gKlBa64Cl5Y86XySr6hJ7O0KZQK/MLdzimx1qZjpSSEeuJ1ihgGyD1UAYCbt5bje0qaCWKAcfFBvLzsIOl7DqOc+S/qDx/Gd8ZD+D78MDVPxlF1RMR84ADO/fuf28WQkZGRaSAsLIy8vLzOHsZZ0el07S7YZS4CLt35V7uhN5jxdXVC3Yr10JKdjSYiAl3v3iicnalNSekQQQxwTa9g3lx5hM+3ZMmCuAFzrVWy/jYI4JJsIzarlH/G1duJ4BhPAqM9COrigU+ISzMrcHQfP0K6ebJ9WSYx/QLQul5c5Wpk/oaonGDsW7DoVtjxPxgshSCZrXbmb8nisw1SPeHpw6J5+KquuGsv/XvyggtiQRCUSGGxo4F8YKcgCMsayjkAIIrik6cc/yjQ55QmTKIoJl6o8Z6gMcu0CGqF9FrtsONuFnHXuJLgl0CBrhzBokNjdcOaX4OrlxM616ZCNiDKg30b8rBbHSjVZ7ZSGOqspOVV8ciVXZvtyztUjleQC7H9A9i+LJOaSjOuXk3dpa6OC2TlnO8x3fMyThoVobP/i9sVVwCgC5COqUtLkwWxjIyMjMwlyQmX6cvaQmw0E9hK/DBIFmLX4cMRNBqcBw6kNmVrh41Fo1Jwx6AIPvj9GBkl1XT1dzv7SX8jRFHEUGKi6HhVowCu1NcBICgE/MJc6TksuFEAnz5vawlBEBh2cyyL39jJ9mWZjLitcyqcNeKwS+V5ekyQ3GsvAjJKqtmcXsa4+KBWQwdkzpHYsdBlJPzxFmKvyazMtPPmysMUVF26ccJnojMsxAOADFEUMwEEQfgOuB441MrxU4CXL9DYWkUpKDlR4EFls6Cy21HbHLiZpYdwcnAyy7TzCcEffaUNRUVNo3XYYrCD1YIGCIh0x2ETKcuvaYwpbo2U42U4xObllqwWO4XpBuJHhNClrx/bl2WSmVZKwpUn6xSLNhuK+f/lle1fkO8XzohF89GcEoen0oLG1xnTHtliIiMjIyNzidKggy9nQ3Gx0UyYt3OL++w1tdhLy9BERALgkpxMzYYNWHJz0YQ39yxrD24bGM5/NmTwRUo2b0zs1SF9XCzYLHZKcqrRZ550fzbXSJlUnZxVBEZ7EDswkKBoD/wj3VE7Kc/SYsv4hLgSPzyEAxvziRsejG9oJy40lKXDqmdh8/tw8wIpM3EnIIoiO7IqmLspk3VHSgB4d/VRZlzRhfuHR6NVn9+1lmlAEGDs24ifDWbdp4/ycOUdl3yc8JnoDEEcApzqX5cPtPhtEgQhAogC1p+yWSsIQipgA94WRfGXVs6dDkwHCG+HH32FQtn4xO1+dB5F9VacLVZcpVwWDA0ZypfuUpbT8vJadMV1RCf6UZ+VRfY35TgsZXjqX8Rn6n2AlFjrbIJ407FS3LQqEsM8m2wvTK/CbnMQ3tMbr0AXvIJcOL77pCC2lZVR8PQz1G3fTtGwsczwHME6nTenXwVdhAc1aWmIooggZyaRkZGRkbnUOGEhvnwNxBQZzPSP9G5xnzVXSqiliZCyPrsOTaYYqE1J6TBB7OPqxMTEEH7cnc8zY7o1C/m6lKk11EuW3wYX6NLcahx26ebzDHAmMsGXoGgPAqM98Ap0RmjHmMoBE6JI31nM5sXp3PBUn86bt4mSuzemSvjyGhj/HvS7+4J1b3eIrDmoZ86mTPbmVeHlrObxkTGM7OHPZxuO88Hvx1i0I5fnx/dgQkKQPL89T0qMZt77w0Q36xjuEVcxe+RdjB457JKOEz4TF3tSrVuBJaIonlrULkIUxQJBEKKB9YIg7BdF8fjpJ4qiOBeYC9CvX7+//Kg8GUMMbpXHGHDcikKEmmhpWw+fHqidFTiUdigtQ3SIeHsJ5D80A0EBnn29MfyyFMMvS9EOe4eiwyVNLLotjJ9Nx0pJ7uLbrH5X3sEKlGoFwTGSUO7Sx49dq7KpM1og4wAFTzyJ3WAg6M03cb3iaqzvbmD1wSKmD+/SpB1dhAeGXUew5uV12INRRkZGRkamoxAvY1dpAJPFjsFkbT3DdHY2AJpISRCrIyJQh4RQk5KC15QpHTauu4dGsjg1j0U7c5lxRfOwr0sBh0OkorAW/fGqRgFsLJOsIEqVAv9INxJHhRHYIIB1bh0r/LUuagZeH83GhUfJ2FVCTL+ADu3vrIx/Dw4vhxVPQOEe6b2q5cRu7YHJYmfJrjzmbckip7yOCB9nXrs+jslJYeg0kjV49u1JbDtezmsrDvHYoj18tTWbl67tSe/TDEsyrXMiTvjTDRnY7CIPDnoKDu9kbO5HIIzt7OF1GJ2RarMAOFUJhjZsa4lbgUWnbhBFsaDh/0zgD5rGF3cYJ2KIBUB0KFA7RJSiiLP5xH4FQ5RuVLhb0VXrAbB+/iHWggJCr/MkaHwwXdasxmPiRNxKjlCw4zjF77yLraKixf6Ol9ZQaDA3c5cGyD1UTnCMJ6qGH4Auff0QRTgwexk5d9yJoNUSufg7PCdNJMzbmbhgd1Yd0DdrRxchlWMy7dnzVy+PjIzMZUp2djY6na6x5jDAPffcg7+/P/Hx8U2OffbZZ+nevTsJCQlMnDiRqqoqAKxWK3feeSe9evWiR48evPXWW2ftd9asWXTt2hVBECgrK2vc/u2335KQkECvXr0YMmQIe/fubdz30UcfERcXR3x8PFOmTMFsln7Ap06dire3N0uWLPlL10Km87hcY4gbSy61WoO4wULsqQRjEYIg4JKcTN2f2xGt1g4bV/dAd5K7+vD11hysdkeH9dOeOBwiBUcr2bEii2WfpDH/qU0sfn0HGxcdI+9wJb5hbiRP7sqNzyVx/8fDmfRMEoMndiWqt1+HiWHRYqEuNRVHw29Vz6HB+IS6svXHDKwW+1nO7mC0nnDb9zD0Kdj9lWQtNha1ezflNfV8+Psxhry9jheXHsTTWcNnU/uy/ukruH1wZKMYPsHgLj4sf3Qo79zYi5zyWq7/NIWnvk9rrNct0zKiKLJiXyEjP9jIe2uOMizGl9+fGs5TE/qjGPUy5G2H/e1T9eFUrHo9NZs3t3u750pnCOKdQIwgCFGCIGiQRO+y0w8SBKE74AVsO2WblyAITg2vfYFkWo89bleUp5RdcthPugvo6k8ek2yDEg8L2vpSlFhg2zoCX5uJc0hDmabgYIJmvkrkTVdh0vqi/3YJGaNGU/Lxx9gNhib9bTwmTfCGxzb106+uMFOpryO850n3KE8PARehluOpelxHjCBqyQ9ou3dv3D8uPpA9uVXNfgycAlxRuLhQJ2delZGR+Qt06dKlSQbnu+66i9WrVzc7bvTo0Rw4cIB9+/YRGxvbKHx/+OEH6uvr2b9/P7t27WLOnDlkN1i2WiM5OZm1a9cS0eAKeoKoqCg2btzI/v37efHFF5k+fToABQUFfPLJJ6SmpnLgwAHsdjvfffcdIIno6667IAULZNqZEzL47+nEd3ZOPNdbtxDnoAoMRLHmaZgzHCpzcElOxlFTg2n//hbPaS/uSY5CbzS3uCB/sSA6RAozqtj03TG+fD6FXz7aw85fs6gzWIgdEMiou3sy7bXB3P1OMuMe6EXiqHACoz1Qqjpu+izabNRu3UrhCy9wbNhwcqbdTv6jj0m5YRQCw2+Joaaynt1rcjpsDG1GoYRRL8NNX0HxIZg7AnK3t0vT2WW1/Ovn/Qx5ez2frEsnKcKLxdMH8cuMIYzvFXRG112lQuCW/uFseOYKHhzRhRV7i7jy/T/4ZF06ps5eSLgI2Zdfxc1ztvHIwj2469QsvH8gc27vR4RPQ9KsxKkQ3Ad+fwnqa9qtX3t1NXnTH6DgmWexV1e3W7vnwwV3mRZF0SYIwiPAGqSyS5+LonhQEISZQKooiifE8a3Ad6Ionrrs2wOYIwiCA0nMv31qduqORCEoEQFBFBEbBLFDAO0pGnNwTTWfu6lRiDa0VYfxnX4/njfcAHM/btJWSJ9wdm4ox+WDz9GunE/57DlUfrsQ77vvwltjR4kUPxzt50KoV9NEGXkN5ZbCGgRxfXo6+Y89jo8ikbzwkfi+Mx3laZmtx8YH8f5vx1hzUM+dQyIbtwsKAV3vBExpe5GRkbn02fDlXEpyMtu1Tf+IaK68a/o5nTN8+PAWBe2YMWMaXw8aNKjRIisIArW1tdhsNkwmExqNBnf3M+dY6NOnZeegIUOGNOkjPz+/8f2J9tVqNXV1dQQHB5/Lx5K5CDk5Jb48LcTFxrMJYqnkEtZCqC2BbyfjctMSUCio3ZKCc9++HTa2K7v5E+njzOdbsriu98XzXRNFkdLcatJ3FpOxq4SaynqUagWR8T507RdAWE9vnHQXdnosOhyYdu/GuHIlxjW/YS8vR+HiguvIq1AHBlE+dy76V18lcOZMgmO86NrPnz2/5dJjcBDuvroLOtYWibsBfGPhu9sa4orfhX73nFdTu3MrmbsxkzWH9KgVCib2CeH+4VHnlbHcTavm+XHduW1AOG+tOsyHvx/jux25/GNcd67rHXzZxxcXn6gnvCsfX1cNb0/qxU39WqgnrFDAuPdg/ijY/IG0CPIXEa1WCh5/nPrMTMLmzEbp1rkZ6TslhlgUxZXAytO2vXTa+1daOG8r0CkpC5WnJNUSHdILgzM4nWIh9jUW06NKyuSscTbg98RLpzcDgF+4G4IA5WZnBn70EeYHHqD0P7Mo++Q/VOqUuERrSKr7jJv8XCl6cV2Tc48Z4tAq3Kn/7B0KHQ6Mq1ahcHGh1/MTyF1hIudAOd0HNU2D39XflRh/V1YdKGoiiBFFdImJlM2eg72mFqXr3yd9uoyMzMXN559/zi233ALA5MmTWbp0KUFBQdTV1fHRRx/h7d1ykqBzYf78+YwbNw6AkJAQnnnmGcLDw9HpdIwZM6aJQJe5NDkRQ3y5TmuLDGd3mXa7+mqgENyCoTIb5cqH0PWKpzYlBb/HHu2wsSkUAncnR/HysoPszq2kb7hXh/V1NkRRpLyglozUYtJTizGWmVEoBcJ7ejPohi5E9fZFo73AIlgUMe/fj/HXlRhXr8ZWXIyg1eJ6xRW4jx+H6/DhKLQNf1eFQPnsOahDQvF98AGGTOpK9t4ytv6YwdgHLpJM3gE9YfoG+PF+WPEkFKa1Oa7Y4RBZd6SEuZuOszO7EnetiodGdOGuIZH4n6GkWFsJ93Hmv9OS+DOznJnLD/H4d2lSfPGEuGaJay8HzFY78zal89kfx7HZRR4YEc0jV3bF7Uz1hMP6Q+8psG0W9JkGPl1aP/YsiKJI0UsvU7t1m5TvKDn5vNtqLy72pFoXDYqGsksAJ1J8VbmCT33DY9hSi7nExIQUJ7ZGK7CGuyAoWnap0WhVeAe7UpJlBEDbvTthn87CtG8fpf93DzXZJvrYD+Nl0FCTfYqrNgLF3QYRYDxM7YFNADj360fQ66+j8vPDNWUrx3eXNhPEILlNz9qQQXlNPT6uTuAVBRlr0fW5DhwOzAf24zJoUDtdLRkZmc7gXC25ncUbb7yBSqVi6tSpAOzYsQOlUklhYSGVlZUMGzaMUaNGER0dfd59bNiwgfnz57NlyxYAKisrWbp0KVlZWXh6enLTTTfxzTffMG3atHb5TDKdy+UaQ1xsNOPmpMLFqfl0zm4wYK+qkizE4lbwi4U+r8GP9+LiPYCyjYXYDQaUHh4dNr7JSaG8/9tRPt+SRd/bLrwgrtTXkrGrhPSdxVTq6xAUAqHdPEkaF0l0oh9alzMIgA5AFEXqjx6VRPCqVVjz8xHUalyGDcP92Wdxu/IKFC5NjRM1dXX4PPoY1sJCSj/+GHVwEB7XXUffsRHsWJ5F/pEKQrv/9QXEdkHnBbcthg1vSJbEkkNSaaZW6hWbrXZ+2VPA/zZncry0lhBPHS9e25Nb+ofh2sI9/VcZFC3FFy/Zlcd7a45xw6cpTOoTwnNju18W9YtFUVIyYz/eTLbBxti4QP5vfPeTrtFnY9QrUiK1316AKYvOdnSrlM36FMPPP+P7yCN4Tpp43u20J7IgbiNKhbLxcXvCZbrSRSCwUtpmyzlM3iZvTL6RCEqBCuOZ3RYDotw5vrsE0SE2puXXJSQQfmsY+foS7q16gbSXR+OsOfkn0mcasL27i7hnpxHT7+lmbUYn+nFwcyEWs63ZSufY+CA+WZ/Bb4eKmTIgHMa8DvNGojP8DoApLU0WxDIyMh3Ol19+yYoVK1i3bl2ju9rChQsZO3YsarUaf39/kpOTSU1NPW9BvG/fPu677z5WrVqFj48PAGvXriUqKgo/PylR4aRJk9i6dassiC9xxGYvLi+KDKbW3aVPJNSKjICsho29JoMhH5dFb1Dm8KX2z+24X91xnhIuTiqmDAhn/pYsCqtMBHt2vHuvscwkieDUYsryakCA4K6eJFwZSnQff5zdL3wZqPrMTIwrV2FcuRJLZiaKBuz0AAAgAElEQVQolbgMHozvjBm4jRqJsiFExOEQSS+uJjWnkl05lezLLmZJzZ38IgxmTexT3BWbg/jPf2H38qXP6IEc3lrE5u/TueVf/VEoOyMtUAsolDDyJQjqDT8/JMUV3/w1hJ+cYxrqrHyzPYcvUrIpq6mnZ5A7/741kfG9glB38Oc4EV88vlcQn/1xnPmbs1h1QM+DI7owfXh0syRdlzqiKHKoyMjyvUV47srkQcBdp2bRzf0Y3MXn3BpzC4Thz8DaVyBjLXQddc7jqfrxJ8o+/RSPiRPxfXjGOZ/fUciCuI0IDfnHBPGkIDa4gkov4Ki3kP/cK9jrFdRfeQ3CoR3UV2Scsb2AKHcObSmkqqQOr8CmKzNGs5X+UV5NxDBA7sFyBAHCerS8Etilrx/7NuSTc6C8WTr+HkFuhHs7s+qAXhLEoUmQdCfK3fPRRCRRJ2ealpGR6WBWr17Nu+++y8aNG3F2PpkfITw8nPXr13P77bdTW1vLn3/+yRNPPAHAyJEj+frrrwkJCWlTH7m5uUyaNIkFCxYQGxvbpI8///yTuro6dDod69ato1+/fu37AWUuOMJp/19u6I31Zy+5FHGKIAZIfhxdeTaKTauo/WV+hwpigDsGRzBvcyZfb8vh+XHdz37CeVBTWc/x3ZIILm7wvguIcmfoTTF06euPq1fHlQNqDUt+fqMIrj9yBAQB5/798b7jDtzGjEbl7U2dxcaOPAO7UtPZlVPJ7twqDCYp+7e3i4ZhISrca+u4kXUE6Gt5pOudvFlUTO2Mh/nf5OeJiuiK+24DW3/LYei4qAv+Gc9Iz+tPiSu+Fsa9Q36XW5mfks3inXnUWewMi/HlgeGJJHf1ueDxvG5aNf8YezK++KO1x1i88+8TX5xRUs2yvUWs2FtIZlktKoXAW/4aqIKfZwxBqTlPi/igGbD7a1j9f/DQCFC23cuiZksKRS+/jMuQIQTNfPWiusayIG4jSuHkipFoBxRQrQNlvUDRGx9gOnyckOQqDquCMDm54VRto7aqEhfPll2EAqKk1cDibGMTQVxvc2C22hke01K5pQr8I91bdfEJ7OKJzk3N8d2lzQSxIAiMiw9k/pYsDHVWPJzVMPJlOLQMnUcVNWl7EUXxoro5ZWRkLk2mTJnCH3/8QVlZGaGhobz66qvce++9PPLII9TX1zN69GhASno1e/ZsHn74Ye6++27i4uIQRZG7776bhIQEHA4HGRkZLcYTf/LJJ7z77rvo9XoSEhIYP3488+bNY+bMmZSXlzNjhrTyrFKpSE1NZeDAgUyePJm+ffuiUqno06dPYwZqmUuXk1mmL08TcbHBTKy/b4v7LNk5oFCgDgtrukMQEK59H+fPt1G7Yw/ikVUI3cd12BhDvZwZGx/Ioh25PDaya7PF/vPFVG3h+J5S0ncWU5hRBSL4hrky6IZoYvoFdEqyKatej3H1aowrV2Hetw8AXWIiAf/8J25XX02Z1o2UnEpStxSxO/cQBwuN2B3SvRvj78q4+ECSIrxIivAiytcFwVQJ7wIh/RhasJ3tcQr+vPZNVM8/wz0rPuHJ4Y8yTOWDaVkmL+3OIj7ai36RXvQN96J7oBuqzrYa+/eA+9dTvfBu3H59ihT7chbb72Zs7wjuGxZNz+AzJ0+8EIR5O/PZ1CS2Z5Yzc4UUX/xlQ/3iPp0Y934+5JTXsmJfEcv3FnJEX40gwOBoH+4bFs3Y+EC8dx+GdZwxQ/dZUTnB1W/Boltgx1wY/HCbTjMfOULB44/j1KULIZ/8G0F9YcMVzoYsiNuIwiEgnkiqZQdUUOskoLALGNesx+/GQbirf8JQBhVO7gRVw/5D2xg0ZHyL7XkFuqDWKinOMjaJ+a2sswA0qz9srrVSkm0kaXxk62NUCEQn+nF0RzE2i72xTvEJxsYHMmdTJuuOFDOpbyg4e8OoV3A+8DwGgyeWrGycoi+yFUYZGZlLjkWLWo4tysho2XPG1dWVH35oXt/w0KFD3Hjjjeh0zSe2jz32GI899liz7fPmzWPevHkt9vPqq6/y6quvnmnoMpcY4mUqhAFsdgcl1eYzukyrg4NRaFpwEVaqcJ08g5o33sb6xX1oHlsOIR2Xcfqe5ChW7tfz0+4Cpg2KOPsJrWCutZKZVkrGrhLyj1QiOkS8Ap3pf00UMf38m3ncXQhs5eUY16zBuHIlptRdAGh79sTnqacoSRrKxnodqTmV7P7iAAVVJmm/WkFimCcPjehCUoQkYD2czyAQet0Egx5C/fODDLM8h/mzd8mZ/jjfHF9M8bMfsOebbK60aliRVc6yvYUAuGiUJIZ7khTuRVKkN33CPXE/U9KkdkYURTYeK+V/mzPZlnEv/3Dy4gHlT0wMMaAZ9y2cpZLAhWZgtA/LHhnKj7vyeXfNUSZ+tpWJfUJ4bmw3gjwugkzerVBYZeLXfUWs2FfI3nyphGtShBevTOjJ+F5B7ZKUrBmxV0PX0fDH29K96ep/xsOtej15DzyIws2NsLlzULq6tv+Y/iKyIG4jJ9bYBETK7XZ0SqhtuMc8xo3CZ7gHpm2+1BqslLoGEwSkHUxpVRArFAL+Ee6Nrj0nSDkiYjZ7crNP0xs4/0glogjhPc/s7x/dR4ojzj1UQXRiU1HdO9STIA8tqw7oJUEM0Od2dHHzYWcpph0psiCWkZE5J5RKJQaDgcTExCa1iNuD+Ph4Pvzww3ZtsyWmTp3K1q1bmTx5cof3JdMxXI6+TWU1FhwiBJwhw7SmoU73ltyr0NeGEfhDOoHRHgR19cBlxJXwxtvUlHvivfBmuG8teEV2yFiTIrxICPXgi5QsbhsQjqWult9mTqeu1kxw0hUEJwwkOLY7zu7NE3xZzDay95WRnlpC7sFyHHYRd18tfcaEE9MvAJ8Qlwvu3WavqqJ67VqMK1dS++d2cDhQRXeh5rZ72BWdxCaTM2l5VdRlSvlkAt21JEV6cd+wKJIivOgR5H7usbK9JktJqxbfjjblYUJe/xd5z84k9Iu3sI18mgObClnxfwOod1OxqyEGOTW7klkbMnCIIAjQLcCNvhFe9GuwQod7O7f7tbPYHCzfW8j/NmdyRF9NgLsTz43rya0DxkLWRDQ/PwhzGuKKIwa3a99/FaVC4Ob+YYxPCOKzDRnM25LF6gN6HhgRzQPDu1w08cWl1fWs3C+J4J3ZUjKjXiEe/HN8d65JCCako2P1BQHGvgWfDYJ1M+H6Wa0eaq+uJu/+6Thqa4n49lvUAQGtHtuZyIK4jSgcJ1+XOWyEqmBnrMDtZXUE/vNJhM1vU6ZOBMDk5ketVsSYlX7GNgOi3En7LbfRmpu5eyfZJSpAxa//focJTz6PUiWt5uUeLEejUxEQeeY6XSHdvHByVpG5p7SZIFYoBK6Ok9yWauttUlZKhQLNtI9RfDsF0+oFeN4qJ5iRkbnU6Mxwh7CwMPLy8jql7/bi22+/bXWfKF6+FshLA/G0/y8f9A01iINasBCLooglOxuP666jtqqefcX9cNHUcGBTAXvXSd9XNx8trkkPUSrU0KNuJT7f3Izi3tWS91g7IwgC9yRH8cTiNNbuOkbR4k+ozDfg51TNrt9Ws3PNbwB4BYUQHNuDwK7dEAlEn6Ug50AldqsDF08nel0ZSky/APwj3C68CK6ppWb9Ooy/rqRmawpYbdQHBHNsxA386hvPJps7Yh0oD5npEaTmpqRQkiK9SYrwaj+B0nUk3Lkcvp2M68F/EfjkDPTvzyUiIJR056vY/H06E5/uS6iXM9cnSnkXauptpOVWSSI5t5LlaYUs3J4LgK+rE0kRnvSL8KZvhBfxIe44qc5P9FWbrSzakcvnW7LRG83EBrjy3uQErk8MQaNqEP89rwPfGCmu+Csprph+90oC6yLC1UnFc2O7M6Uhvvjjteks3pnH850YX1xZa2H1QT0r9hWy7Xg5DlFa4HhmTCzXJgQT6XuBvSN8Y2DQQ7B1llRzugUPE9FiIf+xx6jPyiJ87hy03WJbaOjiQBbEbUQSxNIXwGIXcSih0k1AGV8nuSNV6ykX46WDvTTUuLniVFKF2WamNWeFwCh3HA6pQLx3sIY1c2eh09iJC7KQmrqdXz95j2sf/weCQkHuoQrCunudNYugUqkgKsGXzL1l2G0OlKqmx4+LD+TLrdlsOFrCtQnBAAihfdF1CcB0+DgU7YOghL9wpWRkZC4kWq2W8vJyfHwufFKSvzuiKFJeXo5W+/cvx3GpcmK9QrgMbcR6g+R+25KF2F5RgaOmBk1kJIe3FiGi4Pq4n3C790vK8mooOl6FPtNAQWUMRQ4n9hcNQq03EfDKLwQlDyMwxpvAKA80uvabJo7vFcQnP29l16yZuGBhUo8SwqOCsWq8KN69jnx7OJmEc2TrVg5uXAuAoNDi7hdFl37xdBvUh6CYMNROF+776DCZqNm4kapfV1KzcSOCxYLR3YfNXUewOqAXGR4huOnU9A334skGy2vvMM8Wy2C1G6FJcM8aWDARr9IPsU65hfJF3xE3LYZdGYFk7CppkkfG1UnF0BhfhsZIseZ2h0h6STWp2ZXszqkkNaeSNQeLAdCoFCSEeJAU6SW5Wkd4SaU6z4DeYOaLlCwWbs+lut7GoGhv3prUiyu6+bX8TPLvAfdvgJ/uh1+fbqhX/D6oL77f2VPji1/79cLHF1ebrfx2sJgV+wrZnF6GzSES5evCI1d25drewcQGnNlI1uEMfw72LoZV/5DuyVPKzYqiSNGLL1G37U+C3noLlyFDOnGgZ0cWxG1EIUrrz4IINpsDR8OV0554GlfrKbNei4uHBncPJwy6YLqW1bAjbxvDW2kzIEpyDSrONnLwjxXUVVXQI7SOpAgBl+H3snHBfFZ9+iEDJj1AbVU9YT3btmob3defI3/qyT9aSURcUxfrfpHe+LpqWHVA3yiIAXRXXE/Z7P9h/+lJlA/93uSmlpGRuXgJDQ0lPz+f0tLSzh7K3xKtVktoaGhnD0OmFURBegYLl6ElX2+QLMQtxRCfKLmkjgjn0JpCQtyy8dQZQKUgIMq9MbGn4Tc9x597EfXz71NRZqboiEjqqhzEVXkggE+wK0FdPAjs4kFQFw/cfLTnvfBWkXOca3KWUGexM/ipFwlPmYbDJRh9jzdIr9hP5sEa6u06XNzrCO1qxyVIxGTIpfDYYfb9/gP7fv8BhVKJX0Q0Id16ENzwz8275aRi54vDYqHo9w0U/bIcp+0pqCxmKp3c2BQ6gI0hiZhjutMn0od7IiTrb4y/K4q/kqTofPCLhXt/gwUT8av4GtsVoxG/eR2vCf9m648ZRPbyRe3UsqVXqRDoHuhO90D3xnjukmozu0+4WedU8vmWLObYJXfvKF+XxkRfSRFedBVFFEBBlYkPv9/Lsr0F2B0i43oF8cDwaBJCPc8+fp0nTFkMf7wJm96T6hXf8g24B5/93E5gYLQPyx4eypLd+bzXEF98Q2Iwz43t3u6lxOosNtYfKWH53kI2HC3FYnMQ4qnj3mFRTEgIJi7Y/eJZ/Na6S7WJl86A/d9D71sbd5X9ZxaGpUvxffQRPCfe0GlDbCuyIG4jCofYGKRktzuwK6U3jYK4Rk9ZnT++EW54OcNxoQuxYjrb9q9rVRA7u2tw89ZyfFca2btXYo8bjpttOc4aDf2unYjdamXLd19jKK1HFAcTHte2emFhPbxQOynJ3F3STBArFQJj4gL5ZU8BZqsdrVr6wXQeMBhmz8O0bz+uad9C39vP+RrJyMhceNRqNVFRcuy/zGVKo4X48qPIaEajVODt3DxpliUrG4AShz/VFUUMjm45vt910EB0VgM+xTvo9fjjkPJvLGumURz7D4o8b0CfaeDoDj0HNhUA0rzFP8INpfrcFs1rKjLI27cAldoFg9cE/lhRTVntPRRldsP0expqrZLopEi6+hwm7NhLKI0FEHETTHkFPEIx1VRTdOwIhccOU3D0EPvWrWH3qmUAuPn6EdKtJ8Gx3Qnu1hO/8EgUyjO7/YqiiMFkpaDKRFGVGX15NZbUnXhu30jEoR04W0zY1c6sDU2koM8wvIcMICnKl3vDvfBzu/AlnFrEIwTuWY2w8GaCbCuxxg0kKmUWuxMeZ/eaHAZe1/Y67v5uWsbGBzE2Xkryarba2V9gaIxDXn+khCW78gHoqy3kJ+D1Xw/zh9KL2waEc+/QaMJ9nM/QQwsoFHDVCxCYAL889Nfiiusq4I+3oP994Nft3M9vAwqFwM39whjfK4j//pHB/zZnsfqgVL/4r8YX19vsbDxayvJ9Raw9VIzJasffzYnbBoQzoXcwfcM9Lx4RfDq9p0DqfPj9Zeh+DTi5UfXjj5R99hkeN07Cd8bFU2v4TMiCuI0oHFJRB1FQINhErOoTgtgBVjM2Ux2VNW5Ehrri7WRhA74kA8fT98IZPBr8wnUc2fIDHv4BrPcbSHLlr9QJIq7AwIk3Y7dZ2bZkEc6eFly9rmrTWFVqJZG9fMjcW8aI2xzN3KzHxgWycHsum46VMiYuUPocCQkgCJhs0biubbipOyCOSEZGRkZGpr0JsSmwmG1otJfPtKbYYMbf3alF66QlJwdUKo4dsaJ1VRPtlQ40F85Kd3d0CQnUbkmBxx+HIY+hqcolbOcrhI13hWvvx+EQqSisRX+8iqJMA+X5NTgczZpqFZPxEFUFv6DS+OAddhuiSY1YVkGl4EVwSD0xY/sREefTUBkjDurHQsrHsPU/cHiFVDc5+TGi+/Ynum9/AOw2G6XZmQ0C+TD5h/ZzJGUjAGqtDv/oGFzCu4J/JDUeIehNAkUGE4VVZgoNkgiut1iJK89iRH4aQwv34WGppU6tJat7PxxXjKLL2Ct5KNyn0XBwUeLsDXcsRfj+DkJt67BX9SSgfA971kCPIUHnXXpKq1bSP9Kb/pHeMEJaQMgqq2VXTiX5R62QDhMSgnhzwlV4ubSQxfxc6HndyXrFX10LY9+WhO25CMCiNKkE0K6vYNTLMPChDvN0dHVS8ezV3bm1fzhvrz7SGF/8j7FSfHFbvQWsdgcpGWUs31vEb4f0VJtteDmrmdg3hAkJwQyI8v5r5ZEuFAoFjHsX5o2EzR9Q4zSKopdexiU5maBXXrl4hfxpXD5Pjr+IIAKCQH7ICCIKfqS2YYHQSRShRk+lLQyHKOAb5oZ3TTVlgitoVDiKKyhwEwlppd3ais04rJX0vek5Ptr9E08EV9JTVPFVw/7+191C6srj1FXtYMNXc7nyzulturmi+/iTnlpCYYaB0G5N4xwGd/HBTatiw9GTgljp6opTTAwmiw5Mx2D9a3DtR+d3sWRkZGRkZC4AJxylQ+0KMlJL6Dn04nS57Aj0RnOLCbVAEsRiRDey95eTMDIMZZW91XZchiZTNutTbJWVqLy8YOw7YCiAVc+BRyiKbuPwDXXFN9SV+BHnFj6Q9ttK1n3+EyHdenDDcy+hdXEl8+g+/BfejNGzB8GPrQXlaVNRJ1fJctj3DsnqtPFt2P215JrZ6yYsDtAbLBQqfCgKSqJQF0dhRB1lxcXUFxxHXZaLd0Yhvof2o2gozFWh9qbaLQRn33CuUKvpXXSYyP1/ojFUIGq1OF0xAt9rr8FtxHCSnC4SC3Bb0bjAlO9Q/jKDMOsSarctp9Qrjs3fHuCax/u3SxeCIBDt50q0nyuEVUO6FBPOXxXDJ/DvDvevh5+mw8pnJIE7/oNzjyv26QJr/glHVsINn4HX+Zf4Ohth3s58eltf7hxcwcwVB3licUN88YSe9G0lvtjuENmeVc7yvUWsPlBEZZ0VNycVV8cHMqF3MEO6+Jx79vGLgdB+0Ps2zL/OpuCPZTjFxBDy748vulrDZ0IWxG1E2VA43a50QmMDY0MyN51DhGo9pVbJZdE31BXvvHoQBNyCQ/E21JCiqOHmFtoszswgd/9alJp43kz7EoXXTrwdSvYIVirNlXhpvSjKMKDQJNO1lyd7Vi1HqVIzfOrdZxXF4XHeKNUKMneXNBPEaqWC7oFuHC+tabJd16cPxpUrEadMR9gxG/pMg5Ck87tgMjIyMjIyFwwRa33rou/viN5gJj6keZkikASxPnQEDodI3NBgWNF6O67JyZT9ZxZ1f/6J+7hxkkCdPB++vAaW3AN3rTjnuYAoimxbsohtSxYSnTSAa5/4B2qNE9gsRG98jBqFmgfqHuQnFJw6ZbY7REqr6xusuGqKAl9EaRvDldkfEfnzdPb/8h4v1U9jjyOmSX+ezmqCPHSExCQR2G8oQZ5aAnUCLsZCRH0WhkNpFOccwlqxH4A8mx1TtxBC48cTOfYaAnvEobqEJu/NUKph4hzULr7EWOaSn/4bmYprydlTSESfS2SRSOcJU76TXJ83vQslh+HmBZJreFu55kOoyJSSPP13iFQaqM/tHZrFekCUN8seHsqPu6X6xZM+28r1icH8oyG+2OEQ2ZNXyfK9Rfy6v4jS6np0aiWjewYwoXcww2N9zzuz98WENf5B8mauR6G2ETZn9kVZa/hMyIK4jQiN+ToE1DYo0oBCFKULWFNMmS0SlVrAw0+Hd4W0YuYREoNPfhYpQkUzQVxjrmbRxy9jUttw1Q0lqi6L3eVDeD38B+5RFbKtcBvjo8eTe6gClVrJuIdnsMldReryn1BpNCTffObySBqtivCe3mSmlTLslliE09wuonxd2HC0aRIeXWIiVYsXYwm/BadDP0nZ/+5bB4pL/4sqIyMjI/P3Qzyl3JLddg5+vJc4oiiiN5oZ1aN5TU9RFKnPySU3NJaQaE88A84c26mNj0fh7k5NSookiEGyOt72veQGufCWc6pR7HDYWf/FXPb+9itxI0Yx5oFHT8b0rnsVCveQOfRT9q9148nFaSiEk+7MxUYzNkfTBGnOGi++dX+byS5buc34OT9rXiYn+Br0A/6Bb0gXgjy0OGuaT2frMzIwbt+DceUqLNnZiCoV9oH9qO0RS4VKQVFmBtt3bWP7rm0o1WoComMIju1O3PCr8A1v22e9qFAo4Oo30br4MeSbDyg0DWLjf/9k6ifXotS2kyW3o1Eo4Kp/SdVOfn4Q5p6IK25jhmJBgD5TIWoY/DIDlj0KR36FCZ+AW8fVv1UoBG5qjC8+ztzNmaw5qGdcfBA7siooqDKhUSm4qps/1/YO4qru/i3es5cqdqORvKdewIGWiCH5qI374SKtN9waf5+/RgfjsJ980GrsYFJJCbUEgOoiyq1R+IboEBQC3g0uJAqfEFQ2gQNmsDqJqAGL3cIPx35gww9f0qPYiYpRfoQWuWMzJOKuM9FHrcPDIZBSmNIoiIO6eqBxUjHyngex22z8+eN3KFVqBk265Yxj7tLXn6y9ZRRnGwmMbrqKHOXryvep+VSbrbhppVVRXWJvAOoOZ+A05g346T7Y/ZVUX0xGRkZG5m+NIAhjgX8DSmCeKIpvn7Y/HPgK8Gw45nlRFFcKgqAG5gF9keYVX4ui+NaFGfXlKYgNJitmq6PFDNO2khIqtGHU2rQMaYMLuaBS4TJoELUpW5vWNHf1h6k/wvzR8M1kKavxWXKL2KxWVn36Ice2bab/dTcy7La7TraXvha2zYL+9xF/1VTiDm1hzUE9gR5agjx0DIjyJshDS5CnjmAPLcGeOoI9dLjrVA1tXAX1T0DKx0Rs/Q8RK9ZD8mOQ/DgnprOWnByMq1ZhXLmK+mPHQKHAecAAvO+5G7fRoyWX8FOorapsjEMuPHaYPauWsWvFLySMGsuQm6fi7N6yBf6iRRBg2FN4OPswYNY3bFI8wdbn5zL0o4cvmVhOAHpMAJ8YWDwVvppw7nHFnuFwxzIprnjty/DZICkMMK5jsx27OKl45upu3NI/jHdWH2Hl/iKSu/ry9JhYRvcMaJxv/52Qag0/Tn1WNuGzP0O7+3FY/TxEbQXVJbIQgyyI205DBgkBAY0VLMqTGaZFYzFltkHEhkmp5k8IYounlK1PV61ml3sdecd+YM7eOZhKyrnhSAh+iXE8ff87bFp0lKpNBSTGeKASBAaLGrYVbsNYbqKyqJYeg6V2BIWC0dMfxm6zkrJ4ASq1mn4TJrU65MhePiiUAsd3l7QgiCWf7+yyOnqFSvs0kZEoPT0x7UnDa/Lrkhhe+yr0uA5c2resgYyMjIzMxYMgCErgU2A0kA/sFARhmSiKh0457AXge1EU/ysIQk9gJRAJ3AQ4iaLYSxAEZ+CQIAiLRFHM7uhxn6y2JF5WglhvPEPJpewcCoOG4qSB6D5+bWrPJTmZ6t9+w5KVhVP0KdmJ/WJhyiL4+gYp6dHtv7Qa12kx1bH0gzfJ3Z/G8Gn30P/U+Ul1MfzyIPjHwZjXUSgElj8yFODcShadGl+89hXY+A7WTV9jVI7BmFaI+cBBAHR9+xLwwgu4Xz0GlV/r18DF04uYAUOIGSBZIE3VRrYtWUTab79yJGUjg268lT5jr0WpusSETNKdxD/rw9H3D3LIGkXkex8Q9twznT2qc+P0uOLCNLjmHOKKFQoY9CB0uQp+fgB+uBOO3ATj3wNdx9YQDvN2ZtZtfZsuMP0NkWoNv0jdn38S9PZbuAwdBv5vw8KbYMccGPJoZw+xzVyCkdudg+MUFx6tFeyaWHxrJXeA6vI6LKIzvuFSOukTgrhW54ugUOBrVPOQI4+Z22YS4OzPXfmD0GldufGh5wFwD3VBJUKih+RvP8ThRKmplNRdhwEpHvgECoWSsQ89QeygoWz85nP2rF7e6pidnNWEdvfm+J5SxNNqNEb7SYI4s+xkHLEgCOgSEzGlpUmrcOPfB0uN9NCRkZGRkfk7MwDIEEUxUxRFC/AdcP1px4iAe8NrD6DwlO0ugiCoAB1gAYwdP+STdYgB7LbLpxbxiRrELSXVMqbnUOqbQGwfL1RtzJDskiwJwtotKc13RgyBibMhd5skaltIMV1nNPD9zH+Rd3AfY2c82VQMOxzw83Sor4HJn4NaynysUAjnXb/XZtVRYR5J9v4RZGBDQRwAACAASURBVCwSKPnmdyg9hv/0W+m6fh2RC7/Fe9rUM4rhltC5uXPV3Q9w53uzCIrpxsYF8/ny6RlkpG5vNo+62BF6XsuVD/XGrnIidUstVV//t7OHdO5oPeDWRTD8OUj7Br4YJyV8Oxf8YuHe3+HKF+Dgz/DZYMhY2zHjPY2/sxgGKP3kEwxLl+H3+GN43tBgfY8dAzFjYOO7UFPSuQM8B2RB3FbsJ34IBTQ20Ghvp2fBNQCUlUsrh76hkiB2dVKhVgpU1It4B4cSa9TSAy2fjvyUZ1S3YczM48o778fFU1qhqnCSvjARSA+uIaIkqI/tK8DFQ4N3sEuToSiUSsY/+gxd+g1i/ZdzSd+xtdVhd+nrR3W5mbK8pgm0wr2dEQTIKqttsl2XmIglMxN7VZW0OjdoBuxZAHk7zvWKycjIyMhcOoQAeae8z2/YdiqvANMEQchHsg6fWP5fAtQCRUAu8L4oihUtdSIIwnRBEFIFQUgtLS1t6ZBz4xSNcllZiBsEcYB7c0GcfsiEqFARN6Zrm9vThIaiiYigNqUFQQwQPwlGvyYJirUvN9llLC3hu5eeozw/l+ufeYG4ESObnrv135D5B4x7W5pXnCe2ykoqv/+enLvuJn3EFRS//joOq4Df44/T5d+PEnWdBR/jh6i3vghVeWdvsCWMRWCqwic0nBv/OZNJz7+CQqFk6XuvseSNFynNzT7v8XcGPv2H02uAjsKgZI7950dql3119pMuNk7EFd/yLZQdk+KKc1qf97aIUgUjnpXy4mg94ZsbYcWT0iJNByHa7dRnZnVY+51N5Q8/UP7f2XhMvhGfBx9suvPqt8BqknIGXCLIgriNnLQQC4gICLjjUi+tPJYa3BFw4B0iCVdBkOKIK2st+EdG42PUsFAVSaK2O5sXfkVEQh96Dj9ZU/hwTR0mQURnlDJkBqCkq0cM9Tkqwnp6t7jCpFSpuObxZwnqGsvK/3xAUcbRFscd1dsXQYDju5uu0mjVSkI8dWS3IIgBTHv3ShtG/APcguHXp8BuO7eLJiMjIyPzd2IK8KUoiqHAeGCBIAgKJOuyHQgGooCnBUGIbqkBURTniqLYTxTFfn7naL1rsb1TXl1WgrjBZdrfrakgFkWRzApPvOrz8QlxO6c2XZKTqd2xA4fF0vIBQx6FpLtg6ydQIU30y3KzWfTiM9QZq5j8r9fokjSg6Tn5qbD+deh5A/S985zGA2Cvrqbql1/InT6d9GHD0b/0MraiInwffIDo5cuIXrYU34ceRHP1DHgkVbIkHlkBs/rB+jfAUnv2Tk5lznB4ryssvBX2/UBUz27c8d4srrzrAUoyM1jw3GOsnfcpdUbDOX+WzmLAlIFonZVkdL+ZvH+9hXnt1509pPOjx7WSC7XWQ4or3vG/U2Mm2kZwIkz/Q7qXU7+A2UMh9892H6rDbCb/kUfJHD+e3PunU5+R0e59dCY1mzejf+VVXIYOJejll5vrFN+uMOgh2PMNFOzqnEGeI7Igbiv2k+UcrGpXBEGJzuIDQJk5GE/XWtSak65JXs4aymst+EVGU1OvoK5eZO3/PgVRZPT9jzS5edLyDRh0AlX5J1eqhqpHobI6EdCt9bTlao0TNzz7Ii6envzy7msYSvTNjtG5agiO9WzRbTrK16W5hbhXPCiV1KWlSRucXGHsm6DfD6mft+FCycjIXFD0/8/eecdVVf5x/H3uBO5l7ylLQEQEJ4ojza2ZWo6Wlf3STG2btrVsaWVLTUsrrTQzd2qucgDugYqbvffe3PP746iAgDJE1O779eIlnHvOc56Ll3PO5/mOz2lYOgBy4lt6JnrubhIA5yo/O13ZVpVngNUAoiiGAQaAFfAosE0UxTJRFFOBEKBTs8+YagFidP8hQZySW4yVVo1KUf0xLuFCNgVocTVMavCYmh7BiEVFFB0/UfsOggAeV6K/pQUknItg1awZAIyb9SmOPr7V9y/OgTVPS4vqD3xV74ZIusJCcrdsIW7qVC4G9yBp5huUXrqM5VNP4rb2T9y3bcX6hRdQt65uvSTVF78lCWOfYZJ1zzcd4eSqWtO8a6U4W/KyTQ6XGovO80S+dgIdWumY8NnXBAwcSviuv1n24kSObFpLRXlZ/cZtQdRGSoJGeZGl8STVrjNxr8+h7N+79HnO2lsSxZ79pLriDVOhrLhhYygNYMAceHoLiDpYNgh2vAvlJbdkihW5ucQ+8z/y//0X01GjKDpxgsgHR5A850PKs7JuyTlakuKICBJefAm1lxeOX97Aa7jXdNDaShZY9f37a0H0grieVI0Ql6ilJlSKCiOKdRrSy1yxsqy+omqpVZFVWIpNK2mRfG9EGVEnjtLjkfGY2lS2IhdFkeOx2ahsDMhMLKC0Qvpgueb6IaIj1eLG6RZGpmaMnDkLXXk5az+ZTXF+zfQPj0AbslMKyUoqrLbdzUpDZHpBNaEs02hQe3tJdcRX8R0B7n2kVd67qB5Aj57/BKlnIe4gbHqp4avlevRUchhoLQiCmyAIKmAcsPG6fWKB+wEEQWiDJIjTrmzve2W7BggCzt2eaV/5zIvif6qGOCmnGDtTdY3tZ/bGoygvxLVVw+0Sjbp0AYWi7rTpKkSeOceaD9/ByMSUce/Pq2lTJIrSNSknQfI0NjS74Xi6khLydu4k4ZVXuBDcg4RXXqX4ZDhm48biumolHrt2YvPaaxj4+t68LtPMWTrnhO1gbC81VPrhfog9eNP3BYD3YHjpNDy9TfKwjQmF1eMx/C6QviZHeHLKOBxae7Pnl2V3TX1xm2AHrJy1XPafQJnOiLiZH1Hxz1ctPa3GcbWuuPeMK3XFgxpeVwxSbfzkEOj4JIR8BUv6QFJ4k6ZWlppKzONPUBQejuPnn+Hw0Yd4/L0NszGjyfrtNy4PGkzm8hWIZXf+QkptlCUmEjfpOWSmpjh/9x1yrabunQ1MoN8siD8M4b/frik2Gr0gri9XBbEApSqTa5vTytzJ19lgZVv95mNupCKzoBRrVzcAzsRVYN/am4CBQ6vtl5BdRHp+CY4eZogipOXbSKeLNSRdm8Ch7LCbTs3S0Znhr71FdnISG7/4qMaKpXuAlJZ2+Xh1MetmpSGvuJyMgupi3igggOKT4YhXo+KCIHXlKyuE7e/cdD569OhpAS7tuCtuOnruTERRLAemAn8DZ5G6SZ8RBOF9QRCGX9ntVeBZQRBOAiuBp0RJCSwAtIIgnEES1j+Koti0J8t6ohOrNtW686MQt4rknGLsrqsfLsorJfJEOnbJBzF0b9XgMeVaLYYB7W8qiM9k27D+h+VYOjkzbvbcaov81zj+C5xZC33eBOcuNV8HxLIy8vfuJXHmG1wM7kH81GkUhB3A9MHhuCz/Gc9//8HuzTcxDAhoXHMil65SzejIxZCXBMsGwJoJ9asvlsmgVTcY+hm8cg7Gb5DqqC9ux3LnZEbJfmLUfTbIKkqk+uI5b9/R9cUymUDPsV4UFohkPPMFJblKEmZ9ibjj/btzIVUmkz5bY3+F9Euw6cXGjaM2lrIXHv0DCjPg+76wd16jSgRLo6OJeeRRSuPjcf5uESZDhgCgsLDA/r33cFu/DsO2vqR89BGRwx8kf8+exs25hajIzSVu0iR0xcW4LFmM0tbm5gf5jwPHTlLfgZK85p9kE9AL4npStct0iarSwiiqpCtQ2VDrKpYaSRAbmZiiVeuQy2Dgcy8ik1UXzifisgFo3176YCXn2lJcbkBadB6iUx4hCTdfqQVw9m3HwMkvEncmnB1Lvq22WqkxU2Pnbsrl49UbmFy1XqqRNh0YiK6wkJKLFys3WrWW/P7CV0F0/eakR4+e24ipi+T9p8/i0NNIRFHcIoqilyiKHqIofnhl27uiKG688n2EKIrBoii2F0UxQBTF7Ve254uiOFoUxbaiKPqKojivJeb/nxLEucU1LJfOHUhGVyHikBSCqlXDBTGANjiY4ogIyjNr9kRLvHCWDb9vZ1uSN86e7ox59yOMTGuJ/Kadh62vg1sv6PFyrefRlZYSNWYscRMnkbdrF8YDBuD8ww+03rcX+1mz0HTpgiBveJS7BjIZtB8H045KEcVzf1XWF9e3oZJcAe73wfBv4LWLknjyGoxb1nbGW26kj3MyqRdOseL1aexY8i2FOdlNn3cz4OBpRuvOtpw5L0M7cw4FKQYkfbEUcdOLoKu46fGlRYVE7PuHdYt/5Nvz3fjn7wOUld6aNONG02YYPLsLLK60LJA10k3WawA8Hyb5H++eA8sGSkK7nhSdOUP0o4+hKyzEeuEyLhTYsGTqbL54ZDSb5n9JVlICBl5eOC9ditOihaDTETfpubumvlhXWkr8tBcoiY7B6Zuva5Yr1IVMBoPnQn6KtNBwB6MXxPVEvJb/LlSLEEcWXxHEbtV9es01KnKKyiir0NHdvZQB7ZVYOrnUGPd4bDZqhYx27haYWqlJSVMTT3dEEVz9rInOjSYxP7HGcbXh27MP3R5+lDN7dnFg7apqr3l0sCYjPp+ctMq0aXcrqT45Kq2Oxlonrqsj6vkamDpLdRsVd2e6hx499yxD5kJpofT3qUfPfwTxWhWxSFn2nR2BuFUUl1WQXVhWLUIsiiIR+xOxMilFW5CEqpVro8bWBAeDKFIQJmWniTodFw+HsfKd6ax8Zzrx0UkEWcYyctJTqAyNag5QVixFYZWGMHIJyGoXtRnff0/J2bPYvT+b1iH7cfjoQ7Q9ghEUjRQ0N0OlkSKKU49IomfvXEkYn1jZsPpGhUoST6MWw/RLyMetoENXPyZ4HCXALJ5Tu7aydMp4jvzyNRVldTQna0G6j/JAkMHpQi+spjxPTpQR6cv/lDx6a6nFLSsp5nzYfjZ+8RGLnn2crd9+TlpiEk5GORw7eJpfZrxI8uWLtZzpNmLtLYniBxeAffvGj2NkAaN/lKzBMi9LDbcOLrnp56PgwAEuTZhMgm0Qpx/4jN9+COffH2eRl3YYhcqOCwf+YdnLz7Hx849Iunge4z59cN+0EZuZMyrriz+Yc8fWF4uiSNJbb1N48CAOH85BExTUsAGcOkLA4xC2EDIuN88kbwHNdOW5B6kaIVabo6IAQRAp0FlhJMvEyN6v2u6WV7yIswvLaOdQDpraf9Un4rLxczRFpZBha5ZFfKY7htigMhDpGdiJLxIgJDGE0V6j6zXNbg8/Qk5qMqGrf8XMxo42PfsAUtp0yJpLXD6WRoeB0sqxo7khSrlA5HURYqWTE3JLS4qOH8d83LjKF1RGMOgT+P0xOLgYuk+t15z06NFzG7BsDX3ekHzDIzaA7/UWsnr03IOIlf+U5fw3BHHKlQ7TdqaG17YlXcomO6WQThaxCEZGKGwa18HboG1bZKam5O7bT4xazpHN68hKSsDE2pY+T03Ez74M1foJUJdw3fEOpJyWoqgm9rXuUhIVRcZ3izEZMgTzMWMaNc9GY+YMD/0AXSZKGTXrn4NDS6RnG5euDRtLaSCJ6zYPYDg8n74XttE+7A/2HIxnz6btnNzxF717+uAxZAKCvX+9m4o1J1pzAzoOduXghkjavvAopgmJpK9fj1KzE7Oih2Hcb5TLDYk+eYzzoXu5fOQgZSXFaMzMaXf/QLy798LBpAJhcTDRHT7k762H+e3tVwkaNZauI8cib64FjZthYAqBj9+asfweApfusHEabJ0udS0fsRBMnartVlJUTsSKfzi/6wKZAe+iQ4ci7l9K88PQmlsxdNonqDQurPl0H2r1GWLPHOPioVAcfXzp9MBDeIwfj+mDD5L+zTdkrVxJzqZNWE+dgvkjj9TdqKoFSPvqK3I3bcL6pRcxHT785gfUxv3vSs8l296Ax1bf2gneIvSCuJ7oqvgQFxlYYEgWSnkJJeVarJQxYFQ9QmyhkZpdZBaUUtdtqaxCx+mEHB4PkgSqbel+LugGcOk8OLWxwMPCHTuNHaEJofUWxIIgMGDSNPLS0/j7u68wtrTGydcPEytDrF2MuXy8UhDLZQKtLDVEpefXGMMwMKCy03RVfIZKhtv/fixdNOq44enRo6cF6DYNzqyHv14D157SircePfcwlbEbsT5Zn/cESVc8iKtGiM/sS0RlqMA64wS0atW4mluguKiQaD9vLlw6SWnUKWzcPBj6wnS8gnogk8sh4vo+a1U4t0USl0FTpChqLYiiSPKs2QgGBti+MbNRc7wlOHeBZ3bCqT+kRcRlA6DtqHqlDteKWgvtHsay3cOMeiKbqC3f8++WvWzYcRmXkCnc512OddcHpTpka+9b+lYaSkA/Z86GJLL/j0uMee89ylNTSDh4kGTlBaKPP8ylHFNKioowMDahTY/78O7eCyfftpUlfykRALh6OPHkvKfY/dNiwtasJPLYYQZPeaXWbMi7DhN7eOwPOPYz/P0WLOwGg+dS5jOa6FMZXDySQkx4GjpRjqGJE60DRWLPrSM7KY52fQdw3/j/Xcug6PN4B3YvN8R/wFC0ppc5+td6Nsz7AHMHJzoNG4HvzJmYjRtH6iefkvLRx2StXIXtzBloevVq9N/xrSJr9WoyvluM2eiHsZw0qfEDGdvCfTNg+9twcQe07n/rJnmL0KdM15cqKRPFBhYYko2JXKrVszJKlfLkq2CukVZ3MgvqTpk5l5RHSbmOAGczSD2HbcEOAEqLynG54j8c7BDMgaQDlOnqn6IsVygZ/upbmNrYseGzOWQmSnYs7oHWpEbnkpdZmRZTm/USSI21ymJia9YRCQIM/lRKmd7+Vr3npEePntuAXAEPfgtFmfD3my09Gz16mh2hivFSxX+khLgyQiwtvBfnl3H5WBreXWwRoyMbVT+ck5rM7h8Xs+T5pzidn4FpQTEjJjzP4x9/iU9wb0kM33CABNjwPNj5Q7/36t5t/QYKDx7E5tVXUdwCH+omIZNB+7Ew7Qj0ngnnt4J4C1ZVDM1we2g64xevo+9jj5Oqs2HFcSt2rFpD4VfBsCgY9n1+zcv5dqNQygl+uDWZiXns/zOEc5382d3Wjb/yvLiYIsdTm86oqZN57rvl9J84FRc//xr9b65ioNUyZOqrDH/lTXLTUlkx80WObF5XpczwLkYQoONTlD+zl0jlSP7+6SLLXtnF9qVnSD6ThGPsP3TXbadN/zxO711AWVE+I2e8x4BJL1QrJ2jT3Z52vR059W8KxlZdeear7xn6wnSUajU7lnzL91MncOL0cay+mi/VF4sicZOeI+7ZidV7+dxm8vfuJXn2+2h69sTu3XebLs67TJIy2bbNhPI7r5xAL4jriVjFdqnYwBIjWRYm8hQArExrpmlZVokQ18XxOKleINDFDI6vwEoVj1whfeCcfaXITneH7uSX5XMq7VSD5mug1TJy5iwEuZy1n8yiMDcHj0Dp5hNZpbmWu5WG6IxCKnTVuwzWWUcMUvOCHi/D6T8h8t8GzUuPHj3NjF076PEKnFwJF7a39Gz06GlWqt667oVn8PpwLUJ8JWX6/MFkKsp1tAmyoTQhoUGCOCXyEpu//JSlL0zk5I4teHUN5tHp79I5Kgnz5LT6PQTrKmDtROkh9+EfQVHTDgqgPCuL1E8/xTAgALMx9ct6uy2oNFK5ybQj0H0atB15S4aVKxQEDh/HhAU/Ezh4OKfznFga04PDCcaU7/gAvg6QrH5Cv22cbVAjEEWRxAvniD6xjvKCpRzdNJ+zYftw7diFzllF9I/P4X6LKNwOvow8vf7Oaa27dufJzxbg2r4De1YsZfX7b5KTmtyM76R5qSjXEX0qnZ0/RrDs43i2XhpBghiEt8E/9Mr8jKCdL+Plnc9Z61JC/1yFZ5duPPnZAtw7dK51vODRrbH3NGX38rNkJhXiE9ybxz/+ktHvfIiNmwchv6/ghykTOBxzAcul32P7xkyKwsOJHDGS5Pc/uO31xUVnzhD/0suovb1wnD//1qRwK1Qw6GPIuAQHv2v6eLeYFhHEgiAMEgThvCAIlwRBqJEzIwjCfEEQTlz5uiAIQvZ1r5sIghAvCMK3t2vOVQWxTq5CoyjATCE1u7K2rtme/VqEuLBuQXwiNhsrrRpHYzmcXIncZwA2rUwwtzPCxFK60XW174pMkBGS2PDOzma2dox8/V0KMjNZP+8DtBYKLBw0RJ6oFMRuVhpKy3UkZhdVO9bAzw8UCoqO1yKIAXq8BOausGX6HbnSo0fPf5per4F1G9j8EhTntvRs9OhpNgyUVyNX4n8mQpycU4xWrUCrViCKImf2JWDrZoKpkAPl5TcVxKIoEnX8CKvff5Nf3niJqBNH6DhsBP/7ZimDp7yCfacuqNzdya+HHzEAez+DmP0w9HOw8qxzt9S586jIz8fu/dkIsjswHmPqBAPmNK0xUy0Yao3p89RExs/7Fse2Aey9qODnzJFcdJ+KqKuQsu3m+8KyQXDo+1vuFCCKIilRl9n764/8MO0ZVr7zGuE7t+Lo44NSO4y2/d7lgRnv0uWbhQhFpcQd8aGiXA4/DpE8mOuJxsycB197m4GTXyI1+jI/T59G+K6/73iP5qvodCJx5zL5Z8VZfpyxn78WhBN9Kh3PDjYMfyGAJz/pTeuSDOThUaT7qvg7+TJZSQkMfWE6D7w0A0NjkzrHlitkDHzWD7WRgq3fnaK4oAxBEHDxa89Db8xm/Lxvad21Oye3b2HZa88Tmp2C8fffYT52LFm//87lgYPIXL78tvgXlyUkEPfcc8jNTHFedBOv4YbSuj94DYI9cyEv5daNewu47TXEgiDIkTwL+wPxwGFBEDaKohhxdR9RFF+usv80IPC6YT4A9t6G6V5Dd93Ss1GHAXg7H8V05zuY2dRcETI3kppqZebfQBDHZRPgbIZwfqvkf9ZhPPf3bVOlXhlM1aa0s2pHaEIo0wKnNXje9q29GTztVTbN/4RtC+bjFvAwR7fGUphbipGJqpr1krNFZYqHzMAAgzZtao8Qg9RBcvBc+G0MHFhQp7WCHj16WgCFWuq4ubSf5P83bH5Lz0iPnmbB01bD0Svf63Qt37TodpCSW4ytiRSFTb6cQ1ZyIX2e8KE0RkqvVLm61npchU7k3J5dHNm0lvS4GLQWlvR67Gn8+w1CbVT9oVcTHEz2H3+gKy1FplLVPZmYUNjzCfiPhYBH6tyt4OAhctatw3LiRAy8vBr2hu8RLB2dGTVzFlEnjvLv8h/Y+NdJnNv2oM/IOVhnH4TTaySXgK2vSz0g/B4C1x6NPl9GfCznQvdyPnQvWUmJyORyWvkHEjzmcTw6BaE2MmL/6ouc/CeO9n3ysPb2xvHrr4ib9BwJZwJx7ngeYcVIKervM6Re5xQEAb/7+uHS1p9ti75kx5JvuHzkAP0nTkNrfuf1tBB1IkmROVw6nMKl42kU5ZaiVMtxa29F6062OPtaIFfI0BUUED91CumHDnCuT1eSMtNxNchkoFcMWvv6CX6NqZpBk9qx7otj7Fh6hqFT2yOTSdcsaxdXBk95hR7jxnNs60bCd27lfOheXPz8aT/3I9Rr1l+rL7aZ8Tra3r2bpb64IieH2ImTEItLaLVsWf28hhvKwI9gQVfYNVtqVnaH0BJLdF2AS6IoRoqiWAqsAm7UDvURYOXVHwRB6AjYArc3F/A6QaxxckFhbo+T+jQY29XYXSmXYWKgIKuOCHF2YSmR6QXX0qUxcQSPvphaG2FuV/3GFOwQzJmMM2QVNy5lwqtrML0ee5oLB/aTk7gbRK5Fid2spXNFZ9SsIzYMCKDo9Om6V6S8BoL3UGmlpz5G93r06Ll9OHWEoOfhyDKI2tfSs9Gjp1kQqHwo1In/DUGclFOM/ZV06TP7E1EayGndyZbSmBgAVK41I8QRCQI//KNj20JpcWzQ8y/zv29+oPPwh2qIYQBNcHfE4mKKjh2reyKFmfDns1K22NDP69xNV1pK8nvvoXR2xmrycw14p/cmbgEdeXLet/Sd8BxpMVGs+Pgzdpw3pPDxv2FyGPR8FXLiYNMLkjVUA8hKTuTA2t/5efpUfnr1eQ6uXY2xpTX9J07jucUrGDVzFr69+qI2kgIgnYe5YqhVsu/3C4iiiDY4GPv336fg8HGSEu5DtG4jOYscW9GgeZhY2zD67Tn0efJZYk+d5OfXpnA+7M64D4miSEp0LiFrLrL8rVDWfXaMiNAkHDxNGTTRjwnzetB/Qltc/a2QK2SUZ2UR/fTTXDh7ipD2XqQX5NPvf1MYNfsrtBoVLH8Qtrwu2R7eBDt3U3qN9SI2IpODGyNrvG5saUXvxycwceFP9HrsaTITE9i0fAn/WhlR+PJUdKJI/HOTifvfs7e8vviq13BpbCxO33xTf6/hhmLpAd2mwIlfIf7ozfe/TbREl2lHoKp6igdq7XUvCEIrwA3YfeVnGfA58DjQ70YnEQRhIjARwMWl6R3vqqZMA2jMK+0O0NrWeoylVk1GHTXEJ+KkLPCuFoWwZxf0ml6nX193x+4sPLmQA0kHGOw2uFHz7zRsJDkpSZzcsRFjmyFEHjfHr5cj1lo1WrWCyLRaGmsFBpC1YgXF5y9g6Ne29oEHfSyt9Pz9Joxt2AVTjx49zUyft+D8Fsk+YnKoZJ2m555HEIR2oig2rPHEXY9IxX9EEKfkFuPhYUVxQRmXjqbi080epVpOaXQMMmNj5Obm1fa/+O9Gtp5RYG+lZsBLb+Aa0PGm0SVN586gVFIQElKH76goXVfyU+CZ7aA2rnOsjCXfUxodjfP33yMzNKxzv/8SMrmcwIHDaBN8H2FrfuPE9r84F7qXoFFjCRw8A0WftyDpBJxeC5H/3DCNOzc9jQth+zgXuo+USEkkOfr40vfpSXgF9UBjZl7nsWojJUEPevDPL+e4eCQFr852mI0aSVliIunffoty8kSs3c1h41QpkzH4xXq/R0Emo8OQB2nVvgPbFnzB5i8/5dLhA/Sd8ByG2ro/L82BKIpkJBRw6UgKF4+kkJtejEwu4NLWkm4jPXD1t0JlUFMSlSUmcvF/W9c7kgAAIABJREFU/+MYJaQ4W+Po4cmgyS9jZnfFYeW5fbBzNhxcBJd3wcjF4HTjRYy2PR1Jjc3j2LYYrJ2N8exYMwqrNtLQefhDdBgynHMhezmyaS3/7t6K1tUGn66BWG7bTcGIkZiPHYPVtGkozOv+P67v7yfpzbcoPHQIh3lz0QQ10IKsofR6DU6ukmytntlZozFxS3Cn2y6NA9aI4rW2f88DW0RRjL/ZxVwUxSXAEoBOnTo1uYBBvK7plMZSA1cbMBvXbj1kbqQk6waCWBCgXdpmacMN/NP8LP0wUZkQkhDSaEEsCAJ9n36O3LRUok5uo7TIkOICPww0SlytjGrtNF21sVadgti8FfR6FXbPgUs7wfOG6xR69Oi5naiMYPg38NNQ+OdDGPhhS89Iz+1hoSAIauAn4FdRFHNaeD7NR5VHgf9ChLhCJ5KaV4K9qQEXDiVTUaajbQ8HAEqjo1G5ulYTuylRl9myZAl2hoWMfu8blDbu9TqPTKPBKDCQ/JAQbF59teYOR5ZJ/qwD5oBjhzrHKYmMImPxYkyGDkXbs/Hpv/cqBlotfZ6aiH//wez9ZRl7f/2Rkzu30vvxCXh27obgcH3FoERBdhYXDuznXOg+Es9LFYe27q3p/fgEvLr1xMSq/h28fbrbc3pvAqF/XsbN3xqlWo7VlOclUbxoCcoPZmPmZy6V3xSkQfu6U+Nrw9LRmUc++IyD61dz4M9VxEecYsBzL+IW0LFB4zSGrOQCLh1N5eLhFLKSCxFkAk4+5nQa4opbe2sMNHU3iyq5dImDz0/ipImacpWWXuPG03HYiOodt5WGMPgT8B4MG6bA0v5SU8veM6QmUnXQa4wXGfH57Fp+FnN7IywdtLXuJ1coadv7fnx79SX65DGObPqTI6fDUfk446Exo+jPP8nZ/BfWU56X/ItvVN5wA9K+/IrczZuxfuklTB94oFFjNAi1MfSfDesmQfgqCHi0+c95E1pCECcAzlV+drqyrTbGAVOq/NwN6CkIwvOAFlAJgpAvimKzm9nViBBbVRXEtUeILTRqErKLoJYF0RNx2XhZG6E+tRLce0vCsg7kMjlB9kGEJYYhimKj6wZkcjnDXprBipmvkZ28iZO7/Og6vDNuVlpOxmXX2F9hb4/CxkaqI378sboH7v4CnFgpNdiaHCaZ1evRo+fOwLUHdHoGDiwE3xHgXHsXTD33DqIo9hQEoTUwATgqCMIh4EdRFHe08NRuOZV3QxEdsibdI+8G0vNLqNCJ2JioObMjEZtWxli7SNG20pgYDAMrBVReZjrrP34bQ6GYEaPvq7cYvoomOJi0+fMpT09HYWVV/cUjy6QF8KAptR/MVc/hWQiGhtjOnNGgc//XsHR0ZuSM94g+cZR/Vyxl4+cf4dzWn/vG/w8bV+n/rSgvl4uHQjkfupe4M6cRRR3WLq70GDce7249K6OWDUQmE+g5pjVrPzvG0W3RBD3ogSAI2M+eRXlyMkmzP0CxaCFaI0sI+xYSbpBGX9c55HK6PfQI7oGd2brgC9Z+/B7+/QbR+4lnUBnc2qyB3PQiSQQfSSE9Lh8EcPA0w7+PE+6BNhiZ3Fw0ZoWFsX3OO8SbG2Jl58DQV9/EysW17gPce8PkENj2Juz7DC7+DSOXgK1vrbvLlTIGT2rH6o8Os3XRKUa/0Qm1Ud3iXBAE3AI64hbQkZTISxzetJZzYfs579sKZ5kK5/lfYHW1vvi++xp0Dcz6fTUZixdjNmYMlpMm1vu4JtNuDBz+QfIB9xkGBnU3JbsdtESM+jDQWhAEN0EQVEiit4bTuyAIPoA5EHZ1myiKj4mi6CKKoivwGrD8dohhoJq3g6yiFLVGJXUkNDAFc7daD7HQKMksKKmxXRRFTsRl87DFZciJhcAnbnr6YMdgUotSuZjdtJoBlaERo9+ZjSBXcuyvXwCp03R8ViEl5dX99wRBwDAwkKLjx288qEINQ+ZBZiSEftOk+enRo6cZ6DcLjB2kFezymtckPfceoiheBN4GZgC9ga8FQTgnCMKolp1ZcyKg090dHW0bS/IVyyWzQpHMxAJ8r0SHdaWllCUmXuswXVZczPq5H1CSn8sIn3Q0/ac3+Fya4GAACsLCannRBkZ8d8NUx5x16yk8dOjO8By+S3AN6Mj4ud9U1hfPfPGagPxu0hPsWPIteRkZdB01lqc+X8j4ed/SdeSYRovhq9h7muHVxZYTO+LISZNcRwSlEsevv0Lt4UHCSy9T7DYB+rwNsfXvPH09tu6ePP7xl3R6YBThu/5m+evTiD93pklzByguKOPkrjjWfHqEFW+HEbbuMnKFjB6jW/PkR8GMfLUDfr2d6iWGz/+2gt/mzSZeo6Zzv8E8/vmCG4vhqxiYwogFMG4l5CXDkt6wZx6kX4JaOm1rzNQMnOhHXkYxO36MqJGJWhe27p4Me/F1nvn6e9oPHEqCXGS/tzOhGhknpr9K7IRn6l1fnL9nD8mzZ6Pp3Qu7d9+5vYuJMhkM/lTqqr533u07b13Tud0nFEWxHJgK/A2cBVaLonhGEIT3BUEYXmXXccAq8Q7p137tJisIqMrzpA+NR1+YEQOGZrUeY6FRk1VQxvVvIDqjkOzCMgaU7ABDc2ll5CZ0d+gOQGhC4y9EVzGxssbOsyeF2ZdJj43F3UqDToS4zJoNAQwDAihLSKA8La2WkargeT/4PiitjGVFN3mOevTouYUYmMADX0H6+TvixqOneREEwV8QhPlI99i+wAOiKLa58v091XK88gFOutNWlNXfeykus5DUvOJmmFXzcdWDuOxCLkq1nNadpQy1sthYEEVUrq0QdTq2LviC1KjLDLWPwObBNxvVP8DAtw1yMzMK9lexX9LagFwFoxaDtm6RW56VRercuRgGBmI2+uEGn/u/zNX64me++p4Og4dzLmQPGQlxdBw2kic+/Zqn539H8JjHsHRqen+cqnQb6YkgFwhZUymm5FotzksWIzM2Ju65yZR5PyG5FsgUoGncIodCpaL34xMY++7HIIr8Pmsme35ZRnlpwy08czOK2L/6Ij+/Gcr+Py5SUa6j20gPnpjTjYdndKL9/c5ozWv3xb6espJitr07k80bfkehUDB2+rv0enYKckUDPXh9hsDzB6TGs//MgW87wjwPWPkI7P8SYsKgTPo7dvA0o8eY1sScyuDQX1ENOo2pjS19n5rExIU/ETz2CfItzDjk4cCOrET2j3+MxNmzb+hfXHT6DPEvv4KBjw9OX3yBoGiBpGHHjhD4mNRcuCTv9p+/Ci1SQyyK4hZgy3Xb3r3u51k3GeMnpPqo20OVlRtVeZV62xusplholJRW6NDpRKq2yzoRl4UZeTin7IbOE+qVYmynscPD1IOQxBCe8nuqEW+gOu3uH0DS+e0cWLcBt1FShDoyrQBPm+qNDgwDpEYOhSdOYNK//40HHfgxXNwJ/3ws3SybQPHZsyTOfAP7OR9g2K5do8cpr9Ax4ecjZBaU0K+NLf3a2NLWweSeTqnT8x9CFCHhSpfGm32mW/eD9o/C/vnS4pVd4/+u9NzxfAP8ALwpiuI1k3lRFBMFQXi75abV/OjK67+GPm3lcSw0KpY9dfeUEaTkFqMSIS0iC+8gu2uNgCo7TLuyf9VyLh4K5T7nNDy83cGvcYJUkMnQdO9OfmhIZSq6SxDMjLvpc0vqp3Mlz+HZs+5Mz+G7AAOtlj5PPkuPR8ajUKqa/blFa66m0+BWHFgfSVxEJs6+kk2S0tYW58WLiXnsMeImTqLVr78gb/+IVD/bBJx8/Rg/9xv2rFjGkU1riTp+hMFTX8XWzeOmx6bF5nF8RyyXjqYiAJ6dbQjs74KVU+OadSVdPM/mj98jtyAfT7kBA7/+DoPrywQagsYKxqyA9AsQewDiDkpf569IH5kSHALAuSt+Tl1J7ezEkb+isXY2xj2gYQsNhlpjgkaNpdOwkUTs283hDWs4oVZw/kQo7g/vJuCRJ7AdP75afXFpQgJxkyWvYafvFiHT3EKv4YbS733o+84NG/PdDvRXqXoiXrNdElDrbt5aHaQIMUD5dWkQJ2KzGaMKRaYrrVe69FW6O3bnWMoxisqLbr7zTfDs6IpM5cWlw3twNJbkem2NtQzatkVQKik6XocfcVVMHaVOiLl1lYTXn8xff6Xk/Hnip0ylLLXxJvVL90ex90IaFTr4atdFhn2zn+BPdvPuhtPsvZBGaXn9owl69NxRFGXBqkel7pZ+D4NFPeoDB34Ihhaw/nmoqMNOTc+9wDpRFFdUFcOCILwIIIriPWUHcF2AmIqK+l/T0/JKOBabxR2SiFYvknOLaVeuqNZMC6A0WhLEF+OjObRhDf5tbOigOQeDPmlSB1dNcDAVaemUXKiSgnkTMVxw4CA569djOWHCf9Zz+FaiVKlv2yJ++/udMbEyYN/qC9X+lgy8vXD65mtKIiNJePFFRLF2V5SGojI0ov/EqYyc+R7FBfn89tYrHPhzFbqKihr7iqJIbEQGG748zuqPDhMdno5/Xycen9ON/k+3bZQYrigvY/+q5ax8+1VKs7K4z8yeB378tWli+CqCANbe0PFJyW932lGYfllKqe72vBRlP/Q9wh9P0Dt2ADYGsez8/hhZO5ZD8mnQ1fwd3AiFSoX//YOY8OUSHnztbcxae3HGypjVW/9k8+gRpGzejCiKVJQKxE2ehlhSisuSJShtmsFruCFoLGu1r73d3Oldpu8Yqub2q8X6CmIpzaJcJ1I1YeNEbBbfqPaATQew86v3HIIdglkRsYIjyUfo6dSz3sfVhqFWhZVLN1IvnSPxSAhWWlWtglimUmHQtq3UWKs+3IKLtq64mLxtf2PYqSPFEWdJmPYCLst/RqauX9rLVSLT8vlixwUG+Nqy+ImOZBSUsvtcKjsjUvjjSDzLw2LQqhX09ramfxtb+njbYHqDpgZ69NwxJB6H1U9CbiIMngtdJtbvb8/IQvILXf0EhH4t+V3quRcZD3x53bangK9u/1RuFw1Pmc4tLiOvuJz4rCKcLe4OS7Lk7CICS5VYOWuvNdMCKUKcZWfNwRVLcfHxpq/8VwTfMU1uoqcJlsq1CkJCMPC+ubjVlZSQPGuW5Dn8/OQmnVvP7UehlNNjdGu2LDrF6X8TaH9/ZQ9cTbdu2M/5gKSZb5D0zrvYf/zRLYv+uwd25sl537Jr6SJCVv9C5LHDDJryMhYOTlRU6Lh0OIXjO+LISMjHyFRFt5EetO3pcMNGVDcjPTaaLd9+TlpMFI6ZuXTr0guX92cjyG+N2K8VjZWUUu0zRPq5vASSwlHEHWDQxTD+ODiYrevh4X39UBkqJQsn5yBw7iJ9X48oqiCT4dk5CM/OQSReOEvYj0u4cPkCl35eSCtRR+t0O5QZcTgv/QG1p2fzvde7DL0gridVu0yrxPrVHF2NEFfoKm/QxWUVKFKO46KIhg4vNWgOHW07oparCU0MbbIgBvDo1J60KGuObduMm8cjRNYiiEGqI8767TfE0tJGt3RvCPn//IMuPx/r55+nIj+fhBdeJHnWbOw/+rDeq6Q6nciMP8NRK2TMGeGHIAhYadWM6eTMmE7OFJdVEHIpnZ1nU9h5NpW/wpOQywS6uFrQz9eW/m1scbG8Ox6Q9PyHEEU4+iNsnSE1tXl6a8MfeH2HSynT/34KPg+AtT6Cc68gCMIjwKOAmyAIVZtVGlPpi3BPcf09oaKeWT86nUh+STkA4fE5d40gzk8qxLNM8jKt+t4zoy5z1NYEU1s7HvBKRx4F9HuvyedT2tmh8vSgICQEywlP33T/a57DP/yAzEDvOHE34upvhbOvBYc2R+HVxRZD48rnPrMRIyQ7pq+/oeDAAUwGDcJk6BAM2rVrchTb0NiEYS/NwLNzELuWLmLFjBdw6/AgmSkeFGSXYm6voe94H7w62yFXNl6I63QVHN28npDfV6Co0NEhKom2j43H+oUXbn85nUIt3cOdO2PcHQYGZbLhqxPs1PzIYN/tCPGH4N+PAREEGdi2vSKQu4JLVzB1vuFiuINXGx76eD6ZsdGEfvslF6MuEmXjQauOrTA20dKCidJ3HHpBXE/kWgMgFwDrivh6HWNhJF1Eyisqo8tnEnN5iH8olxui8HuoQXMwUBjQ0bYjIYkhN9+5Hrj4WnJY3Z6MuJ14eWawPb32BwLDwEAyf/qJ4rNnMWxftzn8rSJnw0YUNjYYde2KIJdTMmUK6QsWYODjjcWTT9ZrjF8OxnA4Oot5D/tjY1LzpmyglHN/G1vub2PLhzqRk/HZ7Dybwo6IFD7YHMEHmyPwtjWmn68N/X3t8Hc0RSbT1x3raUFKC2DzyxD+O3jcD6O+l1KNGsOQzyBqr9R1esI2kDXjirie20kokARYAZ9X2Z4HhLfIjG4zFfWsIS4oLb/W+DU8IZuh/k3r0nu7ME0qQScDr86Vdo/F+fnsL8wElZJRTwzHYOMj0Hum5IRxC9AGB5O16nd0xcU3FLklkZFkLFmCybBhaHsE35Jz67n9CIJAj9Gt+f2DQxzYEEmfx32qvW41eTJqdw9yNm0i67ffyPz5Z5ROTpgMHozJ0CGovb2bJCyd2wbR9n4tJ//+mYsH/sDAxI0+T06lTXcvhCY+h2UnJ7Ft0XwSzkXgIChpczoSl9dnYDG+/uWLzYmjtwXBD7Vm/x8XOdruBTpNdoPiHIg/UlmHfHIlHP5eOsDYXhLHVwWynT/Ia0bNLVxcGTb3S/LWzeLYxvWczjZh1XuvY+/lQ+cHRuHRqWt1f+X/IHpBXE9MOrnB2VSsMs5goq1fDa+F9oogrpJufTo6iVHyMMq8h6NohOdWd4fufHbkM5Lyk7DXNrHNvrspao0vYul+rOOPklbehbziMowNqv8xGQYEAFB04kSzC+LyzEzy9+/H4snx19JWrKY8T8mF86R8OheVpyfa4BvfaOMyC/lk6zl6eVnzcMebPxDIZAKBLuYEupgzfaAPMRkF7Dybyo6IZL7bE8mCfy5jbaymXxsb+vva0t3DCgPlf/vCcbsIj8/G00aLkarlL1VlFWXE5cVRUFaAn5Xf7V1JTrsAq8dD2jno8xb0fA1kMnQlJZRGxyCWlWHQ1rf+c9LawKBPYd1EOLQEgvSpjfcCoijGADFAt5aey+1Hus/q6llDnFdcfu37U/E5zTKjW01JYRmOuSKlzkaoDKVrYkV5ORs/m0OBTGBAu86YHf4UTBwh+MVbdl5NcDCZPy+n8OjROu+/oiiS/J7ec/hewcJeQ7s+TpzcHUfbng7YtKp8XhUEAZNBAzEZNJCK3Fzydu0md8sWMpYtI+P771G5uWEyZIgkjt3r732dkZjPiR2xXDiUgqgT8Qp+FmOzyxz76xd2/fAOYsUkfHv1bdS9VxRFwnduY8+KpQiCQMcyOTYXLuH48SeYPnBzp5fbiX9fJ1Jjczm4KQorZ2Nc21lJTi6e90s7VJRDakSlQI49CBHrpdcUhuDYoVIkO3eRSqWuYGxlTO9WUXR/bQ+nQ/Zx9K/1bPz8I8ztHeg4dAS+ve9HqWpYeeK9QqOfMgVBeOVGr4ui+EVjx74TqbbmXM9aVo1Kjkouo6KKIJafXY+xUARdb556VBvBDsF8xmeEJIbwsFfTrAzkShmO3rbEnW5H8eVjGDr5EZNRiJ+jabX9lLY2KBzsKTxxot4R2saS+9cWKC/HdPiD17YJMhkOn3xC9COPkvDKq7j9vgqVq2utx4uiyJvrTiEAH41snGhpZanhmR5uPNPDjezCUv49n8aOiBQ2nUxi5aE4DJVyera2or+vLX19bLDU/jcvHs3N9jPJTFxxFDMjJeODWjG+uytWzfy7FkWRjOIMonOiicqNIjonmujcaKJzoknIT6BClJpcTPKfxNTAqc06l2tzCv+D8tUvU5pvQKn7y5TsL6d0xSRKo6IoS0y85m9o/fLLWE2aWP+B/cfA6TWw633wGgQWtfup67l7EARhvyiKPQRByKP6bUsARFEUG74Ke5dR3xriq4LYQqPiVEIOOp14x2cBhYcloUJA0UayehRFkd3LviPu7Gn849Nw6VsESSek7JFG2CzVhVGnTghKJQUhoXUK4py16yg8fBi792ejuBUNifS0OJ2HuXHhUDL7fr/IqOkdan2ekpuYYDZyBGYjR1CelUXe9h3kbtlC+sKFpC9YgNrHR4ocDxmMytm5xvGiKJJ4IZvjO2KJOZ2BQimjbU9H2t/vjKm1IdAOv/uC2LZwPtsWzufioTAGTJyKkWntdqe1kZeZzvbF3xB94ijOrX1oczgcZWo6TosWoe3Zoym/omZBEATue8yHzMQCdiyLYPTMTpjZVvl7livA3l/66vKstC038YpAPiR1tQ79GnRXFv2svCoFcm4iAEoDAwIHDqN9/8FcPBjGkU1/svOHhYSs/pXAgcMIGDgUQ+Pbd7soLijDQNOyPXyaEnZp2f7Ytx3x2r/1raMVBAELjapahNg/dQPJSmfsXIIaNQsPMw9sjWwJTQxtsiAGcPG1IDq8LaLuML5554hMD64hiAGMAgIoPHa8yee7GTmbNqH28anRvEOm0eC0cAHRD48mbspUXH9fhVyrrXH8H0fj2XcxnQ8ebIuTedMfCMyMVIwIdGREoCMl5RUcjMxkR0QKO8+msD0iBZkAHVuZS5ZOvrZ4WNeck56GU1xWwfubI/Cw1uBhreXr3ZdYvDeShzs68WxPd1ytmlb5UlJRQmxuLNG50UTlVBe+eWWVXnhquZpWJq3wsfBhkNsgXE1cCU0MZXH4Ylqbt2ag68CmvtVr6IqKKI2OpjQqipKoKEovX6I0PIzS5Cx05Vcvt6sQjIxQu7piGBiI6aiRqN3cyNu1m7T585GbGGP+yCP1O6EgSH6SC4Jg04swfsMtaYqnp+UQRbHHlX//M/fnyo/slaZa9awhziuWuqx387Dkr/AkYjILcWvidaW5Obs/kVSZjo7u0oPqsS0bCN+1jQD/jjicXI0qZg14dGq0zVJdyIyMMOzYkYKQEGB6jdfLMzMlz+EOHTB7WO85fK+gNlQQNMKDf1ac48KhFLy73rgTsMLcHPOxYzAfO4ay1FTytv1N7tatpM2fT9r8+Rj4+0viePAg5NY2XD6exokdsaTG5GForKTLA2606+2Egba6MDKztWPMex9x9K8NhKxazk+vTaH/s1No3aX7Td/DuZA97Fq6iPKyMnoNGYHJ9z8jlJXh/NOPt6UEsLEoVXIGT2rHHx8fYeviUzz0esdrFmu1YuIAbUdKXwClhZB4rFIkn9ssef1eQ7pwymRyvLv1wCsomPizpzmyaS2hf/zKoQ1r8OvTj45DR2Jm23wdoIsLyti78jypsXmMfbsLSlXLZV82WhCLojj7Vk7kTudqrZEgAur6N4qw0KgoL5Bu0Fkxp/HXneOA24vYNfLBUxAEgh2D2RG9g3JdOQpZ01JJndtYIJNbYO7ojV/KGSJTajfGNgwIJHfLVsqSk1HaNc8fR0lkFMXh4dhMr3nDBVA5OeH45ZfEPvMMia9Nx2nBt9W6AabkFjNncwRd3Cx4rGurWz4/tUJOLy9renlZ8/6DbTmTmHtNHH+89Rwfbz2Hu7WG/lfEcQcXc+R3eMThTuW7PZeJzyrit2e70t3Dikup+fywL5I/jsTz26FYBrW1Y1JvDwKc614lFkWRtKK0a2I3KifqWtQ3MT8RsUoAzcbIBjdTN4a4D8HN1A1XE1dcTV2x19gjE6o37xjoOpD4vHjeCXnnmliuL6JOR3lysiR4oyTxWxoVSUlUNOVJSZU7CgJKYxkqowJMu7dF3XscKg9PVG5uKGxsaqzUG/fvj66wkOT3P0BmbILpsKH1m5CpEwz4ADa/BMeWS/YQeu56BEEIAs6Ioph35WdjwFcUxYMtO7Pmp741xFcjxMEeVvwVnkR4fPYdLYhTY3LJSyok3LCcYWaGXD56kH9XLKV11+74G1mSDqjkKTDolybZLNWFJrg7aZ9/QVlqag2bltRP51JRUIC93nP4nqNNN3vO7E0gbO0l3Npb3ViUVUFpY4PF+CewGP8EZQkJ5G7bRu5fW0iaN59jvxwgzmMwRTJjTC1V9H7UG58gOxQ3EEMymZzOD4zCrX0Hti6Yz8bPP8K3Zx/6PD0JA03NQERRXi47ly7iQtg+7Ft707tHP/LffheZVovLzz+h9ri513FLY2JlyID/tWXT1yfYvfwsA59tQNajyghce0hfADodZFyCuAOAAIrqgT1BEHD2bYezbzvS42I4snkd4Tv/5uT2rbTu2p3OD4zCzvPWNuCMOZ3B7hVnKc4ro/MwN+Tyln1ebkrK9Nc3el0UxRcaO/adjkxd/07LFhoVFXnSDTondBlaUY6q42NNOn93h+6svbiW0+mnCbAJaNJY5vZGaExVGBl3xiThPMlnjsIA7xr7GQZW1hErBw1q0jnrImfTRpDJMBlWdz2HJqgrtm++QcoHc0j76mtsXnkZkMTP2+tPU1Ku49OH/Js99U0QBPwcTfFzNOXl/l4kZBex60pTrmUhUSzeG4mFRkVfHxv6tbGll5fVHVEHezcQl1nIon8vM8zfnu4eUuqdp42WTx7y55X+XvwUGs2KAzFsPZ1MFzcLJvRwoJVtETH5UoQ3KieK6NxoYnJjKCir7JxuqDCklUkr/K38Ge4x/JrodTVxxUhZ/2wClVzF/D7zGbd5HC/sfoGVQ1diaVi9uVVFfsG1aG9pVBSl0VGUREZRGh2NWFzZpV6m1aJyc8OocyfUbm6o3NxQyVNQHXoPmawCHlwgdYW+CYJSieP8L4h7diKJM2ci02owvu+++r2hjk/B6T9h+9vg2U/yE9dzt7MI6FDl54Jatt0TVD4gNixCnHslQtyxlTlqhYxT8Tk8GHDnfvbP7E8EuUCEsgKD3GT++moetm4eDJ7yCmlvzUBhqEPWYXSTbZbqQtujB2mff0FhWBimD1aWNBUcOEDOhg1YTpqEunXrZjk2PsL7AAAgAElEQVS3npZDkAn0HOvFn3OPcmxbDEEjGi4klY6OGI5+gkvWfTi1O5aSYh1mJYl4XlyJVeZptEldyEsZjEn//sjNbpwKbeXiyqMffsaBtb9zcN1qYiNOMei5l2jlX/ksHHnsMNsXf01RXh49xo3H28iU5Omvo3RywmXpDyjt744GeiAFrbqN9CR07SWOb4+lw8BGBntkMslRoh6uElbOrRg0+SV6jH2CY9s2Eb5jKxcO7MfJ14/ODzyEW0DHJi18lRaXE/rnJc7sS8TCQcOwKe2rWci1FE15Qj96y2ZxF6C7FiEWERpQcH4tZbqiFOvotewWO9Crib5fQfZByAQZIYkhTRbE0qqQBZEnyilVGyM/F4bk2lEdA29vBLWaouPHMWkGQSzqdORu3IQmKAil7Y1Nws0ffZSSc+fJWLIEtbcXpkOHsjk8iR0RKbwx2KdFVvkdzQwZ382V8d1cyS0uY+8Fqe54+5lk1hyNR6WQ0cPTSkqtbmNTa+drPRIfbI5AJgi8NbTNtW2iKJJSmEJUfhTOrtGMNonkYPw5zuXFMP1IVrXj7TX2uJq4Mtxj+LVor5upGzZGNjWivY3FytCKr3p/wWu/P8U3iyYwxWIE5TGxUtQ3MpLy1NTKnWUylI6OqNzd0HTtKoleNzdUbq4orK0rH+h1FfDPR7DvM7BtB2N+Bsv6P3zIDAxwWrSQ2KeeJuHFl3D+fgmaLl1ufqAgwPCvYWF3+OsVeGSVPnX67kcQRfFaqFQURZ0gCP+JFbn6p0xLEWJzIyW+DiaEJ9y5jbVKi8u5eCgFnZMh8ow0QhfNRa3VMmL6OyjVBpSGh6EyrrglNkt1ofb2Rm5pSX5IyDVBrCspIfm9WShdXLCa/FyznVtPy2LnbopXV1uO74ylTbA9ptb1X0DOTink+M5YzoclU1Ghw83fisD+Lth79qX4Qhdyt24ld8sWkt95l+TZ76MNDsZk6BC0ffvWWhYHIFcoCR7zOO4dOrN1wXzWfPg2AQOHETRqLCG/r+DU7u1YObdi1BuzUR4+StLLr2DQzg/n775DYW5+q34tt42A/s6kxuYStv4yVk5aXNo20l2igWgtLOn16FMEjRxD+K6/ObZlI+s+nY2lkwudho3Ep8d9KJQNq/tNvJTNrp8iyM0oJnCAC10fcG+ShdatpCkp0z/fyonc6YhV6oCFBkaIy3UixB5AU15MmMkQBjYxR95UbYqflR+hCaFMCZjSpLFAWoE6F5YMLp2xuLibzMQELByqr5QLKhUG7fwoPHGiyeerjaJjxyhLSMD6hWk33VcQBOzeeZuSyEiS3nqbYlsnZm1Npb2TKc/0aPnGQCYGSob5OzDM34GyCh2Ho6W64x0RKew+l8qb66C9sxlvDWlDFzeLmw94GziXeY41F9aQU9KyD4WpecUczM6kjb8JX4X9ReDvBxAKBArKi641tAJwEeS0UWkwUmioqHAiO19GYbESA7khblbGtLLUoJSnA+nAYSqQvGhuBbrSUspi45BHR/NFaSlwgXTmIjMxQeXmiqZbN1Tu7qjcXFG7uaFs1QrZzfoO5KfCn89IVkiBT8CQeaA0vOlcErKL2BmRwqXUfEYEOtCxlQXO3y8h5vEniJ/8PC4//4yhX9ubvykLd7j/Hfj7TTi1BvxH1+dXoefOJVIQhBeQosIAzwORLTifZuP6pRtdAwWxsYGS9k5mrD4SR4VOvCPLXC4dSaWspIJ0Kxhx/m9KdPmMe38uWgtLiD1AaUo2xp3a1GmzlFuay3cnv+N85v/ZO+/oqKquDz9nSnohIZUU0gikEAIJHULvTTqKFEFpflJEUHlREBVBRAUVBZGOgPQWeu+9hRAglBQgoaSTOpnz/TGhB0iDBM2z1iwyd845s2/IzL377L1/+2Kh7GjtIii/azNfbroLCkGd4EjqRESzfLA3EXsKdi/ia+VLT6+e2Bi9eCO8lOKlTkcPrp6+y4EV4bQe7PfS8beuJHJqawTXzt5FqVRQsZYd/k2dsLB7FLAw8PTEwNMT66FDST8fStKmYJKCN5GyZw9CTw+TBg0wa90Kk4YNURg+ez2096hIr0k/s3/JAk5uWseZrcEAVO/Qhdpd3iFx7jxifvoJ43r1cJw+DYXRm9Fr/GmEEDTu5UX8rfts/es8XT+vniM49nrQMzQisG1HqrZsx6VD+zi2fhVb/pjG/mULqdaqPX5NW+aatv44mqxsjq67xqntkZiVNaDjyGqU88i7MNrroNA7xkIIa+BTwBt4GPaSUjYu7NolE4nIR7N5S2M9ncq0Jp1YaUm2W6MisaJuubrMPDuTxIxEzPWfFcHKD46VdE6ZhXUgSZd3c2TTBlr1H/jMOCN/f+7NX4A2IwNFHpW280ri2nUIQ0NMmzbN03ihp4fj9Glc69KVyCFDUNQfyvcftEalLBk7TQ9QKxXUcbeijrsVX7b15lJsCttCY1hyNIr/+/sk2z5ugLlh8SjrSSk5fOswc0PmcujWIQxVhtga2b584quyB4iOS8XAGLKUidiuisTvUBbxVnqo1EaoFWrUCj3USjUqoYS0J2enatNISEkk7e4NwoXAzFCNuaEaVRHf4AqVErWDI8Z166Ln6sLarOP8mRjM0EYj6O7VI/8LRhyE5e9BeoIuRbrqu88dKqUk5EYS2y7Esj00ltBbut7oekoFCw9HEFjeggFBbjT480+i3n2XqA8+oPziRXlrfVFzEJxfDZtGg1tDMLHO/7mUUlIYBEwHxqL7aO0A8iFB/uaiybPKdBYDVRsxOHeXyg5NmHfwOlfvpFDBtvhT957m/L4bWNgbEXtuJZZpsbQe9QU2Lm6g1ZK9ZhTZGUr0qrd6Zp6Ukg1XN/DD8R9IyEjAz8qvUFkyVyua4X3iLpY3ktGoBDW33yAkoCxXPE0eKdrmA43UMO/8PBaELqCNaxv6+vTFw6JwGXSlvBqMy+gT2Ko8h9dcJTL0Hs7ez0YppVZy7exdTm2NJOZqIvpGKgJalsevkRNGZs/fFBZCYOjrg6GvDzYjR5J2+gxJwcEkbdlM8rZtCCMjTBs1wqxNa4zr1Xtig1mtb0CjvgNwD6zJqc0bCGzbkXKelbg9eTJx8xdg1rYt5SZ+m2cx3JKKWl9Jq0E5Ilt/6ES21PqvV4BKqVLhVb8Rleo1JOLcaY6tW8m+v+dxeNUy/Jq0oFrrDphZPXvfcCcyme3zQom7eR+f+uWo09kjz7Xor5OisGgxsAxog+4i3Ae4UwTrlii0j4lq5Sdl2sL40Yfwn+wg/JyLph1BnXJ1+P3M7xy6dYiWLoVLYTYy08PKyYTUZC1XjV3R37eDpu/2Qf2UeJihvz/M/ov08+cxqlZ0pWjajAySNm/GtFlTFMZ5T3dWlS3LzZETMPt0CNMuLKOCZccis+lVIISgop0pFe1MaeBpw1szDvDtxlC+7/J6lQ41Wg1br29l7vm5hMWFYWVoxfBqw+lasStmesXXlWXG7nC+P3KR+f1qUPf8j1w5eBkjhwy8fv4aKuddufRcdCIz914h+NwtlApBB38HBgS54fmKbnR7aTtzbNd9Jh2bjJuFO9Xt8ljDJyUc/AW2jweL8vDuSrDzfWZYhiabw1fj2BYaw/bQ28QkpSMEBJa34PNWlWjqbYu9uQH/HIti9v5rDFh4AjdrYz76+Bt8vhtFZL/+uCxehNrhJfWRCqXOIf+jHmwaBV3n5ft3UUrJQEp5GyjA7swbyFMq09rMvDlmSelZ9FLtR2xdT5V3mwNwNjqxxDnEdyKTuR2RjJ1LKOaxF4ip1AyPwJq6F88uJTP8AmCN2v3J+t0rCVf45vA3HI89TmWryvze9He8y3oXypasgNuE/92Az2hByvY9pJuY0XH6WrqWLXgKZ1RyFAtDF7ImfA1rr6ylvkN93vN9j0DbwNfb672Ul+LfxJnQA7fY/89lun9hgTInAKHJyubi4RhOb48iITYV07IG1OtWAa869vl2fIRCgVG1qhhVq4rt55+Reuw4ScHBJG/ZQtLGjShMTTFt2hSz1q0wrlULkZOy6+xbBWffKsisLG5++hlJ69dj0asXtp9/9q8RejO3NqJZfx82/HqGXYvCaNbPu1g+I0IIXPyq4uJXldvXr3J8/SpOblrHqc3rqVgniMC2HbFxcUObreXklgiObbiOgamatv9XhfK+ryfduyAUhUNcVkr5lxBimJRyD7BHCHGsCNYtWTwmXJmflOmyTzjEDZjrXDQpAr5WvpjqmbL+ynr0FfmP1gohCLQNxERPl+bg5GXJ6R1RhJr6UiHmCmEH91K5UfMn5hj65whrnTpdpA5xyq7daJOTn+g9nBcS07IYfTadRo360Gf7bGImTMD+66/fiItoZUdzPqjvxh97rtCuSjnqV3j10bjUrFRWh69mwfkF3Lx/E1dzVybUmUAbtzboKYt39/RWYhq/7AinubctDTL2EDNnEdpsE2yq5K56/iIqO5rz6zvViIpLZfa+qyw7HsWKE9E0rmTDwCA3arhaFunfiFKhZFL9SfQM7snI3SNZ0nYJDiYvcT7TEmDth7pWCF7tocOvYPAo0yMhNZNdF2+zLTSWPRfvcD8zG0O1kiBPK0Z6eeba/7pvXVferVWe4JAYZu65woj9cVSr/QHjd/zCtb79cFuy+OX9Qa0rQoPRsPMbXesWr+cL3JVS8hBCjJZSfi+E+IUn+xAD/06xS/FU0rQmjw5xcroGhRCQkYRb9GqM9Nw4dyORzgG5px0XF6H7b6LVXOD6qc2EW/hgUSUnyywjBbZ/RaaqApCAXnmd2E5qViqzzs5i/vn5GKmN+LL2l3Su0LlI9BPUNjboe3pyd+YstMnJ2H09AVUhnGEAJ1MnxtQcw5AqQ1h6cSlLwpbQb0s/fMr60Ne3L02dmxa6m0YpRYNSraBe1woEzzjLuV3RVKplT8jeaM7uiiYtOQtrZ1Oa9/fBvZo1iiLI1hNKJca1amJcqyZ2X4zl/uHDJG0MJnnbNhJXr0ZZpgymLVpg1ro1RoEByIwMoocN5/6+fVgPH07ZgQPeiPvB/FDepyy1OrhxeM1VbMqb4t/UuVjtsXFxo/VHn1Dv7d45beC2cmHfLspV9CNLU4Wke1Z41rAjqIdnsfcZfhlF8S2TlfPvLSFEG+AmUDIKI4uQhxFihUSZj55cFkZ6JEgTDisDSNB3wM2qaPrUqhQqghyD2Hh1I3uj9xZoDS9LLxa1XoSeUg8nL0tObY1E39ARWcaW01s24tuw2RNfJiorK9ROTqQVcR1x4rp1qKytMa6dv97M3wVf4E5yBu981p+y7kruzZyJQcVKWPZ6fsppSWJ40wpsPR/DZyvPsXVEEMb6r+aify/tHn+H/c3SsKUkZSZRzaYan9f8nCDHoCITmSos3268gFZKJtTQkDl3KPFXzCnTthn6xgWXKnCyNOKrDr4Ma+rJwkMRzD90ne6zDlPFqQyDgtxo7mNXZPWCpnqmTG80nXc2vsPQnUNZ2Grh85Wrb52Bf3pDYjS0+A5qDQYhiLh3/2Gt+fGIeLK1EmtTfdr7O9DM24Y67lYYqF+cIqVSKmhfpRzt/Ow5eOUef+y5wqeJ7zHx4CyOdeuF8/z5ODi9pFav7nAIXasT2HKpC4ZvngjJf5jQnH+PF2SyEKIlMA1QArOllJOeet0ZmA+UyRnzmZQyWAjRkycb1PoB1aSUr0Z04gmjHvyQozKdkfXcoY+TnK7Rff6zQXHkDyrbz+BsdMKrsbGAZGVkc37/cbJStuLg7cdvqbUYYZ5TO3jgZ0iJIdOqC4jV6Dk7szNyJ5OOTuLW/Vt0cO/Ax4EfY2lQtLdjxnXrEjd3LoYBAZTp3LnI1i1jUIZBVQbR16cv666sY0HoAkbtGYWDiQO9vXvzlsdb+eoGUMqrwaVyWZx9LDmy7ipH1l1Fk6nF2acsVZs54VDR4pU5oEKtxqR+fUzq10ebMZ77+/eTtDGYxHXrSFi2DKW1FUozczKvXcNuwldYdOv2SuwoCVRrUZ47EckcXBmOlaPJw7LH4sTMyoaGvT+g5ls92PrnMq4c34bUnsXMxgknz26o9Z/tXlPSKIo78G+EEObASOAXwAwYUQTrlixyHGJThzQMmzV/8djHKGuix1tZw5FZEFihTJG2AxpXexy9vXsXaG5YXBjjDo5j+snpfFL9E+w9zFGqFfiqDLjtGIAICSYm/BL2FZ78Izb09+f+4UNIKYvki08TH0/K3r1Y9ur1RE/hl7H/8l2WHotiYAM3/BzLIIcNJePSJWInTULfwx3j2rULbdsrRUoM1Eq+7+JH15mH+H5zGF91eDZdtjBEJEUw//x81oavJUubRWPnxvT16VtoZfKi5uCVu2w4e4sxQZbYBfcj+pw5Qs8Aq/49Yel8kFpdD70CYmmoYlhjdwbUd2HFyWhm773KkMXHcbE0on99N7oEOGKgymctTi4pWC7mLkxpMIUhO4Yw9sBYfmjww5MbDlLCyfkQPBqMrdD22chpUZHtWy6yLTSWy7dTAKhoa8qgBm4087bDz8G8QN8ZQgjqelhR18OK0FZeBM8xpvniHzjcvQ9HBo/jvabeeNk/Jz1eqdalTs9qBFv+B2/NyPf7l1JsdAc2AGWklNPyM1EIoQR+A5oB0cAxIcQ6KWXoY8PGAv9IKX8XQngDwYCLlHIxuvIphBCVgTWvxRnOBU1GXiPEWboIsdoYEiLo7H6OLy66oMnWFlqL4tTtU4TFhdGpQif0lQXX2zi9/RxpcasxLWtNYL/haH87hq25ASRE6sotKncl80gWwtaaYQdGsTt6Nx5lPJjXch4BtgGFOofnYda6Fclbt76ynsMGKgO6VexGF88u7IraxbyQeXx39DtmnJlBj4o9eLvS28+0uSvl9SGEoF7XCqyffoZynmWo2syZsg5FE+jJKwp9fUybNMG0SRO0qamk7N5N0qZNpIddxOHnnzBrnvd79DcRIQSN+3gRF5PKltnn6fp5IGZlX5/I1vNIjktnx/xwblx2w6P2aBzcYzm7Yz3Bv05l39IFBLTuQOXGzdEzLJkbW4V2iKWUG3J+TASKRjGqBPIg90yhkPlKBbEw0iM1R2vM36loFdUMVYYFrgnyLutN6L1Q5ofOp1a5WtRzqEe5CmW4fy2B7VYVcDQw5PTWjc86xFX9SVq/nqwbN9FzLHzPxqTgYNBoMO/w8l6rD7ifoeGzVWdxtTJmRFNdTzWhUFBuyvdc79GDG8NH4LJiOXpOToW2r8hJjYOjf8Kx2eBSj8B20+hT24V5B6/Ttko5qrsUfqfv7J2zzA2Zy47IHagVatp7tKePdx9czF0Kb38Rk5WtZfy687iWUfH+zXGkRSaSfNUMqyHvobbOSe9d9YHuUUgMgV45DwyAVGBLziM/qI1h8H6dMvNT1HWoy8cBH/PD8R+YeXYmg6sM1r2QeR82joQzS7hnW5cZlp+ydmEKd1MOolQIarhY0qOGM828bHEuW7QXC+9yZniP7cs1L0u8xn5Oxl+TaXeuL3Ur2TMwyI3a7mWf3dyyrwL1hsO+qeDbSdefuJQ3gQAhRDmgnxBiAU+JMEsp414wtwYQLqW8CiCEWAp04FHUGXSXwgc7KeboMsKe5m1gacHMzz+PTlAitNlkZ2a/YPQjHkaIy9eD2xdonLCC0ZqPuXw75fmbRXlk+snpHI89zvzz8xkeMJwW5VvkewM5/X4Kh/75GaEQdPnfV1xO123a2ZsbwLbRgCCz0Rii5/UnyuAeR2KOMDJgJD29e6JWvLrURMPKlfHYsf2Vrf8AhVDQxLkJTZybcOr2KeaGzGXW2VnMOz+P9u7t6ePTh/JmBezJWkqhsLAzpvfEOsVtBgAKIyPMWrfGrHXr4jbltaJnoKL1oMos/+4Ym2eG0OmTaqgK2cGmoEgpCTsUw/5/LiElNHq3El517RFCUK1VS66eOs7x9avYvWA2h1YuoUrTVlRt1R4Ti+KPbD9OUahMzweGSSkTcp5bAFOllP0Ku3ZJ4mHXJfFszdKLsDB6dGEqaoe4sHwS+AknYk/wv/3/Y2X7lTh5WRIVGkfsPQ3e9RsRsnsbDXr1x8jsUW2j0YM64tOni8QhTly3Dn1PTwwqVcrznClbLnIjIY1/BtZ+IoVUaWKC04wZXOvajeghQyi/ZClKk9ffkzhX4q/Dod/g5ELQpIFzHV1a6q0zjO44j+0XDPl0xVmCh9V/aVpsbmilln3R+5gTMoeTt09iqmfK+5Xf5x2vd7AyLBoht1fBgkMRXIpNZr/XGsTVI9yOrIPSMgnLfv3B2Aha/6DbRHgFSCTR8Wkcvx7HtbupqJUCXwdzqjlbPF/5O/46nPkbkm7m6hAD9PbuzaX4S8w4PYMKZSoQIOwRy/tglhzOb9ou/BTxFkYx6TSoaE0zL1saVbTB3OjV19a4dm5PgsxEjP2C2dHrGWXSjXdmH6GygzkDgtxo5Wv3ZGQsaDRcWA/rh8OQQ6BfssSGSsmVP9ApSrsBJ3jSIZY5x5+HAxD12PNooOZTY8YDW4UQHwHGQG47Jd3ROdK5IoQYQI7itbNzEdS/PXA0JSikhuysfNYQK1RQcwBWW8fiI65xLtqvUA6xlJKLcRepbledxIxERu0Zxd82fzO6+mh8rfKWBZSt0bBq0rdkpcdRre1QLMs5EHP6BgAu98/B+VUcqdmXb/YM48uoGER1J9a9tQA747yXc71JVLWpStXGVbmWeI0FoQtYG76WFZdWlNisp1JKeR2UsTWiWT8fNs44y+6/L9Kkj9drr5lOTcpk9+Iwrp25S7kKZWjSxwszq0fRaqFQ4B5QA/eAGtwKv8jxdas4tm4VJzauwat+IwLbdqKsY8kIXhVFyrTfA2cYQEoZL4SoWgTrliwe1BAjyU/ZpUqpwNxQTWJaVolziA1UBkwJmkKPjT0Yu38sX1f6HoBy6QL72k04sy2YkF3bqNHhkcKvvqcnwsiItNOnMW/bplDvn3HtGulnzmIz6pM8zzl+PY75h67Tu1b5XKOpes7OOP70I5EfDODmp5/i+Mv04lUYvHkKDkyH0DUglODXHep8BDaV4PoBWNEPo/nNmVP9K5rvcuTn7Zf5rFXeNwcyszPZeHUj887P42riVeyN7fm0+qd0qtCpxNdb3U5O5+dtl/ja/iCO15aTbNmD1NC92H4x9tFGRo3CR4afhwCcch4XY5KZtfcq08/cIPu6pI1fOQYGueHr8FRLs2t7dQ7xS+jp/jFHosIYtWsUc6Njcc4SfKz3BWa+LZjnZUstt7LoqV7/32WZLl3ITkqG779nlYst+zsO4M991/hoySmcLA15v54bXQMdMdJTgdpAlzr9V3OdEnabqa/d3lLyh5RyOjBdCPG7lHLwK3iLt4F5UsqpQojawEIhhK+UUgsghKgJpEopQ15g4yxgFkBgYOAzwl/55ckIsYbsrLxFiJPSs1Aa5syu1hu5exKD5GaO3GhAt+oFv0G7kXKD5KxkWrm2opNHJ1aHr+aXU7/w9sa3aefWjqHVhr7QcZVSsmveTG5dOoeeaQtqd2wAQGxSOgIteoe/5FN7R4Jv78RTYY9JOjSs+w5l/6XO8OO4mrsyrvY4PvT/kCVhS1gatpQdkTvwt/anr29fGjk1KjG6GKWU8jpw8bOiRjtXjq6/hk15U/wavT7n8sqp2+xefJGs9GzqdvGgSmMnxAtKvOw9KtLu489JiLnF8Y1rOL97OyG7tuFWrTrV23XGwcunWEXQiuKbQ5ETFQZACGFJ0TjaJQr50CEmH/FhHWWN9XC2NHpGFbYk4GHhwejqozlw8wAbE1ejMlbholEQp2eJo7cvZ7dvQqt9dIMhVCoMK1cm7dSpQr930vr1IARmbfOmZJuelc3olWcpZ27I6JbPdxqN69TB9tPRpOzYwd1ffy20nflGSgjfDvPbwayGup/rfATDz8Jbv+mcYdAJFg3cC46BeB4azT/2fzN/74U8CbskZyYzJ2QOLVe25MuDX6JWqPmu/nds7LSRd73fLfHOMMDkTRepmn2GdxP+QHq05M62m6jLOxeLGEZFO1OmdqvC3tGNeL++G7vCbtP2l/30nH2YPZfuIOWL79uztZKj1+KYGHyBxlP30GHafjqEabHITueTclZc6bqKn8Z8zIQOvgR5WheLM/yAsv3eo+yggaSsWkXjPf+wbUQQM3sFYG2iz7h156k7aSc/bbvEvZQMcKqhE/06Nlu3gVNKiUYI8SC0+T8hhOXTj5dMv4Fuf+gBjjnHHqc/8A+AlPIQuuKDx1NQegBLCnwCBeGxC7LigUOcmarbvHoOWq0kJUPDw3s3A3NE1XdpJQ5yI+Jqocy5GHcRgEoWlVAqlHTx7MLGjhvp79ufLde30G51O2acnkFqVmqu809tWseZbZvQM6lJxdqNMTDRZY/cTEilltVi3tKPZ5uhmkFVBvGX9zcADxWm/ytYGVrxUdWP2NZlG5/V+Iw7aXcYvms4HdZ0YPml5WRkZxS3iaWU8toIbOWCi58V+5eHc/Ny/Ct/v4zULLbPDWXzzBBMLQ3oNqY6/k2dX+gMP04ZO3ua9h/MB7/NoU7Xnty6fJG1P3yDJrN4P7dFcVc2FTgkhPhaCPE1cBD4vgjWLVHIx37K7w5GS187etQoGSkBudHVsytNnJsw7dTPmLooKK9RcvVOCv7N25B4O5brp08+Md7Q35/0ixfRpqUV+D2llCSuW49x7VqobW3zNGfajstcvXOf7zpVfqkis0WvXph36sTdGb+TtHlzge3MF9lZcGaZro/ros5w9zI0+xpGhECzCWBW7tk5prbQaw3UH0mN+A2sMfiKn5ZtIVOTu4hUzP0Yph6fSrMVzfjpxE+4l3FnZtOZLG+3nLZubV9p7VhRciIijqOnTjDT4BeEVQUSVe3ICA/HZsTHD/sKFgf25oaMae3Fwc8b81mrSlyOTaHPnKO0nr6f1aei0WgffRPcz9CwOeQWI/85Q+A32+g28xSChlEAACAASURBVBBzD1yjimkyB22n8JHYyTSbBsSr1fx+YyYambdUzteB9bBhWLzzDnFz5hD/55+08LFj1ZC6rBhUm4DylkzbcZk6k3byxZoQIv0/BgsXWPcRZBX8M1/Ka+FB+sIJdErTJx57vEx5+hhQQQjhKoTQQ+fcrntqTCTQBEAI4YXOIb6T81wBdOM11g8/jS5lWgvnV+s2JJNu5TrufqYGKXlSsK7mQJRoqXF31XO/f/NCWHwYCqGggsWjvsAmeiYMDxjO2rfWEuQYxO9nfqfdmnasv7IerXz0XpePHmTXgtnYuldFqOrgU193zTh35xxb4z8lxPo8vuizqv0qPvT/EKJ0Jdx65V0KbO+bjJHaiJ5ePdnQcQNTgqZgpDZiwqEJNF/RnJlnZpKYkVjcJpZSyitHKARN3/PG3NqQzbNCSIlPf2XvFRUax9Kvj3LpWCyBbVzo/GkAluUKVppoZGZO7S5v88GMuXQa8xVqfYMitjZ/FIWo1gIhxHGgcc6hTk+pUv4rKEyE+EXRzJKAEIKv6nxF53Wd2SXW4C9bE3UlkXffroVxGQtOb92IW7XqD8cb+lcBjYb0kBCMqld/wcrPJ+3UKbKio7H6vw/zNP5cdCKz9l6lW6AjQZ4v79krhMBu/Dgyr17l5udjyE5IQGH4ilT4stIhYj9c3gFpcTrHt8JgcKwOd1WwZfezcxQKTBo0QGlmBkoVNPkSnGrituIDpiUNY+uKRNr2GPBw+OX4y8w7P4/gq8FIJM1dmtPXp2+BRdWKk2yt5Ls1x5hn8CMGKgXajnO58/YQDKr4YdqiZKhDmhmoGdTAnffqurD29E3+3HuVEcvOsMP0Mr8CkzZdYM6NZDI1WswMVDSuZEMzbzsaqc5gtH4IZGug63wq+7zF+CvrGbN/DN8f/Z7/1fpfcZ8aoPt82I79H9nJydz5+WeU5mZYvP02gS6WzHaxJPx2Mn/uvcayY1EsPhLBMLePGHZjJOyaCM2/fun6iRmJRCVH5blmspSiQUrZNudf1wLM1Qgh/g+dzJwSmCOlPC+EmAAcl1KuQ9dN4k8hxAh0+8R95aP0iSAg6oEo1+vi8evxwwixJueGMDEazOyfmZOcrtucUj6+uW3pRqx9E7rf3M7lG7fxKV+wFOSwe2G4mrlioHr25s7R1JGpDadyMvYk3x/7njH7x7D4wmJGVx+NXaIhwdN/wN7dE7VpayzUAmNnwYRDE1hxaQXG2Sqm3LtLi+5rEWV0peCZERGgVBaJnsebjEqhoqVrS1q4tOBYzDHmnp/Lr6d/5a+Qv+jo0ZFe3r1wNC1Z/aVLKaUo0TdU0WpQZVZMOs6mmSF0HFkVVQH0aJ5HVkY2h1aFc27PDSzsjOg8OgBbl8KJDz5AraePvUfxt2UqqtRmS+C+lHKuEMJaCOEqpbxWRGuXCB5c8wVSJ8TxL8Nc35xJ9SfxfxtG4E9rUiJSUKrUVG7SksOrlpIQG0OZnP7LhjnCWqmnThfYIU5cuw5hYIBp02YvHZup0TJqxRnKGuvxvzZ5dwAVeno4/jKda927EzP+qwLZmX8sgDTYthZY+8KRhgEBlF+44FGNs2cL1IP3cXNmN9qGjeLeqstcrdmJuRcWsu/GPgxVhnSv1J1e3r1wMHlzb4CWHLnOgLuTcVXdRHRdyb31+9HExuLww5RirR/JDX2Vkm6BTnSp5siui7fZvzUK4uFGQhrv1ixPM29bAl0sUAsJuyfB3ilg4w3dFoCVBwDt3NtxMe4i80PnU9GyIl08u7zkXV8PQqGg3MRv0aakEDPhaxQmppi305UveNiYMrmLHyObezL34HVmH1ZhrWlMj4O/ctK4AQF1mjz3/+rcnXOM3DOSW/dvsbr9ajwsPF7naZUCCCE6AjullIk5z8sADaWUa140T0oZjK6V0uPHvnzs51Cg7nPm7gby10y+KHj4d5hTQ/x4dDc59wjxA4f46ZZmitqDsVy1nctHF0P5kQUyJyw+jGo21V44ppptNf5u8zcbr27k55M/89GK/nQ46oy5hSUN+oxkzY9hmDdIp8PaDiRkJNDTrT3v75zFddP6COcaD9fJiohA7eCA0NMrkK3/NoQQ1LCvQQ37Gg83kf+59A9LLy6lefnm9PXti09Zn+I2s5RSXgmW9sY07evNppnn2LvkEo16VSqSe6qYq4lsnxtK4p00qjRxolYHt2JTtH6VFIXK9DggEKgIzAXUwCKec9F8U3mQ1SSQlLB79iIj0C6QXoE9uBd6E1Wcrq+cX9MWHFm9jDPbgmnwrk44XGVhgZ6LC2mnC9ZmUpuZSdLmzZg2bZonFeg/9lwhLCaZWb0Cnq/++xxU1ta4BwejuX27QLbmSvx1XT/ZCxtAmw0ejaFaX7DLezQsZe8+Yr/5hvi/l2D5bs9HL1iUx3DgZsbPfoeLd9cSsn0zlnrmfOj/IT0q9qCMQckSZssv8fczub/la5orTyCbf4fG0p97f47GpFGjAm+uvA4UCkETL1uaGHjDfJjeoyrCNWdzJuUOrOwP1/aAf0+dMrbekzXcIwJGEJ4QzrdHvsXN3I1qti++YX5dCLUah59+JOqDAdz8/HMUpiaYNmz48HUbMwM+bVmJDxt5sPKgHXf3vIXJluG0PTKN9xpUpH2Vcg/roaWULAlbwpTjU7AxtEFfqc/isMWMqz2umM7uP804KeXqB0+klAk51+oXOsRvIo/c4ZyU6czHShOe6xBnAZClUZCWqeVB7pC1b2MurHLFJXw+yI/J78U+IT2BmPsxeFl6vXSsQiho596O2uaBLPjfMNK0Kfztc570LXtRiLJMuz+BiuU8mNlsJp47JpOhlRx1H8rj3YUzr0f85+qH80oFiwp8W+9bhlYdyuKwxSy/uJzN1zdTw64GfX36Us+hXonbgC2llMLiVtWawNYuHA++jo2LGb5BBQ+eZGu0HN1wjVNbIjCxMOCtEVVxqGjx8olvKEURIe4IVAVOAkgpbwoh/nX9OWTOZfff/v050G8gIy2n437Dhkux1/C0dcWjei1Cdm2jTreeqPV0wmCG/v6k7N2LlPmvqU7ZvRttYmKeeg9fik3ml52XaetnT3OfgqWwKQwM0CuK9h6RR+DANLgYDCp9COoJtT+Esu75Xsqi5zuk7N7N7R9/xKRhQ/QcHUjXpLM2fC3zQ+cTZRONYaYlY+Ji6JiVikHNSvCGO8MAm/75nYGsJLFid8xrDebud9+hTU3FZuTHxW1avnj4Nx95GJb3hbR4aP8rVOuV63ilQsnkoMn0DO7JiN0jWNpmKfYmz6ZyFgcKAwMcf59BZN/3uDFsOE5/zsK4Ro0nxpjoq+jTqAoa+xnYLO3B2xnL+WR5O37YcpF+9VxoX7UsP56cyKbrmwhyDGJivYn8dOIn1l9Zz7Cqw974jZw3kNz0Qf51YpdPInNSprMeHUrKrU3yowjxpjAz7p5Iwe3ON/g0bIarfwB7LLsyKO57CN8BFfLXfzssPgyAipZ5S//LSk9ny4/fo0zNpsPosdyP30DWP0bcsbrAZ0Gj6FihI4qoo3B+FTOzO1HW5pHzK6Uk8/p1zKuVjM21koqtsS0fB3zMgMoDWHl5JQtDFzJkxxA8ynjQ16cvrV1bo1a+GdobpZSSF6q3deVOZDL7ll2irIMJ9u7mL5/0FHejU9g+L5R70Sl41bGnXtcK6Bn+uy8hRSGqlZlTQyQBhBAlpPFr0fKoTOrfGyEGXS1OZe+6KKWK79f9RpY2C//mbUhPSebSof0PxxlWrUp2XBxZkZH5fo/EdetQWllhXLv2C8dlayWjVpzF1EDNV+2LKc1Jq4WwjbrWM3OaQ+RBaDAahodA2x8L5AyDzqGy/2o8Aoga+zm/n55B8xXN+ebIN5TRL8PUBlPx05vOgntjUehbwIK3YM8UnT1vKJfPHOSt698SZeyLeddfyIyOJn7JUsp07oS+xxuWVislHPwF5rYGlQH03/ZcZ/gB5vrmTG88nczsTIbtGkaapuQIVClNTHD6cxZqR0eiBw8hLeR8ruNUlVpB5W70zFrO8rdMcbM2ZtL2PTRb2onN1zfT3/tDfmn8C+b65vT06klGdgYrLq94zWdTCnBcCPGjEMI95/EjOmGtfx2PX4+FVoP28T7EyTG5zklKz0KtzeJOigprc4VO5XTK18wa0pf7CUouptuSfSj/HQoeKkxbvlw3RJudzYZpk4m9eoU2w0bjW7kOH7uNwVBjQo8Wbens2RmFBDZ9SqaRHTM1bbE1e1SXnH33LtrUVPRcXPJt538REz0T+vj0YVOnTUysNxEhBGMPjKXlypbMCZlDcmZycZtYSilFgiJHZMvE0oDNM89xPyHv6s1areTklgiWf3eM1KRMWg/xo3Fvr3+9MwxF4xD/I4SYCZQRQnwAbAdmF8G6JYoH/vAWbeC/sob4car5V0CDFsVNQ2acnoGTjx+W5Rw5vXXjwzEP6ojzmzatiY8nZc9ezNu0Qahe/AGbs/8aZ6ISGNfO+/W3rMpKhxPz4bcasPQdXepdqykw4jw0GgMmLxf2ehmxZpKTXXzJPHyM0IW/4Wftx9wWc1ncejHNXZrzbUc/olVO9Fd/j6zcFXZ9A393g9S4IjjB14s2+Q7ma/uQLIwp894yUOlz5+dpCKUSq//7qLjNyz/Bo2DrWKjYCgbuAXu/PE1zM3djctBkwuLC+OLAFy9t5/Q6UVlY4PzXbJTm5kR98AEZV5+jjdRqMsLQgupnvuDthrFYVvgdtV4mqZHv89saZz5beY7w28lUsKhATfuaLAlbQpY2K/e1SnlVfARkAsvQqT6nA3lTMHxjyYkQax7rQ5z8/AixdcYdJILalfQZMGMeb43+EoeK3ogLh9hwzZMlO+I5tWI2aSl5d5QuxF3A1sgWC4MXpxVKKdkx53eunjxGk/6D8AisCcCNi7qWKR7eOR0Jzi6FW6e56PsxaRhgZ/7IIc6MiAD+ey2XCotaqaadeztWtlvJH03/wLWMKz+d+IlmK5rxw7EfiLmf+yZKKaW8SRgYq2k9qDKZGdlsnnXuSW2F55BwO5XVP5zk0OoruPpZ8faXNXD1s3rpvH8LhXaIpZQ/ACuAlejqiL+UUk5/0RwhREshxEUhRLgQ4rNcXv9JCHE653FJCJGQc7y8EOJkzvHzQohBhbU/z0idE3xHlsm3yvSbhru9KTdUEo/kAP469xdHY45SpXkbYsIvEXPlMgD6Hu4ojI1Jzc0hToiAS1t0TuVTJG/eDFlZL02Xjo5PZeq2izT1sqF9lVzaFeWRtOQkdi/8i13zZhEZcoZszUta36TFw76pMM0P1g/V1YJ2mQMfnYKaA0CvcAkQiRmJbLi6geG7htNmdRumOJ4h1tOKIXsM+cn3SwLtAh+m49qaGfBFG2/2RaaxyH4MtPlRV6f6R32IflkHlRKEJpN7c3tglh1PSP3fMbVyJO1cCEkbN2LZtw9qW5vitjD/3L0Ezb+F7ovAIH/pSEGOQQwPGM6W61v489yfr8jAgqG2s8N57hxQKons15+sG0+3oQWMLMloOZEJmRGMOTgWX2sftnRdzc4h7/N2DWeCT0Xw8bjfGPf5N7zj+Q63U2+zI2LH6z+Z/zBSyvtSys+ABlLK6lLKMVLK+8Vt1yvhsVZoCpmNVvPYJtNz2i4lp2uwzdTpStiVUaJQKnEPqEH7kWPoOGUWxywD0KJg5/I1zBzYi/U/T+baqeNotdm5rveAi3EX8xQdPrpmOWe3b6ZGhy5Uadb64fEbl+IxszLA1NIAMlJg+1fgEMipMjrxyVwdYpdSh7ggCCGo61CX2c1ns6ztMoIcg1h0YRGtVrZiyrEppb2MS3njKetgQpPeXsRcTWLfP5efO05Kybnd0Sz75ijxMfdp+p43LQb4Ymjy3xLrK5IYuJRyG7ANdL0IhRA9pZSLcxsrhFACvwHNgGjgmBBi3eOtmqSUIx4b/xG6GmWAW0BtKWWGEMIECMmZm/s2cBGizelELBH/eiEGc0M1t40E5ZPMqKjvw+f7PmdJ04WoluhzeutGWg4ejlAqMaxShbRTTznEni1hz2RdJFNtDO6NoFIbqNACjMuSuHYd+hU80Pd6sejIrot3SM/SMqa1V4F+31KrJWTPdvYunkfG/RSUShUnN61D39gYV/9A3ANr4uofgL5RjoObEAWHf9eJZWWmgHsT6PQnuAYVunA8KjmK3VG72R21mxOxJ8iW2ZQ1KEsf7z709OqJRVAaVzu8RcyECTj++ssT59s10JH1Z28yafNFGo14G8d+VWF5H5jTElpMhBoflPjC9owNn2Add5yfzEczrFELpJTc/uEHlBYWlH3//eI2L39YV4KKraHOUCj/4pT/F/Gez3tcir/EL6d+oUKZCjRyblSERhYOvfLlcf5rNhG9ehPRrx8uixejsnq0SxyVHMXI6yu4YGZKv8T7fNTqM1RG1qRmJdAi4zQ2tzaQcT8F4uD2sao4mzqz8MJCWrq2LMaz+m8hhKiDLlPLBHAWQlQBBkophxSvZUVP3M4LD39WaDVkPh4IeU7KdHJ6FnYZtzHVz8bY4Mm4gIuDDeEO9ahgeoWWsfs57zqC0CNHuHRoH1bOLrQb8RmW5Z5t4ZOuSeda4jWaODd5ob2h+3axf+kCvOo1pF6P3g+PS63k5uUE3KrmZB8d+BlSYqD7ImLOZ6JSCKyMH2VKZV6/Dmo1avuSoUXwJuNd1pvvg75nWLVh/Hn2TxaELmD/jf1MrDcRH6tSVepS3lw8Amy4E+nMyS2R2JQ3xbvukwGmlPh0di4MIyo0DidvSxr3qoSJRfH2Ay4uCuwQCyHM0KVgOQDr0DnEHwKfAGeAXB1ioAYQ/qBXoRBiKdABeF7v4reBcQBSyszHjutTNCnfeUM++kdRsv2PosHWAJIyGWI1mpG3+vP1qe9oX68hF/buokGv/hiamGLo78/dP/4gO+X+I7XoukOh5kC4tk8nPnVxE4RtAKEg06QaaadvYj2oz0ud3NCbiZgbqnG1yn9E9k7kdbbPnsHNi6GUq+hN0/eHUMbGjohzpwk/fpirJ48RdmAPCqUKJw9X3I1v4568EzN1BlTuAnU+ArvKBfmtAaCVWs7fPc+uqF3sitpFeEI4AB5lPHjP9z0aOTXC18oXhcj58zUG66FDuT1lCsmbNmHW+lHEQAjBxI6VafHzXsasDmH+e9URA/bAmsGwaRREHoL200G/hOrYHZuN/un5/K5pR7Pu/4dCIUjZu5fUI0ewHTMGpYlJcVuYP0xs4O0lhV5GCMH42uO5nnidz/Z9xuLWi0tUeyKDihVxmvkHkf36E/n+B5RfMB+lmRm7InfxvwO6XsrTa31Fo9XDiV82nBMGbTm/ZycaTRYegTXxb9WB+ZO+48L2jbwz/B0mHZ3E2Ttn8bPOW2p5KYXmJ6AFumszUsozQoig4jXpFZFzQXa7tp6MMk3JfjyIm5kMGcnPfD/qIsR3sC/zbMRXCEFlB3Pm3GvJn3rrsfHMJOi9+Vw+cpCdc2ey6PMRNBvwf3jVbfDEvPCEcLJl9gsjxBHnTrPl92k4+/rRYvCwRy33gLs3UshI1eDgaQEJkTqNgspdwak6MQdPY2Oq/0SbqMzrEeg5Ob209KiUvONg4sD4OuNpXr45Xxz8gp7BPRnoN5D3/d5HrSgV3irlzaRmB3fuRCazZ8lFypYzwdbVDCkll47Gsm/ZJbI1Whq87YlPkMO/PuD3IgrjUC5ElyJ9Dngf2AV0Bd6SUnZ4wTwHIOqx59E5x55BCFEecAV2PnbMSQhxNmeNya8jOgyPaohzbHgdb1msWDuZkKaQaCL1GBk4kj3Re7hZQaDJyuT8rm0AGFb1B62W9JBzT05W6evUOdv+CB+HwoDdEDSKxLPxgMT85mT4tTps+1Kn0ptLGtr5m0n4lDPL1+86Mz2N3Qv/YuGnQ4m7GU2LQcPoMX4S1s4uqA0M8Khei5aDhzPoj/n0GPQOAS5akq+dYefJRP68HMCC5G4c0DQgNtUw37Wd6Zp09kbvZfzB8TRZ3oR3gt9hTsgcLAwsGBU4iuCOwazusJph1YbhZ+33yBnOwbJPbwwqVybmm2/RxD1ZI+xkacSnLSux99IdVpyIBiNL6LEEmoyD0DUwqxHEPm8/qRi5vh+56VN2ZlflZsAn+DqYI7Ozuf3DVNROTlj06F7cFhYrBioDfm70M4YqQ4buGkpiRmJxm/QERlWr4vjrL2RcuULkwIFMOzCZobuG4mjiyD9t/6GSwod1qW2Ys19JyK6teAU14r0ff6fDJ2Mp71MZy8BGmCVE4pDggYnahEUXFhX3Kf2nkFJGPXXoxfm+bygPLhF6mUkoZBZa+dQ1I5e06fuJCZhmJWFrmvuvpLJjGXbcsyDbrTEc+xMlWirVbUCvydOxLu9K8PQpbJ/9G5rMR3v0YXEvVpi+E3GNdVMnYungSPuR/0OpetLBelA/7OBZBraNAwQ0HQ9ATFL6E+nSoEuZLq0ffjXUcajDqvaraOXaihlnZtAruBdXE56jqVBKKSUchULQvL8vxub6bJp5jns3U9gyK4Ttc0OxsDOm+9ga+DZw/E/4Ni+iMA6xm5Syr5RyJroorjfQQkpZsOa0udMDWCGlfHjVklJGSSn9AA+gjxDCNreJQogBQojjQojjd+7cKbQh8ql//+242phwTZlN5IU43vZ8myDHIH6JnouFuwtntm1CarUY+umiPWmnTj1/ISGgXFVkw89JvGWNUWBV1J0ng5kDHPoN5rSAHzxhzYe6vr6Z98nK1hIWk4xPObM82Sql5PKRg8z9eDAnNqzGt2FT+v30B76Nmj2xA0+2BkJWopjdBIc9gwmyuMB7A9rw3qTvCXq3H3rG5hxZtYxFnw9n1pC+bJ89g2unT6DJyl0QKC49jjXhaxi2cxhBy4L4cMeHbLq2iWo21ZhYbyJ7uu9hTos59PbpjZOZ0wvPQahU2H/7DdnJycR+O/GZ13vVKk91Fwu+3hDK7aR0UCig/sfQex2kJ8KfjeHM0jz9vl4L8RHIf3pzQ9gzTjWMkS10PXsT160n49IlbEYMR+j9t+pTcsPO2I6fG/1MzP0YPtnzCRrtS2rcXzMmdetiOvELUk+fxuab+XR37cQEu2EcmPorf48dSWR0HDU9VHzgeZrmXds/kUrarWdXsoSafas30rFCR7Zd30bs/dhiPJv/FFE5adNSCKEWQnwCXHjZpDeRR9/xAoU2G61UPjkgl17E2bd13RHsnuMQ+zmYo5Vwxb03pMRCyCoATMta0e3LiVRv35kz2zax5ItRJMTo1g+LC8NEbYKjybPp1El377Bq0nj0DA3p9Nn4R6U6j3HjUgLm1oaYJJ+G86ug7jAw1631tEMstdpSh/gVY65vznf1v+PHhj9yI+UGXdd3ZcH5BWjlm9vpoZT/LgYmaloNqkzG/SyWTjjKtbN3qd3RnY6fVKOMjVFxm1ciKEyuzUMvQUqZLYSIllI+q6L0LDeAx70Dx5xjudGD5yhj5vQ7DgHqoxP1evr1WcAsgMDAwEL7sQ+/A/8jOyhuVsZsVGnxTs4i7tZ9vq77NV3WdeGATQTeVyQRZ0/h4h+Anod77sJaT5F26jRZkZFYDRoENTvqBKrSEyF8uy6t+sJ6OL0IVAZkONSjk3ShukUPncjVC0i4fZudixZw7ewZrJ2caDvoCxwqeALZj+Zma+D8ajj0q07wq6wHtJsOft1BbYAlYOnqTfV2nUhNSuTaqeNcOX6E0L07ObMtGLWBIa5VquEeWBOlhw0H446xO2o3p++cRiu12BrZ0t69PY2cGlHdrjp6yoI5egaenlgNHMjdX3/FrE0bTBs/qitVKASTO/vRato+xq4JYWavAN1unmt9GLQPVvSH1QN1KdQtJ4O6GGtAMlJg6TtoMjPplTqGwW8FUMZID216OnemTcOgcmVMW7659aRZ6RmoDYpO9dzfxp8van3Blwe/ZOrxqXxa49MiW7uwHL11lNGpvxHQxoCWB9SkzDzHRs0JTMta07D3B1Ru3Ay9jHswo5ZOhK7XmoffkVaW5mg9AtG/fISG5l+xSC5i2cVlDK02tJjP6j/BIGAauuyrm8AW/q0q0zmXZCl0NcTZvNwhVtyNRgK2prlvQPk56oTy9mur4GldCQ7/BlV6gBAoVSqCer6HQyUfNv/2Iws/G0aLwcMIiwujomXFZ6Is6fdTWD1pPJlpafSY8D2mZZ9VbdXm1A97BNjA9uFgWk5XfoRuwzcmMZ2Gno/EBzWxsciMjNKWS6+BZuWbUdWmKl8d/Iopx6ewK2oX39T7BgeTXBMbSymlxGLtZEqzfj6c33+D2h09sHJ8w0rWXjGFcYirCCGScn4WgGHOcwFIKeXzwnvHgApCCFd0jnAP4J2nBwkhKgEWwKHHjjkC96SUaUIIC6AeulqpV458EBv+jzjErlYmRKizIQ2iQuOp2tyZifUnMmjTACoZunFq60Zc/AMw9PcnZdt2pFb7ZDT2KRLXrUUYGGDavPmjgwbm4NtZ98jOgoiDcHET4uw6Jqm3w9bZsDX39TRawfE4R47cdUIISUObCKoa70Ox6u/nn5RTTZ0QVcXWughrLhiZmePToAk+DZqgyczk+rlTHDuwiUunj3HpyAG0QnLbIgNzF1M+qPEujf3a4GVZMOGv3LAa8AHJW7cSM348RoEBKM0efYzcrE0Y0cyTSZvC2HjuFm39csQRTO2g91pdW6b9P8GNk9BtAVi6FolN+UKrhTWDkbdDGakcg0m5SnSvrtv/il+0CE1MDOUmT37h30pJIFuTRUJMDHE3o4i7Gc2tyxHcjojkflwM2uw0zKyd8W1YH/fAmliXdy30/3/HCh25FH+JRRcW4WnhSccKHYvoTAqGVmqZEzKHP47+So1YR9ySHDjrnIhpcjK1XSpQY+o0VOqclE9DI11qZ/AncGoR2io92bP0EldO3iaodRuOXD7EgTUHaBTYiOWX+gjJTQAAIABJREFUljPAbwAGqv+maMfrQkp5F+hZ3Ha8Dh7/7AmpQfu0Q5z0bFWVfkI0mSZW6KsScl3TxswAOzMDzt1MglqDYf0wuL5ftwGZg3tADXpNns76nyex/sfvMHZLxbV90yfWydZksW7qROJu3qDzmK+wdnbJ9f3uRaeQmabRpUvvCYdKbR92NEjO0JCamY2d+WOCWqUK068VK0MrpjeezprwNUw+NpnO6zozuvpoOnp0/M+nmZbyZuFW1fqRcF8pT1Bgh1jKp/OS8jxPI4T4P3Q71kpgjpTyvBBiAnBcSrkuZ2gPYKl8spjTC5gqhJDoHO8fpJRPFbC+Gh5Z8d/48itf1oj7SpBmKqIu3KNqc2dq2deib5V+nAxfieLkURJvx2Lk70/iipVkXr+OvptbrmvJzEySNm3GtEmTR+JbT6NUg1sDcGvAD1nvcuLYQda0FSjkszv4EZF32bEzhPj4+3hWsKdhQ29MTV5yg12uGjjXzNO5p2alcujmIXZF7WJv9F7izeNRNVRRX1kFnwQ7LK/Gk3DyBhkn93DU8Rr3AmrgHlgLew/PQjt6Qk8P+2+/5Xr37tyeMgX7r79+4vX367my8ewtxq09Tx13KyyNc6LRSpXOKXGqqYsUz2wAHf+ASq2feY9Xyt4pcGEdO50/Yt0lb1b19kGpEGji47k7cxbGDYIwrlnj9dr0HKSUpCYmEH/zBnG3oom7EU38rRvE3Ywm8XYsUvtYapwwRqG0wMjCG5WeCYmxlzi4/G8OLl+MqZU17gE18QishaO3zzO1gXllZOBIwhPC+frw17iau+Jv419EZ5o/EjMS+WLLaJIOhdI9yhlFlhYbXxeqf9gZw117iftjJvd++hmbUZ88uhkM7A/nV6PZPJ5tBytxNSQZfSMVF4OTuW/pgeLMXrp0H8rOqJ1svLqRzp6di+Xc/isIIdzQRYhroav0OQSMeCBm+W9CPCY0pdBq0Ao1Mivj0ZX6qQixlBLTlBiyy3kC4c9dt7KjOWejE6BTd137o8O/P+EQA5hZ29Djq8msnzMNduxGf8VlEt1jMbexRWq1bPl9GlHnz9L6/0bi7Fvlue8V/bB+2AL28MTGe2yiLvHO1uyxlkvXS3sQv26EEHSs0JGa9jUZe2As4w6OY2fkTsbXGY+V4X+nV2sppfxbKRZ5QillMBD81LEvn3o+Ppd524BikSmVOZfXp/U6/q0YqJWUMzckIUuJKjwRTWY2Kj0lH1b9kPfDDyP/n72zDo/i/NrwPbvZZONuGyEuJEQIbkUKxYu3SIEKlJZSqPyoUPtaqAFtKTVqQJFSAUqgWIu7a4QISYi722Z35/tjoyUQwWHv6+qVZPRdOjvzPnPOeU58MYe3/UnfHtrU1/IzZ68piEv270dTWNhk7+EaItKLkToGIOncvcHy0oJ89v7yI9GHjmFh78ioN+biHhJ2Yx+0muyybPal7GNP8h6Oph1FqVFiqm9KT6ee9HHtQ3dFd0z165xKC7MyiD91nPiTRzmxeQPHN/2JkbkFnmGd8OzQGdfAYGQGrYuCGbYLxPqpJ8n98SfMBg3CuFu32nV6Ugmfjgli2NKDvL85gi8eD224s+8geHY//D4F1o3X1qH1fUcrmG81UZth74cU+YxmRkRXxoY5097VEoDcZd+jKS3F7pVXbv04/oNKqaQgI4289FSt+E1Nrv29sqyuNatUpo+xhR1IbJAZuSFqzNEzsMG5rQdeYc64tbPByEwfjVrDnlXRRB2Ox9E9H6k0iYt7/uHsji3oGxrhHhKGZ8cuuIeEITdufkqSnkSPRQ8tYvzf45mzZw7rhq7DwdjhVvyTXJOj53fx+5rFOCcJSLHAt2sPOg4bhb2H1gFbDApFLC4h7+efkZqZYTPjWe2OEgnKAUvY+ul2Uq8U02OsF25BNmxYeBpraSgVqjgu78vA19KX1VGrGeU9ShdZubWsRdvesCbV4HHgV6B5bwXvIWquIxGtIAbQlOZq48Ryi6sEcXFuNnJVGRV21xeTQU7m/BuVSbFaD9MOT2n70+fGg7Vng+2kejLk/QPZk/cH/aPkrH59NgNnvkTapSiiDu6lx+OT8e95/bZqaTH5WNgbYWxxdSlGerUgdjQ3rF2mTExEMDBAz75RCxUdtxCFiYIfB/zI2qi1fHH6C0ZuGsnbXd5mgNuApne+T8iryGN/yn462HfA2fTqmnkdOu5FdH79zUWjDRE/SFM4D1tjErIqsazSkB5XiEtbK2QSGfMHLeTzw0+h3r2d3uOmIjEzo/zsWSxGj2r0OIWbwpFaWzcQdtdCoxGJSivi0VBFvWVqzu3cysF1q1BXKekyejydRoxBpt/6Ok5RFIkriGNv8l72JO/hQo420cDJxIlxvuPo49KHUPvQa7ZaMLdzoP2g4bQfNJyKkhISzp0i/sRRLh05yIXdO9HTN6BNUCieHTrh2b4TRuYWLRqfzQsvUPzPv6S//Q4e4ZuQGNdF1v0dzZjZx4slu2IZFqygn/9/JkWWbvDUDtjxBhxaAiknYczP2tTqW0VmBGx4FtEpjDllTyKXlTF3oLb9iDIllfw1azAfMQK5j88tOb0oipTm55GXlkp+eor2Z1oKeWkpFGVnI9YzQjGxtsHK0Qm/Hr0xMrOjotSE3Ax9spNElCowNJUR0NEG92BbnP0skek3TIaRSCX0neyP3ETG2X+T8erQnme/e5WUqPPEnzzG5dPHuXTkABKpFGf/QDw7dMYzrDPmdk1PXs0NzFnadykTt05k9p7ZrBy48panF4uiSHLkBTav+5qKmFTspQLuvboxYMzTmNs1vGYEQcB+3puoi4vI/uILpOZmWI4fT1mRki0rC8hVtuVh8y/wtZsMtmMYPjuEDYvUlEutSNyzjYlvTOSdI+9wLOMYXRy73NLP9YBjJIriqnp/rxYE4X93bDS3kAYR4uqMInVRtSA2U1zlMp0WGwOAXOEGRVyTds7miCJcTC2ia6dpxP57HDaG4/3MS1dtG50XTaqTkgmTP2P7ksX89ak2sye4/yA6jRh73fFr1BrSYgvw7tj4/SGjSCuIHepHiKsNte720pP7FYkgYVLbSXRz6sa8A/N4Zd8rDL4ymDc7v4m5gfmdHt4t40rRFVZGrGRT/CYq1ZVIBAkD2gxgasBUXb9mHfc8OkHcTGpdph+gqIa7jTGbkwroINUnOUrbtBvAxcyFLoNGk7x8Cz+s/5ARwcGUX8NYS11YSMnevViMf7xZ/RKT88sorlQRqNA+VDLiYvj3p2/IvBxHm6BQ+j01A0vH1plZVGmqOJN5prY/cGqJ1sutnU07ZoXOoo9LH7wsvFocuZKbmODf/SH8uz+EWlVFcuRF4k8eI/7UMeJPHgVBQOHtVyuMrJyatreXyOU4friApElPkPXFEhzmvdlg/cw+Xmy/mMG8jRfp6G6Fmfw/wl0mh6Gfg2tXbf3bdz1hzE/gfgtakZbmwq+Pg4Epe0M+Z/f6FN4d1hZbU+0Li+wlS0AiwfbFWTd8qiplJflpqbWpzflpqbUiWFleXrudnoEBVo7OOHj50rZXXywVzlg5OmHhoKAgS0XCuWwSz+eQe0IbIbZ0NCJ0gA3uwTbYu5k1mGQ3hiAR6D7GG0NTfY5sjKeyTMXA6WF4deiMRqMmIy6m+ho4zp4V37NnxffYuLrhVX0N2Ht4XXMy62nhycc9P+bF3S/yzuF3+KTnJ7ckmqrRqIk7foRj4X+SFR9Hhb6asjArZj31MQ4213ZFFyQSFAsWoCkuIeP9DyiTmLEnworS/EoGzWiH27Fi2DYXPHpj7WTDsBdD+G1+e0xK/kX/siVWcivWRK7RCeJbyzZBEF4H1qF9fD0GbBUEwQpAFMW86+18T1H/uyFUC+KSakNFU0fIjm6weUrMJdRIMFe4Xl8QO2mfQRdSC+jk6s6+ohnIzivxbmTb6LxovCy8sFW4Mv6DhRz4dSVVlRX0fXJGk9/dnJQSlBVqbbp0I2RUR4jtzBrWEBt4eja6vY7bh4e5B6sGr+LHCz+y7NwyTmac5P3u79PdqXvTO99DnMs+x4qLK9h1ZRd6Ej2GeQ7jUc9H2Zu8lz9i/mB74nY6OnRkasBUejr11GX/6Lgn0QniZiI+KLnS9XC3MSZfqcLGzZorUXnUj++OHTCdhRt3knXgOPnendE/eBB1cTFSU9MGxyjath2xqgrz4ddrTV3HxVTtDMXHUo9/f/yGc/9uw9jCkqFzXsOnS48W32hLlCUcTDvI3uS9HEg5QJGyCH2JPl0UXXi63dP0du6NrdHNMxiQ6slwCwrFLSiUvk8+S3ZSQq04PrB2BQfWrsDSUYFHWGcUPn61Ik2vkRZERmFhWE6YQP7q1ZgNGohR+/a16/T1tKnTI785xEdbo/ho1DUqCYLGgUMQ/P4E/PIo9H0Lur90TVOxFqOugj+mQHEmFU9s4a11ufjam/JEF206YkVkJEWbN2M9bRoyh+ZFqEVRpCQvl7zqCK9W9Grre4tyshs0BTe1scVK4UzAQw9jqXDCytEZS4UTplbWtYJTVaUmJTqfyCM5JJ4/TVmhEkEARy8Luo/Rpva2tu1A+0faIDeRsXd1NOFLzjJ0ZjByExkKH38UPv70nDCV/PTU6vT6Yxzb+AdHN/yGiaUVHjXp9QHBV/3/7+3Sm1mhs/jyzJf4WvrydLunWzW+xqhSVhKxdxentmykIDOdchM4F5hH/8GTeab99Kt6ZDeGIJPh9MXnREyfy9btlWBSzvDZ7XH0sgCHr2FZL9j2Goz5CQd3cwbOHMuWTw8QtS6ccTMfY1nkd1wpuoKrmetN+1w6GjCu+uf06p81N87H0Qrkxutb7kFqHwkCiNWtljX1BfHlvVqzv+r7QXp8DDn61rgYG159sHpYmxjgZGHI+ZRCEs7nUKk2pFJtSFFuOWbWDfeNzoumh1MPAPT09ekzZVqzx19TP6zwaTyLKKOoAitjfeQybaaKqFajTE7GtF/fZp9Dx61DT6LHjOAZ9HTuybwD85jx7wzG+YzjlQ6vYCS7d9vZaEQN+5L3sSJiBaezTmOqb8rT7Z5mgt+E2jlTe/v2TAuaxvqY9ayKWsXMXTPxsvBiSsAUhrgPQSZtnZ+GDh13Ap0gbia1LtMPUNK0m402TVff2ZiUfemUFSkxMtNO3AWJhB5DHufIml9YUXKA6aJI+bnzmPRo+Ga0MDwcfU9P5AFtm3XOiNQC/EtjOPrpWiqKi2k/cBjdxk3CwKj5D5aM0gz2JO9hb/JejmccR6VRYWlgSR+XPvRx6UNXRdfb8qASBAE7Nw/s3DzoOmY8xbk5WmF06hhnt2/m1JaN1dtJMLOzw8rRSRvJVDhh6eiMlZMzti/NoWTPHtLnvYX7XxuRGNRFCYJdLJjW04Nl+y8zLEhBN69rGHvY+cG0Pdq2OLvehyvHtIZbRlY3/iG3vwGJB2DkMr6JtSC1IJt107ugJ9VOPrMWLUJqbo71tGeu2rWqoqK6ljelWvym1kZ/qyrrOrjJ5IZYKZxQ+PgT2Kc/VgpnLB2dsHRUXLNOu7xESdKFXBLO53AlMg9VpRqZgRTXACvcg2xoE2iD3OTmPKzbdlcgN5Kx86cINiw6xfDZIZhY1o3L0tGJDkNH0mHoSMqLi0g4c5K4k0eJOrCX8/9uR2Ygx626rZd7aAeMzLSRqWfaPUNMfgxLTi/B29KbXs43Ft0vLy7i7I6/ObNjC+VFhcid7TjUoZBcFwmfPvQ5nR1bVl6amVLBMatRCCVFhJxciFne/wGdwL4tdJqubVXz6FcgM8S3vROb23RCSDyI6fYByFz1WRu9ltc7vX5Dn0lHQwRB6Agki6LoXv33FGA0kAi8d19Fhmuo14dYUxshLtQuMnMEUQ2l2WCqNbrKS7xMpoEHpvKmpz9BzuZcSC0kOk+KnqQKlUZGelxhA0GcU55DbkUu/tb+rRp+WkwBlg5GGJs3XgKUWVjRwFCrKi0Nqqp0hlp3GQHWAfw27DeWnl7KL5G/cCT9CAt6LCDULrTpne8iKtWVbInfwsrIlSQUJuBo7MjcjnMZ5T0KY9nVpqim+qZMDZzKRP+JbEvcxoqIFbx96G2Wnl7KxLYTGesztoH/ig4ddys6QdxMaoNSD1AqiEe1IC4x114myVF5+Haui/K17zuYY7//SlWJgChA+dkzDQSxMjmZ8tOnsX3ppWZFdnNTkinf9BUP5ydh4eVLvzffx9696bQwURSJzouuFcFReVEAuJm58YT/E/R26U2wbTBSSauM0W8aptY2hAwYTMiAwVoxmJZSJwhTtb8nR11EVVlZu4++oSHmAe7ILsWS8MaruIwdh5XCGQtHBTJ9A17q78POyExe23CeHXN6YaR/ja+0gQmM/kmbQr39Da0L9bgV4HQDpmSnVsCJH6DrC1xxHs53f+xjeLCCLh7WAJQcPETJ4SOYzJlFckIceTWR3urPXZKbU3csQcDc1g5LhTPO/gHVLwa0LweMLa2adf0UZJaRcC6HhPPZZMQXIopgbGGAX2cH3IJtcPaxRCq7NTV3HqG2DJsVzN/fnmf9wlMMfzEES4erJw+Gpma07dWXtr36olIqSY44X51af4zY44cRBAkKX388O3TGq0Nn3u/+PklFSby2/zXWDF6Dh0XLA3uFWRmc+nsTF/bsRFVZiVtoGJe8yllRso329u357qGF2BnZNX2geiRdzGX7sgsYWxgw+IWO5M+WkvLc87iuWIFhu8C6enWNunafR5+dwKY3DpIXe4Txklf4U/olM0Nm6iZLN5dlwMMAgiD0Aj4CZgEhwPfAmDs3tFtDza1BBFSSakFcVqx9d23qqF1ZnAam9uSlpaKqLCfL1A7T/5aZNEI7Z3P2n8/gSrFIqOMZIjKDSIstaPAcjMrVPm98LX1bPHaNWkNaXAG+na6dPZNeWIGjeSMO07oexHcdBlIDXu34Kr1devPWobeYsm0KUwOn8kLIC+hLr84Cu5sorCzk90u/syZqDbkVufhZ+fFxz48Z4Dbgml4q9ZFJZQz3HM4wj2EcSjvEiosr+PzU53x//nvGeI9hUttJt90kUoeOlqATxM3lwQsQ42RhiEwqkCKosDGWXSWI5SYmtO3RB83BXVy2y6bk4HZsX3ihdn1huLaDlvmwodc9T1VlBUc3/MbJzRuRIaUg9FFenvv0dQ1DlGolJzNOsjt5N3uT95JZlomAQIhdCC+HvUxvl964m9+BPrzNRCaXY+/hVeveW4Oo0VCcl3tVK6DsvFzSUhM498Un2g0FATMbWywdnZhmbMOvMRV8tqKY2aO7Y2Jl3biAFAToNE3bguqPKfDzQG1f5o7PtPxFT9IR+PtV8OwH/d9n/vKjOCizGG8t5dDvq8lLSSbjyCFKgzxQ79kKe7Sm8gZGxlgqnHBt2642Gm6lcL5m2vj10GhEMhOKauuB8zPKALB2NiFssBvuQTbYupretnomJ19LRr7cns1Lz7Jh0WmGzQrGrs212rFrUyvdQzvgHtqBfk8/T+blOK04PnGU/at/Zv/qn7FSOPNkcC+Wla3nxV0vsmbommabtmRejuPE5g3EHDmIIJHg36M3rv26837sYiJyI3gy8ElmtZ/VrMlOfS4dy2D3yiisnIwZNisEIzN9TH/6iaQJE0meNo02a1bTWKzL28OFUkUA8vSLGF/pSoeKYWyM/YvJAU+06Pw6rou0XhT4MeB7URTXA+sFQWjc6OEep369v0qoAkCtUoOMeoI4A4CMeK2hVqaBXfMixE4WBCj1EEXwt71Ibqk16XENM2su5V8CwNeq5YI4+0oJVRXqa6ZLA2QWVRDsUre+tgexLkJ819LBoQPrh69n4YmFLL+4nIOpB/mwx4f4Wfnd6aFdRVpJGqsiV7E+dj3lqnK6K7ozNXAqnR06t+rZKQgCPZx60MOpB5G5kayIWMHqqNWsiVrDIPdBTAmY0qrvig4dtxqdIG4mmgesDzFoW/y4WhmRkFNGiJ8lyVF5iKLY4CYZMmAwF/fsJMPVCYfz8cTmxuBt7YMoihSGh2PUqRMyheKa54g/dYzdy5dRlJ2FR9fevJnahrk9OzQqhgsrCzmQeoA9V/ZwKO0QpVWlGOoZ0tWxKy+EvkAv515YyW9CGvAdRJBIMLOxxczGljZBdX1o1UVFxAwdSoWVJYYvzyY/K7O2rjYvJpreFeWw6wDf7/oBmYFcm1JcLTatFE61plIyuRycw7StmTY+C1tfhStHYdgSbRT5Omg0aoqys8mPPUPexrfJV7UjT+1JxvTJ+BYX4gsc/kmbAm5qbIJBSSmunbvh2KNn7ViMzC1uSKBWVapJjsoj4XwOSRdyKC+uQiIRUPhYEPiQM25B1lfV991ObF1NGfVqGOFfnuWvz84waEY7XPybviYFQcDB0xsHT2+6j5tEYVZmbXp99M5/6Kk2pkJfzeKz05g0bA7uwe0bTRcXRZGkc6c5sXkDVy6eQ9/QkLChI2g/eDinSyOYdnAOoiiypM8S+rq2vAbx3O5kDv4ei8LbgsHPB2FgqH2EyOztcf35JxInTuLKU0/j9nJ/GpPZPUeO4szX7yPYJuKf3YWzm04wwU+F3u1oC/ZgIBUEQU8URRXQj7oaYrhfn/f17ifqmpRpsfrqM6sWxEVpgFYQCzID8mUWmDVDEAcqzAhUSsHGAAvDAhRmKSQlezYoH4rOi8bZxLlVmQ6pMfX6DzdCpUpNbqmyocN0YiISIyOkNrret3czxjJj3uv2Hn1d+/Lu4XcZ//d4ngt+jqcCn0JPcue/ilG5USyPWM7OxJ0ICLdErLa1bsunvT5ldvvZrI5czfrY9Wy+vPmGRbcOHbeCO/+tvFeoFsQPmreWu40JCTmluHRyJO5UFnlppVg71Qknew8vHL19KUlPw6gymcXrX2LJ0xvQXIiiKukKNtOnN3rcopwsdi//nviTR7F2duWxdz8mRmJL+YqTBCjqImDJxcm1rZFOZ55GLaqxMbRhoNtA+rj0obNj51veluZuQGpmhvO775Iy8wWsL0bj9/zztetEUSQrI4tnv9qGZVUBE/wMKcpMIz32EpeOHGhgQlXTcsjKyRlLhyew8vLG6szPmKZfQHjsF7Dzo6K0pIGRVY2xVX5GGuqqquojOSA3MsTcREO8vjNlzkG8PKYHdi4umFpYkTR8OHqW1ri988ENtwYpLazU1gOfyyY5Oh91lQZ9Qz3aBFrjHmyDa4B1rTC7G7CwN2L0/7SieMvX5+j/ZABeYS1LSTa3s6f9oGG0HzSMitISEs+eYs+ePymIimfz4g/Rk+njGhSCZ1hnPMM6ITcx5dKRA5wMX0/2lURMLK3oNfFJgh4eiFRuwNdnv+bHCz/ib+XP4t6LcTG9tot0Y4iiyPHNCZzcmoh7sA0DnglAT9awBEG/TRtcf/qRpCcmk7R4M27dJFc9YHr36MC+lY6UJB/Co7sHnqc7suG3/YyboDMIukn8CuwTBCEHKAcOAAiC4AUU3smB3Sokkro+xLWCuObKM7YDQVLbizgjPhY9e1dEQdKslOmKzHKsNRJSLbT3MIVpMgDp8QV4hmq/09F50a2O/KXG5GPpaFwrrv9LVpG2fKZBynRSEvpubjohcY/Qy7kXG4dvZMGxBSw9s5R9yftY0GMBbuZut30soihyOO0wyyOWcyz9GMYyYyb5T7rl6cxOJk681uk1ZgTPqE3LnrZzGv5W/kwNmMoAtwF3xUsCHQ82uiuwmYg1oqIZDqz3Ex62xuyPzcbJV/sGOzkqr4EgBggZMIRtX39Grokh8qhExoSPYdyWQoL1BF4W/kS5dWPdxhoRu8hKHM5pTZMywuScaVvIv4kfk1FYgVGbcj6PWIs0SqCwspDEokQAvCy8eCrwKXq79CbQJrBZTrj3G6b9+mE2eDA5336HWf/+GHhrG4AIgoC9oz0vTxnC5J+P46/wZO5T2glalbKSgoz0auOqOgOryP17UJaXVR+5A3qXNZifmEm5vi1lxSW15xQkEizsHbFUOOEW3B6r9H+wzDmM1fgvMQx5lG/2xrNpxyV+eaoT/j5a58ncn5ejSktHsWDBDYnhkvwK/vk5krS4AhDB1EpOQA8F7sE2OHpbIJXevdeAsYUBI19pz99fn2fHjxepLPMloGfr2oXJjU3w6/4Qft0fYsGhD9hxNJxx0r7kxCVy+dRx/hEE5MYmVJQUY+3syiMzZuPXozd6Mhk55TnM/WcWJzJOMMZnDK93eh0Dacv6d2s0IvvXxRCxPxX/bo70nuiL5Br/9nJfX1yWfceVKZO5stcat8pKJPUyDyQSCd59B5MZ/hNGDiIXHS/A/nacd0whqI9zq/59mqSqAhL2w6WtYGAKAz64Nee5CxBFcYEgCLsAR2CnWPvgQoK2lvj+oyZlWgCVRFuzrharpzYSPa0oLk5HraoiO/EyQkAvKKJZKdNRR9LRSOCQUtvSzdYoEz2ZhLRYrSAurSrlStEVhnpcvyyoMdRqDWlxhfh3ubYQqelBbP8fQWwYqOv5ei9hIbdg4UML6eval/lH5zN281heCnuJx/0evy1zmSp1Va3hVWx+LHaGdrwU9hJjfMZgpn/tsp6bjbmBOdOCpjE5YDJb4rewImIFrx14jSWnl/BE2ycY5T3qnnbm1nFvoxPELeRBeyfrbmOMUqWhWCpiYW9EclQeIQ83bJXi06UHe3/5kSuOSsYqrVllYEzAmUQuB1mjZ2Jae5EZpldif7gQgwIVxa5ysrqYoTLVo+b2V6lUIdczxFRfa0ZkaWDJON9x9HbujYtZyyJa9yv2b82j9MgR0t56C7e1axGkdVG6Xj62jAlzZtn+ywxu50igkzkyfQNsXd2wdXVrcBxRFCkrLKhra3T5IgXH1uPoYo5lyBisnFywUjhhbueAtKZ/9P5FEL8Nhr8HoSNILShn6e5YBgY40KtaDKsLC8lZtgzjnj0x7tq11Z+zvFhJ+JKzlBZU0mmoO+7Btlg7Gd+7RWSuAAAgAElEQVRTURG5sYzhc0LYvuwie9dcory4irBBbW7oM8zt+jqXSxL5Nutffp7wM4pyM+JOHiU3JRn/Hr3xCK0rNziRcYK5++dSoixhQY8FDPcc3uLzqas0/LM8kvjTWYQOcKXrSM8mx28UGorTjP6kfLmNvFVrsZnZUIeNHj2YxVvXcX77VnxnhBD563n4DQyM9Bp4FNwQpbkQu0MrguN2Q5W23zT697cgBhBF8Wgjy2LuxFhuC/WuR420JkJcL/pr5ghF6WQnJaJWqdBYKtArETCUXd9ksUqpJu5EJnquxiQV5FBlLyKTaLD3MCcttgCAmPwYRET8rVruMJ2dVIyqUo3iGunSoDXUgroIsahUUpWaitmQwS0+n447zyD3QYTZh/Hu4Xf56PhH7E7ezfzu829ZdLZEWcKfMX+yKmoVWWVZeFl48UH3D+54SyQDqQGjfUYz0ntkbWunT058wjfnvuEx38eY6D8RG0NdSYCO24tOEDcTTfXPBy9lWitOE3JKcfG3IupQGuoqTQO3Xj19fQL7DuDkpj8JSixiocEzpJQdot/0DxjRuzdlhQXsX7OciH27MLO1o+/cZ/EMu7rFS89Pd9PVyYKvB7S/ap0OLXpWVtjPm0faq6+S98sqrJ+c2mD920Pasi8mm7l/nmfTC92RXSOSJwgCxhaWGFtY4tK2HVR0g7QPoP9E6NaIEW30Vtj9AbQbC93nAPDh31GIIrw1tG4ymPP992iKirB79ZVWf0ZluYrNS89RlFvB8BeDUXhfe8J4tyPTlzL4+XbsXhnFsfDLlJco6THGu4ERUIuOJ5Gx+KHFPP7347y09yXWDV1H19HjG2yjETUsv7icL898iaupK8v6L8PH0qfF51JWqNj23QVSovPpNsqL0AHN7xlsGuyGqXM5OT/8hPnI0Q18BAzlBpiEPoT6xFZ8lJP4vu1sFPEu7FoZhb5cintwK/uC58RpBfClrZB8DEQNmCog+HHwHQwx2+Dcb607to67l3pfJalE+6TWiPUm+6YKyE8gIz4WgBJTBabyiiZf7CSczUZZocanswPsyKGkUoWlXILCy5yTWxOpLFcRnRcNtM5Qq65++DqGWtWCuKbtkjIlFdRqnaHWPYydkR3f9PuGP2P/ZOGJhYzcNJI3Or/BMI9hN+2Fb2ZpJmui1vBHzB+UVJXQ0aEj73Z9l55OPe+ql8oSQUIf1z70ce3DuexzrLi4gp8u/MTKiJUM9xzO5IDJeJjfNy3Tddzl3L05h3cZqpruIQ9Yqq5HfUHc1gpVlYb0+IKrtgt+eCAicLk0n7xVq5FaWWHctSvn/tnG8pdmEHVwH51GjGXq4m8aFcOFZVUk55UT4HT70nfuVcyGDMakTx+yv/ii1nG0BnMjGfNHBBKZXsSyffE354RZUbBhGihCYfhSEAQOxeXw94V0ZvbxwtlSG+OvSksjf9VqzB99FLlv64w5VEo1f39zntyUEgZOD7ynxXANUqmEh6e2JaivM+d3p/DvykjUak3TO14DC7kFX/b9kpKqEubsmUOluq5NV2FlIbN3z+aL01/Qv01/1g1d1yoxXF6iZNPnZ0iNKaDvZP8WieEa7EOLAMj85NOr1o2dOAaVIGXfhm0M9R7Mb20WY+lsyI4fIki5lN+8E2jUWkO4f96BrzrCV2Hwz9ugLIFe/4Ppe+HlSBj6GXg/DC1MFddxb1D3klpAT6r9XtWmTIM2QlycTkZ8DIamZhRKTZpVPxx1OB1TazmdOzsiCFBSqY0+K7wtEEXIiC/kUt4lLA0ssTeyb/G4U2MKsFIYY2h6bXf9jKIKDGXSWgMwZVIiAAa6lkv3NIIgMNZnLOuHr8fH0od5B+cxZ88ccstzb+i4sfmxzDs4j4EbBrIyciXdnbqzbsg6fn7kZ3o597qrxPB/CbYN5vM+n7N55GZGeo1ky+UtPPrXo8zaNYtTmafqyhZ16LhFPFjq7gZIydfWW2o0D9aX0tbUAGN9KZezS3HysUAiEUiOunrCam7nQBsPb65YmVFy7Ciqvg/x6/tv8O+PX2Pbxp3Jny6l5/gpjTrjAkSka/1e6htq6WgcQRBweO9dBH190t96G1HTUFw9EuDAkCBHvtwVR2xm8Y2drCwPfh0PMiN4bA3IDKlSa3g3PAJXKyOm96p7e5u95EsAbF9sXamiWq1h+w8XSYsr4OEn2+LW7v5JmRIkAj3GetN5uAcxxzLZ9u0FqpTqpne8Bj6WPnzU4yMu5Fzg/w7/H6IoEpEbwWNbHuNg2kHe6PQGC3stxFh2dS/kpijOq2DjotPkppUy6NlA/Ls5tmqMMmM1Nk8/SfGOHZQePtxgnZOjHVVuoehdPs1A2yGUSYop6HcecztDtn5znszEosYPqiyFqC3w10xY5AM/PwJHvgYzJxi0EOZcgBkHoc+b2hc4d/EEUMfNQV2vD7FBdYRYJcoQa26Lpg5Qnk9G3CUcPL0prlQ3WT9cnFdByqV8/Lo4YGakj4eNMaXVgtjewxyJRCAtroCovCh8rXxbLDTUag3pcQW13hzXIqO6B3HN8WtegMp0EeL7AhdTF35+5GdeCXuFA6kHGBU+il1Ju1p0DFEUOZ5+nOf+fY5R4aP4J+kfxvqMZcvILSx6aBEBNvdWvXkbsza83fVtdozewYzgGZzNPsvU7VOZtHUS/yT9g1rT+uemDh3XQyeIm0u1DtY8YPMrQRBwtzUmIacUfbke9h5mJEflNbpt6KNjUMr0OOHuyPa4CxRlZzHohVcY+86HWDtfvwY4Mk07AQ5Q6CLEzUFmb4/9a3MpO3GCgt9/v2r9/w0PwNhAyv/+PI+6tS9x1Cr480koSoXH14C51hRq5eFE4rJKeGdoW+TVdXgV0dEUhodj+cSk67bZuhaiRmTXiiiSLuTy0HhfvDu2POJytyMIAh0Gu/HQBF+SInIJ/+IsFaVVTe94Dfq16cfzIc+z+fJmXtn3Ck9sfQK1qGblwJVM8J/QqmhAXnopGxaeorSgkuEvBrc+fbkaqymTkLm4kDF/AaJS2WDdgHFj0BNVHA4/Tnen7vye9CuDZgZgaCpj89Kz5KVV1/0WZ8KpFbD2MfjUA36bCFGbwbMPjPkZ5l6GyX9B5+lg0fJIto57nHqzGHn174UFRsRsdEBdVAymCpQaCXmpKTh4+VBcoWpSEEcfSQcR/LpqXwYFOVvURohl+lJs25iSGptPXH5cqxymsxKLUSk1102XBm2E2N6soaGWxNwcPct7P3NGhxapRMrUwKn8NvQ37I3smbN3DvMOzqNYef2X2SqNiu0J23n878d5eufTROZGMjNkJjtH7+TNzm+2uJPA3Ya1obX284zRfp68ijxe3vsyw/8azm/Rv1GhqrjTQ9Rxn6ETxM2kZm6paX2m4z2Lm7VWEAO4+FuRnVxMebHyqu08OnXFWBTINTEk6OGBPPnZd7Tt2adZE/OItCLszQywMdGlNTYX89GjMe7WlayFi6hKS2uwzsbEgPeGB3A2uYDlhxJad4Kdb8HlvTD0c3DpBEBWUQVf/BtLH19b+vnXtRLKWrQYiZnZNdtsXQ9R1LoYx57IpMsIDwJ7tc6N+V4hsJcTjzwTSFZSERsXn6a0oLLpna7Bs0HP0r9Nf/5J+odOjp34Y+gfBNkGtepYmQlFbFx0GrVaZMQr7W9KurrEwAD7N99AefkyeavXNFjXqX0gRZZuFJ7aw2Nej5NTnsOBgj0MfzEEqaAhfOEBir4aBYt9YPNsyIqEsKkwORzmxsPoHyFwNMh1WSUPMrXxIgGMqiPExUo5mioJFbHxYOZIVoUJoiji4OlDUUXVdVOmRVEk+kg6Tr4WmNloe5q3czJHqdKgrC51UHhZkJVYhLpKbJUgrq0fbuI7llFYgYN5wx7Euvrh+xNvS2/WDF7Ds0HP8vflvxkVPoqj6Vf541FWVcaaqDUM3TiU/+3/H6VVpbzdpS6iaiG//kuWew1DPUPG+42vjXib6Zsx/9h8Bvw5gG/Pfkt+RTNLbHToaAKdIG42WlH3oEWIQVtHnJJfRqVKjUtbKxAhJfrqm5AgkTDqtXeY8OrbPPzMTOQmJo0crXEi0gp16dItRBAEHN5/H1GjIf3d966qsRkerKCfnx2Ldl4isfqFRrM5sxqOfQudn4PQSbWLP94WjVKl4Z1hAbUvOkoPH6b04EFsnn0WqXnL/x8e23SZi/tTCR3gSthAtxbvfy/iFWbH0BeCKcqtYP3CUxRkljW9UyNIBAkf9viQZQ8v45t+37R6MpQcmcdfX5xB31DK6P+1x9bFtFXHaQzTPn0weeghcr76iqqsrAbr2j0yDKOqEtIP5+FuaM+ao59g9msXhhvMQlVRyabYCZR2eQ+eOwyzz8OgT8DjIbiDDqn3M4IgDBQE4ZIgCHGCILzeyHpXQRD2CIJwRhCE84IgDK63LkgQhCOCIEQIgnBBEITb0iBeU20WLQIm1YK4vNpUS5mQBKaOpJdrr2cHT+8mI8TpcQUU5VTg37WuVCDIWXtfK61fR6wGuxLX1gniS/lYO5kgN7n2dazRiGQW/UcQJyWh76YTxPcrMqmMF0JfYNWgVcilcqbtnMZHxz6iXFVOTnkOS88sZcD6AXx8/GNsDW35os8XhI8IZ5zvOOR6t+XrdseQSqQ84vYIa4esZfkjywmyDeKbc98w4M8BzD86n+Si5Ds9RB33ODqX6WYi1KRMP1glxAC42xqjESE5rwyPNmYYGOmRHJXXaFqrXVjHFh+/XKkmLquEgQG3rjH8/Yq+szN2L79M5oIFFG7ahMWIEbXrBEFgwch29P9sH69vOM/aZ7ogaY67cfJx2PISePSGAfNrF59IzGPDmVRm9vGsdR8XNRoyFy1CplBgOXFCi8d/ZucVTm1Pom1PBV1HerZ4/3sZF38rRrwUypavzrFh0SmGzQrB1rXlQlSuJ6ebU7dWjyP2ZCb/Lo/E0sGYYS8GY2x+87M07Oe9yeUhQ8latAinT6tNtiqKeNSzhO/0QbPzSya1i+UDGyvOWrsS2n00Q+Xt2PRDMptPuDCipw9yXT3wLUUQBCnwNdAfSAFOCIIQLopiZL3N3gJ+F0XxW0EQ2gJbATdBEPSA1cAToiieEwTBGmh9PUALqJ+0ZVZtqlWp0RpVVSYmgek4MspNMTOVY2RuQXFFFWbXiRBHHU5HJpfiEVqXAdNWYcZZAUqVaiwBB09zRERcSnxpY9YygapWaciIL6Rtj+uXluSWKlFpRByqU6Y1FRWo0jN0EeIHgHa27fh92O98efpLVketZnfybvLK86jSVNHbpTdPBj5JqF3onR7mHUEQBDo4dKCDQwfiC+JZGbGSDbEb+CPmD3o69dS1a2oGLqYuPBn45G3pgX0voRPEzaV6MvZACmIbbaT3cnYpXnamOPtakhyVhyiKN8W1MDqjCI0IbXUR4lZhOXECRdu2kfnRx5h0746ebV3dp4O5nDeH+PPGhgv8euIKEzs3MZkqSoN1E7UmRWOWg1R7i1BrRN7ZFIGjuZyZfbzqNv97K5WRUSg+/QSJQcuEVOTBNA5viMOrgx0PjW+5Mc39gL2bGaNebU/4krNs/Ow0Q54LatJo52ZycV8K+9bF4OhpzpDngzAwujWRV31XV6yeeZrcb7/Dsp0hRlyAhP3INFV0svHkYJqC7i6vYqr6g1UKX0I7PIUDMPg5C7Z8dY4tX51j+OwQ9Juo/dRxQ3QC4kRRvAwgCMI64FGgviAWgRqjB3OgplZjAHBeFMVzAKIo3phdbgsQhbqHspFE+3uRrC0nQ0PomHAcB7k5GRVmODgZIIoiJZXXjhArK1TEnc7Gu4MdMoO6PsVG+noYyfRqI8RyYxnlZgW4lwWgJ2nZNZmZWISqStMsQy2gNkKsvHIFRBH9Nm4tOp+OexNDPUNe6/QavV1689WZr+iu6M6UgCm4m7vf6aHdNXhaePJ+9/eZFTqLNVFr2J64ncjcyKZ3fIDRiBpyK3IRBIGnAp+608O5q9DNLppJzTP3QfS3c7fWRgMTc6vriNtaEX8mm4LMMiwdWu5i+18idIZaN4QgkeA4fz4JI0aQ8cF8nL9c0mD94x1d2HI+jY+2RtPH1w6FheG1D3b0G9A3hinhYGRVu3jtsSSi0ov4ekJ7jPS1tw2NUkn2F19g4O+P2dChLRpz3Kks9qyJxjXAmoentm1e5Po+xdLBmFH/C2Pzl2fZvPQcA54JwCPkxsysmkIURU5uTeT45gTc2lkzYFogMn1p0zu2/EyQfg4ubcNG+jeFRioyvl6D++OmCF1mgO9gvEz82P3C0+w8ns2Y8WNYGbGStJI0FCYKXPyseOTpQLZ/f4Htyy4w5PngBj3QddxUnID6eYcpwH975L0H7BQEYRZgDDxcvdwHEAVB2AHYAutEUby63xYgCMJ0YDqAq+uNm6Cp6906BKmIoFFRoe9HhT6cLJPhlF9AUZUBwaYqSpVqNCLXFMTxp7NRVaobpEvXYGQgpaxcW+8viiLJJpfwyeqIRq1Bco1+742RFpMPgjbt+npkFFUL4poexNUO07oI8YNFZ8fOdHa8ulWljjpsjWyZEzaHOWFz7vRQ7npEUeTVfa+y9PRSOth3aLXnyP2IbmbRXGpMtR7ACLG5kQxrY/0GxloAVyIbd5tuKRFpRZgbynC2vI5Q03FdDDzcsZn1AsU7d1K0fUeDdYIg8PGoINQakTc3Xmi6n9+oH8DOv/bP3JJKFu64RDdPawa3q0trz1+7lqrUVOxefQVB0vxbSVJELv/8HIGjpzkDnw1Eqqe7DZlayRn1ahjWziZsX3aByENpTe/USkSNyIHfYzm+OQHfLg4MnNHuFolhtP2Bl/WCvR8jMTTCfsogKgtk5Nu+qk3Hb9MNa2sr8OmMPPkiPUz6IiCwLnpd7SE8Qm3pO9mf5Kh8dv4cgeYGejjruGHGAytEUXQGBgOrBEGQoH253gOYWP1zpCAI/Ro7gCiK34ui2EEUxQ62tjf+4kcjqbufCQJINNoormlREoVGLuxZvhsAR3kRReXaLO5rmWpFH0nHwt4IB8+rs5WM9KVUqjWUVKrIKM3ginEUgkpKdnJJi8abcqkAG2cT5MbXz8bIKCwH6iLEVTWCWFdDrEOHjlYiCALvdnsXOyM75u6f26Sb+YOEbibaXGqfuQ9mJMvdxpjL2VpBbGZjiLmtISnXaL/UUiLTCmnraPZApszeTKyffBJ5QAAZH3yAKr+h6ZmLlRH/e8SXvZey2Xgm9eqdZYbaljX9/w/8BjdYtWjnJcqUav5veJ2RlrqoiNxvv8O4WzdMundv9hjT4wrY/t0FrBTGDHk+6NYJsXsQuYmMR+eE4OxvxZ5V0ZzekXTTz6FWafhneSQX9qQQ/LAL/Sb7I21BdKvZWHuC3AIU7eHRr+HVWHh6B6YvLsG4W1eyl3yJKq/u/jF8wjgkaDiwYS/9XPvxZ+yflFXVGY35dXWkx1hvLp/JZs+aS4gP4pvJW08qUL9Xi3P1svo8DfwOIIriEUAO2KCNJu8XRTFHFMUytLXF7W/5iAFVbR9iAUEiIhG1gtgvZi0uybtJOB8NgL0kneKKarHcSIS4MLuMtNgC/Lo6NPosMqpuMReXVUJUXhTpZpcBSIstaPZY1VUaMi4XNukuDdoIsVQi1HZeUCYlIbW2Rmp68wzvdOjQ8eBhpm/GJ70+IaM0gw+OfNB0kOQBQSeIm0nN81F8QDWbu01d6yXQRolTYgpQq24sWlOl1hCVUUygky5d+kYR9PRwXDAfdWEhWR9/fNX6Kd3cCGtjyftbIsku/k+rH6lM6+LbfXaDxeeSC1h3Ipmp3dzwtq+biOX+8APqoiLsXn2l2ePLTi5my9fnMbGSM2xWyC2rV72X0ZfrMeT5ILw62HFkYzyH1sfdtIdVVaWard+er21v1X20F8KtSlX3GwKvJ8H4tVqXchNtJFAQBOznzUNTVkb255/Xbh7g60GJvS8VFw4xxn0MxcpiNsdvbnDI4H4udBziRvTh9Jv676KjlhOAtyAI7oIg6AOPA+H/2eYK0A9AEAR/tII4G9gBtBMEwajaYOshGtYe3zJqn8kCCBJtyrS8PAMrSQKel/9CT0hHIrWitECkuFzbLrCxCHH0kQwEAXw7N27uaFhdKhKbWcylvEuU6xdjaisnPa75gjgzsRB1lQYn36bd4DMKK7EzNUBa/R1VJuhaLunQoePmEGIXwsyQmWxL3MZfcX/d6eHcFegEcbPRPpTEBzVCbGtMVnElJdWmIi5trVBVqslMKLyh48Znl6BUaXQtl24Scj8/bKZPo3BTOCX79jVYJ5UIfDI6iDKlmnfDL16983+iIhqNyDvhEdiYGDD7Ye/a5VXp6eT9sgqzYUORt23brHEVZJax+cuz6MulDJ8dgpGZfss/3AOCVE9C/6cCCHzIibP/XGH3L1E3nCZcUVpF+JIzJEfm0WeSH2ED3e5YRoaBpydWkydT8Od6ys+fr13eeegI5Opyov5NIsA6gDXRa9CIDT93x6HuBPVx5tyuZE5uTbzNI7+/EUVRBbyAVtxGoXWTjhAE4X1BEIZXb/YKME0QhHPAr8BUUUs+8BlaUX0WOC2K4t+3Y9zqeqZaSEScU/fjcTkcubkKib4eojIVqcye7XlzKM3Vtv36b4RY1Gh7D7v4W2Fi2Xj7GrlMgkTQRoij86JxM3fD2duStLiCZmcspMYUgACOXs0QxEXl2Jv9t+WSW7POo0OHDh1N8VTgU3R26MxHxz/icuHlOz2cO45OEDeTmmfugxqT8Khus1PTz9bJ1xJBItxwHXFEqs5Q62ZjPWMGBt5epL/7HuqShvVtXnYmzO7nzdYLGWy7kH7d4/xxKplzyQW8OdivQUQle+lXoNFg++Ls6+xdR3FeBZuWnAFg+OwQTK3u736JNwOJRKDX4z7aiOiRDLYtu4hK2TpLv5L8SjYuPk3WlWIemR7YZLuX24HN88+hZ2NDxgfzETVa0fvIw90pNrLlysEdTPCbQEJhAofTDjfYTxAEeoz1xq+LA8c3J3B+j6735M1EFMWtoij6iKLoKYrigupl74iiGF79e6Qoit1FUQwWRTFEFMWd9fZdLYpigCiKgaIozr1dY64RxCIgkYi4XdmBQ/Y5pDINGndXKtVVBAZbkadyIXlXOohg9h9BnHIpn5L8Svy6XW2mVYMAyGVSYjKLic6Lxs/SD0cvCypLVeSlN6/Pe+qlfGxdTJusHwaty7Rjdf2wprQUVXa2LkKsQ4eOm4ZUIuXDnh8il8qZu28ulerKpne6j9EJ4uZSW6f0YFLbeqlaEBsY6mHvZkbyjQritCLkMgketiY3PEYdWiT6+jguWIAqK4ushYuuWj+9lwcBCjPe3hRBQZmy0WMUllXxyfZLdHSzZESIU+3yiksxFG7ciOWkSeg7OzW6b33Ki5WELzmLskzFsFkhN8WV/EFBEAQ6DfOg52M+JF7IYfPSc1SWq1p0jILMMjYsPEVxbgXDXgjGs15v1TuJ1MQEu7n/o+LCBQo3bABAIpHg2nMgpmXZSOJNsTG0YXXU6qv2FSQCfZ7wwz3YhgO/xXLp6PVf7Oi4v9HUS/uv31ZTIhMpdtD2JG3b2ZeOxr9TmADtlNKrUqajDqdjYKSHe/D1e5gayqTEZGeRVpqGr5VvrVN0c9KmVVVqMi4XofBpOjoMkFlUWRshVl65AugcpnXo0HFzsTOyY36P+VzKv8RnJz+708O5o+gEcXN5UJVwNW2sjQBIyK5fR2xJ1pViKkqrWn3ciLRC/BzMauuk7keqlGpyUoqJPZnJib8T2PlTBH9+cpIjf8VTVXlrGnkZBgVhNWUKBb/9Rumx4w3WyaQSPh0TREGZkve3NF7m99k/lygoU/JePSMtgKzPFiMxNcXm2elNjqGyXMXmpecozqtgyMxgbF11ZjCtIaiPM/2faktGfCEbF5+mtLB5b3GzkorYsOgUqio1I14OxdnPqumdbiNmQ4diGBZG1uLPUBdoBcW4x4ZTITXk5JYtPOb7GIdSDzWayiWRShjwTABOvpbs+iWay2ezb/fwddwl1ESIk63NqKxXVy7REyk0kiNoRCydvOhg8gfGFkU8XC5DmVNRu12lSp/LZ7Px7mCPnuz6Jn+GMikZFdrr0d/KHzMbOcYWBs0y1sq8XIRapcHZp2lDLWW1m3VNhFiZmAjoHKZ16NBx8+nl3ItJ/pNYG72WPVf23Onh3DF0griFaB5QJ2S5TIqThSEJOXUpuC5trUGElOj86+x5bTQakci0ovvCUEsURYrzKkiOyuPC3hT2r4shfMkZVr55iO9f3Mdv80+w88cIjm9OID1eO3k6vT2JX//vGAnnbs1k3vbFWcjauJL+1ltoysoarAtQmPNcb082nE5lT3RWg3WRaUWsOprEpC5tGtR2lx49Rum+/dg8Ox2pxfWjHCqlmq3fnCc3pYSB0wOb7Lmp4/r4dHRgyMwgCrPK2LDoNIXZ5dfdPuVSPn99fgY9mZRRr4Zh1+bu+44JgoDD22+hLiwk+8ulAJgYGyJv1x2TrEsEC53Ql+izNmpto/vryaQMfq4dtq6m7PwxgpRLrbsP6bi3UVUL4kqZHhGmdf1aBZmGXJUSs4pKNPlKJIKGNm1OUC7A3uVRtdkWcVnuqKs0102XrsFQX4pErm2J5mPlgyAIKLzMSYsrbNLkLTUmH0EAx2bcC8uryyNqWi7V9iC+CX2bdejQoeO/vBT2Ev5W/rx9+G0ySzPv9HDuCI13p9dxFTUy+EEOFP/XadrezRR9uZTkqDy8wlqeipmcX0ZxpeqeMtRSVqgozConP7OUgowy8jPLKKj+T6WsMwCSGUixsDdC4WWBhb0RFvZGWDoYYW5nVNtqKC2ugH1rL7H12wu4BdnQ8zFvzKxvXi9miaEhjh98wJXJU8he8iX2b7zeYP0Lfb3YfjGDNzdeYOdLvTCVyxBFkXfDL2JhpM/L/X1qtxU1Ghz45TMAACAASURBVLIWLULP0RHLSZOue161WsP2Hy6SFlfAgKcDcGt3/TREHc3DNcCaR+eEsuXrc2xYeIphL4Zg43x1qUH8mSx2/hSBhZ0Rw2aFYGJpcAdG2zzkfn5Yjh9P/q+/YjF2DHJ/f0ZNHMcfZ3ezd/0OBg8YTHh8OLNCZ2FucPV9Ql+ux7BZwWxcfJqt35zn0Tmh2LvffeJfx62jvu1asV5dFoRKTyQnPx/HskoqE5IxNLLGWJXMHis1I3Ir2L0yioEiRGX4YKUwxq5N0xkshjIpUoN0TPSssDHU3tcU3hbEnsyiKKcCc9tr379TYwqwdTXFwLDpaVdZtSCuTZlOTELP3h6JkVGT++poHlVVVaSkpFBRUdH0xjpahFwux9nZGZlM10niXkFfqs+nvT5l3JZxvH7gdX4c8CNSyYPVFvOOCGJBEAYCSwAp8KMoih//Z/3nQJ/qP40AO1EULQRBCAG+BcwANbBAFMXfbtOo//PzwcPdxpi/zqYiiiKCICCRSnDytSQ5Mq92WUuISLs7DbVEjTbaW5BZJ3jzM7Q/SwvqpasKYGYt1wpfbwssHYy1wtfeCCNz/Sb/PRReFoyb15Hzu1I4/ncCv753jA5D3Ah52BWp3s1J3jDu1AmL8Y+T98svmA0aiGFISO06Az0pn44JYvS3h/l4WzQLRrZj09k0TiTm8/GodlgY1TlBF2/fTsXFizh+9BESg2sLLI1GZNeKKJIu5NJ7oi/eHexvyufQocXBw5xRr4QR/uVZNi4+zZCZQSjqOdZGHkxj75po7NzMGPpCcLPMe+40ti/OomjrVjI+mE+bNatxd1VQ4RKILOY4w6a+xV9xf7EhdgNPBj7Z6P5yYxnDXwxhw6JTbP7qLCNfaY+1QudJ8KCgktS9pq4or3thm6cnp6q8Egulisr4ODBVYFSSRZm5Hl37eHLozzj2WQ8ls9iebgMcm/X8ksskSIU0zCR1qcs1jtHpcQXXFMQqpZqMhEKC+rg0uv6/lCm10ev6KdO6+uGbS0pKCqampri53TnH/fsRURTJzc0lJSUFd3f3Oz0cHS3AzdyNeZ3n8daht/jhwg/MCJ5xp4d0W7ntglgQBCnwNdAfSAFOCIIQLopibTGjKIov1dt+FhBa/WcZMFkUxVhBEBTAKUEQdoii2PxGgDfIgx4hLq5QkVuqxMZEK4pc/K1IOJdDYVY5FvYte3sdkVaIVCLgY39nakuV5aoGEV6t6C2lIKscdVVd3EHfUA8LeyOcfS3/E+01bLLmrCmkUsn/s3fn8VFWVwPHf3fW7PueELJCICSEsCOLiOwFBURR3IrV+valtVRxQxStiEoVtVqpWkqLqBVcQIsIr6K4gKwhEJYQQoAQyEbIvs3Mff+YrCQhE0gyBO738+FD8syznISQZ85z7z2HfuNCiRrgx48fH2X75+kc2X6WUbf3JLhn62vNbOH38MOUfPc9WQueIvyzT9EY6hPdfqGezLkunPd+PM7onn4s3nCIviHu3Dqg/o2brKoiZ9lrGHv2xH3qlBavI6Vk60epHN2ZzdBpkcSOaL3oltJ2XkHOTJ+fyBdv7GP960lMuL8P3eO82fP1CbZ/nk5orBcTHohDb+waT3e17u74Pfwnzjy1kKIvvsB96lRGz7iFHa89za7/JjMweiAfHP6Au3rfhU7T/C3L2cPI1If6WZPi15OYPr8/bj7tN9tCuXKZaD4hzhHW+5Gvly9Vx9JhRCAuBcdxddbTd0w3zqQVkpI0CIGFHoNse3BXBWgMOcjK/nXbvAKdMTrryDp6npihzU+7PpteiMUkCbaxoFaTEeITJ3C98UabjlVsU1FRoZLhDiCEwNvbm9xcVdehK5oaOZVtZ7bx9r63GRQwiET/RHuH1GnsMUI8CEiTUqYDCCE+Am4Cmq/uA7cDzwBIKVNrN0ops4QQOYAv0OEJsZoybe1FDNbWS3UJcW/rFLVTh85dQkJcRLSfCw6XmVS2pqK0mrPphXWJb23yW1ZUX2FZCHDzccQjwImQXl541iW+zji66jv8punq5cDEB+PI2J/H1o9S+XzZXnoODmDYjKjL7tmrdXEh8LlnOXX/A+S9/TZ+DzVul/TwuJ5sPpTNA6t2YZHw7t0D0DQoclbw0X+oPnWKbu++g9C2/G+1fV06KVtPkzg+lMTxajSjI7l5OzL9kUS++Os+NizfT/c+3mQk5xE90J8x9/RqtxkGncV9+nQK/vMx2UuX4nLDDYwYmsj/rQihdMc33D75Xv60dR7fnvyWcWHjWj6HryNT/5DAZ6/sYd1re5k+vz/O7lfudHGlfTTsQ1xRVlz3cZ50Rm804h0USuWBFJjUEw/TTtwcdQghuOGeXhQcSsHXvQhndxuSTQd3jmX9At6OFJ2vXyIkNILASI+LFtY6nXoeoRGNZnNcTFmVGU8nPQ56LeaiIswFBaoHcQdQyXDHUN/XrksIwcIhC0nOTeaxHx5j7ZS1zS5XuhrZ411TMNCweWRmzbYmhBDdgXDg22ZeGwQYgGMdEGMTsu7va/c/em0v4vQG64jdfR1x83Hg1KG2t19KySrq8PXDpiozH7+wk/++lcxPa9NI252DqdpCaKwXQ26OYOJv47j96cH89o3rufPPQ/nV//Zl+C3RxI4IJriHJ05urU99bk9hcT7c/sxg+k/sztFd2XywaDsHtp7GYrm8RzEuI0bgfvPN5L/zLhWHDjV6zdGg5cXp8Vgk3DagGwnd6t+0mYuLyXv7bZyGDsF5+PAWz79n0wn2bDxB7xFBDLk58rJiVWzj6Grg5j/1Iyjag4zkPOJGhzD21727XDIMIDQaAp5eiDkvn7y3/gZAzI2Tca48z/m9VQS7BDfbgulC3sEu/Or3fSkrrmb960mXVQFf6RpMDRLiytKSunt1gdkJ/+5hOERFUZ2ZicXoh5vlPB5G6+9zo6OOWT3/xpiY72270OgnOYx1yUxOvndd4SuwriMuzC1vsQL86dQCfENdMdiwfhigvMrUaHQYVIVpRVE6h7PemaUjl5JXnsczPz/TasHAq8WV/s5pFrBWStmoN40QIhBYBfxaSmlp7kAhxANCiF1CiF3tMXWjboT42s2HCfZwRK8VjQprCSEI6eVF5pECzOZm/ymalVNUQW5xZYevH9737SmK8yu48de9mbN0OPe9MoJbHhvAmHt6039CGBH9fPEKckarv3L+K+gNWobcFMmshYPw6ebK9x8c4ZOXd5N7srj1gy/C//HH0Hp6krVgAbK6caIwNNKbzfNG8vy0Po2257/3D8wFBfg9/EiLDwYO/pjFtk+PETXAj1G391RPhztRbVGpGY/1Z8St0Ygu3L7MMS4Oj1tmcG7VKirT0pgxbQJlelcObNrA7F6z2Zuzl5S8lFbPExDuzqT/ieN8ThlfvrmPqoq29W5WuhZzgx95i9lEtVaDRUBxlRMB4WEYI6NASqpKDGiQBOmK6vbXCAsaYeObPf9YDocNxtli4QbTcY7l1ndcqB35bW6UuLrKTPbxIpunSwOUVpnrK0zXtlxSa4gVRekksT6x/DHxj3xz8hvWpK6xdzidwh5ZwGmgYWWJkJptzZkFfNhwgxDCDfgvsEBKub2li0gp35FSDpBSDvD19b3MkKkbIr6WR4h1Wg2hXk6NehEDhPbyorrCTM7xohaObKozCmqVl1SxZ+MJwuK86Tk4AEfXzh3tvVyeAc7c9McExs7pTfG5CtYs2cnW/6TWtQtpK62HBwFPL6Ty4CHyV/yzyevR/q7otfW/Eqqzszn3r3/hNnkyjn1imz1n2u4ctqw+TGisNzfe27vRVGulc2h1GgLC3bvUz3ZLfOfNQ+PkxNnFi9HrdbgPGI1bQQahRT1x0jnZNEoM0C3Gi/H39SEno4ivlu9vVBNAubqYLkhoK/Q6ihyMSDQEhIdjjLLOWKnMt/7eDNJcenuuIw6ORFt0vKD/JxmZ9RPdfEJd0Bm1nEkrrAmqEiqtDzDPHivEYpZtqglRXmVuUFDrBAiBXrVcumK5uHTtIn5vvPEGvXr1Yvbs2fYORbmC3NX7Lq4Luo6Xd77M0YKj9g6nw9kjId4JRAshwoUQBqxJ7/oLdxJCxACewLYG2wzAZ8C/pZRrOynemotb/7o2Jg60LNzHpdEIMUBwT0+EgJNtmDadkmV949C7AxPi3RtOUF1pZsi0rjuFVwhBj0EBzF40mD6jQtj/XSYfPLOd1J1nL2kai9u4cbhOmEDeW29Reeziqw1y//pXpNmM77w/Nvv6iZR8Nq9IITDSnQm/7dMlp+oqVxadlxe+D/2Bsm3bKf56E7fdMQOT0PHD5xuYFj2NjRkbyS2zbcZPRD9fbri7F5mHC9j0jxQslq7/wEBpykQzCbGjte5CQPcwa+9enY7KbOuIrh9tX94DYJEWDhek0jNsDJ4UE7Zzcd1rWq2GwAi3+hHirx6DFRNASmv/YY0gMNK25UESqKg2N5oyrQ8KalQMUbl2SCmxWDr2gd7f/vY3Nm/ezOrVqzv0OkrXohEanh/+PC56F+Z/P59yU7m9Q+pQnf4OVkppAuYCXwOHgI+llClCiOeEEFMb7DoL+Eg2ftd/KzASuFcIkVTzJ4HOUBvFNf6eKsLXmeP5pY3WtDo46/ELcyOzTQlxEd29nXB16Ji2MEV55ez/PpOYYYFXRQsWo5OekbN6MPPxAbh4Gtn8j4Osfz2JgrOlrR98gYCnFqBxdOTMgqeQZnOz+1SmpVH46Wd43XE7hpCQJq+fSTvPxuX78QpyZvLv4ut6KyvK5fKcNQtjr15kv/QSvi6OmCL7o8/YyxivCZgtZv5zxPZOezFDAxk+M5r0pFy2JPVCXstrXq5STUeItZQZ9CAkrt7eCIMBQ/fulJ3KA8DbcmkJcWZxJmWmMnqHDuc/DrfQJ28DpH5d93pglAf5WSXWdetl+ZB9ALL2cvrIefy6u2JwsG39cO29NaBBQqymS3cNJSUljBkzhsTEROLi4li3bh0ATz/9NK+99lrdfgsWLOD1118HYOnSpQwcOJD4+HieeeYZADIyMujZsyd33303ffr04dSpU00vBmzcuJHExET69u3LmDFjADh37hw333wz8fHxDBkyhOTkZAAWLVrEnDlzuP7664mIiOCNN94A4MEHHyQ9PZ2JEyeybNmyjvnGKF2Wj6MPL4x4gWOFx3h558v2DqdD2WVIR0q5QUrZQ0oZKaVcXLPtaSnl+gb7LJJSPn7Bce9LKfVSyoQGf5I6J2rrGynLNZ4Rh3k7U2WykFXY+ElRt15eZB8vorLMtiI2KVlF9OnAglrb16Wj0QgG/Sqiw65hD37d3Zjx2ABGzupBzoliPnp+B7+sT8dU1Xxi2xydjw/+C56kPCmJghaeCOe88ioaZ2e8H2zahy73VDFfvpWMi5cDU36fgNHpyu91q3QdQqslYOFTmM6cIe+dd5h020x00sxPn//MqJBRrEldQ6W5+eJFzek7phsDJ4dx+GQQP52//ZopEHKtMDczZbrUQYfGUIVGY32LY4yMpDLjJFVSi6c575Kuc/jcYQBivGLY0e0+0kUofPEQlFtHhYOiPUBap0jXqt77KTkZRQT3sH26tKXm5zPA3QEppbUHsSqo1SU4ODjw2WefsWfPHrZs2cLDDz+MlJI5c+bw73//GwCLxcJHH33EnXfeyaZNmzh69Cg7duwgKSmJ3bt3s3XrVgCOHj3K7373O1JSUujezAOR3Nxc7r//fj755BP27dvHmjXWdZ7PPPMM/fr1Izk5mRdeeIG777677pjDhw/z9ddfs2PHDp599lmqq6tZvnw5QUFBbNmyhXnz5jW5jqIMCxrGnD5zWJu6lq8zvm79gC5KzXG0mRoiBmsvYqDJtOluvbyQEk4fab0DVmF5NSfPlXXYdOmcE0Uc3ZlN3zHdcPG8+tquaDSCuOtDmP3sEKL6+7FrQwYfPvcLJw7k23wOtylTcB41kpxlr1F1wdPnsp07KdmyBe/770fn2fiN3PnsMr54IwmDg5apDyVcdksoRWmOU2Ii7jdN5dw/VhDr7kiRdyQle79nZuRtnKs4x4b0DW0638BfhRMfcZLj5f2oLFNFtq4mF44QV+p1lBl0YKh/OGuMisSSmUm22RO36ksrsnn43GF0QkekRyTh/p7Mq3wAWZIDmxYA4B/mhkYryEqrvwee2XMAi0US3NP2glq1k68C3B0wFxRgKS5WLZe6CCklTz75JPHx8dx4442cPn2a7OxswsLC8Pb2Zu/evWzatIl+/frh7e3Npk2b6j5PTEzk8OHDHD1qXavZvXt3hgwZ0uK1tm/fzsiRIwkPDwfAy8vaAvPHH3/krrvuAuCGG24gPz+foiJrzZbJkydjNBrx8fHBz8+P7Ozsjvx2KFeRuf3mEucTx7M/P0tWSZa9w+kQKiG2mTURltf4KuII3+YTYv8IN/RGrU3riA92YEEtKSU/f3oMB2c9/a7yXrhObgbG/jqWm+b1Q6vT8OWb+9j49/0Un6to9VghBIHPPovQajmz8Om6UTMpJdlL/4LO3x+vu+9qdEzxuQrWvb4XgKkPJeDq5dD+X5Si1PB9+GGEwUD2C0voN3EKjqZSTvyYQ5RHFKsPrW7TSK8QguFxqcz0X4SDs5rRcDVplBALQYVeS4Veh8WhPiE2REaCxUJOkTvOVW0bIT5bepa/7/s7nx79lAiPCIxaI9H+LuyzRJDX90HY+z6k/R86gxb/MOs64jNfnODkd16cPh+ERgMBEbbPhqqdMh3o5mgtqIWqMN1VrF69mtzcXHbv3k1SUhL+/v5UVFjvx7/5zW9YuXIl//znP5kzZw5gvd8+8cQTJCUlkZSURFpaGvfddx8Azs7O7R6f0Vg/QKDVajGZ1MNBxTZ6jZ6XRr6ERPLY1scwWa6+nx2VELfRtT5l2s/ViJNBS/oFlaa1Wg3BPT1t6kdcW1CrI3oQnzx4jtNHChgwKQyjjT0fu7qQnp7c9tQghtwcwYkD+Xzw7C/s3Xyy1TZY+oAA/ObPp2z7ds7XTLcq/vprKpKT8f3DH9A41Ce85cVVrH89iaoyE1N+n4BnQPvfrBWlIb2fHz5z51Ly/feMdhSUOHiRtmUDs2Nmc6TgCLuyd7XpfEKAg7bta+6VK1uCg1/dx0Y3T8oNeqq1OkzGhiPEUQAUFzniWNH6qFiFqYIN6Rt4YNMDjFs7jjeT3iTCI4KFQxYC0MPfFYDt3X4DPj1h/R+goojAKA9yMgrJ31NI6VkHTpQk4OeWZ/P6YQCzlGg1AjdHnWq51MUUFhbi5+eHXq9ny5YtnKjpIQ0wbdo0Nm7cyM6dOxk/fjwA48ePZ8WKFZSUWAu+nT59mpycHJuuNWTIELZu3crx48cB69phgBEjRtQVx/ruu+/w8fHBza1j21sq14Zurt14eujTJOUm8bekv9k7nHZ3bWQM7UGtOwOsIy3hPs5NRogBuvXyJCM5j8Lcctx9HVs8x8GsIvxcjfi6tu90ZotFsu3TY7j5ONBnVHC7nvtKp9Vp6D8hjOgB/vzwn1R+/iSNI9vPMOr2ngRGtTxdz+PWmRRt2EDOy0txHjaMnGXLMEZH437zTXX7VJab+OKv+yg+V8HUPyTgG+raGV+SouB152zOr11L7osvEjjtDoq3foJrlj8eRg/eP/g+AwMG2jtExc4SHf3YSgYARk8fis7ngxBUOtSPYBjCwpBCQ1WhDkNZ8wmxlJL9efv5PO1zNh7fSHF1MUHOQTzY90GmRk4lxLW+uGCYtzNajeBIXjVTbv4b/GMsbF6If+A8pBSUh/fEszyNcyKchOoNUD0V9LbNqLFIcDLoEEJQdeIEaLXog6+t+1lXNXv2bKZMmUJcXBwDBgwgJiam7jWDwcDo0aPx8PBAq7UWoRw3bhyHDh1i6NChgLV90/vvv1/3+sX4+vryzjvvMH36dCwWC35+fmzevLmueFZ8fDxOTk7861//6pgvVrkmTQyfyLasbby3/z0GBw5mcOBge4fUblRCbCtRO2VaCfdxJjmzsMn2br2sa1hOHTqHu2/LN/CUrCL6BLf/6HDqjrPkny5h3H2x12wLIDcfRyb9Lp7j+/L44T+pfPqXPfQaFsjQ6ZE4ujRd7yuEIPDPz5E+9SYyZt2OOS+Pbn9fjqi5IZuqzGz4WzL5mSVM/J84a+EYRekkQq8n4KkFnPz1HMZXFbJaY2T7+v8y866ZvLf/PU4Vn6Kba7fWT6RctUSD3udGDx8QqQBUNJgyrTEaqfQLQBRVo6kutfYINlof7OXKar44sIJ1aetIL0zHQevAjd1v5OaomxkYMBCNaHovMeg0hHk7kZpdDOMHwNC58PMbaM6WgpxFdb9EMOuRBVqcTxyEtM3Qa4pNX49FSpyM1t+/VSdOYAgJQejVNP8rWe0Ir4+PD9u2bWt2H4vFwvbt2+uKX9V66KGHeOihh5rsf+DAgVavO3HiRCZOnNhom5eXF59//nmTfRctWtTi+TNqZiIoii0eH/Q4SblJPPHDE6yduhYvBy97h9Qurs2s4VLUrrEU1/aUaYAIH2cyC8qoMjWekuvh74SLp5G03Tktru+rqDaTllvS7uuHTdVmflmXjl93V6L6+7V+wFVMCEFEgi93LBpCv3GhHNl+ltXPbOfgT1lIS9N/F0NoKH7z/og5Lw+nQYNwHjkSALPZwsZ3D5CVdp4b5/QmLM6ns78URcF56FBcJ0ygfOVKHMLjccpKYajDSLRCy4eHP7R3eIq9iYYJsXfdx6WOjSvvl/h3w6m4pr5C0RkqTBUsMJQxtvowy3Yvw83gxqKhi9hy6xaWjFjC4MDBzSbDtXr4u5KWY02EGP0kxUVhlH/3Ex4OFeSYQsjVRSGwoD2URcVm20fpLBaJk6E+IdarCtNd3sGDB4mKimLMmDFER0fbOxxFuSxOeieWjlzK+crzLPxp4VXTuUElxDZTI8S1wn2dsUg4ea6s0XYhBAk3hnL6SAG7NmQ0e+zhs8WYLbLdE+LkLZmUFFQydHpUoxGDa5neqGXY9ChuXTAQr0Bntqw6zKd/2UNeZkmTfT3vvBO/Rx8lcPHzCCGwWCTfrDzEif35XH9HT6IH+NvhK1AUK/9H54NGw5hca3XLLZ98w7iwcXx69FNKqpr+PDcrOBH63taBUSr20GiE2LP+oV2xgwm09SOrBb7BuBeXIi1wLv8Iv9n0G77QVnOHxpv1N69n1aRVzOgxAxeDbX3ro/1cyMgvpdJkxlxWxdntThg9qgkNOEt2kR8ni6PwD3NFbxBkf5KELG+9A0NaTjFmKfFyNlpbLqkexFeF3r17k56eziuvvHJJxw8ePJiEhIRGf/bv39/OUSqK7Xp69eThAQ+zNXMrqw81376zq1EJsc3UCHGtcB/rG4bm1hHH3xBCj8H+7PjiOOlJTdtbdERBrYrSavZsPEForDchPW3v93it8A52YdrDiYy5pxfnc8r4+IWd/LjmKFUV9WvshFaL95xfY+jWDSklWz9K5ejObIZOiyR2hFq/ptiXPigIn9/+FsMPW8E1CNPBn5kWOoPS6lLWHVtn20niboHJl/aGVLmCNTNCbNaZKTUAuvo6FbneQWilhYxyI3ftXcrhc4d5pcqJR3VBhLuHt/myUf6uWCSk55aSveRFTIXFBN43lqDCNZgsOvIqggnp7Yvvb+6gLFtP8b/+0uo51yVlIYDu3k6YcnKRZWWq5ZLCL7/8UleJuvZPXFycvcNSrnF3xNzB9d2u59Xdr3Io/5C9w7lsKiG2mUqEa4V717ZeajoyI4Rg9OwY/Lq78n//PEj+6cb7pGQV4eagI8Sz5aJbbbX7qwwqy00Mmx7Zbue82gghiBkayOxnh9D7ukD2fXuKDxb90uz09u3r0knZeprE8aEkXuWtq5Suw2vOr9F3D2Vo5gmMlkr2b06lr29fVh9ajUVevKK6chVr+JDaxfpA1GKsplxoQFdfyOqMRwAAy7TeFJvKeG/ce4w1X/ra3B7+1gfDpzf+H4Wff473/b/B8Z5XCPStf1Ac3MMDzwcexugtyF6xDkt5eYvnk1KyLikLvVaDk0FL1YkMQFWYVhTlyiSE4M/D/oyngyfzt86nrLqs9YOuYCohtlVN0nCtt10CcHfS4+1saHaEGEBn0DLxwXj0Dlo2vJ1MRUl9cZOUrCJ6B7kh2mmkvSi/nOTvMokZEoB3sG1T3a5lDs56rp8dw4z5/XF01fP1uwf48s19FOZaf5Ht2XSCPRtPEDsiiCE3qwcMypVDYzAQ8OSTeGWcQGocyfpxM3f0vINTxafYmrnV3uEpdtLwVlKuMaITFqSxinKNBjT11XqTXU4DEJYH73sMIsEv4bKuG+7jjIupHO/lr2KMjsLnd78DgxNOM17CQ5uJRpgIiHC3Foa7ewymIhP5b73W4vmSTp3n5LkyjDUFIatO1PYgDrusOBVFUTqKh4MHL454kVPFp1j8y2J7h3NZVEJsI3nB39e6MB/nJr2IG3LxNDLxwThKz1ex8d0DmM0WTGYLh88U0acdp0vvWH8cIQSDpkS02zmvBQER7sx8fADDZ0Zz5lghHz67g03vHWDbp8eIGuDHyNt7tttDC0VpLy6jRuE6ejTxp7Nwqcin+pABfyd/3j/0vr1DU+ylweyA4koLYe7nEd5FlNWsLZZS8u+Uf3PI8B657jqm5egILSu67MsadVr+mPoVhqJzBL7wAhpDTRX/sOvo09dCXEQWupriWE7T5+IWWkb+ytVUZWY2e751SVkYdBoMupqCWhkZCL0efWDAZceqKIrSUQYGDOSB+AdYf2w9X6Z/ae9wLplKiG1UO6tUqhFigBZ7ETcUEO7O9Xf25PSRAn5am8ax3FIqTRZig9unoFbuqWKO7DhL3xtCcPWyrcejUk+j1dB3TDdmLxpCeIIPR3flEBrrzY339kajCpMpVyj/Jx4nuLAEYRHs/epLZsXM4pczv5BakGrv0BS7qH9MXVwt6R+QSXHLUwAAIABJREFUjdH3HOVCYLaYeXHHiyzdtRSH6r4U+fRCntdC0ZnLvmrJjz9xXerPfBM3FscL1nP2feBehs+/t36Db0/8xgYDFnJeeqnJuUxmC18mZzEmxo/aX71VJ06g7x5a1wJPURTlSvXb+N+S6JfIn7f9mZNFJ+0dziVRCXEbqRFiq3AfZ3KKKympNF10v5ghgfS9sRv7t2Sy81vrf5L2Kqi17bNjGJ10ap3rZXL2MDL+N324/ZnBTPqfuGu2h7PSNRhCQ/G77z6icvJxzUsjxtQPB60DHxz6wN6hKfbQaITY2mrJUUqqheCPW/7IB4c/4J7e96DPv4cS/+5UnatCFmZd1iXNJSWcWbiQEv8Q/hZ+Q5MWhM3RD5uFT69Cijf/HyU//dTotZ+P5ZNXUsVNCUF126pPnFDTpa9iGRkZODo6kpBQP3U/LCyMuLg4EhISGDBgQN32NWvWEBsbi0ajYdeuXXXbN2/eTP/+/YmLi6N///58++23rV53/vz5xMTEEB8fz7Rp0zh//nyTeBISEnjwwQfrjqmqquKBBx6gR48exMTE8MknnwCwbNkyQkNDmTt37mV/P5SuTafR8eKIF9FpdDy69VGqzdWtH3SFUe98bVQ3QqwGzgBrL2KAjFZGiQGGTYukW28vzv+QTZjU1h17OU4dPMepg+cYMDEMo9OlF0ZR6nkFOqtkWOkSvO+/nzCNA0JKtq7ZwK8if8WX6V9SUFFg79CUztYgIS6qSYidavqtbz29lScGPcEjAx+hpMJMZXAo0iSpPpsHFnOzp7NFztK/YDp7lnP/+ygVQkdGfuv3QfrMwCumFL2vK9kvLEFW179hXJeUhauDjut7+tV8SZKqEydVQa2rXGRkJElJSY22bdmyhaSkpEaJb58+ffj0008ZOXJko319fHz44osv2L9/P//617+46667Wr3m2LFjOXDgAMnJyfTo0YMlS5Y0iScpKYnly5fXbV+8eDF+fn6kpqZy8OBBRo0aBcC8efN47rnnLulrV64+gS6BPDfsOVLyU3hj7xv2DqfNdPYOoOuoabukpkwD1l7EAOl5pfQJvviIr0arYdx9sfz1yR+ZWmqgvLDqsqY4S4vk58/ScPV2IG5UyCWfR1GUrknj6EjYE48TtPR5Th3bwVj/51ibupa1qWu5P/5+e4fXZQkhJgCvA1rgPSnlixe8Hgr8C/Co2edxKeUGIUQYcAg4UrPrdinlg3SGhglxhfXjsOpqXCU8P3oZN4TegJSSkkoTpjBrgllZKDCU5l3S5Uq3b+f8f/6D1733IocPhj0/kJpdTA9/14sf6B6MJnI4/qWZZG44xrnVq/G+914qqs18nXKWSXEBOOit06NN5yuQVVUqIe4kz36RwsGsy19X3lDvIDeemRLbLufq1atXs9v79etX93FsbCzl5eVUVlZiNBqb3R9g3LhxdR8PGTKEtWvXtnr9FStWcPjwYQA0Gg0+Pj6tHKFcq8Z0H8NtPW9jZcpKBgcOZnjwcHuHZDM1HGSj+jXECkBYbeulixTWasjopGO9axV6KdjwdjLVVZf+dD51ZzZ5p0oYclMEWr36EVaUa5Hr2LGE+XZDg2Tnmq0MDRzKR4c/otrS9aZqXQmEEFrgLWAi0Bu4XQjR+4LdngI+llL2A2YBf2vw2jEpZULNn85JhqH+5gwU1SzhGVlewY/VPtwQegMApVVmLBKomYJcWaiH4rZPm7aUlnJmwVMYunfH96E/EOHrjEbA0eymLQibFXcLLq7HcR7Ul7w338KUl8c3h3IoqTRxU0J9v/eqmplXqgfxtUUIwbhx4+jfvz/vvPNOm4795JNPSExMvGgyfKEVK1YwceLEus+PHz9Ov379GDVqFD/88ANA3ZTqhQsXkpiYyMyZM8nOzm5TbMq15ZEBjxDlEcWCHxeQV35pDx7tQY0Q28jfvxsns6NV26UaDnotwR6OzfYibs6pc+WcMplwGx1K3jdn+Pbfhxh3X2ybKxmbqs38si4d31BXogf4X0roiqJcBYQQ9Hv+zxz8/QOc3/UNs26dx0Nb/8DmjM1Miphk7/C6okFAmpQyHUAI8RFwE3CwwT4SqK2K6A5c3mLc9tBwhLi8/mGIRlff6764wrrdycsDnY8nVUVll1RYK+eVV6nOyqL7+6vQODriAIR6OZGWY2NC3Gsq4r+P4D/Wj/S9B8l5dRnrYqbh52pkSIR33W5VedY2eIYwNULcGdprJPdy/fjjjwQHB5OTk8PYsWOJiYlpMk26OSkpKTz22GNs2rTJ5mstXrwYnU7H7NmzAQgMDOTkyZN4e3uze/dubr75ZlJSUjCZTGRmZjJs2DBeffVVXn31VR555BFWrVp1yV+ncnVz0DmwdORSbv/v7TzxwxP8fezf0Ygrf/Dqyo/wCuHk6IpWH4FUrWjqhPs4czzftkbcKVmFACQMDmTITRGk7cphz9cn2nzNA9+fpvhcBUOnRyJUJWRFuaYZIyIIiOiD1Fg4t+4QYW5hqgXTpQsGTjX4PLNmW0OLgDuFEJnABuD3DV4LF0LsFUJ8L4QY0aGRNtRghLiwQUKMrn6krLjCOnLs6qDDGBlBZZEOituWEJfu2EHBBx/geeedOPXvX7c92t+V1Oxi207i5AXR4zDmbML77rso/PRTsrbtYkrfILQN7mdVuaUIR0d0fn5tilHp2oKDrf/d/Pz8mDZtGjt27Gj1mMzMTKZNm8a///1vIiMjbbrOypUr+fLLL1m9enXdoITRaMTb2/pQpn///kRGRpKamoq3tzdOTk5Mnz4dgJkzZ7Jnz55L+fKUa0iUZxSPDnqU7We2szJlpb3DsYlKiJVLFu7jzPHcEqRsfSJ5SlYRWo2gZ4ArieO7Ez3Qn+3r0jmebPt0iorSanZtyCC0txfdYrwuJ3RFUa4SY55/DmO1mbzv/ssdPWaxP28/+3L32Tusq9XtwEopZQgwCVglhNAAZ4DQmqnUfwI+EEI0219PCPGAEGKXEGJXbm7u5UfUaA1xg64Huvo6FbUjxK4OegzRvags0rWp0rSlrIwzTy1E360bfvP+2Oi1aD8XjueVUm1uvdI0AHG3QPEZvCfGU+3uxW/2fspN8Y17DVfllWEIDVW94K8hpaWlFBcX1328adMm+vTpc9Fjzp8/z+TJk3nxxRe57rrrGr129913N5tQb9y4kZdffpn169fj5ORUtz03Nxez2bqULT09naNHjxIREYEQgilTpvDdd98B8M0339C794UrKRSlqVuib2Fc93H8dc9fSc5Ntnc4rVIJsY1Ulemmwn2cKaowca60qtV9U7IKifJ1wUGvRQjB6Lti8O3myuYVKZzLsm0d8p6vT1BZbmLodNuegiqKcvUzurvhEpFAqVEQ/N8sXPWuvH9QjRJfgtNAtwafh9Rsa+g+4GMAKeU2wAHwkVJWSinza7bvBo4BPZq7iJTyHSnlACnlAF9f38uPuoUp0w0T4qKGI8TR0UiTBtOp4zZfIue116g+eZLA559H0yCJAIj2d8FkkZywpdI0QI8JYHBBe+wLvhw6g57nT9FtR+N2OVV5pWr98DUmOzub4cOH07dvXwYNGsTkyZOZMGECAJ999hkhISFs27aNyZMnM378eADefPNN0tLSeO655+raJeXk5ACQnJxMUFBQk+vMnTuX4uJixo4d26i90tatW4mPjychIYFbbrmF5cuX4+VlHXh46aWXWLRoEfHx8axatYpXXnmlM74lShcnhOCZYc/g5+THo1sfpbjKxpk0dqLWENtIFdNqqrbS9PG8UrxdLl7I4UBWESOi6ysT6g1aJj4Yx5oXd7Hh7WRueXwADs4tt08qPldB8reZ9BwcgE9IK9U8FUW5pkx+4lFWPXgXp77dyG2PTuKfJ9ZytvQsAc4BrR+s1NoJRAshwrEmwrOAOy7Y5yQwBlgphOiFNSHOFUL4AueklGYhRAQQDaR3Ttj1d2eTpcGdutEIsTUhdnPQYYyyPlCtzMhEb8OPR9mePRSseh/PO27HefCgJq9H+1nvR6nZJUT52XBvMjhBzK+wHFzHvxze4PrwnmhfXYbbuHFo3dyQFqjKL8dVVZi+pkRERLBvX/MzW6ZNm8a0adOabH/qqad46qmnmmwvKioiOjqakJCmXTjS0tKavcaMGTOYMWNGs691796drVu3Xix8RWmWm8GNl0a+xL0b7+XP2/7MSyNfumJnvqgRYptZ/wHH9wm0cxxXjnDv+tZLF5NTXEFucSWxQY3bM7l6OTDxgT4Un6tg03sHsFxkytmO9db3VoOnRlxm1IqiXG18vT2RoXFkuzow/L+5SCQfHf7I3mF1KVJKEzAX+BprC6WPpZQpQojnhBBTa3Z7GLhfCLEP+BC4V1rXzIwEkoUQScBa4EEp5bnOCbyF+0ajNcQNpkxHWO8hladbn65tqajgzJML0AcG4vfww83uE+nrgmhLpWmA+JloKosYpdmH34KnMBcUkPfWWwBUlwAWqVouXeW0Wi2FhYUkJCS0+7nd3NxYs2ZNu5/3QsuWLWPJkiW4uTW7OkJRAEjwS+B3Cb/jq4yv+Dztc3uH0yKVENuodsr0yzPi7RvIFSTE0xGdRnC8lYQ4paa/X2xQ01+agVEejLqjJ6cOFfDzp8eaPT4vs4TDv5wlfnTIZfUvVhTl6jXuvvuwCEHm7l3MlP1Ze3Qt5aZye4fVpUgpN0gpe0gpI6WUi2u2PS2lXF/z8UEp5XVSyr417ZU21Wz/REoZW7MtUUr5RecF3VJC3HSE2NVBh87TE62rgUobEtjcN/5KVUYGgc//GY2zc7P7OBq0dPN04mhOG6YDhl/PeeHOPS47iBo+AI+ZMzn3/moq09KoKrQ+fDeEh9l+PqXL6datG6dOnSIpKcneoVyyefPmceTIEV544QV7h6Jc4e7rcx+DAgaxZMcS0s930uShNlIJcVtdmSP9dqHTagj1dmq1F3Ftw/vezSTEAL2vCyJ+dAj7vjnF4W1NK39u++wYRkcdiRPUE3NFUZoX1zuKMp8ITni7MWVdDkUV5/ky/Ut7h6V0NBtHiLUagaNeC4AxyJuqcxa4yAOT8qQkzq1cicett+I8bNhFQ4j2c2nTCHFafgWfVw9msGknVBTh+8eH0Dg7c3bxYqqKahJiNUKsKMpVQqvRsmTEEhy0Djy69VEqzZX2DqkJlRDbSNZU07pS577bS4SPsw0jxIWEejnh5tDyGuHrbokiJMaTLasPcza9sG575uFznEzJp//EsIuuMVYURek/ZQbVOi05Z3K47Xggqw+utqkKvtKFXfDvq6EmQdY37ENswtVBV99ipnuwtdJ00dlmT2mprCTryQXo/P3xe3R+qyFE+7uSnleCycZK0+v3ZbHOch06SxUc/hKdlxe+f/gDZdu2U3BQg8ZBh9ZLdVJQFOXq4efkx/PDn+dIwRFe3fWqvcNpQiXENqq95ap0uDFrL+JSLJaW33SmZBXRJ/jia0w0Wg3jf9MHFw8jXy3fT0lBJdIi+fnTY7h4GYm7/sJ2mIqiKI1NHD+CEkdvUgN9mfL1ebKy09h2Zpu9w1I60gUjxAZqWi81GCEuKq/G1aG+hqghMgJLtQZTYVmzp8x78y2q0tMJfO45tC4urYYQ7edCtVly4lzz52sUrpSsSzqNU/hg8OgOyR8D4DnrNow9elBVJDD4OKmH74qiXHVGhozkzl538sHhD9hycou9w2lEJcS2qs331HeskXAfF6pMFrIKm596VlRRzYn8siYFtZrj4KJn0u/iqa4089XyZA5vP0PuyWKGTI1AVzPVTVEUpSUajYbg4eOp0AuKTGbu2m5g9aHV9g5L6UgXjBA7iJrWSxesIXY11s8wMva09netLGo666h8/wHyV6zAfcZ0XEYMtymEaH9r0nw0u/V1xPsyCzmRX8ZNCSEQNxOOfw/F2QidDv8FCwAw+Di1chZFUZSuaV7/efTy6sXCnxdytrT5WTr2oNI7G7m7VhJm3KGe2l4g3Ke+9VJzWls/fCHvIBdu/HVvck4U8+2qw3iHuNBjkGqdoiiKbWbdfjMVWgeSIyO54ZcK0vd+T0Zhhr3DUjrKBSPERqwJsdQ2XENsajRCbOzTH4CqwsadJy1VVZx58kl03t74P/aYzSFE+dUmxK2vI16XdBqDTsOEuACIv9Uaf8pnADgPHkTAdWa8RobbfG2la8rIyMDR0bGuyvSpU6cYPXo0vXv3JjY2ltdff71u30WLFhEcHFzXa3jDhg11ryUnJzN06FBiY2OJi4ujoqLiotedP38+MTExxMfHM23aNM6fP98knob9iQGqqqp44IEH6NGjBzExMXzyySeAtcp0aGgoc+fObbfvi3L1M2gNvDzyZarMVTzxwxOYLWZ7hwTYKSEWQkwQQhwRQqQJIR5v5vVlQoikmj+pQojzDV7bKIQ4L4To1Gop0d0LmOy5BI1GJcQNRfhePCG+WIXpFs+Z4MvgqREI4LrpUQj1PVcUxUYuzk4YYodRSgXl7m7ct1nygRolvoo1HiHW10yZNmvrR4iLKqpxbVDDQhsUhtZgobKocUKc9/bbVB49SsBzz6JtQysZJ4OOEE9HjuZcPCE2mS18se8MN/T0s9bU8O0JAXGw/+O6fTxjLDiGtj6jSun6IiMj66pM63Q6XnnlFQ4ePMj27dt56623OHjwYN2+8+bNIykpiaSkJCZNmgSAyWTizjvvZPny5aSkpPDdd9+h11+81srYsWM5cOAAycnJ9OjRgyVLljSJJykpieXLl9dtX7x4MX5+fqSmpnLw4EFGjRpVF9Nzzz3Xbt8P5doR5h7GgsEL2JW9i3f3v2vvcADQtb5L+xJCaIG3gLFAJrBTCLFeSln3P19KOa/B/r8H+jU4xVLACfht50SsXIyfqxEng/YiCXEhvq5G/Fzb1i5pwKQwYkcG4ehiaI8wFUW5hkyffSufJH/Hrrh+jNr6Hd+tX0tR4u9xM6h+mVedFqpMW7T1947iChNuDUaIhUaDwVvXKCGuOHiQ/Hfexf2mqbhef32bw4j2cyG1lSnT29LzySup5KaEoPqNcTNh89OQfwy8I9t8XaUdfPU4nN3fvucMiIOJL9q8e2BgIIGBgQC4urrSq1cvTp8+Te/evVs8ZtOmTcTHx9O3b18AvL29W73OuHHj6j4eMmQIa9eubfWYFStWcPjwYcC6LMXHx6fVYxSlNVMjp7LtzDbe3vc2gwIGkeifaNd47DFCPAhIk1KmSymrgI+Amy6y/+3Ah7WfSCm/AdrQ8E/pSEIIa2Gti0yZ7tOG0eGGVDKsKMqliAwLoTw4loLibKqiwpi1uYJ1yf+xd1hKR2ghITZpGibEjYtqARj9nKkq1COlRFZVkfXkArRenvg/8cQlhdHD35X0vNKLVppel5SFq1HH6Bi/+o19ZgACDnxySddVrj4ZGRns3buXwYMH12178803iY+PZ86cORQUFACQmpqKEILx48eTmJjIyy+/3KbrrFixgokTJ9Z9fvz4cfr168eoUaP44YcfAOqmVC9cuJDExERmzpxJdnb25X6JioIQgqcGP0WwSzCP/fAYJVW2t67rCJ0+QgwEA6cafJ4JDG5uRyFEdyAc+LatFxFCPAA8ABAaGtr2KBWbhfs4s/90YZPtFdVmjuaUcGMvfztEpSjKtWzU9Ons/uuz7Bk4iiEfZrD3nfcwJ85Bq1EF+q4qLY0Qa6xriKWUlFSaGk2ZBjCG+HB+XzHmMhMF775L5eHDhLz1JloPj0sKI8rPWmDyVEF5XW2NhiqqzWw8cJaJfQJwaFgk0j0Eul9nrTY9svUWT0oHaMNIbkcrKSlhxowZvPbaa7jVTNv/n//5HxYuXIgQgoULF/Lwww+zYsUKTCYTP/74Izt37sTJyYkxY8bQv39/xowZ0+p1Fi9ejE6nY/bs2YB1hPrkyZN4e3uze/dubr75ZlJSUjCZTGRmZjJs2DBeffVVXn31VR555BFWrVrVod8H5drgYnBh6cilHMg7gLO+6e/NznSlF9WaBayVUrZ5xbWU8h0p5QAp5QBfX98OCE2pFe7jzKlzZVSZGr8xOXK2GLNFtmn9sKIoSnu4fvhAilyDOHkshZIxA7n+xyJ+2KZGia86LfSZNtUkxKVVZiySJiPEhu4hABQdKCDv7eW4TZ6Mqw2JREui/V0BWpw2/e3hHEoqTdyU0EwLwbhbIP8onNl3yddXur7q6mpmzJjB7NmzmT59et12f39/tFotGo2G+++/nx07dgAQEhLCyJEj8fHxwcnJiUmTJrFnz55Wr7Ny5Uq+/PJLVq9eXd+b22ism3Ldv39/IiMjSU1NxdvbGycnp7p4Zs6cadM1FMVWsT6x3BZzm92LFtsjIT4NdGvweUjNtubMosF0aeXKFO7jjEXCyQt6MNYX1FIFQhRF6Xw9bpiES2UBmcOmYtILSpe+gWwhgVK6qAYjxA3fT9UmxMUV1qrTbo4XjBBHRQGQvek0Wnd3/J9acFlh1FaaTmuhsNa6pNP4uhoZGtnMOs/eN4FGD/vXXFYMStclpeS+++6jV69e/OlPf2r02pkzZ+o+/uyzz+jTx9o2bPz48ezfv5+ysjJMJhPff/993Zrju+++uy5xbmjjxo28/PLLrF+/Hien+vZeubm5mM3Wsaf09HSOHj1KREQEQgimTJnCd999B8A333xz0XXNitJV2SMh3glECyHChRAGrEnv+gt3EkLEAJ7Atk6OT2mjllovpWQV4uqgo5uXoz3CUhTlGjdjxiTK9C4k/fg92bePxlhWTWlBjr3DUtpTg4TYxVA/CmwS1jXExRXWqtMXjhDrQqPQ6C1ggYCnn0bn6XlZYbgYdQR7ODbbi7iwvJoth3OZEh+EtrmuCU5eED3Wuo74CmlBonSun376iVWrVvHtt982aa/06KOPEhcXR3x8PFu2bGHZsmUAeHp68qc//YmBAweSkJBAYmIikydPBqztmIKCgppcZ+7cuRQXFzN27NhG7ZW2bt1KfHw8CQkJ3HLLLSxfvhwvLy8AXnrpJRYtWkR8fDyrVq3ilVde6YxviaJ0qk5fQyylNAkh5gJfA1pghZQyRQjxHLBLSlmbHM8CPpIXPM4XQvwAxAAuQohM4D4p5ded+CUoF6hPiEuA+vXCKVlFxAa52X0ahKIo1yYHowG3fqMw7fgvXvctZuCjf0VorvSVQkqbSAt+xhKOGrpbk95K6+baolq1I8QXriEWbsG4BFUgvLrhNn4c7SHKz4XUZnoRf33gLFVmS+Pq0heKuwWObGj5deWqNnz48BZnr1xsve6dd97JnXfe2WhbUVER0dHRhISENNk/LS2t2fPMmDGDGTNmNPta9+7d2bp1a4sxKMrVwC7vDKSUG6SUPaSUkVLKxTXbnm6QDCOlXCSlbNKjWEo5QkrpK6V0lFKGqGTY/jycDHg5GxqNEJvMFg6dKVLTpRVFsatb77wVk9Cy+ZPPVTJ8NZKSuyL2ku0X3ijprR0hLmphhBjXAIKHnidoavsV3ezh78Kx3BLMlsaJzedJpwnzdiI+5CL3wx4TweDSbrEoVzatVkthYSEJCQntfm43NzfWrOn46ffLli1jyZIldcW/FKUrU+8OlHYR7uNMem59QpyeV0qlyaIKaimKYleB/t6YwhPRHd/Dmex8e4ejtDtr8mlBNEp6qzWNp0y7NUmIA9s9kmg/VypNFjIL6utpZBdVsC09n5sSgi8+W8rgBDG/aveYlCtTt27dOHXqFElJSfYO5ZLNmzePI0eO8MILL9g7FEW5bCohVtrFhb2IU7KsbZjUCLGiKPY24baZ6KSZj9//2N6hKO2tZg1xk4RYNC6qdeGUaXQGcPJp11Ci/K0jvA2nTX+xLwspYerFpkvXipvZrvEoiqIotlEJsdIuwn2cySmupKTS+jQ+5XQRRp2GSF/79hVTFEXpn9CbIq9wivZ+T0Vllb3DUdpTXUKsaVRJuqqVoloA+MeCi1+7hVJbafpoTn1hrXVJWcQFuxPpa8N06IjrwT0UnFWrSEVRlM7U6UW1lKtTRE1hrYy8UvoEu5OSVURMoBs6rXrmoiiK/Q2dfhvHU49iMpnBaO9olHZTkxBLGie9Jmm99xRXVKPVCBz12qbH3vEfEM1sv0RuDnoC3R1IqxkhPpZbwv7ThTw1uZdtJ9Dq4Hc/g051ZlAURelMKiFW2kW4b33rpdggN1KyCvlVXxumiCmKonSCCWOHw9jh9g5DaW81lXklmsZFtWoKWxVXmHB10DW/flff/olnlJ8LqTUjxOuTshACprTlXmh0bfeYFEVRlItTw3dKuwjzrk+IMwvKKaowqYJaiqIoSsdqYQ2xydw4Ie4s0X6upOVYK02v35fF0Ahv/N0cOu36SteQkZGBo6NjXZXpOXPm4OfnR58+fRrtN3/+fGJiYoiPj2fatGmcP38egOrqau655x7i4uLo1asXS5YsAaC8vJyEhAQMBgN5eXmd+0UpShemEmKlXTjotQR7OHI8r1QV1FIURVE6R01CDAJfl/q58CaLdXtxRTWuRn0zB3aMHv4uVFRb+OrAGY7nlV6897ByTYuMjKyrMn3vvfeycePGJvuMHTuWAwcOkJycTI8ePeoS3zVr1lBZWcn+/fvZvXs3f//73+uS7KSkJIKC1M+dorSFmjKttJswHyfS80pJySpCqxHEBKipX4qiKEoHqkmIn72pDyEJwfCldXPtCHFRZ48Q11SafnVzKgathgl92r+9k9K+XtrxEofPHW7Xc8Z4xfDYoMds3n/kyJFkZGQ02T5u3Li6j4cMGcLatWsBEEJQWlqKyWSivLwcg8Gg+gErymVQI8RKuwn3ceZ4bgkHThcS5euCQ3NFTBRFURSlnUX4uWHQ1b+labyGuPNGiKP8rA+C03NLGR3ji7tj511bubqtWLGCiRMnAnDLLbfg7OxMYGAgoaGhPPLII3h5edk5QkXputQIsdJuwn1cKKowseP4OcarP0pDAAAgAElEQVTHBtg7HEVRFOVqVztlWjR+vl87ZbqovJpenThbyd1Rj7+bkeyiSm5KCO606yqXri0jufayePFidDods2fPBmDHjh1otVqysrIoKChgxIgR3HjjjURERNg5UkXpmtQIsdJualsvlVaZ6a0KaimKoigdrYWE2Fw3QlzdqVOmwVpYy9Wo44aY9utxrFy7Vq5cyZdffsnq1avrqqV/8MEHTJgwAb1ej5+fH9dddx27du2yc6SK0nWphFhpN+E1CTGoglqKoihKJ2hQVKuharNESklJZedOmQZ4dEJP/npHP7VsSLlsGzdu5OWXX2b9+vU4OTnVbQ8NDeXbb78FoLS0lO3btxMTE2OvMBWly1MJsdJuQjwd0Wmsb0rUCLGiKIrS4VocIbZQWmXGIun0EeL4EA+u76lGhxXb3X777QwdOpQjR44QEhLCP/7xDwDmzp1LcXExY8eOJSEhgQcffBCA//3f/6WkpITY2FgGDhzIr3/9a+Lj4+35JShKl6bWECvtRqfVEOrthMksVSERRVEUpeNJ69ToCxPiarOkuKIaoNNHiBWlrT788MNmt6elpTW73cXFhTVr1nRkSIpyTVEjxEq7umdoGL8ZEW7vMBRFUZQ2EkJMEEIcEUKkCSEeb+b1UCHEFiHEXiFEshBiUjOvlwghHum0oC+yhri4wgR0/gixorRGq9VSWFhIQkJCu563vLychIQEqqur0WjUW3xFsZW6Syjt6p5hYfYOQVEURWkjIYQWeAsYC2QCO4UQ66WUBxvs9hTwsZTybSFEb2ADENbg9VeBrzopZKu6hNi6XKd8zGLu/6qE682WBiPE6q2OcmXp1q0bp06davfzOjo6kpSU1O7nVZSrnXp8pCiKoijKICBNSpkupawCPgJuumAfCdQWiHAHsmpfEELcDBwHUjoh1gYRNU6ILYMe5EdLHGaLpKhuhFhNmVYURVFaphJiRVEURVGCgYZDVpk12xpaBNwphMjEOjr8ewAhhAvwGPBsx4d5ocZriHVaa2JsajBl2t1RjRAriqIoLVMJsaIoiqIotrgdWCmlDAEmAauEEBqsifIyKWVJaycQQjwghNglhNiVm5t7+RFdUFRLV7Nu0qSKaimKoig2Uo9NFUVRFEU5DXRr8HlIzbaG7gMmAEgptwkhHAAfYDBwixDiZcADsAghKqSUb154ESnlO8A7AAMGDJCXHfUFRbW0GoEQYLJYVFEtRVEUxSZqhFhRFEVRlJ1AtBAiXAhhAGYB6y/Y5yQwBkAI0QtwAHKllCOklGFSyjDgNeCF5pLhDlGbECPqNuk0ombKdDVajcBRr+2UUBTFVhkZGTg6OjaqMh0WFkZcXBwJCQkMGDCgbvuaNWuIjY1Fo9Gwa9euuu2bN2+mf//+xMXF0b9/f7799ttWr3sp5/rwww+Ji4sjPj6eCRMmkJeXB8D8+fMJCAjgL3/5y2V9LxTlSqAemyqKoijKNU5KaRJCzAW+BrTACillihDiOWCXlHI98DDwrhBiHtbFu/dKKS9/lPdyNNOHWKfRYDJbqDRZcHXQIYRo4WBFsZ/IyMgmFaG3bNmCj49Po219+vTh008/5be//W2j7T4+PnzxxRcEBQVx4MABxo8fz+nTF07qaKyt5zKZTDz00EMcPHgQHx8fHn30Ud58800WLVrE0qVLcXZ2vozvgKJcOVRCrCiKoigKUsoNWItlNdz2dIOPDwLXtXKORR0SXIsXbNqHuH6E2KSmSyutOvv/7N15fBXV/f/x1yc7S8KWsCXsguxEVgFFcWG1rqgoVHGpdf9Vq3W3Smvtt7Zat7pU0RZRK+JWqwgoLqgICGEXiOx7AgHCFrKc3x8zCZeQkASS3Jvk/Xw87iNzz5w5c+bk3kw+c86c+dOfyFr+U7mWGd2pI03vv79cyurUqVOR6aecckrBcpcuXThw4ABZWVlER0eXW1lhYWE459i3bx+NGjViz549nHTSScd5JCKhS0OmRUREpGoqKiAOt4JJtWKjNaGWVA1mxpAhQ+jVqxcvv/xymbadMmUKPXv2PGYwfDxlRUZG8sILL9CtWzeaN2/OsmXLuO666054HyKhRpdORUREpGoq9BxigPCwMHL85xCrh1hKUl49uSdq1qxZJCYmsn37ds4991w6duzIoEGDStxu6dKl3HPPPUybNu2E61C4rOzsbF544QUWLFhA27Ztue2223j88cd58MEHT3hfIqFEPcQiIiJSReXfQ3w4II4MN3Jy8/wh0+ohlqohMdF77Hfjxo256KKLmDNnTonbbNy4kYsuuoh///vftGvX7oT2X1RZ+fc4t2vXDjPjsssu47vvvjuh/YiEIgXEIiIiUjUVMWQ6PMzI9WeZjlMPsVQB+/btIzMzs2B52rRpdO3a9Zjb7Nq1i5EjR/LnP/+ZgQOPvLX/qquuKlVAXVJZiYmJLFu2jPxnhk+fPr3Y+5BFqrKgBMRmNszMVphZqpndW8T6p8wsxX+tNLNdAeuuNrNV/uvqyq25iIiIhIwiZpmODA8jW5NqSRWybds2TjvtNHr06EHfvn0ZOXIkw4YNA+D9998nKSmJ77//npEjRzJ06FAAnnvuOVJTUxk/fjzJyckkJyezfft2ABYtWkTz5s2P2k9Zy2revDm///3vGTRoEN27dyclJYX7Q2SIuUh5qvQzhZmFA88D5wIbgblm9pE/eyUAzrk7AvLfBpziLzcEfg/0xhsn9aO/bUYlHoKIiIiEgmJ6iHNy89ibpSHTUjW0bduWhQsXFrnuoosu4qKLLjoq/cEHHyzyXt49e/bQvn17kpKSTrgsgBtvvJEbb7yxpEMQqdKC0UPcF0h1zq12zh0C3gYuOEb+K4C3/OWhwHTn3E4/CJ4ODKvQ2oqIiEhoyg+IOXwPcUSYkXkwh9w8px5iCUnh4eHs3r2b5OTkci87Li6OyZMnl3u5hd1999288cYbehaxVAvBOFMkAhsC3m8E+hWV0cxaAW2AL46xbWIF1FFERERCXRFDpiPCjR37DgGoh1hCUosWLdiwYUPJGUPYE088wRNPPBHsaoiUi1CfVGs08K5zLresG5rZDWY2z8zm5U8GICIiItVIUc8hDgsjoyAgVg+xiIgcWzAC4k1Ai4D3SX5aUUZzeLh0mbZ1zr3snOvtnOudkJBwAtUVERGRkFRkQGxk7FdALCIipROMgHgu0N7M2phZFF7Q+1HhTGbWEWgAfB+Q/BkwxMwamFkDYIifJiIiIjVNQUAccA9xuJGV46VryLSIiJSk0i+dOudyzOxWvEA2HJjgnFtqZuOBec65/OB4NPC2c/k3CIFzbqeZ/QEvqAYY75zbWZn1FxERkVBRxD3EYYeX9RxiEREpSVDuIXbOfeKc6+Cca+ece8xPezggGMY594hz7qhnFDvnJjjnTvJfr1VmvUVERCSEFNNDnE89xBKK1q5dS61atQpmmd6wYQODBw+mc+fOdOnShaeffrog7yOPPEJiYmLB84E/+eSTgnWLFi2if//+dOnShW7dunHw4MFj7vehhx6ie/fuJCcnM2TIEDZv3gzApEmT6N69O926dWPAgAEFj4A6cOAAycnJREVFkZ6eXt7NIBIyQn1SLREREZGiFTXLdFhgQKweYglN7dq1IyUlBYCIiAj+9re/sWzZMmbPns3zzz/PsmXLCvLecccdpKSkkJKSwogRIwDIyclh7NixvPjiiyxdupQvv/ySyMhjXwC6++67WbRoESkpKZx33nmMHz8egDZt2vDVV1+xePFiHnroIW644QYAatWqRUpKCs2bN6+IJhAJGTpTiIiISNVU5HOIveA4PMyoHRUehEpJVfLNOytJ37C3XMuMb1GX0y/rUOr8zZo1o1mzZgDExsbSqVMnNm3aROfOnYvdZtq0aXTv3p0ePXoA0KhRoxL3ExcXV7C8b98+zB9ZMWDAgIL0U089lY0bN5a67iLVgXqIRUREpGoqooc43B8yHRsTUfAPv0hVsXbtWhYsWEC/fv0K0p577jm6d+/OtddeS0ZGBgArV67EzBg6dCg9e/bkL3/5S6nKf+CBB2jRogWTJk0q6CEO9OqrrzJ8+PDyORiRKkI9xCIiIlI1FfHYpciwwwGxSEnK0pNb0fbu3csll1zC3//+94Le3JtuuomHHnoIM+Ohhx7it7/9LRMmTCAnJ4dZs2Yxd+5cateuzdlnn02vXr04++yzj7mPxx57jMcee4zHH3+c5557jkcffbRg3cyZM3n11VeZNWtWhR6nSKhRD7GIiIhUTUUExOH+kOnYaE2oJVVHdnY2l1xyCWPGjOHiiy8uSG/SpAnh4eGEhYXxq1/9ijlz5gCQlJTEoEGDiI+Pp3bt2owYMYL58+eXen9jxoxhypQpBe8XLVrE9ddfz4cffliq4dci1YkCYhEREamaiphlOjJcPcRStTjnuO666+jUqRN33nnnEeu2bNlSsPz+++/TtWtXAIYOHcrixYvZv38/OTk5fPXVVwX3HF911VUFgXOgVatWFSx/+OGHdOzYEYD169dz8cUXM3HiRDp0CJ0ec5HKorOFiIiIVFFF3ENcMGRaPcRSNXz77bdMnDiRbt26FTyK6U9/+hMjRozgd7/7HSkpKZgZrVu35qWXXgKgQYMG3HnnnfTp0wczY8SIEYwcORLwenuLmhn63nvvZcWKFYSFhdGqVStefPFFAMaPH8+OHTu4+eabAW/W63nz5lXGoYuEBAXEIiIiUjUVdQ9xuLccpx5iqSJOO+00XP4EcYVMnDix2O3Gjh3L2LFjj0jbs2cP7du3Jykp6aj8gUOkA73yyiu88sorZaixSPWiIdMiIiJSNZ31kPezyB5iBcQSmsLDw9m9e3dBb3B5iouLY/LkyeVS1oEDB0hOTiY7O5uwMIUMUn3pbCEiIiJV0+l3eq8AEeEaMi2hrUWLFmzYsCHY1ShRrVq1SElJCXY1RCqcLveIiIhItRGhHmIRESkDBcQiIiJSbUTkP3ZJPcQiIlIKCohFRESk2lAPsYiIlIUCYhEREak2IsLze4gVEIuISMkUEIuIiEi1EalJtSTErV27llq1ah0xy/S1115L48aN6dq16xF57777bjp27Ej37t256KKL2LVrFwDZ2dlcffXVdOvWjU6dOvH444+XuN/nnnuOk046CTMjPT29IH3SpEl0796dbt26MWDAABYuXFiw7qmnnqJLly507dqVK664goMHDwIwZswYGjZsyLvvvntCbSESChQQi4iICGY2zMxWmFmqmd1bxPqWZjbTzBaY2SIzG+Gn9zWzFP+10MwuqvzaH5b/2CU9h1hCWbt27Y6YwXncuHFMnTr1qHznnnsuS5YsYdGiRXTo0KEg8J08eTJZWVksXryYH3/8kZdeeom1a9cec58DBw5kxowZtGrV6oj0Nm3a8NVXX7F48WIeeughbrjhBgA2bdrEM888w7x581iyZAm5ubm8/fbbgBdEn3/++SfSBCIhQ2cLERGRGs7MwoHngXOBjcBcM/vIObcsINuDwDvOuRfMrDPwCdAaWAL0ds7lmFkzYKGZ/dc5l1O5R+GpEx2BGTSoExWM3UsVM/P1l9m+bnW5ltm4VVsGj7uhTNsMGjSoyIB2yJAhBcunnnpqQY+smbFv3z5ycnI4cOAAUVFRxMXFHXMfp5xySpHpAwYMOGIfGzduLHifX35kZCT79++nefPmZTkskSpBAXFptR8CdRqDhQe7JiIiIuWtL5DqnFsNYGZvAxcAgQGxA/L/464HbAZwzu0PyBPj5wua83s0p3WjOsTXjQ5mNUTK3YQJE7j88ssBGDVqFB9++CHNmjVj//79PPXUUzRs2PCE9/Hqq68yfPhwABITE7nrrrto2bIltWrVYsiQIUcE6CLVhQLi0mrWw3uJiIhUP4nAhoD3G4F+hfI8Akwzs9uAOsA5+SvMrB8wAWgF/LK43mEzuwG4AaBly5blVfcjxESG07fNiQcGUjOUtSc3WB577DEiIiIYM2YMAHPmzCE8PJzNmzeTkZHB6aefzjnnnEPbtm2Pex8zZ87k1VdfZdasWQBkZGTw4YcfsmbNGurXr8+ll17KG2+8wdixY8vlmERChe4hFhERkdK4AnjdOZcEjAAmmlkYgHPuB+dcF6APcJ+ZxRRVgHPuZedcb+dc74SEhEqruEhV9vrrr/Pxxx8zadIkzLx75N98802GDRtGZGQkjRs3ZuDAgcybN++497Fo0SKuv/56PvzwQxo1agTAjBkzaNOmDQkJCURGRnLxxRfz3XfflcsxiYQSBcQiIiKyCWgR8D7JTwt0HfAOgHPue7zh0fGBGZxzy4G9QFdE5IRNnTqVv/zlL3z00UfUrl27IL1ly5Z88cUXAOzbt4/Zs2fTsWNHAM4++2w2bSr89S3e+vXrufjii5k4cSIdOnQ4Yh+zZ89m//79OOf4/PPP6dSpUzkdmUjoUEAsIiIic4H2ZtbGzKKA0cBHhfKsB84GMLNOeAFxmr9NhJ/eCugIrK2siotUB1dccQX9+/dnxYoVJCUl8eqrrwJw6623kpmZybnnnktycjI33ngjALfccgt79+6lS5cu9OnTh2uuuYbu3buTl5dHampqkfcTP/PMMyQlJbFx40a6d+/O9ddfD8D48ePZsWMHN998M8nJyfTu3RuAfv36MWrUKHr27Em3bt3Iy8srmIFapDrRPcQiIiI1nD9D9K3AZ0A4MME5t9TMxgPznHMfAb8F/mlmd+BNnDXOOefM7DTgXjPLBvKAm51z6cXsSkSK8NZbbxWZnpqaWmR63bp1mTx58lHpy5Yt45JLLqFWrVpHrbv99tu5/fbbj0p/5ZVXeOWVV4rcz6OPPsqjjz56rKqLVHkKiEVERATn3Cd4j1IKTHs4YHkZMLCI7SYCEyu8giLVRHh4OLt37yY5OfmIZxGXh65du/Lkk0+Wa5lFGTNmDN999x2jRo2q8H2JVDQFxCIiIiJSozjnCiaoqmwtWrRgw4YNJWcMYZMmTSoy3bmgPnVN5LjoHmIRERERqTFiYmLYsWOHgrdy5pxjx44dxMQUOcm8SMhSD7GIiIiI1Bj5E0ulpaUFuyrVTkxMDElJScGuhkiZKCAWERERkRojMjKSNm3aBLsaIhIiNGRaREREREREaiQFxCIiIiIiIlIjKSAWERERERGRGslqwgx7ZpYGrCuHouKB9HIop7pTO5WO2ql01E4lUxuVTnm2UyvnXEI5lVUj6dxc6dROpaN2Kh21U8nURqUT9HNzjQiIy4uZzXPO9Q52PUKd2ql01E6lo3YqmdqodNRO1ZN+r6WjdiodtVPpqJ1KpjYqnVBoJw2ZFhERERERkRpJAbGIiIiIiIjUSAqIy+blYFegilA7lY7aqXTUTiVTG5WO2ql60u+1dNROpaN2Kh21U8nURqUT9HbSPcQiIiIiIiJSI6mHWERERERERGokBcQiIiIiIiJSIykgLiUzG2ZmK8ws1czuDXZ9KpuZTTCz7Wa2JCCtoZlNN7NV/s8GfrqZ2TN+Wy0ys54B21zt519lZlcH41gqipm1MLOZZrbMzJaa2f/z09VOAcwsxszmmNlCv50e9dPbmNkPfnv8x8yi/PRo/32qv751QFn3+ekrzGxocI6o4phZuJktMLOP/fdqo0LMbK2ZLTazFDOb56fpO1dD6Nysc3NJdG4uHZ2bS0/n5pJVuXOzc06vEl5AOPAz0BaIAhYCnYNdr0pug0FAT2BJQNpfgHv95XuB//OXRwCfAgacCvzgpzcEVvs/G/jLDYJ9bOXYRs2Anv5yLLAS6Kx2OqqdDKjrL0cCP/jH/w4w2k9/EbjJX74ZeNFfHg38x1/u7H8Xo4E2/nc0PNjHV85tdSfwJvCx/15tdHQbrQXiC6XpO1cDXjo369xcyjbSubl07aRzc+nbSufmktuoSp2b1UNcOn2BVOfcaufcIeBt4IIg16lSOee+BnYWSr4A+Je//C/gwoD0fzvPbKC+mTUDhgLTnXM7nXMZwHRgWMXXvnI457Y45+b7y5nAciARtdMR/OPd67+N9F8OOAt4108v3E757fcucLaZmZ/+tnMuyzm3BkjF+65WC2aWBIwEXvHfG2qj0tJ3rmbQuVnn5hLp3Fw6OjeXjs7NJyRkv3MKiEsnEdgQ8H6jn1bTNXHObfGXtwJN/OXi2qvGtKM/LOYUvCusaqdC/OFGKcB2vD9wPwO7nHM5fpbAYy5oD3/9bqAR1b+d/g78Dsjz3zdCbVQUB0wzsx/N7AY/Td+5mkG/t6Lp818MnZuPTefmUtG5uXSq1Lk5oiIKlZrHOefMTM/wAsysLjAF+I1zbo93MdCjdvI453KBZDOrD7wPdAxylUKKmZ0HbHfO/WhmZwa7PiHuNOfcJjNrDEw3s58CV+o7JzWZPv+H6dxcMp2bj03n5jKpUudm9RCXziagRcD7JD+tptvmD2nA/7ndTy+uvap9O5pZJN4Jd5Jz7j0/We1UDOfcLmAm0B9viEz+RbrAYy5oD399PWAH1budBgLnm9lavGGgZwFPozY6inNuk/9zO94/cH3Rd66m0O+taPr8F6Jzc9no3FwsnZtLqaqdmxUQl85coL0/i1wU3o3xHwW5TqHgIyB/xrergQ8D0q/yZ407FdjtD5H4DBhiZg38meWG+GnVgn9fyKvAcufckwGr1E4BzCzBv/qMmdUCzsW7p2smMMrPVrid8ttvFPCFc8756aP9WRzbAO2BOZVzFBXLOXefcy7JOdca7+/NF865MaiNjmBmdcwsNn8Z77uyBH3nagqdm4umz38AnZtLR+fmkuncXDpV8tzsQmAmsqrwwpsBbSXe/RQPBLs+QTj+t4AtQDbeGP7r8O6D+BxYBcwAGvp5DXjeb6vFQO+Acq7FmzwgFbgm2MdVzm10Gt49E4uAFP81Qu10VDt1Bxb47bQEeNhPb4t3QkgFJgPRfnqM/z7VX982oKwH/PZbAQwP9rFVUHudyeGZLNVGR7ZNW7yZOhcCS/P/Nus7V3NeOjfr3FyKNtK5uXTtpHNz2dpL5+bi26bKnZvN35mIiIiIiIhIjaIh0yIiIiIiIlIjKSAWERERERGRGkkBsYiIiIiIiNRICohFRERERESkRlJALCIiIiIiIjWSAmIRERERERGpkRQQi1QBZtbazJaUIf84M2teijzPnWC9xpvZOSdShoiISFWkc7NI9RAR7AqISIUYBywBNlfkTpxzD1dk+SIiItXIOHRuFgk56iEWqToizGySmS03s3fNrLaZPWxmc81siZm9bJ5RQG9gkpmlmFktM+tjZt+Z2UIzm2NmsX6Zzc1sqpmtMrO/FLdjMws3s9f9/Sw2szv89NfNbJSZ9fb3leKvd/76dn75P5rZN2bWscJbSUREpPLo3CxSxSkgFqk6Tgb+4ZzrBOwBbgaec871cc51BWoB5znn3gXmAWOcc8lALvAf4P8553oA5wAH/DKTgcuBbsDlZtaimH0nA4nOua7OuW7Aa4ErnXPznHPJ/v6mAn/1V70M3Oac6wXcBfzjxJtBREQkZOjcLFLFaci0SNWxwTn3rb/8BnA7sMbMfgfUBhoCS4H/FtruZGCLc24ugHNuD4CZAXzunNvtv18GtAI2FLHv1UBbM3sW+B8wragKmtnlQE9giJnVBQYAk/19AUSX8ZhFRERCmc7NIlWcAmKRqsMV8f4fQG/n3AYzewSIKWOZWQHLuRTzN8E5l2FmPYChwI3AZcC1gXnMrCvwCDDIOZdrZmHALv/KtIiISHWkc7NIFach0yJVR0sz6+8vXwnM8pfT/Su+owLyZgL59yKtAJqZWR8AM4s1szJdDDOzeCDMOTcFeBDvSnPg+vrAW8BVzrk0KLjavcbMLvXzmH/iFhERqS50bhap4tRDLFJ1rABuMbMJwDLgBaAB3oyVW4G5AXlfB140swNAf7x7kZ41s1p49yiV9XEMicBr/pVlgPsKrb8Ab0jXP/OHYPlXn8cAL5jZg0Ak8DawsIz7FhERCVU6N4tUceZc4ZEeIiIiIiIiItWfhkyLiIiIiIhIjaQh0yJyBDP7gaNnnPylc25xMOojIiJS0+ncLFJxNGRaREREREREaiQNmRYREREREZEaSQGxiIiIiIiI1EgKiEVERERERKRGUkAsIiIiIiIiNZICYhEREREREamRFBCLiIiIiIhIjaSAWERERERERGokBcQiIiIiIiJSIykgFhERERERkRpJAbGIiIiIiIjUSAqIRUREREREpEZSQCwiIiIiIiI1kgJiERERERERqZEUEIuIiIiIiEiNpIBYJIjMbJyZzTqB7V80s4fKs04VzcyWmtmZlbCftWZ2TiXs5xEze6Mcy7vfzF4pr/JEREREpHgKiKXc+AHIATPbG/Bq7q972cxWmFmemY0roZwkM5tiZulmttvMlpS0TU1QVPDsnLvROfeHYNXpeDjnujjnvjyRMso7CC2i/C/N7PqKKv9YnHN/cs4FZd8iIiIiNY0CYilvv3DO1Q14bfbTFwI3A/NLUcZEYAPQCmgE/BLYVp6VNLOI8ixPRERERESqHgXEUimcc8875z4HDpYiex/gdefcPudcjnNugXPu0/yVZnaamX1nZrvMbEN+77GZ1TOzf5tZmpmtM7MHzSzMXzfOzL41s6fMbAfwiJ9+rZktN7MMM/vMzFoVVykzOzVgvwvzh/2a2eVmNq9Q3jvM7KOS6lVom9Zm5gKD9fyeSjPrBLwI9Pd73nf56183sz8G5P+VmaWa2U4z+yi/h95f58zsRjNb5R/D82ZmxRxrXzP73s+3xcyeM7OogPVD/B7/3Wb2DzP7Kr9H1czamdkXZrbD7+WfZGb1A7YtGMrs9/S+47dPpj+cundA3nvMbJO/boWZnW1mw4D7gcv9tlhY3O8M6GNmy/zf72tmFuOX28DMPvZ/Jxn+cpK/7jHgdOA5v/zn/PQuZjbdb9ttZnZ/wH6iijuG4hR1bNeNqAkAACAASURBVAFt8oa/nF+H/FeOmT3ir2tu3kiKNDNbY2a3l7RPERERETmSAmIJRbOB581stJm1DFzhB6yfAs8CCUAykOKvfhaoB7QFzgCuAq4J2LwfsBpoAjxmZhfgBVYX+2V9A7xVVIXMLBH4H/BHoCFwFzDFzBKA/wInm1n7gE2uBN4sZb1K5JxbDtwIfO/3vNcvnMfMzgIeBy4DmgHrgLcLZTsP74JDdz/f0GJ2mQvcAcQD/YGz8Xr4MbN44F3gPrwe/BXAgMCq+PVoDnQCWuBfgCjG+X496wMfAfkB6MnArUAf51ysX9e1zrmpwJ+A//ht0eMYZY/xt2sHdAAe9NPDgNfwRiG0BA7k79c59wDeZ+FWv/xbzSwWmAFM9Y/rJODzko6hOMUdW+F8zrn8OtQFTgMygA/9Cyr/xRt5kYj3+/mNmRX3+xQRERGRIigglvL2gd+ruMvMPjjOMi7FC0geAtaYWYqZ9fHXXQnMcM695ZzLds7tcM6lmFk4MBq4zzmX6ZxbC/wNb7h1vs3OuWf9XucDeAHm48655c65HLwgK9mK7iUeC3zinPvEOZfnnJsOzANGOOf2Ax8CVwD4gXFH4KNS1qu8jAEmOOfmO+ey8ALW/mbWOiDPn51zu5xz64GZeBcUjuKc+9E5N9tvq7XAS3jBPMAIYKlz7j2/3Z4BtgZsm+qcm+6cy3LOpQFPBmxblFl+u+biDZfPD3BzgWigs5lFOufWOud+LkN7ADznnNvgnNsJPIb/O/I/N1Occ/udc5n+umPV8Txgq3Pub865g/7v8odSHENxynRs/oWXD4DbnHML8C5qJDjnxjvnDjnnVgP/xPusiYiIiEgpKSCW8nahc66+/7rweApwzmU45+51znXB681NwQu0Da+3sajAIR6IxOsVzbcOr/cs34ZC27QCns4P4IGdeL2biRytFXBpQLC/C6/Hrpm//k38YAsvaP/AD5RLU6/y0jxwP865vcCOQvvaGrC8H6hbVEFm1sEfRrzVzPbgXSyID9hPQVs65xywMWDbJmb2tj8ceA/wRsC2RSlcpxgzi3DOpQK/wetd3u6X2byoAo4h8He+zq87ZlbbzF4ybwj7HuBroL5/AaMoxX3ujnkMxWUuy7GZWSRej/ybzrn8Hv9WQPNCn8f78b4vIiIiIlJKCoglpDnn0oG/4gUyDfECnHZFZE0HsvEChXwtgU2BxRXaZgPw64AAvr5zrpZz7rsiyt8ATCyUt45z7s/++ulAgpkl4wXG+cOlS1OvfPv8n7UD0poeo/6FbQ7cj5nVwRvSXNS+SvIC8BPQ3jkXhxds5d9vvAVICtiPBb7HC54d0M3fdmzAtmXinHvTOXca3nE54P/yV5WyiBYByy3x2gjgt8DJQD+/joP89Px6FvVZaVuGqpfoGMdW2LPAHg4P986vz5pCn8dY59yI8qyjiIiISHWngFgqhZlF+RMaGRBpZjFWxMRSft7/M7OuZhbh37t5E5DqnNsBTALOMbPL/PWNzCzZH6r6Dt69wbH+sOc78Xoni/MicJ+ZdfH3W8/MLi0m7xvAL8xsqJmF+/U/M38iJudcNjAZeAIvcJ/up5e6Xv7w4k3AWH8f13Jk8L8NSLKAya0KeQu4xsySzSwaLzD9wR/yXFaxeEHYXjPriPc7yPc/oJuZXej3gt7CkYF7LLAX2O3fe333cewfMzvZzM7yj+Ug3n2+ef7qbUDr4j5DAW4x7zFeDYEHgP8E1PEAsMtf9/tC223jyAD4Y6CZmf3GzKL932W/4zmuUhxbYL5f4w3lHuOcC1w/B8g0b2KuWv7npWvArQUiIiIiUgoKiKWyTMP7p38A8LK/PKiYvLWB94FdeJNgtcKbtAj/3tcReD18O/GGU+ffr3kbXi/ramAWXi/thOIq5Jx7H69X7m1/2OwSYHgxeTcA+ZNwpeH10N3Nkd+hN4FzgMn+vbX5ylKvX/nl7gC6AIG91V8AS4GtZpZeRB1n4N13PQWvF7cdx39P6V14Q78z8e5NzQ8k83vtLwX+4tezM9791Fl+lkeBnsBuvOD5veOsQzTwZ7xe9q1AY7z7osG7+ACww8yO9SivN/E+e6vxhjznz8j9d6CWX/ZsvMmyAj0NjDJvBupn/PuMzwV+4ddlFTD4OI8Ljn1sga7AC8w32+GZpu/3L7Sch3cP+Bq/nFfwJm8TERERkVIy7/Y/EZHj4/fSbsTrxZwZ7PqIiIiIiJSWeohFpMz8oeP1/SG/+fcXzw5ytUREREREykQBsYgcj/54Q5DT8YYRX+g/ykp8ZtYyYJhz4VfLkksQCQ1mNsHMtpvZkmLWm5k9Y2apZrbIzHpWdh1FRESOl4ZMi4iISLHMbBDeRHn/ds51LWL9CLy5EkYA/YCnnXPHPemciIhIZVIPsYiIiBTLOfc13iSGxbkAL1h2zrnZeM/0bnaM/CIiIiEjItgVqAzx8fGudevWwa6GiIhUEz/++GO6cy4h2PUIEYl4M+/n2+inbSmc0cxuAG4AqFOnTq+OHTtWSgVFRKT6O95zc40IiFu3bs28efOCXQ0REakmzGxdsOtQFTnnXsZ79B69e/d2OjeLiEh5Od5zs4ZMi4iIyInYBLQIeJ/kp4mIiIQ8BcQiIiJyIj4CrvJnmz4V2O2cO2q4tIiISCiqEUOmRURE5PiY2VvAmUC8mW0Efg9EAjjnXgQ+wZthOhXYD1wTnJqKiIiUnQJiERERKZZz7ooS1jvglkqqjoiISLmq0CHTZjbMzFaYWaqZ3VvE+qfMLMV/rTSzXQHr/mJmS81suZk9Y2bmp/cys8V+mQXpIiIiIiIiImVRYQGxmYUDzwPDgc7AFWbWOTCPc+4O51yycy4ZeBZ4z992ADAQ6A50BfoAZ/ibvQD8Cmjvv4ZV1DGIiIiIiIhI9VWRPcR9gVTn3Grn3CHgbeCCY+S/AnjLX3ZADBAFROPdq7TNzJoBcc652f4QrX8DF1bUAYiIiIiIiEj1VZEBcSKwIeD9Rj/tKGbWCmgDfAHgnPsemAls8V+fOeeW+9tvLE2ZIiIiIiIiIscSKo9dGg2865zLBTCzk4BOeM8yTATOMrPTy1Kgmd1gZvPMbF5aWlq5V1hERERERESqtooMiDcBLQLeJ/lpRRnN4eHSABcBs51ze51ze4FPgf7+9kmlKdM597JzrrdzrndCQsJxHoKIiIiIiIhUVxUZEM8F2ptZGzOLwgt6Pyqcycw6Ag2A7wOS1wNnmFmEmUXiTai13Dm3BdhjZqf6s0tfBXxYgccgIiIiIiIi1VSFBcTOuRzgVuAzYDnwjnNuqZmNN7PzA7KOBt72J8nK9y7wM7AYWAgsdM791193M/AKkOrn+bSijkGCKzfPlZxJRERERETkOEVUZOHOuU+ATwqlPVzo/SNFbJcL/LqYMufhPYpJqrEfVu9gzCs/cHHPRH475GSaxMUEu0oiIiIiIlLNhMqkWiJHmDJ/I2FhxvsLNnHmE1/y1PSV7D+UE+xqiYiIiIhINaKAWEJOTm4e05dtY3jXpnx+55mc1akxT3++ijOf+JL/zF2vodQiIiIiIlIuFBBLyJmzZicZ+7MZ3rUpLRvV5vkrezLlpgEkNqjFPVMWM/KZb/hmlR6lJSIiIiIiJ0YBsYScqUu3EhMZxqAOhx+X1atVA967aQDPX9mTfYdy+OWrc7h6whxWbM0MYk1FRERERKQqU0AsISUvzzF1yVbO7NCY2lFHzvlmZozs3owZd57BgyM7sWB9BsOf/pr73lvE9syDQaqxiIiIiIhUVQqIJaQs2LCL7ZlZDOvatNg80RHhXH96W766ezDjBrTh3R83cuYTX/LM56s4cCi3EmsrIiIiIiJVmQJiCSlTl2whMtw4q1PjEvM2qBPFw7/ozPQ7zmBQ+wSenL6SM/86k8nzNmjiLRERERERKZECYgkZzjmmLt3KwJPiiYuJLPV2rePr8OIvezH5xv40rVeLu99dxC+encW3qekVWFupFHvTwOnihoiIiIhUDAXEEjKWbt7Dhp0HGH6M4dLH0qd1Q96/aQDPXHEKuw9kM+aVH7j29bms2qaJt6qkzK3w1/bwwU2Qmx3s2oiIiIhINRRRchaRyvHZ0q2EGZzTqclxlxEWZpzfozlDOjfh9e/W8vwXqQz9+9ec3akJZ3RI4IwOCbRoWLscay0VJisTcLDwLdiXBpf+C6LrBrtWIiIiIlKNKCCWkPHpkq30a9OIRnWjT7ismMhwbjyjHZf1bsE/Zqby6ZKtTF+2DYDWjWpzevsEBnVIoH+7RtSN1tcgpLUfCqnT4V/nwZWToW5CydvUJFl7YdU06HwhhGnQj4iIiEhZKBKQkJC6PZPU7Xv55amtyrXchnWiePC8zjwwshOr0/fxzco0vl6Vzrs/bmTi7HVEhBk9WzVgUPt4Tm+fQNfEeoSHWbnWQU7MoS6jiOp9DUy+BiYMgbHvQcM2wa5W6Fg1Dd69Bi6Pgk7nBbs2IiIiIlWKAmIJCVOXbAVgaJfju3+4JGZGu4S6tEuoy7iBbcjKyWX+ul18vSqNb1al8ddpK/nrtJU0qB3JwJPiGdQhgdPbx9OsXq0KqY+ULHX7Xk4C7npnITMjB3F67Uf5v4w/4J4fzJsn/Y2cJt2JrxtNQqz3iq/rvaIialgvaZ7/qLHZLyggFhERESkjBcQSEqYu3copLevTtF5MpewvOiKc/u0a0b9dI+4Z1pH0vVl8m5rO1yvT+WZVGh8v2gJAhyZ1Ob29Fxz3a9OIWlHhlVI/gX99t5Y/AOf1aEaj2kmkZSbwcMaT3JP+AL9ccTO/XnwHs/K6HbVd/dqRXqBc93Cg7P2MKgieE+pG07BOFBHh1Sh4XjcLtiyEZj2CXRMRERGRKkMBsQTdhp37WbJpD/eP6Bi0OsTXjeaC5EQuSE7EOceKbZl8vTKNb1alM3H2Ol6dtYaoiDD6tm7IoA7e8OqOTWMx0/DqirBk026+/TkdomFI56YM6dbFX9MT9pwOk0YxMe0JMoY8zfrE80jPzCJtbxZpmVmk+z/TMrNYtHEXaZlZ7DuUe9Q+zKBRnaiAgPlwsBwfG0VC3ZiCQLpB7SjCqsJQ+u//ARe/FOxaiIiIiFQZCogl6D5b6g2XHtalWZBr4jEzOjaNo2PTOG4Y1I4Dh3KZs3anf/9xGn/65CfgJxJiozm9fTyD2idwWvt44sthMjDx/H3GyuInO4trBtd8gr09hoZTb6HhkAwYcNsxy9t/KIf0zEOk7T1IWuahIoPnNen7SMvMIisn76jtw8OM+Lpe8Nw0LobuSfXp06YBp7RoEDqjBk46B5ZMgXMfhdiKufVAREREpLpRQCxB9+mSrXRuFkfLRqH5OKRaUeEFj2wC2Lr7IN+s8ibnmvnTdt6bvwmALs3j/Nmr4+nVqgHRESESKFUxizbuYsby7fxhYBL8WEymmHowdgq8dwNMexD2bIEhfyx2luXaURG0bBRR4mfMOUdmVo7X45yZRfreQ6RlHiRtb5YfUGexIWM/X6zYjnMQGW50TaxH39YN6dO6Ib1bN6B+7agTbIHj1O9GSP0c5vwTzn4oOHUQERERqWIUEB+vlwZBi34w4olg16RK277nID+uy+DOczsEuyql1rReDJf2bsGlvVuQl+dYsnk336xK56uVabzyzWpe/OpnakeFc2rbRl4PcocE2sbX0fDqUvr7jFXUrx3JRT0Tiw+IASKiYdRr8FlTmP087N0GF/7DSz9OZkZcTCRxMZG0TSj+mce7D2Qzf10Gc9buZO6anbz27Vpe+no14N133qd1Q/q28YLk5vUraWK2Bm2g40iYNwEG3QWRmhBOREREpCQKiMsgNzuPKU/8yIBLTiJpy0JvAhsFxCckf7j08K5Vc4hnWJjRPak+3ZPqc8vgk9iblcP3P+/gm1Xe/cdf/LQdgMT6tQqC44Ht4qlXOzLINQ9NKRt28cVP27l76MnUjXYlbxAWBsP+DLHNYMbvYV8aXP4GxMRVaD3r1YpkcMfGDO7YGICD2bks2ribuWt3MmfNTj5K2cykH9YD3u++T+sG9GnTkL6tG3JS47oVd3Hk1Jvhp49h0X+g17iK2YeIiIhINaKAuAwO7s8mbX0maeszSQp2ZaqJqUu30jahDic1Lr43riqpGx3BuZ2bcG7nJoA3YdjXq9L4ZmU6/1u8hbfnbiDMoHtSfR4Y2Yk+rRsGucah5e8zvEdfXT2gNWSuKd1GZnDab6BuE/joVnh9BIyZArFNKrSugWIiw+nbxusVvmUw5OY5lm/Zw9y1O5m3NoNvf97BBymbAWhQO5JerRrSt00D+rRuSNfEekSW12zXrQZA0+7eI5h6Xu21jYiIiIgUSwFxGeRm55GbtZyDmY2DXZVqIWPfIWav3smNZ7SttsOJWzSszZh+rRjTrxU5uXks3LiLr1amM3neBu58J4UZd56he41989dn8OWKNH437GRvQq3MMhaQfAXUSYB3roJXz4Gx70P8SRVS15KEh3n3FndNrMc1A9vgnGPdjv0FQ6znrctgxvJtANSKDOeUlvXp3drrQT6lZX3qFDehWEnMoP8t8P6v4efPvYm2RERERKRYCojLYO+u3WTv/5StqaE5+VNVM335NnLzXMjMLl3RIsLD6NWqIb1aNaRP6wb88tU5/Ou7tdwwqF2wqxYS/j5jFQ3rRHF1/9bHX0j7c2Dcf2HSZTBhCFz5DiT1Lrc6Hi8zo3V8HVrH1+Gy3i0A2J55kHlrM5izZidz1+7kuS9Wkef8YLp5nD9Jl/dZaXSMGcwPbdnBrkWxxGdlEQbQ5WKY/nvvEUwKiEVERESOSQFxGWQfPOT9PHQwyDWpHqYu2Upi/Vp0TazY+z1D0entExh8cgLPfp7KJT2Tjhnw1AQ/rtvJ1yvTuHd4x+PvHc2X2AuumwYTL4J//QIu/Rd0GFI+FS1HjWNjGNGtGSO6eReEMg9mM3/9Luau2cmctTv59+x1vDLLGzbeLqEOfds0pHcrb1h2UoNamBm5u3ez/vGJZG+NJeKjz2h4UxeIiIK+18MXf4TtP0Hj4D3fW0RERCTUKSAug9zsHABysg6BRrmekMyD2cxalc4v+7eqtsOlS3L/iE4Me/obnv58FeMv6Brs6gTVU9NX0ahOFFf1b1U+BTZqB9dNh0mj4K3RcP6zcMqY8im7gsTGRB7xeK+snFyWbNrNnDUZzF27k/8t2sJbczYA0DQuhr6t6nHle09SL20XUXHZ7HjzPepfdythUVHQ61r4+q/wwwvwi6eDeVgiIiIiIa2cZnKpGbKz/IA4OyvINan6vvhpO4dy86rs7NLloX2TWK7o24JJP6wndXtZb5itPuau3cms1HRuPKMdtaPK8RpdbBO45hNoMwg+vNkLEF0pZq4OEdER4fRq1ZCbzmzHhHF9SHl4CJ/+v9P5wwVd6NOmIW3fnUC9pQuY2aMHTXvuJidtB+899gLfrEojMzwOul8OC9+GfTuCfSgiIiIiIUsBcRnkHPIC4tzsQ0GuSdX32dKtJMRG07Nlg2BXJah+c04HakeG8/gnPwW7KkHz1PSVxNeNZuyp5dQ7HCg61ruPuNtl8MUf4JO7IS+3/PdTCcLCjE7N4vhl/9b8IWo1Q5fPJPKyK+h9+SBqNznE5saJxP/3P4z75/d0f3Qav1rZF3IOsvijp0jdvpe8vKpzMUBERESksiggLoOcbO8f6dyc7MOJVajHKVQcOJTLzJ/SGNqlCWFhNXO4dL74utHcctZJfP7TdmatSg92dSrdD6t38N3PO7jxjLbUiqqg+xAiouCil2DAbTD3nzB5HGRX3XkA9s+fz5ZHHqXOgAG0e/h++rZphBn0u/t6muzP4K02Gfzm7A4catiBb+lB458mMvzJzznlD9MZ99ocnvl8FbNWpZN5MLvknYmIiIhUc7qHuAzy7yHOzQnoIc7JgsiYINUoBG1OARw0P6XYLF+vSuNAdi7Du9aM2aVLMm5Aa96YvY4//m8Z/7v9dMJr0EWCp2asJCG2gnqHA4WFwZA/Qt2mMO0BeGMHjH4TatWv2P2Ws+zNm9l42+1ENm9G4lNPYhGH/4TXObUXMZ07E/nBm9z+vzFYRHvyVj5M2JuX8Ea/jbyfdzrz12fw1co0nPOe0HRyk1hOadmAni3r07NVA9rG16mx9/SLiIhIzaSAuAyy/SHTeYEBcfb+mhsQ52RB+krYtvTw6+fPvXW/WwO1Gxa52dQlW6lfO5K+bYpeX9PERIZzz7CO3PbWAt79cQOX92kZ7CpViu9/3sHs1Tt5+LzOxERW0ix1A26F2Kbw/o3w2nAYOwXimlfOvk9Q3v79bLjlVlxWFi3+/S/C69U7Yr2Z0eimG9l02+3s+XQq9X5xHmHtz4aEjvTb9jb9fn0zmLHnYDYp63cxf30G89fv4uNFm3lrznoA6teO5JQW9enZsgG9WjWgR4sTeCayiIiISBVQof/pmNkw4Gm8OZlfcc79udD6p4DB/tvaQGPnXH0zGww8FZC1IzDaOfeBmb0OnAHs9teNc86lVOBhFMj1h0znBQ6Zzj5QGbsOLudg90Yv4N2eH/wugx2rIM+7SEB4FCScDE27wdbFMPMxGPm3o4o6lJPHjOXbGNalKZHhGrGf77zuzZjw7Rr+Om0l53VvXu2DEOccT81YSePYaK7sV8kXALqNgtqN4D9j4ZVz4ZfveZ/dEOacY/N995P100+0ePEFotsV/ezq2LPPJrp9e9JfepG4kSOwsDA49Sb47/+Ddd9B64HExUQyqEMCg/zZrPPyHKlpe5m/LqMgSJ65Ig2AMIOTm8Z5PcgtG9CzVQNaN6qtXmQRERGpNirsv24zCweeB84FNgJzzewj59yy/DzOuTsC8t8GnOKnzwSS/fSGQCowLaD4u51z71ZU3YuTP2Q6Ly8wIN5f2dWoWAf3wPZlh3t8ty/zgt+s3Yfz1GsJTbpAxxHez8ZdvMfchEd66z+9B+a8DD2vgmY9jij+u5/TyTyYw7AaPLt0UcyMh87rzMX/+I6XvvqZO4eEdoB2or77eQdz1uzkkV9UYu9woHaDvRmo3xgFrw6BK/8DLU+t/HqUUvoLL5D52Wc0vvtu6p5xRrH5LCyMRjf+ms2/vYvMadOJGzbUm216xqMw+x/QeuBR24SFGR2axNKhSSyj+3oXJ3bvz2bBBi84XrA+g49SNjPpB68XuWGdKK8XuVUDTmlZnx5J6kUWERGRqqsi/4vpC6Q651YDmNnbwAXAsmLyXwH8voj0UcCnzrmgR545+QFxbqEh01VRbg7sSD2yx3fbUti9/nCe6Dgv4O02yvvZpAs07gQx9YovF+DM+2Dxu96Mvtd+5t2s6Pts6VbqRkcw8KT4Cjqwqqtnywb8okdzXv5mNaP7tqR5/VrBrlKFcM7x1PSVNI2LKQjAgqJZD7h+Oky8GP59AYyaAB1HBq8+xdgzfTrpzzxLvQvOp+G115SYP27YMNKfe570F18kdugQLLIW9L4Wvvkb7FwNDduWWEa92pGceXJjzjy5MQC5eY7U7Xu9HmS/J/nzn7YDXi9yx6Zx9Gzl9yK3bEAr9SKLiIhIFVGRAXEisCHg/UagX1EZzawV0Ab4oojVo4EnC6U9ZmYPA58D9zrnjnowsJndANwA0LJl+fzTfbiHOOdwYlUdMv36SNgw21u2cIjvAC36QO9xXo9vky5QL+mIYLbUatWHcx6Bj26FRf+BHqMB75/qaUu3Mbhj4+D0ClYBvxt6Mp8t3cpfP1vBk5cnB7s6FWJWajrz1mXwhwu6BP9z0KA1XDcNJl3qDaEe+ST0LjnorCwHV6xg8z33EtO9O03Hjy86yMw98s+fhYfT6Nc3sOXe+9g780tizxoMfX8F3z4NP7wEw/+vzPUIDzNObhrLyU1jucK/iLFr/yEWFNyLnMH78zfxxmzvglqjOlGc27kJj13UrUZNEiciIiJVT6iMcxsNvOucO+IBoWbWDOgGfBaQfB+wFYgCXgbuAcYXLtA597K/nt69e5fLs5Fys/O8sgOHTB/aVx5FV75d66Dtmd7Mu/EdICK6fMtPHgM/vgbTHoKTR0BMHHPX7mTHvkMM13DpYrVoWJtrB7bhxa9+ZtzA1nRPqlqzIJckv3e4eb0YLuvTItjV8dSJh3EfwztXw8e/gb3b4Ix7ju9iUDnK2bmTjTfdTHjduiQ99yxh0f53dM8WWPet91r7LaSv8NKjahdsW2/kSK+X+IUXqDv4TCy2KXS9BBa8AYPvL3mURynUrx3F4I6NGdzxcC/yym2ZzF+fwdcr03h77gbO6dSEczo3OeF9iYiIiFSUipzVaBMQ+B9vkp9WlNHAW0WkXwa875wriECdc1ucJwt4DW9odqXIzfHi9SMC4nLuIT6QuYd9uzLKtcxi1W/pTYJV3sEweI+5GfEE7EuDr7weqalLthIdEcYZ/mQ+NZHLySFz5kw23f079n79dZF5bh7cjkZ1ovjj/5bjqtlzrr9elc789bu4efBJREeE0CiBqDpwxVvehZwvH/cC49yckrerIO7QITbd/v/I2bGDpMcfIHLzDPjwFng6GZ7sCFOug0WTve/wOY/Ar78+YrZsi4yk0Q2/4uDixeyb9a2XeOpNcGgvzP93hdQ5PMzo1CyOMf1a8dyVPWkcG80bP6yrkH2JiIiIlJeK7CGeC7Q3szZ4gfBo4MrCmcysI9AA+L6IMq7A6xEOzN/MObfFvLGDFwJLyrvixcm/hxiXU/Acz/IMiLOzDjLpgTsBuPaplwgLD6GA4Xgk9vIm1vrhRfKSxzJ1yVbO6JBQIyfgyVqzht3vvceuDz4gNy0dgH3ffUe7Tz8hPC7uiLxxMZHccW4HHvxgCZ8t3VZtJiDL7x1OrF+Ly3qHSO9woPBIuOB577FM3/wN9m6HS149oue1wjmHS09l6+8fZv+8FJqfZdSafqm3rlYDaDnAG/7cagA07Q5hxf+NqH/hhaT/4wXSX3iBU7OFjQAAIABJREFUOqcNxJonQ6uB8MPL0O8mCK+472FkeBij+7Tg2ZmpbNi5nxYNK7ENRURERMqgwnqInXM5wK14w52XA+8455aa2XgzOz8g62jgbVeoK8zMWuP1MH9VqOhJZrYYWAzEA3+smCM4Wm5Ofo9RDvvy/H/wsstvyPT3U95m97at7N62lZWzZ5VbuUF19sMQVYd9H/yWrXsOVJvgrjTy9u1j15T3WDtmLKuHj2DHhNeo1bUbSc89S+s7ziR3507Snnm2yG1H92lB+8Z1+fOnyzmUk1fJNa8YX65MI2XDLm4ZfBJRESH6yC0z7zM7/AlY8SlMvBD276y4/eXleRPazfknTB4HfzuZjN8MYtcXKTTqnku9M/vAiL/CTd/B3avhijeh/y3Q/JRjBsMAFhVFo+uv58D8+eyfM9dLPPVmb+K8nz6uuGMCOLiHq5pvwoA356wvMbuIiIhIsFRoV51z7hPgk0JpDxd6/0gx267Fm5ircPpZ5VfDssl/DjE4MvPqsisrgmn//JRLWgynXuMTu08ubd0a5v33PbqccQ6bVy5n7n/f4+QBg6r+TK114uGsh4j95C7OC+/D2Z2GVuruXV4eUx7/PRYWxjnX3XzCv6cS9+ccBxYsYNeUKez5dCpu/36i2rSh8V2/Je7884ls7N1vyQ9X0aB9DBlvvkn9Sy4mplOnI8qJCA/j/pGduOa1uUycvY7rTmtTofWuaIG9w6N6JQW7OiXrdwPUbQzv/QomDIOxU6B+OfRq5+XCtiXevb/rvvWeDXzAD7jjEtmX14NtC5ZSd2AfEl6eACc4SqT+qEtIf+lFr5e4X184ebg3kdjsf0CXC0/8eIqz8C3iP/0dt7f8ExPnRvGbc9qH1hB5EREREV+IdtOEpvx7iAF258WSfrAOGRn7+HzCCyd0r2deXi7TXn6WmLqxnHHVdfQ+7yK2r/mZDUsXlUe1g871GseqsNY8GjOJeuHZJW9QjpZ/+xXrFi1g/eKF/OuuW0iZ9gkur/x7XLO3byf9n/9k9fARrLtyDJmfTiVuxHBavfkmbT/5H42uv/5wMOxL6LqT8GjY+uj4Iut0ZocETm8fzzOfr2LX/kNHra9KvvhpO4s27ub/s3fX0VFcbwPHv7MSdzeSEMGhuLu7FWuhFC2uNawObakCxYtbgeKluLsG1xiShLh7dnfePyZESAIJZIH+3vmcwyGM3TubkDN3nnufZ1zLtzg6/KzK3eGD7ZAUDsvbSGXJSkqbBSGX4NQcWN8HZpeFJU1h/1RpYFy+A3RbCOOvkvnuXkK3PsbAywuXuQsQSmHJhMLICNshQ0k9d45UvytSVLneKHh8HkIuv/L1i6SRMl+PSpxLZko8+26G668tmUwmk8lkslfwH3kyfTvkHRCnaY3QiNLHF3zl0nOnOKecWkeaX9FTFK8d2EN4wH1afDgcYzNzKjVtiYmlFRd3bS29zr9BdyPTmJY2EFttFJx6toKW/mgyMzm9aS0OZb0ZMmcxzuUqcHj5Qv7+bjrxEa/+gC5mZZF06BCPR40moEVLon79DaWtLc6zZuF78gQuM2diUrNGkVF+pUs5HKrGkHb1KgnbdxTYLwgC0ztVJCk9i7mH/V+5v2+KKIrMOeRPGRtj3v0vRIfz8mwMQ/ZKX6/oAA9esJRBkyFFfU/8DGu6w48esKwVHPpKqgFcpQf0/BMm3YIJ16D7QqjRH62BPY/HjEEEyixciNLMrNRuwbpvH5TW1kQvXiRtqNFfqjF+bkGptVEUw7QIfjDdyPrz8rRpmUwmk8lkbyd5QFwCOm3ugDhTNMgZENt7lOXoqqWkpyQXPCk1lj2bUtm/qfDM0Umx0ZzauAaPajWo0KgZACoDA2q078KDa35EPQwu/Rt5zfbeDOcyFUiv+K5UCzUm8LW0e3X/bhKjImnafzCWDk70mv4dbT4aR0RwAKs/HYPf3n9eKlqcERBAxOyf8G/egpCx40i/eRPbIUPw2rsHz/XrsHq3JwpT0xdfyLcNlj16YmyXSeTsH9AmJBQ4pIKTBX3ruLP27EOCogr5+foPOHQnkhuhCYxr6Yta+R/8leNYGYYeBHNHWNsDbuV5eZGZCkHH4ej3sLIT/FAGVnaAIzOlpFw1+kPvVfCJP4y7BF3mQrU+Uo3vbKJWS9gnn5IZ/AC3uXMwKKW66U8pTEywGTSIlBMnSbtxEwzNpWR3t3ZAQlGJ/0tJneF01h7C8OEx7kck6bctmUwmk8lkspfwH3w6fTOu3L9FbFjugCRLZ4BGp0AQoN3ICaQmJHByw6oC52kvrSc6y4PQBBdSEjIK7D+yYgk6rY7Ww8bkiyS+07YjakMjLv2zTS/38zrtvxlOHU8bjDrMAqUB7J+m9zbTk5M5v30zntVr4VG1OiBFXKu1aseHvyzErWIVjq5awuZvpxEXHvbC62mTkojbtJngvn0J6tyF2LVrMalZE7fFi/A5egSHjydjWLbk63yFjj/h1NIcbVIyUb/8VOgxk9uUw1Cl4Me9d0t8/TdNig7fx8PWhJ41CqQE+O+wKgND9kuD478/hE0fwPK28KM7rOkqRYQzk6HOMOi3AT4LhtFnpNJjlXtI65GLEDV3HsnHjuE4bSqm9evrpfvW/d9HYWFB9JLF0oa6HwEiXFiql/ZyNJ+KxsaXH9XL+Pv0bf22JZPJZDKZTPYS5AFxMZ0/dRvIjSZqdCo0ogKVUsDRy4eaHbty/dA+Qu/meejT6Yg7+y861IBA8LXofNf0v3iWgItnadDrPawc82dfNjYzp2rLttw9c4LE6Cg93pl+BUUlcy8iScoubeEMzT6H+/vg/n69tnt+x2bSU1No+v6gAvss7OzpOeVr2o2cQNTDYNZ8Og6/PTsLRItFUSTlwgXCPp+Cf5OmhH/1FWJqKg5TPsf3+DHc/piHefPmCKpXyE1naIbRiBVY+6YSt2UbaTcLVhGzNzdkdAsfDtyO4GxgzMu39QYcuB3BrbBExrX0RfVfjA7nZWIjZXwGuLMLRJ2U8bn/Fvj8AYw4Du2/hwqdpGOLIWH3v8QsXYpVnz5Yv1+gKl2pUZqZYfPBByQfOkz6vXtg7QEVu8DlVZBZepnyC1AZouqxEGchBp9rP5OS8eZqO8tkMplMJpMV5j/+hPr6GNRJ5rZ97vrBLK0KjU7B0/xADfv0x9zOnoN/zkeryU4cFXiEmFgDANTKTAL9InPOz0hN5ciKxdi7e1KrU+HZXmt16o4oivjt2amfm9KDxH37if7zTzRx0hTxvdnJdNpVzh7w1xsJtr6w93PIStdPH6IjubLvHyo3bYm9R+FRW0EQqNKiDR/+ugD3KtU4uvpPNn49hdiwULLCw4levJjAdu15NPBDkg4fxrJbNzw3b6Lsrl3YDhqEyta29DrsWhP7saNQGmoJnzKh0GncQxuXxcXSiJn/3kane/kEbq+TTietHS5rZ0r36i5vujulw7UWjLkA08Jg2CFo8w34tgEjyxJfKu3GDZ5Mn45x7Vo4zZiu94zyNgM/QGFqSvTi7Chx/TGQHg9XN+i1XcrUJbLyEPoKBzl/eLt+25LJZDKZTCYrIXlAXExqpYoI89z1vBpRiUZUoFRIgxMDI2NaDxtNTMgjLu7MToZ1cRnRVEIpZFHF+Rah9+NJS5ayBZ/etJbkuFjajBiHsogIo4W9A+UbNOH64f2Fr08OOgaXVpbqfb4KbWIiT6ZNI+rX3who3oInX3zB1ROXeaeMFS5WxtJBKgPoMBviguHsfL304/SmdYD0kgKQPqeTv8HZhXBxOVxZBze2wJ1/MI+5QvfezWnXqzPRQf6snjSCA316EDlnLmoHO1y+/w7f40dx/uZrjKtV09ugRdn6MxxbO5MeEEb86kUF9huplXzeoQK3whLZdkXP6z5LyYHb4dx5ksi4lj7//ejwU4IA9uXBoBhrxJ8jKzKSkDFjUdna4jZvHoKBQSl1sGhKS0us+/cnad9+MoKCoExdaYB/bpFUD1mPHLt9R6jCmUqXpiNmyGuJZTKZTCaTvT30Wof4f4laoUYgdzCk0akQRCVqRW60zqtGHco1aMK57ZsoV9kLm/v7iDFajk1mFL72AVwJqUHwtWisHZO4sn831dt2wtmn/HPbrd2lJ3dPH+f6oX3U7dYr/06/NXBrO3g1B5s3X6c27q+N6FJTcf39N1LOnSd+xw4mZ2whrsI7JFXKxKxZMwSFAnxaSdM1T/4K7/TLl2DoVUU+COL2yaPU6dITC7vsdZt7PoXo+4Uenx6vIj7IBIMHxjTWGXDbw467zrbEexjQ3u0wltd3w3VAoQKVEagMc/9WGub/d87febfn2ac0yP237pnyUwoFFl9sIO5iC6LmLsC8S29UdvnXnXap5sKKU8H8sv8eHas6YWLw9v731elEfj/oj5edKV3f+R+JDpcSXUYGIePGoU1OxvOvDahsije9ujTYDPqQ2LVriVmyBJfZs6H+aNg6FPwPQPn2emtXMDDlRu3vaXt+CNE7p2PfZ57e2pLJZDKZTCYribf3ifoto1KoUOQJomhEJYJOgUqRP7LSctBHPLzmx6HFv9PbSiA6xQ4P48vYmUZjbmtEwKVw4kNWYGZtQ+N+A1/YrmNZb9yrVsdvz05qduyGSq3Of4CokzI3d5lTGrf50nQZGcSuXYtp48ZYdOiARYcOHGjQg5tL1zAk6hIho0aj9nDH5oOBWHbvjrLd9+BfBw7MkLLwlpKTG1ZhZGJK3W6983ROC5W6Sxl+NRloY6NI2H+IhN0HSb8fhKBWYVa/Ki4ta/NORTfu3QrgyIFLrH1Yl4b1vKhd1QmFLkMqqaPNAE269PWzf6cnFNyuzZT+1hWydtIk/7RrwdIVp6mfEDzpN6I+H4zz8n/z7VcoBGZ0rkTvxWf580QwE1r7ltrnVtr23gznXkQSc/pW/9+JDpcCURQJ//JL0q9dx3XeXIzKP/+FWGlT2dhg3bcvsWvXYjdmDAaVusHBL+HcQr0OiAEatezC+vPt+eD2anjQFzwb6bU9WekSBKE9MBdQAstEUfzxmf3uwGrAKvuYKaIo7nntHZXJZDKZrITkAXExqRQqhDxLN7U6JYgKlM8MiE2trGnSbwCHVizhin0H0pK12LlFIAjgXdOBy7u3kpX6gK4fT8PQxKRYbdfp0pOt33/JnVNHqdqibcEDrq6H5lPA3KngvtckYfsOtNHR2A4blrNt94MUkpp2o8KYH0k6eJDY1WuImDmTqDlzsOrVC+sKwzG4+QfUGgxezV65Dw+vX+XBNT+afTAUo2fquIooSb1ym/it20g6eBAxMxPDChVwnD4di86dUFlb5xxbsTq4d4rj0LIFnDxzDv9IBe1HTcTW7RXK4Wg12YPppwPlLLAsU+Awo/YfYbN1G7EnA7Hatxrj9h/m21/H04aOVZ1YfDyQfnXL4Ghh9PJ90hOdTmTu4ft425vS5VWiw6J+p/G+CbErVpKwcxd248Zi0baQ/8uvgc2QwcRt2EDMn3/i/N13UHc4HPoawm+CUxW9tWtupMa/6mQe3biM644xKEefAYPi/Q6UvVmCICiBBUAbIAS4KAjCLlEU86YOnwFsFkVxkSAIlYA9gOdr76xMJpPJZCUkh26KSaVQYZynapJGFKSkWkLByF81pzRcjBM4fS8TUZeKrXEEAI4eIlmpZ3H0ro5v3YbFbtujWg3sPcpy6Z/tBRMuGVpI0cezC17qvkqDqNUSs3IFRlWrYlKvLgCRSelcehhH+ypOCGo1Fh074rlpI56bNmLWrBmx69YR+OV2Qs67krpsMqIm89X6oNNxYv1KLOwdqN6uM9rkZNKuXSN+23YiTmcR+Jsfj4YMJfnUKax696bstq147diOzQcD8g2GnzK1sqbrx9PpNP5T4iPCWfv5eM5v35yvFnWJKFXSulMTG7BwkbL8Kgr/72c3ezVKEwXhM39ATCqYYfzz9hXQ6kR+2X/v5fqiZ//eeML9iGQmtC6HUvGSa679D8GGvtLXVqVbl/dNST5+nMhffsG8fXvsRo9+Y/1QOzhg1asX8Tt2khUWBrUGgdpEWkusZ30alOezrI9QxgfDke/03p6s1NQFAkRRDBJFMRPYCHR75hgRsMj+2hJ4cT07mUwmk8neAvKAuJjUCjUKMffhXqtTSGWXKDhAEi4tp02FdLKysshKO46dcQSiKHJ1/1oEQYG5fbsStS0IAnW69CQ29DFBVy7m32nmAJV7wqUVkBb3Uvf2qpIOHiLr4SNshw3LSTp14FYEoggdqjjnO9b4nXdw/fUXfA4dxHbYMFKjjXm4LZXgjq2J37EDXWbJBsba+HhS/fy4/NtPRD4IpEJiBsFt2nK/dh0e9O3Hk2nTiLuhwcDWCNfffsX3xHGcvpiBUaVKL7y2IAhUaNSMwb8uxLtWPU5tXMOGGZ8Q/ehBifpYUkobRxwnjSE9WiD+m/4g5s8q7WFryqBGnmzxC+FWWIJe+1JSWp3I3MP++DqY0amq84tPeFb8Y9g0ANa/CwolfLBDSv70H5cRFETox59gWKECLt/P0ntG6RexHS7N5IhZtgyMraH6+3BjMyRHvuDMV1PF1ZJ014bsUHdAPLcIHp3Ta3uyUuMKPM7z75DsbXl9DQwQBCEEKTo8rrALCYLwkSAIlwRBuBQV9d8tKSiTyWSy/x3ygLiYVAoVyjxjX60oZA+In0mOFH4DHp/DrumH2Hs2Q5d5h4jETO6Ganl43Y8y1TrxJFBLZnrJ6nGWa9AEc1t7Lu7aVnBn40mQmQwXlr3Enb0aURSJWbYMAw8PzFu3ytm+/1Y4Ze1MKedoVuh5aicnHCZPwuf4SZw6OiMmhPNkylQCWrYiav4CNNG5NZtFUUQTE0PK+QvEbthA+Lff8fDDQdxv3IT79RsQ1L8/F04dxSI9E9eUdEzq1sF+0iTcFszH+49JlO8VhvsXH2LRsSMKQ8MS36OJpRVdJk+l88QpJEZHsnbKRM5t3YhWo7+aqhYDRmNSwYXIAw/RHFtYYP+YFj5YGauZ9e8dRPHtKcO0+3oYAZHJTGjtW7LosCZTygS+oK4UHW71JYw6A94t9NfZ10SbkEDIqNEIBgaUWTAfRTGXSuiT2tkZq+7did+ylayISKg3SlrvfnG53tseUN+DaUm9yTB1gZ1jICtN723KXov3gFWiKLoBHYG1giAUeMYQRXGpKIq1RVGsbW9v/9o7KZPJZDLZs+QBcTGpFWqUuqcP+AJaXfaUabLylyy5uFzKJFz9fZSG9VAb23LwriHHbmbh5FOOBu92R6vR8fBmTInaV6pU1OrUndC7twi7fzf/Tqcq4NsOzi+CzNRXu9ESSj1/nvSbN7EZMgRBqQQgPjWTs4Ex0nTpF0TCFCYmWE9bilfHWMoMqYZx5cpEz59PQIuWPB4zlgf9B+BfvwH+jRrz6MMPifj2OxJ27UJMT8esaVMcPv2UxFHDSDNU0+bbH/HauhXXn37CbsRHmNevjsGV2QhuNaDO8Fe+1/INGjPo14X41mvI6c3r2DD9Y6IeBr/4xJcgCAJOsxeh0yiI/OVXiMqfJdvSWM3E1uU4ExjD4Tv6jeoV19PocHlHczpWKUF0OOg4LG4Eh78B75Yw9gI0+VjKxv0fJ2o0hE6aTGZYGG7z/0Dt8vZk3Lb9aDiiVkvsihVg5yP9Drm4TG/1wZ/qXM0ZtbE5S60mQUwAHP1er+3JSkUokDfpgVv2tryGApsBRFE8CxgBdq+ldzKZTCaTvQJ5QFxMKoWK3ApLKrRa0IoKVIJOSpIEUpbh65uhSi+0aisSIjMpV78fCWkK0rKgzfCxuPjaYGxhQKBfyQcxVVu1xdDUlEv/FBIlbjIZUmOkUkyvUcyfy1Da2WHZPXc52aE7kWh0Iu0rFzPJl50PQqOxmKXuo8z0wXjt3YNVnz5k3LkDApi3a4fjtKmUWb4Mn+PHKHfxAp6bNuLy/SxM+/XhyrVLeFSrgec7NfNfd9/n0vek2wJpDW8pMLGwpPOEz+g6eRrJcTGsmzqRM39vQKvJevHJJWRYvhw27/chIdCQtPkfSAm58ni/njte9qZ8v+cOWdo3n4Bq17VQgqJSmNDaF0VxosOJT2DLEFjTVYpOvv839Fufs2ZYExND6OSPiVm+It+Mgf+SyJ9/JuXMGZy//gqTmjVffMJrZFCmDJadOxO3aROamBhoMBpSo+HmFr22a6RW0ruWG/OCXUmrOkCqRx5ySa9tyl7ZRcBXEISygiAYAP2AXc8c8whoBSAIQkWkAbE8J1omk8lkbz15QFxMUoRY+lpAjajNjhArdJCVHZW9tgmyUqDOUGLDU9DpRLxr16SpTwatq6lx8PRCoRDwqm7Pw5sxZGWWLEGTgZEx77TpiP/Fs8Q9eeblvHt9cG8IZ/6Qpp++Bum3b5Ny+jQ2Awfmm4q87+YTXCyNqOZmWfyLNfkEzF1gzycYerjj9MUMfI4cxnPdOpy//QabgQMxa9QItaNjvqjzhZ1bSE9Jpmn/wfmvd28v3Pgbmn4CjpVf9VYL8K3XkEG/LqRc/cac3bKB9dMmExEcWOrt2E38DJWNBU8ORCMe/CbfPrVSwfSOFQmKTmHD+Uel3nZJaLQ65h0OoIKT+YtfhGiz4Mx8mF8b7uyGZlNg9Dkol5t1WZeeTsjoMSTu30/kzz/j36w5j0eNJjE7Q/h/QfzWrcSuXoP1wA+wevfdN92dQtmOGIGYkUHsqlVQthk4VoGzCwusWy9t79dzR6MTWWM2DMydYcdovUemZS9PFEUNMBbYD9xByiZ9SxCEbwVB6Jp92MfAcEEQrgF/AYPEt2k9h0wmk8lkRZAHxMUkRYilgZggKNHpRGkN8dMIsShK0w1daoJrTWJCkgGwdTWjjkcW1TxyI5TeNezRZOp4fCu2YEMBh+HWjiL7UbNDV5RKJZf/LeSYJpMhMURKjvMaxCxfgcLUFOt+fXO2JWdoOOEfTbtiTJfOx9AM2s2E8Ovgt7pYpyRGR+G3ZyeVmrTAwdMrd0daPOyeBA6VofHk4vehhIzNLeg0/lO6fTKD1IR4NkyfzOnN60o1Wqw0M8Xxi6/JiDMgbt1KCDySb3/LCg409LZlzqH7JKSWfpS6uHZeDSM4OoWJL4oOPzwDS5rBgeng3gDGnIMWU0FtnHOIqNMRNnUqadev4zrnd7z+3Y3t4EGk3bxB6Ljx+DdvQcQPP5J+737R7bxhqX5+PPn6G0wbNsDxs8/edHeKZOhVFosO7YlbvwFtQgLUHwWRtyD4uF7b9bI3o5GPLav9YtF2ngvR9+D4bL22KXs1oijuEUWxnCiK3qIozsre9qUoiruyv74timIjURTfEUWxuiiKB95sj2UymUwmKx55QFxMaoUahQ4EUUQQlWh1IhpRKUWIAR6ckh7q6kjZW6NDklGqFVg5GBe4lks5K4xM1QQUNm367ALYOgyiAwrth6mVNZWatuTmsUOkpj8zTdanNThVhVNzQPeS5YGKKTMkhMS9e7Hq2xelhUXO9qN3I8nU6Apkly6Wyj3Bswkc/hZSC3lZ8Iwzm9cD0KjvgPw7Dn4ByRHQbT6oDErejxLyqVOfD39dSIVGzTi3dSPrpkwkPNC/1K5v3r49JvXqEnXTCs2GkZCSO31YEASmd6pIfFoW84+WXpslodHq+OOIP5WcLWhbqYjocHIkbB8JKztARiL0XQ/9/wYbrwKHRs2bR9LefTh88jEWbdpg6O2Nwyef4Hv0KG6LF2FSuzaxGzYQ3K0bwe/2Inb9erTx8Xq+y+LLCgsjZNx41C7OuP72G4Lq7S73bjtiJLrUVGLXrIUqvcDUXooS69mAeh6EJaRzVFMNqg+A03Mh1E/v7cpkMplMJpPlJQ+Ii0mlUKHQCQgiCCjQitJgVCVkDzwvLgMjK6jSE5AGxLYupiiUBT9ipVJB2XfseHAjGm3Ws2s/RdBlwd5Pi5y2WKtzD7RZWVwJeCaBliBIEdEYf7i7+5Xu90ViV6wEpRKbDwfm2/7v9SfYmRlQy6Ngbd8XEgToMBvSE19YozTqYTC3ThymRvsuWNg55O4IPCqto244Hlxf35pNYzNzOoyZTI/PvyI9OYkNMz7m1MY1aLJePWorCAJOX32FTqsk8mwW7Byb72ejsoslvWu5serMAx7GpLxyeyW1/UooD2JSC48O67Rw4U/4ozbc2CL9fI45DxU7S9/vZ8Rv207M4iVY9e6NzZAh+fYJKhXmzZvjNm8uvieO4zhtGqJWS8R3M/Fv0pSQSZNIPnkK8WVrRZcCXWoqj8eMRczIoMzChSitrN5YX4rLqHw5zNu0JnbtWrTpWdJLPf/9EK3fFyytKzniYG7IuvMPod0sqYTczjEF1srLZDKZTCaT6ZM8IC4mlfA0qZYIgoAWqeSOSqGDpHBpAFpjAKiNpVJEIcnYuhVecgjAu6YDWelaHt8pIhIaeATuPJuzRGLrWgbv2vW5GphGlvaZQUWlbmDjLZWw0dPyLU1sLPHbtmHZtQtqR8ec7XeeJLL/dji9apUpWcmdvBwrQ92P4NJKCLta5GEnN6zCyMSUet375G7MSIZ/xoOtDzSf8nLtvyKvmnX48NeFVGrakvPbN7NuygSeBNx75esaenlhO3gwCUFGpJ4+DJfyl8f5uG151EoFP+69W8QV9CNLq+OPIwFUcbWgTSXH/DsfX4SlzWHPJ+BSXSqj1PorMDAt9Fop5y/w5KuvMG3YAKcvv3julHuVtTU2Az/Aa8d2ym7bilXfvqSeOcvj4cMJaNWayN/nkPngQendaDGIokjYtOlk3L2L66+/YOjt/VrbfxW2I0aiS0oibv0GqD0ElAZwbpFe21QrFfSrU4bj96N4nGYAnedA5G048Yte25XJZDKZTCbLSx4QF5NaKU2ZVoigUygQs+sPx1wzQ3tqGeg00oMkkJqQSXpKFnbPGRC7VbDGwFhF4JVCpk271JCS2+ybCpmFR/zqdOlJeqbIjfBnpmQrlNB6BZW+AAAgAElEQVRoAjy5CkFHX+5mXyBu3XrE9HRshw7Nt/3n/fcwN1QxqtkrDgSaTwFTO9jzaf6SVtke3bxG8NXL1OvRByOzPJ/x4W8h/rGUVVpdcKr662Jkakb7URPpOeVrMtJS+WvGp5xYvxLNKyaDshs1CpWTE+E3XBH3TofIOzn7HC2MGNHUm703w7n44MXTzUvLdr9QHsWmMrFVudwBbGos7BoHy1tDShT0WgEDd4J9uSKvkxEUTMj48Ri4u+M6Zw6CWl3sPhhVqoTTjOn4nDyB65w5GJYvR8yffxLYvgMP+g8gfutWtMn6j5xHL1pE0j5pqrdZs2Z6b680GVepjGmzpsSuWoVOMIWqfeDaX8VauvAq+tV1RwDWn38E5dtDtX5w6jd4cl2v7cpkMplMJpM9JQ+Iiymn7JIoolUI6JAGN0KmQNrJf8G7FdhKA8HoPAm1iqJUKfCsZkvwtWi0z5bMUaig4y+QGAonfi70fNcKlXCxVXM02JIVk0Zy8M/53Dl9nOTYGHinn5S59eRvr37jz9ClphK3fj1mrVph6JW7/vNCcCxH7kYyqrkPlibFH8wUytgKmn0OIRcg6k6+XaJOx4n1KzG3s6d6u865Ox6ehQtLpeiye/1Xa7+UlK1Rm0G/LKBKi9Zc3LWVtZ+PL1hDugQUJiY4Tp1KRmQGcYEW0lrzPJl5hzcti5OFETN330an039y1yytjnlH/KnmZkmrig7Sy4vLq+CPmnBlPTQYC2MvQpV3C50e/ZQmLo7HI0ciKJWUWbI435r0klAYGGDRvh3uS5bgc/Qo9h9PRhsTw5PpM/Bv2pSwqdNIvXgRfSS+TTx4kOh5f2DZrWuBqd7/FXYjR6KNjydu4yapBFNWarET3L0sFytjWlV05O9Lj8nQaKH9D2BiK2Wd1r65JHEymUwmk8n+/5AHxMWkErLXEANZKgBpEKvU6UiP0uUk0wKIDkkCnj8gBvCu4UBGqobQe3EFd3o0gHfel8rTRBWeTbdrQ0uaeiZi7eTM3dMn2DPvZ5aM+pDlH49lf0pzbl+7S+KNQy9zu0WK37IFbUICtsNyo8OiKDJ7310cLQwZ1NCzdBqydJP+1uaPqt49e5KIoAAa9xuIyiA7YVZWGuwaC1ZloNWXpdN+KTE0MaXtiPG8O+1bsjIz2PjlZxxbu5yszJdbJ2netg2mjRoRdcMMzYPbcDi3FJOJgYpP25XnWkgCu66FldYtFGnL5RBC4tKY2NoX4ck1WN4G/pkADpVg5ClpXaih+XOvocvMJGTsODTh4bgtmI+Bm1up9E3t6IDd8OF47d2Dx4YNWHbqSNKBAzz8YCCB7doTvWgRWU+elEpb6ffuEfb5FIyqVcPp229Lll39LWJSowYmDeoTs3IlOktvqQzT+aV6H5gOqO9BTEom+26Gg4kNdPoNIm7o5YWeTCaTyWQy2bPkAXExqRQqKaGWKKJV5m43MNCQnmwJ5drlbIsJTcHMxhAj0+dHSt0r2aAyVBJ4JarwA9p8A2qTIhNsmRopqOOWQo/Pv2LMir8Y8MMcmn0wFFs3d/yD49gbVoE/Z85h2bih7Fs4h1vHD5MQGfFS9w8gZmURs2oVxrVrYVKjRs72Q3ciufwwjgmtymFsoHzOFV6NJiuL0xvXYO/pRcVGeaakHvsRYgKgyzypfNNbyPOdmnz48wKqtW7H5d3bWfvZeMIDSl42SBAEHGdMR8zSEhlaC84tBP/clx49arhSxdWCn/bdJT1Lf8mlMjU65h8JoKGrihaBP8OfLSD+IfRYAoP+BcdKL7yGKIo8mTGDtMuXcfnxh3w/U6VFEARMatbA+bvv8D15ApfZP6J2diZq7jwCWrbi0ZChJOz+F136y9XA1cTGEjJqNEozM9z++CNfPe7/IrtRo9BGRxP/9xZoMAaSwuD2Tr222cTHDncbE9afy66lXbGzNKvgxM8QcUuvbctkMplMJpPJA+JiUivVKEUpQqzN86kZmWWRnmQmrd3NFh2SjJ3b8yNjACoDJZ5VbQm+GpUzxTU5Q0Pa04GMmQO0nAFBx+B20bWJARQKJY5ePtTu3IPun37B6OUb+KDXO7RwDMTeyZ5AvwvsW/g7y8YNZemYwewNdCAyrmRRysQ9e9CEPcF2WG40XKsT+Xn/XbzsTOlTu3Sie0W5dmAPCZERNO0/GEGR/U0I9YMz86DmQPBuodf2X5WhiQmth42h14yZaLIy2fT1FO6eLnm9V8OyZbEZOoSES6GkZpWDHaMgWXqpolAIzOhUibCEdJafCi7tW8jx96VH1Evcz6rkUQiXlkOd4TD2kjRdv5gR0uiFC0nc9Q/2E8Zj0bGj3vr6lMLEBMtu3fBYvQrvQwexGzWKzAcPCPvkE/ybNOXJ11+TduNGsadUi5mZhI6fgCYmBrcF81E7Orz4pLecad26GNeuRcyyZejcm0kJ6s4u0FuCPpB+Zt+v586FB7HcC5dm19DhZzCyzJ46rdFb2zKZTCaTyWTygLiYVEJuhFiX51NTWRuRFZWENkl6kNNkaYmPSMXWtfBMus/yruFAWlIWt65FMnnzVfwexuEfmZwb3as9RKotvG+alEW5mBQKJQ6dPqamYwLdKsYyeuk6Bv48n5aDR+DsXY7AeFO2H4sgPaV41xRFkZhlyzH09cGsadOc7duvhHI/IpmP25ZHVUiJqdKSnpLMue2b8KhWA89q2ZFETaZUgsjMCdrO1Fvbpc2janUG/DAHR28f/p33M2f+Xl/ida12I0agcnEm3M8GMTUBdo7OGbTU97KlbSVHFh4NIDLp5SKfz5MZdoPK+/vxm8Fi1PZe8NEx6PiTtPa7mBL+2U30H/Ox7NYN25EjS72PL2Lg5ob9+HF4HzqI+8oVmDVvTsL2HTzo3Yfgrl2JWbESTXR0keeLokj4zFmkXrqE88yZGFet+hp7r192o0ahiYggYecuqDcSwvzg8Xm9ttm7lhsGSgUbzj+UNpjaQqdfpeSAZ+bqtW2ZTCaTyWT/v8kD4mJSoUSpVSCIoFXkDl78aw4HIP2OlPwpNiwFUScWK0IM4FjeClEpMH/NdXZfe4KDhREarY5157IfDJUq6PirNHXxxE8l67SJDdQeDDe2ICQ8wt7dkxrtu9Bl8lTeLR9GSpqWY6uXFetSKSdOkOHvj83QoTnR2fQsLb8fvE81N0s6VnUqWd9K6OLOLaQnJ9Hk/UG5G0/9BpG3oPPvUjTpP8TEwpJeM2ZRuVkrzm75i3/n/lSidcUKY2Ocpk0jI/gRsfQA/wNSUrFsUztWJEOj4/eDJZ+WXaT0RNg3DdWfzXDXPeZ+ve8RhhwA53dKdJlUPz+eTJuGSe3aOH33ZtfcCgoFpg0a4PrzT/ieOonTN9+gMDEl8qef8G/egsejx5B06BDiM/Wk4zZsIH7zZmyHD8eyS+cirv7fZNqwIUbVqhGzdCli5T5SffVzC/Xapq2ZIR2rOrHNL5SUjOyIcOXuUhm5Yz/my6guk8lkMplMVprkAXExXdq5FbM0KeqrVeYOiFNcPAFIv30byM0w/bySSyBFmPbceEL7+afwV2ioqFFxcGJTKjiZY26oZvHxINIys6PE7vWg+gBp6mJUCWvaNhgDggLO/JFvs7NZBnUrWXLr+CECL1944WVi/lyGytkZy06dcratO/eQ0Pg0Pm9fQa+DmqSYaPz27KJi4+Y4ls0u6RRxS6pXWrWPVK7lP0ilVtNu1ESavD+Ie+dOsfmbqSTHFb/MjVmrVpg2bUL0rstkObWAA1/krLksa2fKwAaebLr4mLvhia/WUVGEG1tgfh3EcwvZKbRiksNyfNuPBkXJfoVkPnpEyJixqF1ccP1jHoqnidHeAkpzc6z79sFz00a8/t2NzYcDSbtxnZCx4/Bv1pyIH2eTfv8+KefOEfH9D5g1b479xAlvutulThAE7EaNJCs0lIT9R6DWILjzD8Q91Gu7/et7kJShyZ8QruOvYGAGO8fIU6dlMplMJpPphV4HxIIgtBcE4Z4gCAGCIEwpZP/vgiBczf5zXxCE+OztLfJsvyoIQrogCN2z95UVBOF89jU3CYLwWp6oDYykurY6hYCYJ0KcYWGLysGBjOwIcUxoMioDBRb2RdfBvRWWQN+l5xi93g9zIxVt2nqizhIxiM+E9AScrYyITs7IjRIDtP4aDEyl2rwlmV5r4QLV34Mr6yA5f83j+lWtsXP35ODSP0hLTiryEmlXr5J66RK2gz7MqQ+blJ7FgqMBNPaxo5GPXfH78xJOb16HKOpo3PcDaYNWIz0gG1lC+x/12ra+CYJA3W696PrxNKIfP2T99MlEPggq9rlOM2YgZmURebeM9HlsHSZl3QbGt/LB3EjNrH/vvHypoaj7sKYrbB0K5k7sqbeWSamDGN62dolfgmgTEng8YiTodJRZshiVtfXL9ek1MPT2xvHTT/E9ehS3RQsxqVWL2PXrCe7ajUfDhmNQ1hOXX35GUOovidybZNa8OYYVKxKzZAliraGAkG8Ggj7U9rCmvKM56849zP15NbOHjj9D6GU4t0Cv7ctkMplMJvv/SW8DYkEQlMACoANQCXhPEIR8qWdFUZwkimJ1URSrA38A27K3H82zvSWQChzIPm028Lsoij5AHDCU18DAWBrgahUCOmVu3eAsFBhVrJgTIY4JScbGxQyFouBgITo5g6nbrtP5j1P4RyQxq0cV/h3fhDaty6JQCgRuWguhlzGr3IHGPnYsORFIamZ2VMTMHlp+AcHH4db2knW+0USpfNG5hWSma7hzJoydIeO4G1qWDmMmk5aUyJEVi4s8PWb5chSWllj16pWz7c8TQcSlZvF5+wol60sJRYWFc/v4Eaq374KFfXbSonMLIOwKdPpFWmv4P8C3TgPe+1aqOb3xy88IuHiuWOcZuLtjO2wYifsOkeI1CSJvw0Gp9JSViQHjW/ly0j+aY/eLyGRelMwUOPQ1LGoIT65Bp19JH3SQb68YU8fTmkY+JfvcxcxMQsZPIDMkBLf5f2Dg6Vmy/rwhgkqFeYsWuP0xD98Tx3GcNhWz5s0os3AhSrO3M6N5aRAEAbuRI8l8+JDEMzek6ct+ayCj6BdnpdHmgPru3ApL5FpIQu6OKu9Chc5wZFaRJehkMplMJpPJXpY+I8R1gQBRFINEUcwENgLdnnP8e8BfhWzvBewVRTFVkEJSLYEt2ftWA91Lsc9Fehoh1goKNHkixFmCCqPKlcgIDEKbmpqdYTr/g7JOhEexqbT4+Rh/XwphcMOyHPukBf3reaBUCBhmRlDG9C5BTxwQ2/0IzT5jYmtfopMz80eJaw8Bp2qwf1qJHkxFGy/CXIZzeC+s/OwUR9bcJTzdi9O3q2Js4UL9nv24e/o4/ufPFDg3IyiYpEOHsX7/PRSm0pTxqKQMlp0KplM1Z6q66Xft7sndBzEwMaZejz7Shmh/6cG4Qmeo9Fq+9a+Ng6cX/Wf9hm0Zd3b+OosLO7cUK7Jr+9Fw1K6uhK/Yg1hnpBTJu78fgA/qe+Bpa8Ksf++g0epecCWk2Qd3/oEF9eDU71CtD4y9DHWG8delUCISM5jUulyJosOiKPLkm29IPX8e5+++xaROnWKf+zZRWVtjM3AgZebPx8Dd/U13R+/M27TG0NeHmCWLEeuOgoxEaaaJHnWv4YqJgTL/7z1BkGoTq42lmSE6/ZUTk8lkMplM9v+PPgfErsDjPP8Oyd5WgCAIHkBZ4Eghu/uRO1C2BeJFUXy6mKzIa5Y2A2MTAESFgDZP9FcjKDGsWBF0OmKv3CUjVVNgQPwgJoWAyGRqe1qzb2JTvuxSCUuT7BrFoZfhzxZ4q06QpHUgyqU/CAK1PW1o4mvHkuNBuVFihVJ6MEx6AsdfnGArOS6Dy/sesP6rc2y/3J7A1Dr4lomk56e1eM/jO0QEzmwLoG733jiU9ebgsgWkJibku0bMiuUIBgbYDBiQs23+EX8yNDo+blPuZT7KYnuUYknwnfvU694HYzNz0Olg1zjpwbjTr8Uu7/NfYmZtQ5+vfqB8/cac3LCK/YvmonkmodOzFEZGOE6fTmZgILFh3uBYRSpXkxSBgUrB1I4VCYhM5q+Lj597HWICYX1v2DRAmn49ZD90Xwhm9qRnaVl4LJC6ZW1o4F2y6HDMsmUkbN2G3ehRWHX/33qJ8b9MUCiwHTGSDP8Aku7GQZl6cH6xXgek5kZqulV35Z9rYcSnZubZ4QgdZkPIBakPMplMJpPJZKXkbUmq1Q/YIopivictQRCcgarA/pJeUBCEjwRBuCQIwqWoqBJOFy3E0wgxgEaZOxDbFTMFja8ULYq4ItV9tc0zIA6ITCYpXYOnnSkrB9fFxyHPYPnmNljZEZSGlB0+HYVCINAvd53vxNbliEnJZM3ZPNGSMnWgxgAp62t0wemDWo2OQL9Ids+/xppppzm3IwhTS0NafViRQfXX0lKcinMZNRbqWGp63yfgUiThQUl0GD2JzNQUDi1bkBOVzIqIJHHnLqze7YnKVhoEPYpJZcOFR/StUwYve/1NGRV1Iiciy2JubUmN9l2kjReXwaOz0P4HMNdvVus3SW1gSKcJn9Gg13vcOn6ILTNnFHhR8Szzli0wa9GCqEVLyGoyGzKTpfrEOh1tKzlSr6wNcw7eJzG9iMG1JhOWt4VH56R12R8dB/f6ObvXn39EVFLJo8OJ+/YT9etvWHTsiN24ccU+L6dbWVkvv/5Z9sosOrTHwMOD6MWLEeuNgrgHcG+vXtscUN+dDI2OLZdD8u+o1hd828Hh76SXNzKZTCaTyWSlQJ8D4lCgTJ5/u2VvK0zeKHBefYDtoig+fYqPAawEQVC96JqiKC4VRbG2KIq17e3tS9z5Z6mNjHK+zlRIiXREAeJ1D7koBqO0tCQqKA4AW9fcgeKmi48QAFerPEm2RFGK8G4ZLJWsGX4Eo7JVcC1vRaBfVM4AoJaHNU3L2bP0RFBuKRKA1t9ICbbCb+Rsig5J5uTm+6z6/DT7lt4kOiSZmu096P9tfXp8XJMKDZwxaD4eUqNzpj3W9LqPuY0RJzf5Y+PqToPe/fE/f4Z7Z04AELtmNaJWi83gwTnt/HrwHkqFwIRWvq/8mT7P9cu3iEg3p1GHVqgMDKQMt4e+Bp/W8M57em37bSAIAg1796fT+E+JCPRnw/TJxIQ8eu45jtOngVZLxPKt0G4WBB6G84sRBIEZnSoRk5LJwqNFDCQ06dLPRrPPoP4oqdxXtrRMLYuOBdLAy7ZE0eG069cJ+/xzjKtXx/mH70s0kE6Jj2P/4rnM/aAnW2ZOL3aiMVnpEpRKbEeMIOP2HZIjLMDSXe8lmCq7WFLD3YoN5x/lfxkiCNBlDigNpJkiumIsAZDJZDKZTCZ7AX0OiC8CvtlZoQ2QBr27nj1IEIQKgDVwtpBr5FtXLEpPR0eR1hUDfAjsLOV+F+ppUi2ADKX0sYnZU6fPPjmHYaWKxMXqMLc1wtBYGkxkanRs9QvFwliNoSr7o85Kh23D4egsKeIxcJeUMAvwquFAQlQaMaHJOW1NbO1L7DNRYp2xLWkNvyZW48aN2AZs/v4im2Ze4OaJUNwqWNN53DsM/L4h9bt5Y+VgknsTHg2hTH04Mw+0WaiUOhr18iEmNJlbJ8Oo06Unzj7lObxiMYkhj4nfuAmL9u0wKCO917gVlsDOq2EMblQWR4vcFwSlLSI4kKN7T+FpGkulWu9ILxD+GS89EHee8z8xVToxJo3MtBeXkanQqBl9vvqBrIwMNsz4hOCrl4s81sDNDduPhpO0dx8pmRWhfEc49BWE36CqmyU9a7qy4lQwj2NTi25QKPgrYf35h0QnZzCpBFPks0JDeTx6DCp7e9wWLkBhaFis87QaDZf/3cGKiSO4feIIFRs3J/LhA9ZOmcC+hXNIio0udh9kpcOyS2fUrq5EL12KWPcjeHgawq7qtc0B9TwIik7hbGBM/h0WLtD+e6kPF4tXQ10mk8lkMpnsefQ2IM5e5zsWabrzHWCzKIq3BEH4VhCErnkO7QdsFJ+ZFykIgidShPn4M5f+HJgsCEIA0pri5fq5g/yeriEGSFdIlZ502QPiM2FnMKpYiQSdOXYupjnHHboTQWxKJjam2ZWhkiNhdWe48beUMbrHElDnDiy9qtsjCOC37yHXjjzm/K4gEk9GMFQwI3TbA9Z9dY7ln5xk0ZijrNjgyV/Rf3DiSU9EUaRJX18G/9iYdsOr4FHZttAs1wA0mQwJj6VoIOBVwx7X8tac3xVEZpqO9mMmocnIYO8P36BNScF22LCcU3/efw9LYzUjm3mXymdamIzUFHb//iPGJkZ0cLmHoFDAlbUQdAzafAtWZV54jbdZSkIGh1bdZu30s6z94iw3joWge0GyK2ff8vT//ncsHRzZ/uM3+O39p8hpxLbDhqF2dyd85izEDr+DsQ1sGQqZqXzarjwKBczed7fY/U3N1LD4eCCNfGypW9amWOdok5J4PHIUYkaGVF7JpnjnPbjmx5pPx3JszTJcyldk4M/z6Tj2Y4bOXUrtzj24e/oYKyaM4PTmdWSmpxX7HmSvRlCrsR0+nPRr10nJLC/VBdZzlLhTNWcsjdWsO19I7ePq/cG7lfSyJzZYr/2QyWQymUz2v0+va4hFUdwjimI5URS9RVGclb3tS1EUd+U55mtRFAvUKBZF8YEoiq6iKOqe2R4kimJdURR9RFHsLYpihj7v4am8a4jTsksfP02u9STlCfFl7Ek1tsfKJLc7Gy8+xsXSCHMjFcQ/gj9bQvhN6LMGmn5SINJpYmGAWwVr/C9FcmqzP5f3PiDoahSuSjUarY4EtYh3TQdqd/CkSV9f2n7gSd8J7vSdXpdqLcpgZKZ+8Y34tpWSLmUTBIEmfXzJTNdyflcQNi5uNOz1HiHR4UTXqYFRJalS1rmgGI7di2JUc28sjYvRzksQRZH9i+eSEBVB597tMFFpIPEJ7J8BHo2h1uAXX+QtpdXouHLwEeu/Oof/xQjeaVUGWxdTTmy8z8aZF3l4K+a551vY2dPv25/wqlWXo6uWcHj5IrSaghFmhaEhTjOmkxkcTMzf/0CPRRB9Dw7MwNnSmI+aerP7+hMuP4wrVr/XnXtIdHImk1oXLzosajSETppMRnAwbvPmYuj94pcn8eFP2PHzd2z9/kt0Wi3dP/uSnlO+xtZVevlhZGpGswFDGPz7Yrxr1+Pc1o0sHz+c64f3oZMzDr8Wlj17oHJyInr5GimHwc2t0v/NPHQ6LclxsUQEBRAYFMmjlJfPQG+kVtK7lhsHbkUQmZief6cgQNd5ICjlqdMymUwmk8lemerFh8gAVHmmfKYIT0swCVjgQyIB+CmzQFBglhYBwOPYVE76RzG+pS9CMBB1F8ydYchecKlRZDvtR1QlOS4DY3M1hibqnEjv4JUX2Pc4npM9a2Nu9AoDUkGAxpNga275ZltXM6o2c+XGsRAqNXHBK0PLjeQ0rtkYUiMmGjMbW2bvu4ujhSGDGnq+fNsvcGXfbvzPn6Fp/8G4epjAGeDAdKmGctd5oHhbcsCVzKPbMZza7E9ceCoeVWxp3NsXK0cTRFEk+Fo0p7cGsPuPa7hXtqHRu77Y5JllkJeBkTHdPp7Gyb9Wc3HXVuLCw+gycQpGz9TDNWvaFLPWrYheuAjLTrtRNxgLZ+eDT2tGNG3LXxceMfPf22wb1fC563pTMjQsPh5EE187anu+OMoriiLhs2aRcuoUTt99i2mDBs89PjM9jQs7/ubSP9tQKFU0eX8QNTt2Q6Uu/Ofb0sGJzhM+o1bHbhxbs4yDS+dzZe8/NB0whLLVa72wf7KXI4oi6elpCL3f5dGqFcQEVSMr0oXk36eTbOJFSlwMyXGxpMTHIeYbnFZleEw0Fi4vl3yvf30Plp0KZtPFx4x7NmeBpRu0/Q52T4TLK6HOaylHL5PJZDKZ7H+QPCAuprwDh0SkAYtOocAUL6zMM7kZHYcLYBIpZX7+OztDau/abhDrBgjQb720Bu45DIxU2DgX/LZMaF2O7gtOs/rMA8a2fMWEVpW6SwMkm9zoXZ3OZbl/MYKTG+9T7fBq6phYckSAA0vmYdp1FFcexfNDz6oYqZWv1nYRngTc4/ja5XjVqkvtzj3APzuxeGwQtJ0Ftvqbpq0vidFpnPrbn+Br0VjYG9NpdDU8q9nl7BcEAa/q9nhUseXGsRAu/vuAjTMvULmJC3W7lMXYzKDANQWFgqb9B2PjWoaDS+ez4YtP6PHZF1g7568+5jhlKkGdOxPx42zcfvsJgk/AzjGYjjrDp23L89nW6+y+/oQu7xT987jm7ENiUzKZWMzocNyaNcT/tRHbYUOx7t27yONEUeTu6eOcWLeC5LhYKjVpQZP3B2FmU7yEXc6+5en37U/4XzjDyfWr2PbDV3hUq0GzD4Zi7+5ZrGvIpO9DZloqybGxJMfFkBIXS3JcLMmxMSQ/HeRm/8mZjeDlAidOAB4YxYZh5mKEmY0dtmU8MLO2xczaBlMbG3S3/mH33us8uHGdai6eL9W/snamNPax468Ljxjdwgfls8tAag2C2zvg4Jfg2was/vdrQ8tkMplMJit98oD4JSQjRYhFQYFSZ0lDl4Y8uZFJGTET5X0/tDqRvy89pomvPW7WJtB7tZSs6BWSQVUvY0XLCg78eTKYDxt6vlqUWKmC4Ufz9cfIVE39bl4cW38Pm1Rbak7oT1OFliMrFnMj8W+87KvRu5bby7f5HOnJyeyeMxszGxvaj54krRt+yrW2lPX4PyQrU4vf/odcOfAIQYB63byo3roMqiJeJihVCqq3dqd8fScu/BPMrZNh3L8QQe2OnlRr4YZSVTAyXqV5a6wcndj56/dsmP4xXT+eRpnK1XL2G7i5YjdyBFFz5pJ8tjdm7y6HJU1hx0je7b+NlWce8OPeu7Sp5FjoS47kDA1LTwTStJw9tTysX3jPSUeOEPHjbMzbtMF+8uQij4sIDuTIyjUIuh8AACAASURBVCWE3buNo5cPXSZPxaVcxRde/1mCIFCuXiO8a9Xl6v5/Obd1I2s/G0/l5q1p1HcAZtbFW7dcUmlJmdw6FUalRi6YWBR8YfG2yEpPzxnUJsfFkhKb5+u42Jx9moyCK04MjE0ws7bBzMYG1wqVpa+tbTC1tkV35iypq9ZQ7vvJmF+eCJ37Qu2CSxnEjAuYqTJ4eOMa1dp1LbC/uPrXc2fUej+O3I2kTSXH/DsFAbrMg4UNYNd4+GD7/0TCPZlMJpPJZK+XPCB+CRoMABUIShQ6Sxq6VOVQsj8m6iQy7t7lxN0IniSk82Vnaf0titKJqk5s7UvX+adZdfoB41r5knjwIBn372M3YgSCqoTfykIeHCs0dMZv9SkCyvWmYbNWVDdScfrQUco/OEqPTi1QKUt/yrIoiuxb9DvJsbH0+2Y2xmbm0g4bb3CoDN0WlNrnp2+iKBLoF8Xprf4kx2bgW9uBhu/6YGZdvIzcxmYGNHuvPFWbuXF6qz9ntgZw80QojXr6ULa6XYHpzW4Vq9B/1m9sn/0NW2Z9Qauho6nWql3OfpshQ0jYvoOImTMx+WcXivY/wO6JKM8vZEan9+i/7DyrzjwoNEna6jMPiEvNYlLrF89GSLt1i9CPP8GocmVcfpqd/4VGttTEBE5vXMv1I/sxNreg7YjxVGneutBjS0KpUlOrU3cqNWvF+W0bubLvX+6dOUHtLj2p06VnvnJpryoxOo1//rhGfEQqt0+F0en/2LvrgCqvN4Dj3/cWcenOC0ijgCDG7Jg62+l0djudm+u5Lteue9OZv+k25+yazi4sBAxQDFqUlO7398d1GBggoW7n8w9w73nPOS8C3ueec57niSCs73A7cH0pKy7m9OH9JERFkJeZURXwlhQWVGur0hhgYmWFiaU19k288fwn0LWyvirotbomX8L1KoNCOP3HcrLXhGMaGgThP+hXaq/72ZQkCXdtNnHHj1JZUYFCeWe/ww8G2GNnasAv4QnVA2IASzfo/g6sfwEiFkGLsXc0jiAIgiAI/10iIK6hS+vWEZCSgYFSptLfECQ1sqSECjPC7MM4UlhIuUkqcmEhf206iLXWkG7+N3gBVwdBLhY86G/HnF1nGeVtzMWXXqaysJCiqCicP/8CpcmNz57WVPGhQ3hFLyIi9HkiNifRvLc7K7Xt6K44R8m2X5HbNa1zAHO9w2tXcObQfrqMewxHb98rT9j6wLS99TpWQ8pMzWf30jiSY7Oxdtby4HMBOPvcfmX1RqyctPSb3pyE45nsWXaaDT8dxdnHgnaPeGOrM72mrYW9AyPe+5S1X37M5tnfkJWSRMdR41EolCg0Guxff52kyZPJmjcPmylT4PTf8Pc7tJvUgW5+dny39TSPtHDB5qq/BHnFZczZdZYuvraE6G59D2VpaSQ/Pg2lhYW+vJLRtcFURXk5UZvXs/ePxZQVF9Oid3/aDB6OobZ+A0kjE1M6j5lM8x592bVkAfuWLeHolo20e3Q0AZ26oqjjmyqZKfms+TqS8rJKOo3w5eDac/w563BVVvfGVFFeTsLRI8Tu3sHpg+GUlRRjbG6Bub0D1i6uuAU2R3s5wDWxtMbESh/oGhhra1UL+kYUWi1W48aS/uVXFPV6DKOot+H0FvB+sFpbd5NsjqUUcv70KZx9a78LAECtVDCslY5vtsaRmFmIztq4eqOwiXBiFWx6XV+n3Ny5ehtBEARBEISbEAFxDZWnpeGecQnJSIlcYaDPcCopkcvNkPNVGFQYcc40ngeAC4ejGTx8EJobbHWtq2ce9KHvN7s58so72JeVEjswkIA1e0kYNQrXn35EbX/nQXjmzz9jrb6ET5gtRzYnclRdzuliA57oN4KzKxcQuXk9IT371tu9pJyMYeeSBXi3bkvIQ/3qrd/GVFJUzsE154jenozGUEnHYT407eCEoh5W092aWuPqZ8nxXakcWHOOpR8exP8BR1oPaILW/EqSNwNjLQ+/9BbbF/3M4XUryT6fQp+nXtRvfe3QHtMePcj48SfM+vZD0/8b+KEd/DmJVwetp8d36Xz59ynee8itqr+Fe+PJKSy77dnhyoICkh6fRmVBAW5LlqC2s7vm+YSjkWxbMJvM5ETcgkLoMnYy1i4Ne87TwsGRfs+9QkrsCXb8by5//fgVERtW02nURNyCmt9Rn+dP57Du+2hUagUPPx+KtbMJbs2sWfd9NOu+jaL9UB+CujTMcYJ/yLJM6qlYYnZv59S+XRTl5WKoNcG/fWf82nfCxa/+36y6GcuRI8mcN5/Mv0/j4uEA4d/dMCB20+YgSRLxUYfvOCAGGN7KlW+3xrHkQCIv9/Kr3kCh0Cfd+6EdrHkaRv4htk4LgiAIglBj92fa3rtAcXkrr1xWCZUGVJq0JNfSkcoyUzKT8wHYbxpNpUqFR3Yyj7ZsmHq5zZzNmWCUgePhXfzZuoK3/WP4apiWooSzxA99lOLYmteYvVrxyZMU7NqF1ehRtH3EB4VC4tSmJDp42zBw2GDcm7dg5+L5ZKel1st9FOZeYu1XH2Nma0fPqU/XeeWqscmVMjF7U1n85j6itiXh386RkTPbENjZpV6C4X8olAoCO7sw6t02NO/mysn9afzyZjiH1sdTXlpxVTslXcdPodvEaZyLPMyvb84gN/0iAPavvAySxIWPPgRjK3j4R8g8jWfE+4xqrWPJ/kTOpOt/hovLK5iz6xzd/OwIdrW4+f1XVJDywouUnDyJ8xefY+h7JXi+dPECqz/7gGXvvU55aQkDXnidwa/ObPBg+GrOfgEMf+9T+jw9g5LCQpa9/zrLP3yLjKQb1LW9hfjoDFZ9FYmRqYZBM1pg7axf2Ta1MmTQC6G4Bdqw6/dT7Pz15G3rSd+JjKQEdv+2iJ+nT+K3N1/k+LbNuDYLZsCLbzDlp//R/bEncQ0IbLRgGEBpaorVqFHkbdlKsdMgOLMVLsZUa2eoLMfRy5v4qIg6jedobkQ3f3uWHkqipPwmZbasmkC3t+D0ZohcUqfxBEEQBEH4bxEBcQ0p/tmOXC4jVxpSYaxD1thSUaEhM0UfTGSYnifR0oLQkgt42jbM2UK5tJT+OxeQZiGxr6M/v/X9jfwW3rw0vILs4mzOjRhB/q5dte438+e5KIyNsRw+HK2FAUXeJrgXK5js7YgkSfR4bDpKpYq/fviyzrVf5cpKNnz3OUWXcuj3zMsYGNdtq3dju3Aul2WzDrN1USzmtsYMfaUlXUb63TArdH0xMFbT7hFvhr/VGp2/FftXn2Xx2+GcOpiGLMtV7Zr36M2gV94hLyOdxa89R8rJGNSOjthMe5z8v7eQv2MHNOkE7Z6Gwwt4QReH1kDFp5tOAnDgXBaXim6/Onxx1izyt23D/vXXMOnYEdCfZ92z9BfmPzeVc1GHaT9sDOM++wGvlm3uyhsekiTh17Yj47/4kY6jJpB6KpZFL05n85xvKci5fR3mmL3nWf/jUaydtAx6IRQz62u3g2sMVfSaGkjz7jqO7khh7XfRlBRVrw1dW7kZ6RxYtYxFM6az8IUnOLByGVZOzjw07Vken/ML/Z55Ca+w1jctT9UYrMaMRmFsTOb+PFAZ6s8S34B7YDBpZ+IozL1Up/FGtXEjq6CUjcfSbt6o1WOgewD+eqVajWRBEARBEISbEQFxDSmvrvUqqyjQ5GJaakNZRSUZyfmY2RqiMTDglI2EW3byNUFKfTrz01cYpKSxuLsVyfEjcTH2YVGvRYzu/wZvjTckwbSExClTyPjt1xr3WZqcQu769VgMHYrS3JyLecXMuZBBqaGCpC2pVJRXYmptQ5dxj5ESe4KI9avrdA8HVi0jPvIwXcY9hn0Trzr11ZgKc0vZuiiGZR8fIj+rmAfH+TPoxdBq53obkoWdMb2mBjLw2RAMtWo2zz3Bn7MOk3b2SsDhHhTC8Pc+RWNoxB8zXyFm1zasx45F4+FB2vsfUFlSAl1eA8fmmG56lpfbmbMrLgOA/eeyeNDfnkAX85vOIWvJErIWLsJyzGisRozQl1Hau5N5z00l/M/f8G7Vlglf/ETrh4ei0tz9TMwqtZqW/QYx4avZNH+oD8e2bWbu048Rvvx3ykqKb3hNxKYEti6KwcXXggHPhmBkeuP7UCgk2g32ostoP1Jis/nz40NcSi+q9RyL8vOI/nsjv7/9MnOeGM+uJQtQqTV0GTeFKT8uZPCrM2naqRsaoxucob0LlBYWWI4cQe6mrZQ49IXo36Egs1o798BgkGUSjkbWabwOXja4WRuzODzx5o0UCn0SvvISWPssNNDfYEEQBEEQ/l1EQFxDCtOrgx6JQuMczIptKKuQyUjOx8bZFG2lHwnOBajy8yhPrZ+txVfLPBdLwez5RPpqGD72J/IKDZi76ywKScGjfo+yYPRqtr7clUh3SH97JsfffRm58vbbOLMWLABJwmqcPkPrN1tOU1JZSdshXuRcKOTodn1N5YCOXWnSohW7f1tEZkrSHd1D0omj7Pn9F3zbdiTowV531Edjq6ioJGpLEovfCudkeBrNu+sY+U4bfNs43rWt3s6+lgx5pSVdRvuRl1nMn7MOs2nucfKy9AGetbMrI97/DCcff9Z/+xl7lv+O/RuvU5aYSObPP4NKA4PnQnkJw5PfQ3c5E3ZxWQXP3CKzdP6uXVx4731MOnfG/qWXuBh/lqXvvMK6r2ZhZGrGo29/RJ+nXsTU2uamfdwtxmbmdB03hbGffo9bYHP2/P4/5j07lRM7t1b9nsiVMnuWxbFv+Rm8w+zo80QwGsPbp1oIaOdEv6ebU5hbyrKPD5F6Oue215SVFBO7dycrP3mXHx8brV+5vpRD26EjmfjVHEa8/xmhvfqhtbiz5GwNzWrcOCQDAzKPaqC8GA7Nq9bG3qMJhiamJNRx27RCITGilY4D8VmcTMu7eUNrT+j6OpzaAEf/qNOYgiAIgiD8N4iAuIYUV2XFNVApqDQrxrhUi6K0kkvpRZjYG5FyXsc5J31dz+KY6mfq6qKkooSdL01AlmX8Z35CN6+m9Al05Nttp1m8X38u0l5rz6xe32D/3ZfsammMYvEqtozrQ35+1k37Lc/OJmfZMsz79kXt4EB8RgG/Hkjk0ZautG7nglszaw6sPUfBpZKqrdNqA0M2fv8FlRW12zpdkJPNuq9mYeHgSI/Hnrwvzg0nx2ax9P2D7P4jDgcPM4a92Yp2g73QGN39fHQKhURAOydGzmxDi15unI1MZ/Fb4YSvOkNpcTlGpmYMfm0mgV17sH/F72wL34HxQz3JnD2H0qQksPGCXrNQJOzia51+m72vvSnNnG+8Olx88hQpzzyLga8vVu+8xZb5P/HLy8+QkZzIg5OeYNSHX+Di36wxvwV3xMrJmQEvvMajb32E1tySDd99zi+vPkv80Si2LIwh8u8kAju70H1C0xvWgL4ZF19LHnkpDANjFau+PEJsePVtu5UVFcRHHmbDt5/xw2OjWffVLC6ciSOkVz9Gffgl4z//gQcGD8fCwbE+b7lBqKytsXx0KJf+3kWpVQc4OEe/OnsVhUKBW1AI8VERdd4180gLFzRKRdXfu5tqMw1cWsL6FyHvQp3GFARBEATh308ExDWkNL02IB7eehAAzgUyyHCqpISSXC8S7UBWSBSfOFFvY1fKlfwwewp+0dkUjuhN8+AeAHwyJIjOvna8tuIYn/wVW/WCs5tHD4b/vJ2jQ0NwPhDPjkFd2HFs7Q37zl68BLm4GOtJEwH4fPMpVEqJp7vpVwnbD/GmoqyS8FVnAdBaWNJtwlTSTp/i4JrlNb+HygrWf/MpJQUF9Hv25Xtm6+fN5GUVs3H2UVZ9GUl5aQW9pgbSd3owlg733nlnjaGKNgM8GflOG5o0t+XwhgQWvxlOzN5UFAoV3R+bTqfREzl1YC87VWUUa9Rc+OBD/cUhoyBgAJ4nZwPQK/DGgVh5ejpJU6eCVkvWiEdY8MozRG/ZSPOefZj45WyCu/eqc2mjxuYS0IyR739G7+kvUJSby5/vvcbRLT8S2MmIDo96Iylq/4aNhb0xj7wUhqOnOVsWxBC+8gyVFZWknopl6/yf+Onxsfz54VucOXwA3wc6MOSND5j8/Xw6j56IfROv++JNoqtZTZiIpFSSecYe8i/Asep/E9yDQynIySY94VydxrI2MaB3oAPLI1IoKLnFWW2FEgZ8D2VFsO45sXVaEARBEIRbEgFxDSmuOkNsqFbiodOXWXEr0r+AXZuYQVN7HS42nmTaG1F8ov5WiL/f/yVBi/ZT5GhJmxc+rHrcWKNi9ugWDG/lynfbzvD80ihKy/VbP00NTBk6cwllM5/BNbWMiskv8vbvU0gvTK+6vrKwkOxffsGkSxcMvLw4lnKJ1VGpTGjngZ2Zfguthb0xwd1cid17ngvncgHwbdsRn9bt2PfHYjIS42t0D+F//kbisSi6TpyKrZtHPX1n6l95WQUH151jyVvhJBzNpHV/D4a/1ZomzW3v+WDF1MqQHhObMnhGC0ytDdm6KJalHx4kNS6HsL4PM/DFN8hOv8g+fzdSwveQt3WbvjxNv6+QzJz0fRhUX/muLCoiadoTXCwpZG+wNzuWLcHOowljPv6aruOnYGjSMAnkGoOkUOAR0g4L18dQGbVHIaVyaPWHbJ3/4x0ngjLUqun3VHOaBCsJX7GEbyeM49c3XiB6y0Zc/JrS//lXmTr7F3pOfQpds6D77o2Eq6nt7bB4ZDA5Ww9QZugD4d9XC0Ddg0MB6pxtGvTJtfJLylkVeZsjKbY+0OUViF0Lx2v+xp0gCIIgCP89IiCuIYX2ysqggVqBuZ0+46xzqYRSo+BIZj7DWupo69SWWOsSik4cr5dxV8StIH3eXByzwefdT1Bcl6RIpVTwwcOBvNDDh+VHUhi/4AC5xWVVzwcNnYL7wkXYlBvR+6OdPP91H5aeXEqlXEnOn8upyMnBevIkAGb9dRJzIzVTOnleM0ZYb3eMzTXs/P0UcqWMJEl0mzQNjbGWDd99QUX5rTPrJkRHsu/P32jaqRvNOnevl+9LfZNlmbOR6fz6zn4OrDmHW6ANI95pQ1hvD1Tq+ytgcWhizuAZLeg+MYDi/DJWfn6EDT8exdq1GcNnzkJpYkq4tytHPvmQyuJiMLKEQbNBUoDBtcGtXFlJ3PPPsTf3IuGuNpTLlfR/7lUeef19bHTud+cG61F+djHLP40gK6WEvk9PYPJ3cwl6sBdRmzcw96nJHFi1jPLS0hr3l5eVwaE1y1ny2rOc2P4JFSUHqCg3wcp1AGM+mUu/517Bu1Xbu5ohur5ZT5wIMmSm+kBaNCTsueZ5E0srbN08iI88XOexWrhZ4udgyuL9Cbffgv3AdHAK1W+dzk+/dVtBEARBEP6zREBcQ5LySlBkoFKiMVQhGypQIlForMBQo6B/cycecHqAM/YyFRfTKc/IqNOY+1L38cOGt3lkH5j07IFJ+3Y3npsk8WRXbz4dEsz+s1kM/XEfaZeuZM81axGG358rMbN15sX/5bNl3jtMWDeGC/PmYBQainFoKHvPZLDzVDrTOntibnTti3WNoYq2g7y4GJ9LbLi+7ImxmTndJz3Bxfgz7F+x9Kb3kJ+VybpvPsHa2ZVuEx6/J1dZs9MKWPNNFBt+PIpKo2TAM8156LFmmFoZ3u2p3TFJkvBp6cDId9rQur8HiTFZ/PrOfk4dLGfIG7OwcXLhoJmGbW+8rA8s3NvDU0cgeERVH2WlJfz9/FOsT08g3cqctkNHMu7zH/Bu3bZR/h1T8lOYHT2bzQmbyS/Nr/f+s84X8OeswxRkF9NvejCeIXYYm5nz4MTHGfvpd7gENGPXkgXMe3YKMbu33zRBXXFBPke3bmLpzFeZPW08O36Zh0KhoPOYyUz5YSEDX5pJaZkPq7+KIT3xFgmh7lNqZ2fMBw4gZ/sxymQr2Pd9tTbuwaGknIyhtKiwTmNJksTI1jqOp+YSmXSbxGVKlT7rdHEurH+hTuMKgiAIgvDvdfczA92HDC4n2pFM1VBcwsniEvqEOmFmqCbMPowfHFVAKcUxMZh06HBHY8Rlx/Hc9ud4cbsGtUqJwyuv3PaaR1q4YGdqwOO/HGbQ93tYMKEVPvb67NganQ6v3/8g+cnpPLPqMNEnouF8GfGT++Amy3y88SSO5oaMbet+w759WtlzbEcy+1aeoUmILQZGKrxbt8WvXSf2r/gdz7DW2Htcu7JcWVHBuq8/oaykmH7PfoTa8N4KMEuLyjm47hzRW5NRGShpP9SbwE7OKJT/nveJVBolYb098G/rRPjqs0RuSSI2PI0WfV/CYPHrRCaepmjWTB569hVUlu6AfrU8bv8etv30DfmFBejMrenx4aeY29o3ypyT85L5+ejPrDq9inJZv/tApVDR0r4lnVw70dGlI66mrnUaI+3cJdZ+G4VCqWDg86HYul5bOsva2ZWHZ7xJ4rFodvwyl/XffErE+lV0Gj0RF/9mlJeWcjbiADG7d3DuyEEqysuxdHTigcHD8WvXCSsn56q+TCxh8IuhrPsumuWfHqb7hKY0aW5bp/nfa2wee4xLy1eQld4ce2k9mNhd87x7cAsOrv6TxONH8QprXaexBoY48+GGWH4JTyREd5sM3PYB0Pkl2PoenFgFAQPqNLYgCIIgCP8+/55X/o3oSkCsfz/hvFTB8Fb6F+jGamMsmjUHoPj4nSXWSi9M54ktT9DyjIT/iXxsn3wCtYNDja7t6GPL0qkPUF4pM/iHvew7c6U2qMrSEt38eZj17UtQXBnp9oY8VbqIKRteJSo5nWce9MbwJtuDJUmiw6M+FOWVcmjdleQ4XSdMxcjMnI3ff0F5Wdk11+xZ+gvJMcfoPvlJrF3qFsDUJ7lSJjb8PIvfCidySxK+D+hXUoO7uv6rguGraS0M6DbGn6GvtMTKUcueZecodX4c13wDTkYc5I93X6PwUg7pifH88e5rrPniI6SsbDobWfHIj/MbJRhOzkvm7b1v029FP1afWc0Q3yFsHLyR+T3nM8p/FGmFaXx04CN6L+/NwJUD+fzw50RciKC88tZb9q+XeDyTVV8cwcBYzeAXqwfDV9M1C2LUB1/w0LRnyc/O4ve3X+bXN2fww2OjWPPFR5yPiyW4Rx9GfvAF47/4ibZDRlwTDP/DxsWUR14Ow8pRy4afjhKxqQZbfu8jGp0Os759yN4bT3mpWl+X+CrOfv6oDQzr5RyxqaGagSHOrI1OJaewBtvZ2z0DDkGw7vkb1koWBEEQBOG/TawQ14Jrz3IOqQIwUOmDRpW5hjIK0NgY0MLtykpFmGdH0iwOoDkWSW2rsRaWFfLk1icpyM9m6jZjNF6eWI0ZU6s+mjqZs3xaW8bNP8jYeQf4dGgw/YP1SZMUGg1On8zCuHUrXHy9GVOyhYUnFmDpFUVr7x9v2a+dmxkB7ZyI3pqMfzsnrBy1GJmY0n3yk6ycNZPwP3+l/TD9XM8eOciBlX8Q2K0nAR261PK70HDSE/PY+dtJ0s7mYu9hRu9pQdi7m93taTUaW50pA58L4VxkBnuWnybd9QksM7dw4cxxFrzwBMX5eWgMDGmWnoen1hSPb75DUjXsn4nkvGTmHJ3D6tOrkSSJIb5DmNBsAg5a/ZtAzibOhDmE8XzY8yTmJrIjeQc7knfwv+P/Y/6x+ZgbmNPeuT2dXTrT1rktZpqb/3ueOpDGlgUxWDlr6Te9OcZmmpu2/YekUNC0Uzd82rTj8LpVnNi5Fe9WbfFv3xnXZoE1ToqlNTfg4edD2bIwhn3Lz5CTVkinEb61Ku10L7OZOpXcNWvJygzBTrP/mueUKjWuzYKIjzyELMt13nI/qrUbS/YnsuxwMpM6NLl1Y6UaBn4PszvDX6/oz8sLgiAIgiBcJgLiWjCxLyRHYYmBWv8C1sDDhEPH0+nR1vOaF3htndqyz0HC+lh0rfovqyxjxs4ZxGbFMjelO1LqOhwWfoJ0Bwl4XCyN+XNqWyb/7xBP/XqEtEtFTO7QBEmSkCQJyyFDAHA9aEVhUiXW7ssZsX4Y77Z/l266bjftt82AJpw+fJHdf8TRb3owkiTh2aIVTTs9yIGVy/AMa43WwooN336OrZsHXcY9Vuu5N4Si/FLCV53lxO5UjEzUdB3jj18bhzsqrXO/kySJJiG2uDWzJmpLAgeXt0dVEYAsb8O/dShu6zaiKqnA7X8/oTS9+eppXSXlJfHz0Z9ZfXo1CknBUN+hTGg2AXvtzVejdWY6RgeMZnTAaPJK89ibupedyTvZlbyLdWfXoZJUhNqH0tGlI51cOuFu7l51bdSWJHb/EYezjwW9Hg/CoJa1pNUGhrQZ9ChtBj16p7eMSqOkx8SmWNgbc2h9PJfSi+g1JRBDk/s/yZZBkyaYPtST7B07sO4poTS4dgXcI7gFZw8fICctFUvH6qvotRHgZEaozoIl+xOZ2N7j9gG2QyAED4MTq+s0riAIgiAI/z4iIK4pWYbSAgoNNFVbppv6WLPE4zwftbp2O7CvlS8rnI1Rx2ZRkZuL0uz2K5CVciVv7nmTHck7eNdtGiaf/ohp375oW7e64ymbG6tZNKEVz/8RxQfrY0nNKeaNvgEoLweBxWUVfPl3HM0s2/Jt/2G8uPNFntn2DGMCxvBMi2dQK6q/SDcy1dCqnwe7l8ZxLiqj6ixkl3GTSTgWycbvvsDAxITKinL6PvMyao3BHc+/vpzcn8au309RVlxBcFdXWvb1qHUw9G+kVCsIfcgDd206O2ftJ8V5OPExFZRoC2n7Zh/UznULWm4mKS+JOdFzWH1mNUpJWaNA+EZMNab0dO9JT/eeVFRWcDTjKDuSd7A9aTufHvqUTw99iruZOx2dO+J5qg3nd5fRJMSW7hMC7mrmcEkh0bp/Eyzsjdn6vxiWfXyIPk8E3ZM1rmvLZupU8jZsJOuiP7au1x4Zubr8Ul0DYoCRrd14/o8o9p7JpJ1XDfbiGJiLmsSCxk61NwAAIABJREFUIAiCIFTz79ir1xjKSwCZAllTtWU6zN2Kv57tiJX22m2XCkmBtlkgAEUxtz9HLMsynxz8hLVn1zK9+ZOELj6CpFZjN+PFOk/bUK3km2EhTGrvwYK98UxbfJjisgoAFu2L5/ylYl56yBdXM1cW9VrEMN9hLDqxiPEbx5NWkHbDPpt1csbKScueZXGUX+7LwFhLzylPkZWazPlTsfSY8tQNz1I2tuKCMrb9EouFvTGPvt6K9kO8RTB8HasOrWgVUESb/TOxT9vPece2LPv1EpvnHSczpf6yOyflJfHmnjfpt6If686uY5jfMDYM3sArrV+pdTB8PaVCSXO75jwd+jQrBqxg4+CNvNr6VZyNXUhaW8753WXEOR5gq88iNiSuJ6f4NhmKG4FvawcGPhtKaXE5f846TFJs1t2eUp0Z+vpi0q0bWVGlVDQdBZorQb6FgyMWDo71co4YoE+QIxbGan4JT6iX/gRBEARB+G8SAXFNlenLhRRUqKtWiG/Fq5W+3m7SoR23bTs7eja/xPzC6IDRDEvzoGDXLmyfmo7azu6219aEQiHxet8A3ugbwKYTFxgxJ5yEzAK+23aGjj62tPXUr65olBpea/Man3T8hLjsOIasGcLulN3V+lMqFbQf6k1uRjGRm5OqHncPDqXjyPF0HDke3wfuLLt2fTu5P42Ksko6jfDFyun+X4FrKHYvvICFg5ZOA10Z/UE7grq6cDYqg9/ePcC676I4f+bSHfedlJvEG3veqBYIv9zqZeyM6+dn/HrOJs4MaTKU/mcfx/dia6zbyZh1K+TgxYO8uvtVOi3txNgNY5l7dC5ncs7ctQRXjp7mPPJSGFoLA9Z+HcXxXSl3ZR71yWbqVCrzC8nOCobrtjK7B4eSeDy6WgK+O2GoVjKkhQubTlzgQm7x7S8QBEEQBEG4AbFUVkPbjiXQBcit1FSdIb6VVv7dOWn6LpVRB/C/RbvfY3/n28hv6e/ZnyeUD5L82hQM/PywHDmy3ub+j4ntPXA0N+SZ3yPp/sVOSssrmdHTt1q7hzwews/Kj+d2PMe0v6cxOWgy04KnobwqeZCrnxWeIbYc3hiPbxuHqpq9LfsPrvd53ylZljm+MwV7D7NbZhIWQGVri+eG9VVft3/Em7CH3Inenkz0tiSWf3IYJ28LQh9yQxdgVaOkSEm5Scw+Ops1Z9agUqgY7jec8c3GN1gQfLWSwjLWfR/N+TOX6DjMh8DOLkA3KuVKjmccZ0fyDnYm7+TLiC/5MuJLnE2c6ezamY4uHQmzD0OjvH2yrfpiZmPE4Bdb8NfPx9i++CTZFwppO8gLxX16vt0osBnaDh3Imj8fs9690LheOVLiHtyCyL/WkRJ7HLfA5nUea0RrN+bsOsfvB5N4qpt3nfsTBEEQBOG/R6wQ19DFrGwAMkqUVVumb8XW2JZ0FxOkU+du2mb92fW8v/99Ort25mWDASRPmozS2grXH75vsMy+vQMdWTypNcYaJYNCnWnmbH7Ddu7m7izuvZiBXgOZHT2bxzY/RkZRxjVt2g72QpZh3/LTDTLXujp/+hLZaYX4trOnuFysINWWoYmaVn09GPN+W9o94sWl9CLWfhPF0g8OEnfoApWVN15VTcxN5PXdr9NvZT82nNvAcL/hbBi0gZdavdQowXDBpRJWfHaEC+dy6TGx6eVgWE8hKQi0DeTJkCdZ2m8pmx/ZzBtt3sDLwotlp5YxZfMUOvzWgee2P8fK0yvJLGqcMj0aIxV9pgUR1MWFqL+T2PBDNKXFtSsndS+xn/EiyDIJY8ZSmphY9bhr00AUSlW9bZv2sNHS3suGXw8kUl5RWS99CoIgCILw3yJWiGvIUq0/K1uMpkZbpgEUvt5YrTpCfm4mJmbW1zy3O2U3r+1+jVD7UGYaPUrKlMdR29mhW7gAtX3D1nxt6W5F+CvdUN+m5q6RyoiZ7WYSah/K++HvM2TNEGZ1nEVLh5aAfmUrtIeOg+viadYpGydvy1v219j2/R1DpbqcqedGkHsmB1O1KTbGNtga2WJjpP9oa3zl83+eM1Gb1LkszL+JxlBF8wd1BHZ24eT+NI5sSmTTz8cxtz1LSA8dfm0cUaoVJOYmMjt6NmvPrq1aEZ7QbAK2xraNNtecC4Ws/jqSovwy+j4RjGuA1S3bO2gdGOo7lKG+QykqL+LA+QNVZZ02J2xGQiLQNpBOLp3o5NIJH0ufBvvZUCgVdHjUBwt7Y3YtjWP5JxH0eSKoavfF/cTA2xvdwgUkjhtPwpixuC1cgMbNDY2hES7+AcRHRdBp1IR6GWtUGx1Tf4lga+xFejStWb12QRAEQRCEf4iAuIYs1Pozb0UYYFjDDLVOIW1RrDzC0fC1PNBjbNXjkRcjeXbbs3hbevOJ8WguPD4dtbMzbgvmo7JtnOChpvcAMNBrIAHWATy//XkmbZrEk82fZGLgRBSSgpCebsTsO8/O3+IY+moYitsE2Q2tpKKETfGbWB69ipDoocQ6hNPJvQMe5h6kF6WTUZRBemE6UelRZBRlUFJRUq0PQ6WhPkg2vkngfPk5CwMLFNJ/Z5OFUqUgoJ0Tfg84cvZIOhF/JbB98UnC15zmglcsK9TzkNUVjPAfwfim4xs1EAa4mJDL2m+jkGV4+LkQ7NxqV1/aSGVEJ9dOdHLthCzLxGbFsj15OzuTdvLNkW/45sg3OGgd6KbrxlMhT2GsNm6Q+wjs7IK5nRF/zT7GHx8dovfjgTh43Hgnx73M0M+velDs7o57cAt2Lp5PXlYGpla1rdRe3YP+9tibGbB4f6IIiAVBEARBqDURENeQuepyQCzXfIXYv20fkviOxEPbqwLiU9mnmLZlGvZae77Ujidz+vNodDp0C+ajsra+TY93j4+lD7/1/Y139r7D10e+JuJiBB+2/xALQwvaDfbmrznHOLE7lWadXG7fWQOIvxTPslPLWHlmJZdKLtEpaxBKWcWMMZNx93C84TWyLJNXlkdGYQbpRen6gPnqz4syiMuOY1/qPvLLqmdbVkkqrI2sr1ldrva5kQ3WRtaoFP+eXzWFQsKrhR1qryIWbtpJyUETnCK8Gat5l6adnWjj74ORceOdwQVIjs1i/Q9HMdSq6f90cyzs6xasSpKEv7U//tb+PB78OBlFGexM3smOpB0siVmChMRLrV6qp9lXpwuwZvCMMNZ9H8XKz4/Qbaw/3mENu3OkIRj6+qJbsIDEceNIGDMW3cIFuAeHsnPxfBKijtCsS/c6j6FSKhjWUsfXW+NIzCxEZ90wb1QIgiAIgvDv9O95ld7AzJRXVohrGhBrXdwp0qoojokB9GVnpmyegpHKiO+MJ5L7zMtoPDzQzZ+HyurWWzvvBVq1lo87fkwL+xZ8fPBjhqwdwqedPiUoNAhnXwvCV5/Fq4U9hibV6xc3hLLKMrYlbmPpqaXsP78flaSii64LQ3yGcOZHMPbS3DQYBn3QY6Yxw0xjRhOLJrccq6i8qFqwnF545fOU/BSiLkaRXZJdfRwkLA0tr1lhtjO2w8nECTczN3SmOuyM7e6bbdoJuQlVW6M1Cg1DhwzlQVMvzmzL4cSmi5zankFAeyeaP6hrlO2+pw9fZPP841jYGdP/qeZoLeq/9rWNkQ2DvAcxyHsQ74W/x5LYJfTz7EeAdUC9j/UPKyctj7wUxoafjrLp5+PkXCgkrLf7ffNz8g9DX5+qleLEMWNxXTAfraUV56Ii6iUgBhjWypVvt51m8YEEXul1qzSGgiAIgiAI1xIBcQ1pFVcCYhfLmq1ASJJEqacLNknxHM84zgs7XqCssoz52mkUvfAWGm8vdHPnorK8t87e3ookSTzq9yjNbJrx/I7nGbdhHM+FPUevIQNZ+sEh9q85S6fh1TNX16fU/FSWnVrGitMryCjKwFHryPSQ6Tzs9TC2xrYkx2YRcTGSln086m1MI5URrmauuJq53rJdWUUZmcWZ1wTL6UXppBde+TwuO47M4kwq5Ipr+zd1rQqQ3czc0JnpP1obWt8TQdD1gfAo/1GMbzYeGyP9tldfPx1ZqQVEbErg6PYUju1Iwae1A6E9dFg6NEzJq2M7ktnx2ykcPc3p/XgQhtqGfzPmqdCn2JK4hZn7ZrK49+Jrsq/XNyNTDQOeDmHb4lgOrDlHdlohXcf4oarFkYd7gaGPD24LF5AwbjyJ48bh2rc78dFHqKysQFEP3z9HcyO6+dnxx6FknuvuU6PEh4IgCIIgCNDAAbEkSQ8BXwFK4GdZlj+67vkvgC6XvzQG7GRZtrj8nA74GXAFZKC3LMvxkiQtADoB/xRGHSfLcmRD3geAVioFoEg2wMe+5iV8LIJCMf41nknrx1GhlJhnOIXyVz/SbyWc+zNK8/vvbCBAU5um/N73d97Y8wazDs4iQhdB3/YTOb4zhaYdnLBxqd8yRxWVFexO2c3SU0vZlbwLgI4uHRnqO5R2Tu2uCUqO7UzFQKvCM7Rxz7ACqJVqHLQOOGhvfZaxorKCtMI0EnITSMxN1H/MSyQuO45tidsol69kGNaqtehMdejMdFXB8j8Bs6WBZYMHy/GX4pkdPZt159ahUWgY7T+acc3GVQXCV7Ny0vLguABa9fMgcnMSJ/akErvvPE2a29LiIbdan+u9GVmWObj2HAfXxeMeZEPPSU1RaRonCDLTmDGj5Qxm7JzB7yd/Z4T/iAYdT6lW0G2sP5YOxoSvPEteZhG9pgZhbNa429LrysDbG7dFC0kYOw7jv7dTbGVM2uk4nHz86qX/UW3c2HTiAhuPpTGguXO99CkIgiAIwr9fgwXEkiQpge+A7kAycFCSpNWyLJ/4p40sy89e1X46EHJVF4uA92VZ3ixJkglwdU2NF2VZXtZQc78RZXkRAEVocLE0qvF1js3bkvrLcuwvlPK6yySUb32JYdMAdHPmoDSrn+DgbjE3MOerLl+x6MQivjj8BWcM4ulr9BS7/4hjwDMh9RKopRemszxuOX/G/cn5gvPYGNkwOWgyg70H42TiVK19YW4p5yLTCezqck+voikVSpxNnHE2caatU9trniuvLOd8/nkS8hJIyE2oCpqPZxxnc8JmKuUrvwqmGlPcTK+sJuvMdFVfmxvU7c2W6wPhMQFjGNt07A0D4euZWRvRcZgPYb3did6WxNHtKZw9ko6LnyUtHnLD2ffOA/nKSpldv53i2M4U/No60mWkb6Mnc3vI/SFWxK3g6yNf86Dbgw1eTkqSJFo85I6FnTF/zz/Bso8O0eeJIKydTRp03Ppm4OmJ26KFlI4bB7LM6a2b6y0gbu9lg5u1Mb+EJ4iAWBAEQRCEGmvIFeJWwGlZls8CSJL0GzAAOHGT9sOBty63DQBUsixvBpBluXpGo8ZWVgjoyy4pFDV/IW/YVH/G8L1TQSgX/IRRUBCuc2ajNLm/XsjejCRJjG06liDbIF7Y8QI77VfwwMmBfLziW8qdLt2+g1u4WHiRXcm7KJfLaePYhhdbvkhn186oFTffFhuzN5XKSpmm7asHy/cLlUJVtT27vXP7a54rqygjOT/5mlXlhNwEIi9GsuHcBmSu1Aa2MLC4JkC+OmA20dz85+/cpXPMjp7N+nPrax0IX8/YTEObAZ6E9nDj2M4UorYkserLSOzczWjR0w2PYBukWvw+VZRVsnn+cc5EpBPaU0ebgZ53ZTu5JEm83uZ1Hl71MLMOzuLTTp82yrieoXaYWhuy7vto/vzkMD0mNsU9sO6ZmhuTQZMmeC9YyIHnH+f0xnW06toTQx+fOverUEiMaKXjww2xxKbl4udwf7/hKAiCIAhC42jIgNgZSLrq62Sg9Y0aSpLkBngAWy8/5APkSJK0/PLjfwMvy3LVocv3JUl6E9hy+fFqtXMkSXoMeAxAp9PV/W4uDz2jd7NaXaZxc0NhbAy7DmEU1gLXH39CadIw5ynvphC7EP7o9wfv7JpJQWoOpfss2NJiGdQhVjFQGjAqYBSP+DyCm5nbbdvLlTIndqfi7GvRYGdW7za1Uo2HuQce5tXPR5dUlJCcl3xlG3ae/uOBtAOsObvmmrZWhlbVzitbGVqxPG4568+tx0BpwNiAsYxtOhZro7pnP9cYqQjt6UZQVxdi96VxZFMCG346iqWDMaE93fBuZY/yNqu8pUXlrP/xKCkns2n3iBfNH6yH3+s60JnpmBw0me8iv2Og18Bqb140FDs3M4a8HMa676NZ/3007YZ4E9TF5Z44Z15TBk088OrVj4N/r+f0+PF4zZuPoW/dg+IhYa58tvkUi8MTeXdg7f5WC4IgCILw33SvJNUaBiy7KuBVAR3Qb6FOBH4HxgFzgVeANEADzAZeAmZe36Esy7MvP09YWJh8/fN3akK72iVqkhQKTHv2pCInB+fPPtUHx/9SVoZWfNX9S2K0qWxdFMt832U0CWm8c7xJMVnkZhTTZqBno415LzFQGuBp4YmnRfX7LyovIikvqdrK8t7Uvaw6s6qqnZHKqF4D4eup1EqadXQmoJ0jpyMuErExkS0LY9i/+izNu+sIaO+E+gZngQtzS1n7bRSZyfk8OD4A39b3Rr3ZCc0msO7sOt4Lf4+VA1ZiqGr4rNoAJpaGPPx8KH/PP8HupXFkJufjFWaHlaMWrYXBfREce3XuysEtG8gwMUIzdiy6BfMx9Kvb9mkrrYY+gY6sOJLCy7380BrcK//FCYIgCIJwr2rIVwsp6BNi/cPl8mM3Mgx44qqvk4HIq7ZbrwTaAHNlWT5/uU2JJEnzgRfqddYNwOnDD+72FBqVb2sHIv5KJHz1WdyDbWq1xbwuju1MwchUTZPmjZ9M615npDLCx9IHH8vqq3CFZYUk5iWSmp9KsG1wgwTC11MoFfi0dMA7zJ6EY5lEbExg99I4Dq2PJ7irC806uVRljM7NKGL1V5EUXCqh97Qg3JrdO/W6NUoNbz7wJhP+msDs6Nk8FfpU441tqKLXlEDCV50h4q9EYvbq/zSqDZVYOWqxdNRi5aDF0tEYK0ctplaGtdqe3tAcPH0w1JpQGNYGad0WEseO0wfF/nUrmzSqjY4VR1JYFZnKiNZ3dxeBIAiCIAj3voYMiA8C3pIkeaAPhIcB1dKxSpLkB1gC+6671kKSJFtZltOBrsChy+0dZVk+L+mXQAYCxxrwHoQ7oFAqaN2/CX/NOUbcgTR829y8FnB9yc8uIf5oJiHdXVHWsE60oGesNsbPyg8/q/pJblQbkiThHmiDe6ANqadziNiYwP7V54j4K5FmHZ1xDbDi7/knqKioZMAzITg0ufeysrd0aEl/z/7MPz6fPk363HCFvqFICokHHtZvH886X0D2+QKyzheSdb6AxGOZxO49X9VWpVFgeVWAbOmgxcpRi5mtUaO9aXU1hVKJLrA5SSdP0G3RQhLHjSNh3Hh08+Zi1LTpHfcbqrPEz8GUX8ITGN7K9b5YLb8f3K5qxOU2Q4G30VeGiJJluWFTsAuCIAhCPWiwgFiW5XJJkp4E/kL/H+g8WZaPS5I0Ezgky/Lqy02HAb/JsixfdW2FJEkvAFsuB76HgTmXn14sSZIt+tOpkcDUhroH4c55hthi42rCgbXn8Aqzb/AgNWZvKnKlTMB9nEzrv87JywKnJy3ISM4j4q9EIv9O5MjmREwsDRjwbAusHO/dc+HPhz3P9qTtvBv+LvN7zm/0IMzIVIOzqQZnn2trmhcXlJF9voDstMKqgDn1VA6n9l+oaqNUKbCwN8bK0Vi/qnx5ddnczui257rryr15KKfCd5NLJW6LFpE4ZiyJEyaimzsXo2Z3FhRLksTINm68sfIYR5JyCNXdP3Xe71U1qRohSZI3+iNN7WRZzpYkqWFTrwuCIAhCPWnQA1ayLK8H1l/32JvXff32Ta7dDATd4PGu9ThFoYFICok2Az1Z+00UMXtSadbJpcHGqrycTMs1wApz23/vGe3/ChsXU3pMbErr/h7EHbyAbxtHTK0a52zunbIytOK5Fs/x9r63WXVmFQO9Bt7tKQFgqFXj6GWBo5fFNY+XFpVfEyRnpRVwIT6XuEMXq9ooFBLmdkZXtl9f/mhhb1RvJc3cg0MBiI88TMv+g9EtWkTimDEkTpigD4oD7ywx1sMhzny0PobF4YkiIK4fNakaMRn4TpblbABZli9W60UQBEEQ7kEi44jQYHQBVjh6mXNwfTy+DzjeMFlSfUg8lkl+dgnth3o3SP/C3WFua0xY79olsbubHvZ+mFVnVvHZoc/o5NIJS8N7NxDTGKmw9zDD3uPa0kRlJRXkXNAHyv8Eyxkp+ZyNTOefPTySBGa2VwXKDvqVZUsHLWqD2v2Om1rZYOPqRnxUBC37D0bj4ozb/xaRMGasPiieNxejwMBa35+JgYqBIc4sO5zMG339sTDW1LoP4Ro1qRrhAyBJ0h70u8LelmV54/Ud1XsFCEEQBEGoIxEQCw1GkiTaDPBkxWcRHN2WTGjP25dOuhPHdqVgbKbBPej+qscq/LsoJAVvtHmDoWuG8vnhz3m33bt3e0q1pjZQYqszxVZnes3j5WUV5FwoqlpN/uescsLRTCorryTxN7U2rAqU7d3N8AyxvW0iL/fmLTiyYTVlxcWoDQ1RO18VFI+fgG7uzxgFB9f6Xka2dmPx/kSWHU5mUocmtb5eqDUV4A10Rp9Ec6ckSYGyLOdc3aihKkAIgiAIwp0SAbHQoJy8LdA1tSbirwSadnTGwKh+f+RyM4tIOJZJWC/3Bj/vKAi3423pzZimY5h3bB4DPAcQ5hB2t6dUL1RqJTYuJti4mFzzeEVFJZcuXg6Ur0rqlRSbRWW5jLOvJV1H+2FmY3TTvt2DQjm0ZjlJJ47SJLQlAGonp6tWiifi+vMcjENCajXnACczQnUWLN6fyIR2Hoi/DnVSk6oRycB+WZbLgHOSJJ1CHyAfbJwpCoIgCMKdEa8RhAbXZkATSgrLidycWO99x+zRZ9H1b9fwmawFoSamBk/F2cSZd8Pfpayi7G5Pp0EplQqsHLV4htrRso8HPSY1Y9gbrZjyVSc6j/TlYnwuv717gOO7Urgqb+I1nP0CUBkYcC7y8DWPqx0dcfvfIpQ21iRNmkxhxJFaz29UGzfOZRSw72zmHd2fUKWqaoQkSRr0yTBXX9dmJfrVYSRJskG/hfpsY05SEARBEO6ECIiFBmerM8Uz1I7ILUkU5pbWW78VFZWc2JOKW1NrzKxvvgIlCI3JSGXEq61f5eylsyw8sfBuT+euUCgVNO3gzLA3W2Hnbsb2xSdZ+00UeVnF1dqqNBp0TYOIjzpc7Tm1gwNuixahsrEhadIkCiMiajWP3oGOWBir+SU84Y7vRdBXjQD+qRoRAyz9p2qEJEn9Lzf7C8iUJOkEsA14UZZl8U6EIAiCcM8TAbHQKFr396CitIKIv+rvhWlCdCaFl0pp2tG53voUhPrQ0aUj3d2682PUjyTlJd3+gn8pM2sjBjzdnI7DfEg9ncNvM/cTs/d8tdVit6BQctLOk5N2vlofant7dIsWobKzI3HSZAoPHarx+IZqJUNauLDpxAUKSsvrfD//ZbIsr5dl2UeWZU9Zlt+//Nib/5RQlPWek2U5QJblQFmWf7u7MxYEQRCEmhEBsdAoLB20+D3gyLEdKTdcJboTx3alYGJpgFtTq3rpTxDq04yWM1BKSt7f//5Ntwv/F0gKicDOLgx7ozU2rqZsXRTD+u+jKcgpqWrj0fxy+aWoG68Aq+3t0C1aiNrensTHplB4sObHUke0dqOiUuZkWl7dbkQQBEEQhH8lERALjSasjzsyMofWnatzX5fSi0g6kUVAeycUIpmWcA9y0DowPWQ6e1L2sClh092ezl1nbmvEwGdDaD/Em+TYbH6duZ+T+9OQZRkLByfM7eyJj775lmi1nR1uixaidnAg8bEpFBw4UKNxPWy0dPC24eSFPP67b0sIgiAIgnAzIpIQGo2ZtRHNOjgTsy+NnAuFderrxO4UJIWEf1unepqdINS/YX7D8Lfy5+MDH5NXKlYoJYVEcDdXHn29FZYOxvw9/wQbfzpGUV4Z7sEtSDwaRUX5zRORqWxt9UGxsxNJU6ZSEL6/RuOObO1GQUk5FZWV9XUrgiAIgiD8S4iAWGhULXq5o1RJHFhz58lHK8oridl7HvdAa0wsDepxdoJQv1QKFW8+8CYZRRl8e+Tbuz2dRlNSUUJi7s2zylvYG/PwCy1oO8iLhGOZ/PrOfjTaJpSVFJMSG3PLvlU2NrgtXIjGxZmkqVMp2LfvtvN50N8OY42SsgqxRiwIgiAIwrVEQCw0KmMzDcFdXYk7dJGM5DtbMTsbmU5RXplIpiXcF5rZNGOY3zB+jf2V4xnH7/Z0GlxBWQGT/prEwFUDyS7Ovmk7hUIipIeOoa+2xMzGkOjtgKTg9KHbnw9WWVujW7gQjU5H0tTHKdi799btlQp8HcyolGUKSkRyLUEQBEEQrhABsdDomnfXYWCsYv+qO1slPr4rBVNrQ3T+IpmWcH+YHjIdGyMb3tn3DhWVFXd7Og2msKyQaX9PIzI9krLKMqLTo297jZWTlsEzWtBmoB8KlTPRf+/h7JH0216nsrJCt2A+Gnd3kh6fRv7uPbdsH+xijrFGhdZAVeP7EQRBEATh3++2AbEkScaSJL0hSdKcy197S5LUt+GnJvxbGWrVhPTQEX80k/Onc2p1bXZaASknc2jawQlJITXQDAWhfplqTJnRagYxWTH8dvLfWY2msKyQaVumEZUexbvt3kUlqYhKj6rRtQqlgrBe7jTv0Z6Ksous/2Efm+cdp7jg5ueJ4aqg2MOD5GnTyN+1++ZtFQrEXwxBEARBEK5XkxXi+UAJ8MDlr1OA9xpsRsJ/QlAXV4zMNISvOlurkjTHd6eiUEj4PeDYgLMThPrX060n7Zzb8c2Rb7hQcOFuT6deFZUX8eTWJzly8QgfdviQgV4D8bHyqdEK8dWadtT/N+PWrJDThy7y6zv7iY/OuOV0WIqvAAAgAElEQVQ1KktLdPPnofH0JPmJJ8jfufOO70MQBEEQhP+emgTEnrIszwLKAGRZLgTxRrtQN2oDJWG93EmNyyEpJqtG15SXVRC77zwezW3QmotkWsL9RZIkXmv9GuWV5Xx88OO7PZ16U1RexPQt0zl84TDvt3+fXh69AAiyCeJoxtFabRG31bljbG6BgiQeeTkMI1M1676PZsvCE5QU3iL7tKUlbvPnofHyJPmJJ8nbvr2utyUIgiAIwn9ETQLiUkmSjEBfwlGSJE/0K8aCUCdN2zthamVI+MqarRKfiUinpKBcJNMS7luupq5MCZrC5oTN7Ey+/1cyi8uLeWrrUxxIO8B77d6jb5Mrp2mC7YIpLC/kdM7pGvcnKRS4B4cSH30EaxdjhrzSkha93Di5/wK/zjxAwvHMm16rtLDAbd48DHx8SJ7+FHlbt9Xp3gRBEARB+G+oSUD8FrARcJUkaTGwBZjRoLMS/hP+z96dh1dVnX0f/66cBEiAMCQnCiQhIYAJSc4JCbOKtZS5Dgy2UKizqC2tBQesCiItIlhFLX0dHsEBqT7iUKmPRbCgSBUZkzBDChESFcI8Q4b1/pEQA4SM5+Qk4fe5rlycvfbea9/71KvkZq11L0eAH11/Hk3OrqPsSC2/iM7GL7Np5gwkvGOLGohOxDtujb+Vds3a8eQ3T3Iy76Svw6my0/mnuW/pfXzz/TdMuXIK18Vcd855t9MNUOF1xGdFJaVw6ugR9uzIwOHvR48bYhj2UAoNGjn4+K9pLH1rC2dOll4p2tG8OZFzZtMoNpas++7j6JIlVXs5ERERuWSUmRAbYwywBRgK3Aq8DXSx1n7u9cjkknBF98tocXkQ33y0g4KCi48S7//uGN9nHKaTimlJHRfgCGBij4lkH8vm5bSXfR1OlZzJP8Mflv6Br777iid6PcGN7W+84JrwJuG0bNSy0uuI2yYmgTFkpq0tbrssKphfPNqVzv0i2fyf73j7T9+we0vpSy0czZoROftVGsXFkXXfHzj62WeVezkRERG5pJSZENvCeayfWGv3W2v/z1r7sbW27AonIpXg5/Cj23XtOPjDCbZ988NFr9v45Xf4+RviVExL6oEul3fhxvY38sbGN9h+cLuvw6mUM/lnGPf5OJZnL+fxno8zpMOQUq8zxuByuio9QhwU3IzL27UnM3XtOe3+AQ56DW3P0AdTcPj7seC5VL54eytnTl04WuwIDi5MijvFkfWHcRxZvLhSMYiIiMiloyJTptcaY7p6PRK5ZMUkO3FGNmXlP3eSn1twwfncM/lsXfEDMZ3DCGzawAcRinje+JTxNGnQhD+v+DMF9sL/7muj3Pxc7v/8fpZlLWNij4kM7zi8zOvdTjeZRzI5fPpwpZ4T5U7m++1bOXXs2AXnLm/XjF8+1g33TyPYsCyb//3zSr7bfvCC6xxNmxI5ezaBCQlkjxvPkbTsSsUgIiIil4aKJMTdga+NMf81xqQbY9YbYyo3B06kDMYYetzQjqMHTrHpP99dcD5j9R7OnMwjoXdrH0Qn4h0tGrVgfMp41u5dyz8y/uHrcMqVW5DLA188wOdZn/No90f5xRW/KPees+uIKzttOsqdgrUF7NqQWur5gAYOrvpFB4aM7wzAh8+s48t3t5F75tyK1o4mTYh49X8ITEwk+41VHMn0r1QcIiIiUv9VJCHuD8QAPwWuA35e9KeIx0R0aknrDs1Z9UkmuafP/aV245ff0eLyIFq1b+6j6ES848b2N5Iclsyza57lwKmKbT/mC7kFuTz0xUMs2b2EP3b7IyNiR1TovviQePyMX6WnTbfqcAUNgxqz87xp0+dr3aEFIyZ2J/En4aQvyeJ//7yS7/977mi0o0kTIv7nfwhs24Lvv25E/qFDlYpFRERE6rdyE2Jr7bdAcwqT4OuA5kVtIh5jjKH7De04eeQM6z/PKm7P2X2UPTuPEH91GwprvInUH8YYJvWcxPEzx3lm9TO+DqdUuQW5TFg2gc92fcaErhP4VdyvKnxvUEAQHVt0rPQIsZ/DQWSim8z0teVuyRbQ0EHvER254Q9JFORZPvjLGv7zfgZ5uT/+w5qjSWMi77mSyL4ncDTXP6yJiIjIj8pNiI0x9wHzgLCin7eMMb/zdmBy6WndvjltE0JY++m3nD6RCxSODjsC/Liix+U+jk7EO2Kax3Brwq0s+O8CVv2wytfhnCOvII8/fvlHFn+7mAe6PMDoTqMr3Yfb6Wb9vvWVXicd5U7h2P597M/aVaHrw2NbMmJSNzpd1ZrUxbt4d+oq9uw8Unzer6E/gaF1Y622iIiI1JyKTJm+A+hurZ1krZ0E9ADu8m5Ycqnqfn07Tp/IY93iXZw5lce2b36gfUoYjRoH+Do0Ea8Z4xpDmyZt+NOKP3Em/4yvwwEKk+FHlj/Cp5mfMj5lPLfE31KlflxOF8dyj7Hj0I5K3RflTgY4Z/ul8jRo5M+1o2K57vduck/n8/6M1Xz9j/+WWqxPREREBCqWEBug5KLO/KI2EY9zRjalfZcw0pZkkb5kN7mn80no3cbXYYl4VaB/II/1eIydh3fy2obXfB0O+QX5PPafx/jXzn9xX/J93JZwW5X7OltYq7LriINDnYSER1YqIT4rslMIIyZ244qerVi78FvenbaKnENNK92PiIiI1H8VSYhfA74xxkw2xkwGVgCzvRqVXNK6X9eO/NwCvlmwk5A2jbksOtjXIYl43VVtrqJf2368kv4Ku45UbJqwN+QX5DPpq0n8347/43edf8ediXdWq7/IppE0b9ic9H2V35wgyt2ZrM0byD19qtL3NgwKoM/NcQz+jYtTx3OZv6w737R9h/w8jRaLiIjIjypSVOtZ4DbgQNHPbdba57wdmFy6ml8WRGzPwjXDKqYll5IJ3SYQ4Ahg6jdTyy0m5Q0FtoDHv3qcBf9dwG+TfssY15hq92mMweV0kba3ciPEULiOOD83l6xNG6r8/ChXKCMndadD18vYvsVBQX7Nf68iIiJSe1WkqFYPYLu19gVr7QvAf40x3SvSuTFmgDFmqzEmwxjzcCnnZxpjUot+thljDpU4F2mMWWSM2WyM2WSMiSpqjzbGfFPU5/8aYxpU9GWl7uhxQwxJfSOJ7dnK16GI1JiwoDB+1/l3fPXdV3ya+WmNPrvAFvDE10/w0X8/4l73vdzjvsdjfbudbv57+L8cOXOk/ItLCI9LwL9BQ3amranW8xs1DqDvbfEMf7gLAQ0d1epLRERE6peKTJl+EThW4vhYUVuZjDEO4G/AQKATMNIY06nkNdbacdbaJGttEvBX4IMSp98EnrbWxgHdgL1F7dOBmdba9sBBCot+ST0TFNyAK4e11y+vcskZccUI4kPimb5qeqUTyKoqsAX8acWf+GD7B4xxjeFe970e7d/ldAGwIadyI73+DRoQ3imBzLR1HolDxflERETkfBUqqmVLzN2z1hYA/hW4rxuQYa3dYa09A7wD3FDG9SOBtwGKEmd/a+3iomces9aeMIVzZ38KvFd0zxvAjRWIRUSkTnD4OZjYcyIHTh3ghbUveP151lqmrpjKe9ve487EOxmbNNbjyxQSQxMxmEoX1gKIdidz8LssDu/d49GYRERERKBiCfEOY8zvjTEBRT/3ARXZP6MNsLvEcVZR2wWMMW2BaGBJUVNH4JAx5gNjzDpjzNNFI84hwCFrbV55fYqI1FXxIfGMjB3Ju1vfZX3Oeq89x1rL1G+m8u62d7kt4TZ+3/n3Xlmz3zigMe1btCdtX+UT4rZV2H5JREREpKIqkhDfA/QCsot+ugPVr7RyrhHAe9bas9s7+QNXAw8AXYF2wK2V6dAYM8YYs9oYszonJ8eTsYqIeN3YpLE4A51MWTGFvIK88m+oJGstT618iv/d+r/c0ukWxiWP82oBO7fTTXpOOgW2clWeW7YOJ9gZRmY11xGLiIiIlKYiVab3WmtHWGvDin5+Za3dW959FCbPESWOw4vaSjOCounSRbKA1KLp1nnAP4BkYD/Q3Bhzdsr2Rfu01r5ire1ire3idDorEK6ISO3RpEETJnSbwJYDW3h7y9vl31AJ1lpmrJrB37f8ndFxo7m/y/1er+budro5euYomYczK3WfMYYoVzK7NqSRn+f5fxgQERGRS9tFE2JjzF3GmA5Fn40xZo4x5rAxJt0Yk1yBvlcBHYqqQjegMOldUMpzYoEWwNfn3dvcGHM2k/0psKloLfNSYHhR+y3ARxWIRUSkzunbti9Xt7maWetm8cPxHzzSp7WWZ1Y/w1ub32JU3Cge6vpQjWxtdrawVlXWEUclJXPm5Em+37bF02GJiIjIJa6sEeL7gMyizyMBN4VTl8cDz5fXcdHI7ljgU2Az8K61dqMxZoox5voSl44A3jmvcFc+hdOl/22MWQ8Y4H+KTk8AxhtjMihcUzy7vFhEROoiYwyPdH+EAlvA9JXTq92ftZaZa2fyxqY3GBk7kgldJ9TYPt9RwVEENwiuUkIcmeDGz+Go9vZLIiIiIucrq1p0nrU2t+jzz4E3rbX7gc+MMTMq0rm19hPgk/PaJp13PPki9y4GXKW076CwgrWISL0X3jScu9138/za5/li9xdcE3FNlfqx1vLCuhd4bcNr/PKKX/LHbn+ssWQYwM/4kehMJH1feqXvbRjUmFYdYslMW8vVI2/xQnQiIiJyqSprhLjAGNPKGNMI6AN8VuJcoHfDEhGRs26Jv4X2zdsz9ZupnMg9Uen7rbXMSp3Fq+tfZXjH4TzS/ZEaTYbPcjvdZBzM4NiZY+VffJ7opBT27vwvxw8d9EJkIiIicqkqKyGeBKymcNr0AmvtRgBjzDVUbNslERHxgAC/ACb2mMj3x7/npfSXKn3/i2kv8kr6KwzrMIyJPSbiZyqywYDnuUPdWCzr91V+K6moou2Xvk1f5+mwRERE5BJ20d+KrLUfA22BOGvtXSVOrQZ+6e3ARETkR8mXJTO0w1DmbpzLtoPbKnzfS2kv8WLai9zY/kYm9Zzks2QYINGZiMGQnlP5adNhUe0IDG6m/YhFRETEo8r8zcham2etPXhe23FrbeXnu4mISLWMSx5H0wZN+dPXf6rQfr6vpL/C31L/xvUx1zO552SfJsMATRs0JaZ5TJUKaxk/P6LcyWSmrcUWVG4vYxEREZGL8e1vRyIiUmHNGzXn/i73k5qTygfbPyjz2lfXv8pf1/2Vn7f7OVN6TcHh56ihKMvmcrpI35dOiY0FKizKnczJo0fYm6lVOyIiIuIZSohFROqQ62Oup8tlXZi5Zib7T+4v9ZrXNrzG82ufZ1D0IP585Z9rTTIMhYW1Dp8+zLdHvq30vVGuzgDsTNX2SyIiIuIZVUqIjTGxng5ERETKZ4xhYs+JnMg7wTOrn7ng/Bsb3+DZNc8yMGogU6+aWquSYQBXaOFuelWZNh3UrDlh0TFkaj9iERER8ZCqjhAv8mgUIiJSYe2ateP2hNv5545/8s333xS3z900l7+s/gv92vbjyaufxN+vrK3mfaNd83Y0DWhapcJaULj90nfbtnD6xHEPRyYiIiKXoosmxMaYFy7y81egeQ3GKCIi57kr8S4imkbw5xV/5kz+GeZtnseMVTPo27YvT/V+qlYmwwB+xo9EZ2KVRogBolzJ2IICdq2v2v0iIiIiJZU1QnwbsAFYc97PauCM90MTEZGLaeTfiMe6P0bmkUzuXnw3T618ip9G/JTpvacT4Bfg6/DK5HK62H5oOydyT1T63lYdY2kQGKjtl0RERMQjyhpCWAVssNZ+df4JY8xkr0UkIiIV0qtNLwZGDeRfmf/iJxE/4S/X/KXWJ8NQWFirwBawYd8GurXqVql7Hf7+RCYksTNtDdZajDFeilJEREQuBWUlxMOBU6WdsNZGeyccERGpjEd7PErXVl25IeYGAhy1PxkGSAxNBAoLa1U2IYbC7ZcyVn3Nge+yCGkT4enwRERE5BJS1pTpJtbays9nExGRGtOsYTNu6ngTDRwNfB1KhTVr2IzoZtFVLqwV5U4GIDNV06ZFRESkespKiP9x9oMx5v0aiEVERC4RbqebtJw0rLWVvrdZ2GW0aB2u7ZdERESk2spKiEsuzGrn7UBEROTS4XK6OHj6IFlHs6p0f7Q7maxNG8g9c9rDkYmIiMilpKyE2F7ks4iISLW4nW4AUnNSq3R/VFIKeblnyN60wZNhiYiIyCWmrITYbYw5Yow5CriKPh8xxhw1xhypqQBFRKT+iWkWQ+OAxlXejzg8Lh5HQACZ6VpHLCIiIlV30YTYWuuw1gZba5taa/2LPp89Dq7JIEVEpH5x+DlICE2ocmGtgIaNCI9LYKcKa4mIiEg1lDVCLCIi4jVup5ttB7dxIrdqGxpEuZM5kL2bI/v2ejgyERERuVQoIRYREZ9wO93k23w27d9Upfujk1IAyEzTKLGIiIhUjRJiERHxCVeoC6DK64hbtomgSUio9iMWERGRKlNCLCIiPtG8UXPaBretckJsjCHancy361PJz8vzcHQiIiJyKVBCLCIiPuN2uknPScfaqu3uF5WUwpmTJ/g+Y6uHIxMREZFLgRJiERHxGVeoi/2n9pN9LLtK90cmuDF+fpo27WXGmAHGmK3GmAxjzMNlXDfMGGONMV1qMj4REZGqUkIsIiI+4w5zA1R5+6VGjZvQqkOsCmt5kTHGAfwNGAh0AkYaYzqVcl1T4D7gm5qNUEREpOqUEIuIiM+0b96eQP/AKq8jBohyd2bPzgxOHDnswcikhG5AhrV2h7X2DPAOcEMp1/0JmA6cqsngREREqkMJsYiI+Iy/nz8JoQnVSoij3SlgLd+mr/NgZFJCG2B3ieOsorZixphkIMJa+381GZiIiEh1KSEWERGfcjvdbD2wlVN5VRtYvKxdewKbBpOZusbDkUlFGGP8gGeB+ytw7RhjzGpjzOqcnBzvByciIlIOJcQiIuJTrlAXeTaPTfs3Vel+4+dHW1dnMtPXYQsKPBydANlARInj8KK2s5oCCcDnxphMoAewoLTCWtbaV6y1Xay1XZxOpxdDFhERqRivJsTlVaU0xsw0xqQW/WwzxhwqcS6/xLkFJdpfN8bsLHEuyZvvICIi3uVyuoCqF9YCiHInc+LwIfZ+u9NTYcmPVgEdjDHRxpgGwAig+O9la+1ha22otTbKWhsFrACut9au9k24IiIiFefvrY5LVKXsS+F6o1XGmAXW2uIhAGvtuBLX/w7oXKKLk9baiyW7D1pr3/NC2CIiUsNCAkOIaBpRzcJayQBkpq7hsugYT4UmgLU2zxgzFvgUcABzrLUbjTFTgNXW2gVl9yAiIlJ7eXOEuKJVKc8aCbztxXhERKSWcjldpOWkYa2t0v2Nm7fAGdWOzHRtv+QN1tpPrLUdrbUx1tqpRW2TSkuGrbU/0eiwiIjUFd5MiMutSnmWMaYtEA0sKdHcqKjwxgpjzI3n3TLVGJNeNOW64UX69GzhjtjrYPhrYBzV70tERM7hdrrJOZnDD8d/qHIfUe5kvtu6mdMnTngwMhEREanPaktRrRHAe9ba/BJtba21XYBfAc8ZY87OgfsjEAt0BVoCE0rr0OOFO8JiIWEo+NWWr0xEpP44u464etsvJVOQn8/ujVVfiywiIiKXFm9md+VVpSxpBOdNl7bWZhf9uQP4nKL1xdba722h08BrFE7NFhGROqxji440cjSqVkLc+oo4AhoFkpmm7ZdERESkYryZEJdZlfIsY0ws0AL4ukRbi7NToY0xocCVwKai41ZFfxrgRmCDF99BRERqQIBfAPGh8dWqNO3wDyAywcXO1LVVXossIiIilxavJcTW2jzgbFXKzcC7Z6tSGmOuL3HpCOAde+5vL3HAamNMGrAUeKpEdep5xpj1wHogFPizt95BRERqjsvpYtOBTZzOP13lPqLcKRzJ2cPB77/zYGQiIiJSX3lt2yUorEoJfHJe26TzjieXct9XQOJF+vypB0MUEZFawu1081rBa2zev5mksKptMV+8/VLaGlq2LrWOo4iIiEgxVYgSEZFawe10A9UrrNX8sstp0ao1mWnafklERETKp4RYRERqhdDAUNo0aVOtdcRQOG1698b15J0546HIREREpL5SQiwiIrWGy+mq1ggxFE6bzjtzmqwtGz0UlYiIiNRXXl1DXJvl5uaSlZXFqVOnfB1KvdOoUSPCw8MJCAjwdSgiUse4nW7+tfNf/HD8By5vfHmV+ojolIjD35/MtLVEuTp7OEIRERGpTy7ZhDgrK4umTZsSFRVF4Q5O4gnWWvbv309WVhbR0dG+DkdE6piz64jTc9KrnBAHNGpEm9h4MlPXwK/v8GR4IiIiUs9cslOmT506RUhIiJJhDzPGEBISopF3EamSK1pcQUNHw+pPm05KYX/WLo7u3+ehyERERKQ+umQTYkDJsJfoexWRqgpwBNAppJMHCmud3X5J1aZFRETk4i7phFhERGofV6iLTfs3kZufW+U+QiPa0qRFSyXEIiIiUiYlxB7QpEkTX4dQLS+88AJxcXGMGjXK16GIiOAOc3Om4AxbDmypch/GGKKSUvh2/ToK8vM9GJ2IiIjUJ0qIazlrLQUFBV59xv/7f/+PxYsXM2/ePK8+R0SkIs4W1vLE9kunjx/n+4xtnghLRERE6iElxB507Ngx+vTpQ3JyMomJiXz00UcATJo0ieeee674ukcffZTnn38egKeffpquXbvicrl4/PHHAcjMzOSKK67g5ptvJiEhgd27d5f6vIULF5KcnIzb7aZPnz4AHDhwgBtvvBGXy0WPHj1ITy9chzd58mRuv/12fvKTn9CuXTteeOEFAO655x527NjBwIEDmTlzpne+GBGRSggLCuPyxpdXOyGOTEzCGD9NmxYREZGLumS3XfKGRo0a8eGHHxIcHMy+ffvo0aMH119/PbfffjtDhw7lD3/4AwUFBbzzzjusXLmSRYsWsX37dlauXIm1luuvv55ly5YRGRnJ9u3beeONN+jRo0epz8rJyeGuu+5i2bJlREdHc+DAAQAef/xxOnfuzD/+8Q+WLFnCzTffTGpqKgBbtmxh6dKlHD16lCuuuIJ7772Xl156iYULF7J06VJCQ0Nr7LsSESmL2+mudmGtwCZNubx9BzLT1nDlL7QkRERERC6khNiDrLU88sgjLFu2DD8/P7Kzs9mzZw9RUVGEhISwbt069uzZQ+fOnQkJCWHRokUsWrSIzp07A4UjzNu3bycyMpK2bdteNBkGWLFiBb179y7e67dly5YALF++nPfffx+An/70p+zfv58jR44AMHjwYBo2bEjDhg0JCwtjz549hIeHe/MrERGpEleoi08zPyXnRA7OIGeV+4lyp/D1+29z8ugRApsGezBCERERqQ+UEHvQvHnzyMnJYc2aNQQEBBAVFVW8H++dd97J66+/zg8//MDtt98OFCbQf/zjH7n77rvP6SczM5PGjRt7PL6GDRsWf3Y4HOTl5Xn8GSIinuAOK1xHnJ6TTp+2farcT3RSCl+/93e+TV9H7JXXeCo8ERERqSe0htiDDh8+TFhYGAEBASxdupRvv/22+NyQIUNYuHAhq1aton///gD079+fOXPmcOzYMQCys7PZu3dvhZ7Vo0cPli1bxs6dOwGKp0xfffXVxcWxPv/8c0JDQwkO1qiIiNQtcS3jCPALqPY64sti2tOoSVOtIxYREZFSaYTYg0aNGsV1111HYmIiXbp0ITY2tvhcgwYNuPbaa2nevDkOhwOAfv36sXnzZnr27AkUbt/01ltvFZ8vi9Pp5JVXXmHo0KEUFBQQFhbG4sWLi4tnuVwugoKCeOONN7zzsiIiXtTA0YC4kLhqJ8R+fg7aJiaRmbYWay3GGA9FKCIiIvWBsdb6Ogav69Kli129evU5bZs3byYuLq7GYigoKCA5OZn58+fToUOHGnuur9T09ysi9c+MVTN4d+u7fP2rrwnwC6hyPxuWLubTl57n19NfICyqnUdiM8assdZ28Uhnl6jS/m4WERGpqqr+3awp0zVg06ZNtG/fnj59+lwSybCIiCe4nC5O559m24Hq7SMc5U4G0LRpERERuYCmTNeATp06sWPHjirf3717d06fPn1O29y5c0lMTKxuaCIitVaSMwmAtJw04kPjq9xPk5YhhEZGkZm2lm43DPdUeCIiIlIPKCGuA7755htfhyAiUuMub3w5YUFhpOWk8au4X1Wrryh3MmmLPiHvzBn8GzTwUIQiIiJS1ykhFhGRWsvtdFe7sBZA1+uH0XPYCCXDIiIicg6tIRYRkVrL7XSTfSybfSf3VaufoOBmNAgM8lBUIiIiUl8oIRYRkVrL5XQBkJ6T7uNIREREpD5SQiwiIrVWp5BO+Pv5KyEWERERr1BC7COZmZkEBgaSlJRU3BYVFUViYiJJSUl06fLjFlrz588nPj4ePz8/Su7ZuHjxYlJSUkhMTCQlJYUlS5aU+9wHH3yQ2NhYXC4XQ4YM4dChQxfEk5SUxD333FN8z5kzZxgzZgwdO3YkNjaW999/H4CZM2cSGRnJ2LFjq/19iIiUpqGjIXEt4zyyjlhERETkfCqq5UMxMTGkpqae07Z06VJCQ0PPaUtISOCDDz7g7rvvPqc9NDSUf/7zn7Ru3ZoNGzbQv39/srOzy3xm3759mTZtGv7+/kyYMIFp06Yxffr0i8YDMHXqVMLCwti2bRsFBQUcOHAAgHHjxtGiRYtzknQREU9zOV18sP0D8gry8PfTX1siIiLiOfrNAnjinxvZ9N0Rj/bZqXUwj19X9X0zS4qLiyu1vXPnzsWf4+PjOXnyJKdPn6Zhw4YX7atfv37Fn3v06MF7771X7vPnzJnDli1bAPDz87sgYRcR8Sa30828zfPYfnA7cSGl//+hiIiISFVoynQtYoyhX79+pKSk8Morr1Tq3vfff5/k5OQyk+HzzZkzh4EDBxYf79y5k86dO7DtX+AAACAASURBVHPNNdfw5ZdfAhRPqZ44cSLJycncdNNN7Nmzp1KxiYhUx9nCWpo2LSIiIp7m1RFiY8wA4HnAAbxqrX3qvPMzgWuLDoOAMGtt86Jz+cD6onO7rLXXF7VHA+8AIcAa4NfW2jPVidNTI7nVtXz5ctq0acPevXvp27cvsbGx9O7du9z7Nm7cyIQJE1i0aFGFnzV16lT8/f0ZNWoUAK1atWLXrl2EhISwZs0abrzxRjZu3EheXh5ZWVn06tWLZ599lmeffZYHHniAuXPnVvk9RUQqo3Xj1oQGhpKek86I2BG+DkdERETqEa+NEBtjHMDfgIFAJ2CkMaZTyWusteOstUnW2iTgr8AHJU6fPHvubDJcZDow01rbHjgI3OGtd6hpbdq0ASAsLIwhQ4awcuXKcu/JyspiyJAhvPnmm8TExFToOa+//joff/wx8+bNwxgDQMOGDQkJCQEgJSWFmJgYtm3bRkhICEFBQQwdOhSAm266ibVr11bl9UREqsQYgyvUpRFiERER8ThvTpnuBmRYa3cUjeC+A9xQxvUjgbfL6tAUZm8/Bc4ufH0DuNEDsfrc8ePHOXr0aPHnRYsWkZCQUOY9hw4dYvDgwTz11FNceeWV55y7+eabS02oFy5cyIwZM1iwYAFBQUHF7Tk5OeTn5wOwY8cOtm/fTrt27TDGcN111/H5558D8O9//5tOnTpd0K+IiDe5w9zsOrqLA6cO+DoUERERqUe8mRC3AXaXOM4qaruAMaYtEA2U3DeokTFmtTFmhTHmbNIbAhyy1uaV12dds2fPHq666ircbjfdunVj8ODBDBgwAIAPP/yQ8PBwvv76awYPHkz//v0BmDVrFhkZGUyZMqV4u6S9e/cCkJ6eTuvWrS94ztixYzl69Ch9+/Y9Z3ulZcuW4XK5SEpKYvjw4bz00ku0bNkSgOnTpzN58mRcLhdz587lmWeeqYmvRESkmNvpBmB9zvpyrhQRERGpuNpSZXoE8J61Nr9EW1trbbYxph2wxBizHjhc0Q6NMWOAMQCRkZEeDdYb2rVrR1pa6dMBhwwZwpAhQy5of+yxx3jssccuaD9y5AgdOnQgPDz8gnMZGRmlPmPYsGEMGzas1HNt27Zl2bJlZYUvIuJVnUI64W/8SctJ45qIa3wdjoiIiNQT3hwhzgYiShyHF7WVZgTnTZe21mYX/bkD+BzoDOwHmhtjzibyF+3TWvuKtbaLtbaL0+ms6jt4jcPh4PDhwyQlJXm87+DgYObPn+/xfs83c+ZMpk2bRnBwsNefJSKXtkD/QDq27Eh6TrqvQxEREZF6xJsJ8SqggzEm2hjTgMKkd8H5FxljYoEWwNcl2loYYxoWfQ4FrgQ2WWstsBQYXnTpLcBHXnwHr4mIiGD37t2kpqb6OpQqGzduHFu3buXJJ5/0dSgicglwhbpYv289+QX55V8sIiIiUgFeS4iL1vmOBT4FNgPvWms3GmOmGGNKVo0eAbxTlOyeFQesNsakUZgAP2Wt3VR0bgIw3hiTQeGa4tneegcREak93GFuTuSdIONQ6Us/RERERCrLq2uIrbWfAJ+c1zbpvOPJpdz3FZB4kT53UFjBWkRELiFnC2ul5aRxRcsrfByNiIiI1AfenDItIiLiMeFNwmnZqKX2IxYRERGPUUIsIiJ1gjEGl9OlwloiIiLiMUqIfSQzM5PAwMDiKtO7d+/m2muvpVOnTsTHx/P8888XXzt58mTatGlTvNfwJ5/8OAs9PT2dnj17Eh8fT2JiIqdOnSrzuQ8++CCxsbG4XC6GDBnCoUOHLoin5P7EAGfOnGHMmDF07NiR2NhY3n//faCwynRkZCRjx4712PciIlIWt9NN5pFMDp+u8C58IiIiIhdVW/YhviTFxMQUV5n29/fnmWeeITk5maNHj5KSkkLfvn3p1KkTUFjR+YEHHjjn/ry8PEaPHs3cuXNxu93s37+fgICAMp/Zt29fpk2bhr+/PxMmTGDatGlMnz79gnhKmjp1KmFhYWzbto2CggIOHDhQHFOLFi1YvXp1tb8LEZGKKLmOuHd4bx9HIyIiInWdEmKAfz0MP6z3bJ+XJ8LApyp8eatWrWjVqhUATZs2JS4ujuzs7OKEuDSLFi3C5XLhdhf+ghgSElLuc/r161f8uUePHrz33nvl3jNnzhy2bNkCgJ+fH6GhoeXeIyLiDfEh8fgZP9Jz0pUQi4iISLVpynQtlJmZybp16+jevXtx26xZs3C5XNx+++0cPHgQgG3btmGMoX///iQnJzNjxoxKPWfOnDkMHDiw+Hjnzp107tyZa665hi+//BKgeEr1xIkTSU5O5qabbmLPnj3VfUURkSoJCgiiY4uOKqwlIiIiHqERYqjUSK63HTt2jGHDhvHcc88RHBwMwL333svEiRMxxjBx4kTuv/9+5syZQ15eHsuXL2fVqlUEBQXRp08fUlJS6NOnT7nPmTp1Kv7+/owaNQooHKHetWsXISEhrFmzhhtvvJGNGzeSl5dHVlYWvXr14tlnn+XZZ5/lgQceYO7cuV79HkRELsbtdPPxjo/JL8jH4efwdTgiIiJSh2mEuBbJzc1l2LBhjBo1iqFDhxa3X3bZZTgcDvz8/LjrrrtYuXIlAOHh4fTu3ZvQ0FCCgoIYNGgQa9euLfc5r7/+Oh9//DHz5s3DGANAw4YNi6dcp6SkEBMTw7Zt2wgJCSEoKKg4nptuuqlCzxAR8RaX08Xx3OPsOLzD16GIiIhIHaeEuJaw1nLHHXcQFxfH+PHjzzn3/fffF3/+8MMPSUhIAKB///6sX7+eEydOkJeXxxdffFG85vjmm28uTpxLWrhwITNmzGDBggUEBQUVt+fk5JCfnw/Ajh072L59O+3atcMYw3XXXcfnn38OwL///e8y1zWLiHhbycJaIiIiItWhKdO1xH/+8x/mzp1LYmJi8VZMTz75JIMGDeKhhx4iNTUVYwxRUVG8/PLLALRo0YLx48fTtWtXjDEMGjSIwYMHA4XbMbVu3fqC54wdO5bTp0/Tt29foLCw1ksvvcSyZcuYNGkSAQEB+Pn58dJLL9GyZUsApk+fzq9//Wv+8Ic/4HQ6ee2112riKxERKVVk00iaN2xOek46wzsO93U4IiIiUocpIa4lrrrqKqy1pZ4ra73u6NGjGT169DltR44coUOHDoSHh19wfUZGRqn9DBs2jGHDhpV6rm3btixbtuyiMYiI1CRjDC6nSyPEIiIiUm2aMu0jDoeDw4cPF48Ge1JwcDDz58/3eL/nmzlzJtOmTSsu/iUiUlPcTjc7Du/gyJkjvg5FRERE6jAlxD4SERHB7t27SU1N9XUoVTZu3Di2bt3Kk08+6etQROQS43K6AFif4+E95KVUxpgBxpitxpgMY8zDpZwfb4zZZIxJN8b82xjT1hdxioiIVJYSYhERqXMSQxMxGNJz0n0dSr1njHEAfwMGAp2AkcaY86srrgO6WGtdwHvAjJqNUkREpGqUEIuISJ3TOKAx7Vu01zrimtENyLDW7rDWngHeAW4oeYG1dqm19kTR4QrgwiIWIiIitZASYhERqZPcTjfpOekU2AJfh1LftQF2lzjOKmq7mDuAf3k1IhEREQ9RQiwiInWS2+nmaO5RMg9n+joUKWKMGQ10AZ6+yPkxxpjVxpjVOTk5NRuciIhIKZQQ+1BmZiaBgYHFlaZvv/12wsLCSEhIOOe6Bx98kNjYWFwuF0OGDOHQoUMA5Obmcsstt5CYmEhcXBzTpk0D4OTJkyQlJdGgQQP27dtXsy8lIlJDzhbW0rRpr8sGIkochxe1ncMY8zPgUeB6a+3p0jqy1r5ire1ire3idDq9EqyIiEhlKCH2sZiYmOJK07feeisLFy684Jq+ffuyYcMG0tPT6dixY3HiO3/+fE6fPs369etZs2YNL7/8cnGSnZqaSuvWrWv0XUREalJUcBTBDYKVEHvfKqCDMSbaGNMAGAEsKHmBMaYz8DKFyfBeH8QoIiJSJf6+DqA2mL5yOlsObPFon7EtY5nQbUKl7unduzeZmZkXtPfr16/4c48ePXjvvfcAMMZw/Phx8vLyOHnyJA0aNNCewCJyyfAzfiQ6E5UQe5m1Ns8YMxb4FHAAc6y1G40xU4DV1toFFE6RbgLMN8YA7LLWXu+zoEVERCpICXEdM2fOHH75y18CMHz4cD766CNatWrFiRMnmDlzJi1btvRxhCIiNcftdPNi9oscPXOUpg2a+jqcesta+wnwyXltk0p8/lmNByUiIuIBSoih0iO5vjJ16lT8/f0ZNWoUACtXrsThcPDdd99x8OBBrr76an72s5/Rrl07H0cqIlIz3E43FsuGfRvo2bqnr8MRERGROkZriOuI119/nY8//ph58+ZRNB2Nv//97wwYMICAgADCwsK48sorWb16tY8jFRGpOYmhiRiMpk2LiIhIlSghrgMWLlzIjBkzWLBgAUFBQcXtkZGRLFmyBIDjx4+zYsUKYmNjfRWmiEiNa9qgKTHNY0jPSfd1KCIiIlIHKSGuRUaOHEnPnj3ZunUr4eHhzJ49G4CxY8dy9OhR+vbtS1JSEvfccw8Av/3tbzl27Bjx8fF07dqV2267DZfL5ctXEBGpcS6ni/R96VhrfR2KiIiI1DFaQ1yLvP3226W2Z2RklNrepEkT5s+f782QRERqPbfTzQfbPyDzSCbRzaJ9HY6IiIjUIRoh9iGHw8Hhw4dJSkryaL8nT54kKSmJ3Nxc/Pz0P7GI1G9upxtA06ZFRESk0jRC7EMRERHs3r3b4/0GBgaSmprq8X5FRGqj6GbRNA1oSlpOGje0v8HX4YiIiEgdouFDERGp0/yMH4nORI0Qi4iISKV5NSE2xgwwxmw1xmQYYx4u5fxMY0xq0c82Y8yh884HG2OyjDGzSrR9XtTn2fvCvPkOIiJS+7mcLrYf2s7x3OO+DkVERETqEK9NmTbGOIC/AX2BLGCVMWaBtXbT2WusteNKXP87oPN53fwJWFZK96OstdpwV0REgMJ1xAW2gA37NtC9VXdfhyMiIiJ1hDdHiLsBGdbaHdbaM8A7QFmLu0YCxWWWjTEpwGXAIi/GKCIi9UBiaCKgwloiIiJSOd5MiNsAJStGZRW1XcAY0xaIBpYUHfsBzwAPXKTv14qmS080xpiL9DnGGLPaGLM6Jyenqu/gNZmZmQQGBp5TYToqKorExESSkpLo0qVLcfv8+fOJj4/Hz8+P1at/HBhfvHgxKSkpJCYmkpKSwpIlS8p9blX6evvtt0lMTMTlcjFgwAD27dsHwIMPPsjll1/OX/7yl2p9FyIi1dWsYTOim0WTlpPm61BERESkDqktVaZHAO9Za/OLjn8DfGKtzSol3x1lrc02xjQF3gd+Dbx5/kXW2leAVwC6dOlivRZ5NcTExFxQDXrp0qWEhoae05aQkMAHH3zA3XfffU57aGgo//znP2ndujUbNmygf//+ZGdnl/nMyvaVl5fHfffdx6ZNmwgNDeWhhx5i1qxZTJ48maeffprGjRtX4xsQEfEct9PNF7u/wFrLRf6tVEREROQc3kyIs4GIEsfhRW2lGQH8tsRxT+BqY8xvgCZAA2PMMWvtw9babABr7VFjzN8pnJp9QUJcGT88+SSnN2+pThcXaBgXy+WPPOKRvuLi4kpt79z5xyXX8fHxnDx5ktOnT9OwYUOP9eXn54e1luPHjxMSEsKRI0do3759Fd9ERMR7XE4X/8j4B7uP7iYyONLX4YiIiEgd4M0p06uADsaYaGNMAwqT3gXnX2SMiQVaAF+fbbPWjrLWRlproyicNv2mtfZhY4y/MSa06L4A4OfABi++Q40yxtCvXz9SUlJ45ZVXKnXv+++/T3JycpnJcFX6CggI4MUXXyQxMZHWrVuzadMm7rjjjmo/Q0TE09xON4CmTYuIiEiFeW2E2FqbZ4wZC3wKOIA51tqNxpgpwGpr7dnkeATwjrW2ItOaGwKfFiXDDuAz4H+qG6unRnKra/ny5bRp04a9e/fSt29fYmNj6d27d7n3bdy4kQkTJrBoUfXrj53fV25uLi+++CLr1q2jXbt2/O53v2PatGk89thj1X6WiIgnxTSLoXFAY9Jy0rgu5jpfhyMiIiJ1gFfXEFtrPwE+Oa9t0nnHk8vp43Xg9aLPx4EUT8ZYm7RpU1hzLCwsjCFDhrBy5cpyE+KsrCyGDBnCm2++SUxMTLWeX1pfZ9c4nz3+xS9+wVNPPVWt54iIeIPDz0FCaIIqTYuIiEiFeXPKtFTC8ePHOXr0aPHnRYsWkZCQUOY9hw4dYvDgwTz11FNceeWV55y7+eabWblyZYWff7G+2rRpw6ZNmzhbqXvx4sUXXYcsIuJrbqebbQe3cSL3hK9DERERkTpACXEtsWfPHq666ircbjfdunVj8ODBDBgwAIAPP/yQ8PBwvv76awYPHkz//v0BmDVrFhkZGUyZMoWkpCSSkpLYu3cvAOnp6bRu3fqC51S2r9atW/P444/Tu3dvXC4XqampPFJLppiLiJzP7XSTb/PZuH+jr0MRERGROqC2bLt0yWvXrh1paaUXghkyZAhDhgy5oP2xxx4rdS3vkSNH6NChA+Hh4dXuC+Cee+7hnnvuKe8VRER8zhXqAgoLa3W9vKuPoxEREZHaTiPEPuJwODh8+DBJSUke7zs4OJj58+d7vN/zPfjgg7z11lvai1hEao3mjZrTNrit1hGLiIhIhWiE2EciIiLYvXu3r8Oolqeffpqnn37a12GIiJzD7XSzPHs51lqMMb4OR0RERGoxjRCLiEi94na6OXDqANnHsn0dioiIiNRySohFRKRecTl/XEcsIiIiUhYlxCIiUq+0b96eQP9AJcQiIiJSLiXEIiJSr/j7+ZMQmqDCWiIiIlIuJcQ+kpmZSWBgYHGV6d27d3PttdfSqVMn4uPjef7554uvnTx5Mm3atCneH/iTTz4pPpeenk7Pnj2Jj48nMTGRU6dOlfnciRMn4nK5SEpKol+/fnz33XcAzJs3D5fLRWJiIr169SreAurkyZMkJSXRoEED9u3b5+mvQUTEK9xON1sPbOVUXtn/nygiIiKXNiXEPhQTE0NqaioA/v7+PPPMM2zatIkVK1bwt7/9jU2bNhVfO27cOFJTU0lNTWXQoEEA5OXlMXr0aF566SU2btzI559/TkBAQJnPfPDBB0lPTyc1NZWf//znTJkyBYDo6Gi++OIL1q9fz8SJExkzZgwAgYGBpKam0rp1a298BSIiXuF2usmzeWzav6n8i0VEROSSpW2XgC/f3ca+3cc82mdoRBOu/kXHCl/fqlUrWrVqBUDTpk2Ji4sjOzubTp06XfSeRYsW4XK5cLvdAISEhJT7nODg4OLPx48fL96SpFevXsXtPXr0ICsrq8Kxi4jUNomhiUBhYa3ky5J9HI2IiIjUVhohroUyMzNZt24d3bt3L26bNWsWLpeL22+/nYMHDwKwbds2jDH079+f5ORkZsyYUaH+H330USIiIpg3b17xCHFJs2fPZuDAgZ55GRERHwgJDCGiaYQKa4mIiEiZNEIMlRrJ9bZjx44xbNgwnnvuueLR3HvvvZeJEydijGHixIncf//9zJkzh7y8PJYvX86qVasICgqiT58+pKSk0KdPnzKfMXXqVKZOncq0adOYNWsWTzzxRPG5pUuXMnv2bJYvX+7V9xQR8TaX08U333+DtbZ4NoyIiIhISRohrkVyc3MZNmwYo0aNYujQocXtl112GQ6HAz8/P+666y5WrlwJQHh4OL179yY0NJSgoCAGDRrE2rVrK/y8UaNG8f777xcfp6enc+edd/LRRx9VaPq1iEht5na62XdyH98f/97XoYiIiEgtpYS4lrDWcscddxAXF8f48ePPOff99z/+Mvfhhx+SkJAAQP/+/Vm/fj0nTpwgLy+PL774onjN8c0331ycOJe0ffv24s8fffQRsbGxAOzatYuhQ4cyd+5cOnasPSPmIiJV5XK6ALT9koiIiFyUpkzXEv/5z3+YO3cuiYmJxVsxPfnkkwwaNIiHHnqI1NRUjDFERUXx8ssvA9CiRQvGjx9P165dMcYwaNAgBg8eDBSO9pZWGfrhhx9m69at+Pn50bZtW1566SUApkyZwv79+/nNb34DFFa9Xr16dU28uoiIV3Rs0ZFGjkak5aQxIHqAr8MRERGRWkgJcS1x1VVXYa0t9dzcuXMvet/o0aMZPXr0OW1HjhyhQ4cOhIeHX3B9ySnSJb366qu8+uqrlYhYRKR2C/ALID40XoW1RERE5KI0ZdpHHA4Hhw8fLh4N9qTg4GDmz5/vkb5OnjxJUlISubm5+PnpPxcRqVtcThebD2zmdP5pX4ciIiIitZAyHB+JiIhg9+7dpKam+jqUMgUGBpKamkp2djYtW7b0dTgiIpXidrrJK8hj8/7Nvg5FREREaiElxCIiUm+5nW4ATZsWERGRUikhFhGReis0MJQ2TdooIRYREZFSKSEWEZF6zeV0aeslERERKZUSYhERqdfcTjd7Tuzhh+M/+DoUERERqWWUEPtIZmYmgYGB51SZvv322wkLCyMhIeGcax988EFiY2NxuVwMGTKEQ4cOAZCbm8stt9xCYmIicXFxTJs2rdznzpo1i/bt22OMYd++fcXt8+bNw+VykZiYSK9evUhL+3F64cyZM4mPjychIYGRI0dy6tQpAEaNGkXLli157733qvVdiIh4k9YRi4iIyMUoIfahmJiYc6pM33rrrSxcuPCC6/r27cuGDRtIT0+nY8eOxYnv/PnzOX36NOvXr2fNmjW8/PLLZGZmlvnMK6+8ks8++4y2bdue0x4dHc0XX3zB+vXrmThxImPGjAEgOzubF154gdWrV7Nhwwby8/N55513gMIk+vrrr6/OVyAi4nVXtLiCho6GmjYtIiIiF/D3dQC1wdLXX2Hvtzs82mdY23Zce+uYSt3Tu3fvUhPafv36FX/u0aNH8YisMYbjx4+Tl5fHyZMnadCgAcHBwWU+o3PnzqW29+rV65xnZGVlFR+f7T8gIIATJ07QunXryryWiIhPBTgC6BTSSSPEIiIicgGNENcxc+bMYeDAgQAMHz6cxo0b06pVKyIjI3nggQc8slfw7Nmzi5/Rpk0bHnjgASIjI2nVqhXNmjU7J0EXEakL3E43Ww5sIbcg19ehiIiISC3i1RFiY8wA4HnAAbxqrX3qvPMzgWuLDoOAMGtt8xLng4FNwD+stWOL2lKA14FA4BPgPmutrU6clR3J9ZWpU6fi7+/PqFGjAFi5ciUOh4PvvvuOgwcPcvXVV/Ozn/2Mdu3aVfkZS5cuZfbs2SxfvhyAgwcP8tFHH7Fz506aN2/OTTfdxFtvvcXo0aM98k4iIjXhlvhbuDPxTgL8AnwdioiIiNQiXhshNsY4gL8BA4FOwEhjTKeS11hrx1lrk6y1ScBfgQ/O6+ZPwLLz2l4E7gI6FP0M8EL4tc7rr7/Oxx9/zLx58zDGAPD3v/+dAQMGEBAQQFhYGFdeeSWrV6+u8jPS09O58847+eijjwgJCQHgs88+Izo6GqfTSUBAAEOHDuWrr77yyDuJiNSU0MBQmjVs5uswREREpJbx5pTpbkCGtXaHtfYM8A5wQxnXjwTePntQNBJ8GbCoRFsrINhau6JoVPhN4EZvBF+bLFy4kBkzZrBgwQKCgoKK2yMjI1myZAkAx48fZ8WKFcTGxgLQp08fsrOzK/yMXbt2MXToUObOnUvHjh3PecaKFSs4ceIE1lr+/e9/ExcX56E3ExERERER8R1vJsRtgN0ljrOK2i5gjGkLRANLio79gGeAB0rpM6vE8UX7rItGjhxJz5492bp1K+Hh4cyePRuAsWPHcvToUfr27UtSUhL33HMPAL/97W85duwY8fHxdO3aldtuuw2Xy0VBQQEZGRmlrid+4YUXCA8PJysrC5fLxZ133gnAlClT2L9/P7/5zW9ISkqiS5cuAHTv3p3hw4eTnJxMYmIiBQUFxRWoRURERERE6rLaUmV6BPCetTa/6Pg3wCfW2qyz04MryxgzBhgDhaOcdcHbb79dantGRkap7U2aNGH+/PkXtG/atIlhw4YRGBh4wbnf//73/P73v7+g/dVXX+XVV18t9TlPPPEETzzxRFmhi4iIiIiI1DneHCHOBiJKHIcXtZVmBCWmSwM9gbHGmEzgL8DNxpiniu4Pr0if1tpXrLVdrLVdnE5n1d7AixwOB4cPHyYpKcnjfSckJPDss896vN/zjRo1ii+++IJGjRp5/VkiIiIiIiKe5s0R4lVAB2NMNIVJ6wjgV+dfZIyJBVoAX59ts9aOKnH+VqCLtfbhouMjxpgewDfAzRQW46pzIiIi2L17d/kX1mLz5s3zdQgiIiIiIiJV5rURYmttHjAW+BTYDLxrrd1ojJlijLm+xKUjgHcqsXXSb4BXgQzgv8C/qhFjVW+VMuh7FRERERGRusCra4ittZ9QuFdwybZJ5x1PLqeP1yncd/js8WogobqxNWrUiP379xMSEkJV1ynLhay17N+/X9OoRURERESk1qstRbVq3NlKyzk5Ob4Opd5p1KgR4eHh5V8oIiIiIiLiQ5dsQhwQEEB0dLSvwxAREan1jDEDgOcBB/Cqtfap8843BN4EUoD9/7+9ew+2qqzDOP59BARMUhB1FDCgvDGYSOhIOWZqeMmRZkLBNDGZHLXMchpHR4cx+0un7OI1SsVKxcQuJzUVFceyQCC5G3JEChBFU9EmRcVff6z36GJzNmed276c/Xxm1px122u/6zlr7XXes971bmByRKytdDnNzMzaqzt7mTYzM7M6J6kXcCNwEjAKOEPSqJLVpgGvR8SngB8D11S2lGZmZh3jCrGZmZntyBFAc0SsiYh3gVnAxJJ1JgJ3pPHZwHFyBx1mZlYHXCE2MzOzHRkC5L8ncH2a1+o66VsmNgN7VKR0ZmZmndAQzxAvWrToED6kdAAACmNJREFUVUn/6oJNDQZe7YLt9HTOqRjnVIxzapszKqYrc/pEF22noUg6DzgvTW6RtLya5ekBfO53njPsPGfYec6waxzYkRc1RIU4Ivbsiu1IWhgR47piWz2ZcyrGORXjnNrmjIpxTh22ARiWmx6a5rW2znpJvYHdyDrX2kZEzABmgH8fXcEZdp4z7Dxn2HnOsGtIWtiR17nJtJmZme3IAmB/SSMk7QxMAZpK1mkCpqbxScDjEREVLKOZmVmHNMQdYjMzM+uYiHhf0reAh8m+dum2iFgh6WpgYUQ0AbcCv5bUDLxGVmk2MzOrea4Qt8+MahegTjinYpxTMc6pbc6oGOfUQRHxIPBgybzpufF3gNPauVn/PjrPGXaeM+w8Z9h5zrBrdChHuUWTmZmZmZmZNSI/Q2xmZmZmZmYNyRXigiSdKGmVpGZJl1W7PJUm6TZJm/JfkSFpkKQ5klannwPTfEn6WcpqqaSxuddMTeuvljS1tfeqV5KGSZoraaWkFZIuTvOdU46kfpKelrQk5fT9NH+EpPkpj3tS5z1I6pumm9Py4bltXZ7mr5J0QnX2qPtI6iXpGUn3p2lnVELSWknLJC1u6V3S51xtaev6uaPj1zIFMrwkXXuWSnpMkr8WrETRv+MkfUVSSHKPvyWKZCjp9NzfQXdVuoy1rsC5vF/6W/KZdD6fXI1y1jK1UicpWV72Wl9WRHhoYyDrROR5YCSwM7AEGFXtclU4g6OBscDy3LxrgcvS+GXANWn8ZODPgIAjgflp/iBgTfo5MI0PrPa+dWFG+wBj0/gA4DlglHPaLicBu6bxPsD8tP+/Baak+bcAF6TxC4Fb0vgU4J40Piqdi32BEekc7VXt/evirC4B7gLuT9POaPuM1gKDS+b5nKuRocj1s9zx66FdGX4B2CWNX+AM259hWm8A8CQwDxhX7XLX0lDwONwfeKbl8xPYq9rlrqWhYIYzctf2UcDaape71gZaqZOULG/1Wr+jwXeIizkCaI6INRHxLjALmFjlMlVURDxJ1nNo3kTgjjR+B/Dl3PxfRWYesLukfYATgDkR8VpEvA7MAU7s/tJXRkRsjIh/pPG3gGeBITinbaT9/W+a7JOGAI4FZqf5pTm15DcbOE6S0vxZEbElIl4AmsnO1R5B0lDgS8Av07RwRkX5nKsdRa6f5Y5fy7SZYUTMjYj/pcl5ZN8VbR8p+nfcD4BrgHcqWbg6USTDbwA3ps9RImJThctY64pkGMDH0/huwIsVLF9dKFMnySt3rS/LFeJihgDrctPr07xGt3dEbEzjLwF7p/FyeTVMjqnJ32Fkdz+dU4nUFHgxsIms8vE88EZEvJ9Wye/zh3mk5ZuBPej5Of0EuBT4IE3vgTNqTQCPSFok6bw0z+dc7SiSbbnj1zLtPT6nkd0dsY+0mWFqVjksIh6oZMHqSJHj8ADgAElPSZonyf9Y3FaRDK8CzpK0nqxn/4sqU7Qepd3XdH/tknWJiAhJ7rIckLQrcB/wnYh4M3+jwzllImIrMEbS7sDvgYOqXKSaIukUYFNELJJ0TLXLU+OOiogNkvYC5kj6Z36hzzlrJJLOAsYBn692WeqJpJ2A64BzqlyUetebrNn0MWStFJ6UdEhEvFHVUtWXM4CZEfEjSePJvt99dER80NYLreN8h7iYDcCw3PTQNK/RvdzSBCH9bGkaUy6vHp+jpD5kleE7I+J3abZzKiNdJOcC48matLT8ky6/zx/mkZbvBvyHnp3T54BTJa0la1J1LPBTnNF2ImJD+rmJ7J8rR+BzrpYUybbc8WuZQsenpOOBK4BTI2JLhcpWL9rKcAAwGngife4eCTS5Y61tFDkO1wNNEfFeekznObIKsmWKZDiNrL8QIuLvQD9gcEVK13O0+5ruCnExC4D9lfXwujNZpx9NVS5TLWgCWnpjnQr8MTf/7NTL25HA5tR88WFggqSBynp9nZDm9QjpmbdbgWcj4rrcIueUI2nPdGcYSf2BL5I9bz0XmJRWK82pJb9JwOOR9ZrQBExJPdSOILvoPl2ZveheEXF5RAyNiOFknzePR8SZOKNtSPqYpAEt42TnynJ8ztWSItfPcsevZdrMUNJhwM/JKsN+bnN7O8wwIjZHxOCIGJ4+d+eRZbmwOsWtSUXO5T+Q3R1G0mCyJtRrKlnIGlckw38DxwFIOpisQvxKRUtZ/8pd68trq9ctD9v0WPYc2bOOV1S7PFXY/7uBjcB7ZP8BnEb2jNdjwGrgUWBQWlfAjSmrZeR6agTOJevYpxn4erX3q4szOorsecalwOI0nOyctsvp02S9UC4lq7xMT/NHklXWmoF7gb5pfr803ZyWj8xt64qU3yrgpGrvWzfldQwf9TLtjLbNZiRZL51LgBUtn80+52praO36CVxNVuHY4fHroXCGjwIv5649TdUuc60NbWVYsu4TuJfpdmeYPmOvA1amz9gp1S5zrQ0FMhwFPJWua4uBCdUuc60NtF4nOR84Py0ve60vNyi90MzMzMzMzKyhuMm0mZmZmZmZNSRXiM3MzMzMzKwhuUJsZmZmZmZmDckVYjMzMzMzM2tIrhCbmZmZmZlZQ3KF2MzMzMzMzBqSK8RmdUDScEnL27H+OZL2LbDODZ0s19WSju/MNszMzMzMqqV3tQtgZt3iHGA58GJ3vklETO/O7ZuZmZmZdSffITarH70l3SnpWUmzJe0iabqkBZKWS5qhzCRgHHCnpMWS+ks6XNLfJC2R9LSkAWmb+0p6SNJqSdeWe2NJvSTNTO+zTNJ30/yZkiZJGpfea3FaHmn5J9P2F0n6i6SDuj0lMzMzM7OCXCE2qx8HAjdFxMHAm8CFwA0RcXhEjAb6A6dExGxgIXBmRIwBtgL3ABdHxKHA8cDbaZtjgMnAIcBkScPKvPcYYEhEjI6IQ4Db8wsjYmFEjEnv9xDww7RoBnBRRHwG+B5wU+djMDMzMzPrGm4ybVY/1kXEU2n8N8C3gRckXQrsAgwCVgB/KnndgcDGiFgAEBFvAkgCeCwiNqfplcAngHWtvPcaYKSk64EHgEdaK6CkycBYYIKkXYHPAvem9wLo2859NjMzMzPrNq4Qm9WPaGX6JmBcRKyTdBXQr53b3JIb30qZz4SIeF3SocAJwPnA6cC5+XUkjQauAo6OiK2SdgLeSHeNzczMzMxqjptMm9WP/SSNT+NfBf6axl9Nd2Mn5dZ9C2h5TngVsI+kwwEkDZDUrn+GSRoM7BQR9wFXkt0Fzi/fHbgbODsiXoEP70S/IOm0tI5SpdrMzMzMrCb4DrFZ/VgFfFPSbcBK4GZgIFlv0i8BC3LrzgRukfQ2MJ7sOeHrJfUne364vV+VNAS4Pd31Bbi8ZPlEsubWv2hpHp3uDJ8J3CzpSqAPMAtY0s73NjMzMzPrFooobYVpZmZmZmZm1vO5ybSZmZmZmZk1JDeZNrNtSJrP9r1Bfy0illWjPGZmZmZm3cVNps3MzMzMzKwhucm0mZmZmZmZNSRXiM3MzMzMzKwhuUJsZmZmZmZmDckVYjMzMzMzM2tIrhCbmZmZmZlZQ/o/C7kbeI+925MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1152x1440 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<Figure size 1152x1440 with 6 Axes>,\n",
       " array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f8c81091860>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7f8c501dc240>],\n",
       "        [<matplotlib.axes._subplots.AxesSubplot object at 0x7f8c745f9390>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7f8c74384cc0>],\n",
       "        [<matplotlib.axes._subplots.AxesSubplot object at 0x7f8c502392b0>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7f8c50568390>]],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_mlp_summ_performance(mlp_perf_metrics23, 'batch_size', 'layer_conf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following conclusions can be extracted from the graph above:\n",
    "\n",
    "* Models with architecture [512, 256] and [256, 256] reach top values for the most metrics.\n",
    "* Evolution respecting `batch_size` is very disruptive. For value 1250, higuer metrics can be outperformed for the most architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.860427</td>\n",
       "      <td>0.812808</td>\n",
       "      <td>0.323961</td>\n",
       "      <td>0.438386</td>\n",
       "      <td>10</td>\n",
       "      <td>0.824032</td>\n",
       "      <td>0.848148</td>\n",
       "      <td>0.711180</td>\n",
       "      <td>0.906712</td>\n",
       "      <td>0.773649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>0.842182</td>\n",
       "      <td>0.822660</td>\n",
       "      <td>0.366579</td>\n",
       "      <td>0.415356</td>\n",
       "      <td>49</td>\n",
       "      <td>0.823375</td>\n",
       "      <td>0.834798</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.892325</td>\n",
       "      <td>0.779327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>0.821018</td>\n",
       "      <td>0.368339</td>\n",
       "      <td>0.404629</td>\n",
       "      <td>41</td>\n",
       "      <td>0.822062</td>\n",
       "      <td>0.852886</td>\n",
       "      <td>0.704615</td>\n",
       "      <td>0.909507</td>\n",
       "      <td>0.771693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.839080</td>\n",
       "      <td>0.824302</td>\n",
       "      <td>0.371704</td>\n",
       "      <td>0.408708</td>\n",
       "      <td>126</td>\n",
       "      <td>0.821405</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.733846</td>\n",
       "      <td>0.886598</td>\n",
       "      <td>0.778140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.862251</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.330685</td>\n",
       "      <td>0.425597</td>\n",
       "      <td>18</td>\n",
       "      <td>0.820749</td>\n",
       "      <td>0.820380</td>\n",
       "      <td>0.737578</td>\n",
       "      <td>0.881684</td>\n",
       "      <td>0.776778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  n_layers  layer_conf      lr  dropout  batch_size  \\\n",
       "11          11         2   [256, 32]  0.0001      0.1         1.0   \n",
       "7            7         2  [512, 256]  0.0001      0.1      1250.0   \n",
       "0            0         2  [512, 256]  0.0001      0.1      1000.0   \n",
       "41          41         2  [256, 256]  0.0001      0.1      3000.0   \n",
       "25          25         2   [256, 32]  0.0001      0.1        20.0   \n",
       "\n",
       "    accuracy_train  accuracy_val  loss_train  loss_val  epochs  accuracy  \\\n",
       "11        0.860427      0.812808    0.323961  0.438386      10  0.824032   \n",
       "7         0.842182      0.822660    0.366579  0.415356      49  0.823375   \n",
       "0         0.842000      0.821018    0.368339  0.404629      41  0.822062   \n",
       "41        0.839080      0.824302    0.371704  0.408708     126  0.821405   \n",
       "25        0.862251      0.816092    0.330685  0.425597      18  0.820749   \n",
       "\n",
       "    precision    recall  specificity  f1_score  \n",
       "11   0.848148  0.711180     0.906712  0.773649  \n",
       "7    0.834798  0.730769     0.892325  0.779327  \n",
       "0    0.852886  0.704615     0.909507  0.771693  \n",
       "41   0.828125  0.733846     0.886598  0.778140  \n",
       "25   0.820380  0.737578     0.881684  0.776778  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics23.nlargest(5, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>0.842182</td>\n",
       "      <td>0.822660</td>\n",
       "      <td>0.366579</td>\n",
       "      <td>0.415356</td>\n",
       "      <td>49</td>\n",
       "      <td>0.823375</td>\n",
       "      <td>0.834798</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.892325</td>\n",
       "      <td>0.779327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.839080</td>\n",
       "      <td>0.824302</td>\n",
       "      <td>0.371704</td>\n",
       "      <td>0.408708</td>\n",
       "      <td>126</td>\n",
       "      <td>0.821405</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.733846</td>\n",
       "      <td>0.886598</td>\n",
       "      <td>0.778140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>0.840540</td>\n",
       "      <td>0.821018</td>\n",
       "      <td>0.378525</td>\n",
       "      <td>0.409411</td>\n",
       "      <td>47</td>\n",
       "      <td>0.820092</td>\n",
       "      <td>0.821918</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.880871</td>\n",
       "      <td>0.777958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.829228</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.395538</td>\n",
       "      <td>0.411314</td>\n",
       "      <td>105</td>\n",
       "      <td>0.818779</td>\n",
       "      <td>0.815878</td>\n",
       "      <td>0.743077</td>\n",
       "      <td>0.875143</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.837256</td>\n",
       "      <td>0.821018</td>\n",
       "      <td>0.376550</td>\n",
       "      <td>0.407989</td>\n",
       "      <td>82</td>\n",
       "      <td>0.820092</td>\n",
       "      <td>0.823024</td>\n",
       "      <td>0.736923</td>\n",
       "      <td>0.882016</td>\n",
       "      <td>0.777597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  n_layers  layer_conf      lr  dropout  batch_size  \\\n",
       "7            7         2  [512, 256]  0.0001      0.1      1250.0   \n",
       "41          41         2  [256, 256]  0.0001      0.1      3000.0   \n",
       "14          14         2  [512, 256]  0.0001      0.1      1500.0   \n",
       "55          55         2  [256, 256]  0.0001      0.1      4000.0   \n",
       "35          35         2  [512, 256]  0.0001      0.1      3000.0   \n",
       "\n",
       "    accuracy_train  accuracy_val  loss_train  loss_val  epochs  accuracy  \\\n",
       "7         0.842182      0.822660    0.366579  0.415356      49  0.823375   \n",
       "41        0.839080      0.824302    0.371704  0.408708     126  0.821405   \n",
       "14        0.840540      0.821018    0.378525  0.409411      47  0.820092   \n",
       "55        0.829228      0.816092    0.395538  0.411314     105  0.818779   \n",
       "35        0.837256      0.821018    0.376550  0.407989      82  0.820092   \n",
       "\n",
       "    precision    recall  specificity  f1_score  \n",
       "7    0.834798  0.730769     0.892325  0.779327  \n",
       "41   0.828125  0.733846     0.886598  0.778140  \n",
       "14   0.821918  0.738462     0.880871  0.777958  \n",
       "55   0.815878  0.743077     0.875143  0.777778  \n",
       "35   0.823024  0.736923     0.882016  0.777597  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics23.nlargest(5, 'f1_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the top architectures and `batch_size=1250`, let's grid some values for **dropout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid parameters\n",
    "params4 = {\n",
    "        'lay_conf': best_arch1,\n",
    "        'lr': [1e-4],\n",
    "        'dropout': [1e-2, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4],\n",
    "        'max_epochs': [300],\n",
    "        'batch_size': [1250],\n",
    "        'seed': [123456]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters {'batch_size': 1250, 'dropout': 0.01, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_70\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_71 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_200 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_130 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_201 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_131 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_202 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.7333 - acc: 0.4501 - val_loss: 0.6782 - val_acc: 0.5698\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.6676 - acc: 0.5762 - val_loss: 0.6523 - val_acc: 0.5846\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.6411 - acc: 0.5815 - val_loss: 0.6268 - val_acc: 0.6847\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.6108 - acc: 0.7075 - val_loss: 0.6114 - val_acc: 0.6864\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.5915 - acc: 0.7048 - val_loss: 0.5965 - val_acc: 0.6831\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.5719 - acc: 0.7196 - val_loss: 0.5775 - val_acc: 0.7323\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.5531 - acc: 0.7451 - val_loss: 0.5618 - val_acc: 0.7455\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5357 - acc: 0.7542 - val_loss: 0.5468 - val_acc: 0.7438\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.5192 - acc: 0.7577 - val_loss: 0.5327 - val_acc: 0.7488\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5056 - acc: 0.7729 - val_loss: 0.5209 - val_acc: 0.7603\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4934 - acc: 0.7800 - val_loss: 0.5110 - val_acc: 0.7504\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4825 - acc: 0.7856 - val_loss: 0.5016 - val_acc: 0.7685\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4730 - acc: 0.7896 - val_loss: 0.4937 - val_acc: 0.7603\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4641 - acc: 0.7933 - val_loss: 0.4871 - val_acc: 0.7701\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4558 - acc: 0.7971 - val_loss: 0.4807 - val_acc: 0.7718\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4496 - acc: 0.8000 - val_loss: 0.4756 - val_acc: 0.7800\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4439 - acc: 0.8033 - val_loss: 0.4708 - val_acc: 0.7882\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4380 - acc: 0.8046 - val_loss: 0.4671 - val_acc: 0.7898\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4326 - acc: 0.8084 - val_loss: 0.4643 - val_acc: 0.7915\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4286 - acc: 0.8123 - val_loss: 0.4610 - val_acc: 0.7947\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4239 - acc: 0.8141 - val_loss: 0.4582 - val_acc: 0.7997\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4209 - acc: 0.8194 - val_loss: 0.4552 - val_acc: 0.8046\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4176 - acc: 0.8201 - val_loss: 0.4533 - val_acc: 0.8046\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4138 - acc: 0.8214 - val_loss: 0.4538 - val_acc: 0.7997\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.4101 - acc: 0.8225 - val_loss: 0.4516 - val_acc: 0.8013\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.4089 - acc: 0.8228 - val_loss: 0.4514 - val_acc: 0.8013\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4045 - acc: 0.8263 - val_loss: 0.4478 - val_acc: 0.8030\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4018 - acc: 0.8280 - val_loss: 0.4473 - val_acc: 0.8046\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3992 - acc: 0.8272 - val_loss: 0.4471 - val_acc: 0.8062\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3963 - acc: 0.8305 - val_loss: 0.4457 - val_acc: 0.8095\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.3939 - acc: 0.8307 - val_loss: 0.4449 - val_acc: 0.7964\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3925 - acc: 0.8280 - val_loss: 0.4443 - val_acc: 0.8079\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3890 - acc: 0.8316 - val_loss: 0.4429 - val_acc: 0.8013\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.3870 - acc: 0.8316 - val_loss: 0.4462 - val_acc: 0.8046\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3858 - acc: 0.8352 - val_loss: 0.4428 - val_acc: 0.7931\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3839 - acc: 0.8338 - val_loss: 0.4447 - val_acc: 0.8013\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3809 - acc: 0.8352 - val_loss: 0.4417 - val_acc: 0.8046\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3789 - acc: 0.8351 - val_loss: 0.4416 - val_acc: 0.8095\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.3752 - acc: 0.8396 - val_loss: 0.4398 - val_acc: 0.7980\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 0.3753 - acc: 0.8347 - val_loss: 0.4440 - val_acc: 0.8013\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.3732 - acc: 0.8413 - val_loss: 0.4399 - val_acc: 0.8030\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.3698 - acc: 0.8409 - val_loss: 0.4388 - val_acc: 0.8079\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.3669 - acc: 0.8425 - val_loss: 0.4375 - val_acc: 0.8030\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.3644 - acc: 0.8425 - val_loss: 0.4377 - val_acc: 0.8062\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.3628 - acc: 0.8429 - val_loss: 0.4388 - val_acc: 0.8062\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.3606 - acc: 0.8469 - val_loss: 0.4377 - val_acc: 0.8030\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.3582 - acc: 0.8442 - val_loss: 0.4383 - val_acc: 0.8030\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.3560 - acc: 0.8469 - val_loss: 0.4374 - val_acc: 0.8030\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.3535 - acc: 0.8469 - val_loss: 0.4384 - val_acc: 0.8013\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.3504 - acc: 0.8484 - val_loss: 0.4375 - val_acc: 0.8062\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.3479 - acc: 0.8513 - val_loss: 0.4369 - val_acc: 0.8046\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.3459 - acc: 0.8493 - val_loss: 0.4391 - val_acc: 0.8013\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3436 - acc: 0.8531 - val_loss: 0.4370 - val_acc: 0.8062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3411 - acc: 0.8542 - val_loss: 0.4370 - val_acc: 0.8013\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3391 - acc: 0.8533 - val_loss: 0.4398 - val_acc: 0.8079\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.3376 - acc: 0.8564 - val_loss: 0.4378 - val_acc: 0.8046\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.01, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_71\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_72 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_203 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_132 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_204 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_133 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_205 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.8003 - acc: 0.4319 - val_loss: 0.7403 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.7100 - acc: 0.4720 - val_loss: 0.6800 - val_acc: 0.6043\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6685 - acc: 0.6088 - val_loss: 0.6551 - val_acc: 0.5911\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6506 - acc: 0.5796 - val_loss: 0.6398 - val_acc: 0.5944\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6332 - acc: 0.5935 - val_loss: 0.6244 - val_acc: 0.6437\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6146 - acc: 0.6824 - val_loss: 0.6114 - val_acc: 0.7356\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5995 - acc: 0.7305 - val_loss: 0.6014 - val_acc: 0.7225\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5863 - acc: 0.7209 - val_loss: 0.5913 - val_acc: 0.7225\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5735 - acc: 0.7261 - val_loss: 0.5797 - val_acc: 0.7291\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5617 - acc: 0.7364 - val_loss: 0.5687 - val_acc: 0.7438\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5499 - acc: 0.7497 - val_loss: 0.5578 - val_acc: 0.7406\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5370 - acc: 0.7513 - val_loss: 0.5471 - val_acc: 0.7438\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5254 - acc: 0.7572 - val_loss: 0.5361 - val_acc: 0.7455\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5137 - acc: 0.7612 - val_loss: 0.5256 - val_acc: 0.7635\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5038 - acc: 0.7707 - val_loss: 0.5159 - val_acc: 0.7750\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4934 - acc: 0.7783 - val_loss: 0.5074 - val_acc: 0.7767\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4856 - acc: 0.7796 - val_loss: 0.4996 - val_acc: 0.7750\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4787 - acc: 0.7853 - val_loss: 0.4925 - val_acc: 0.7783\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4697 - acc: 0.7902 - val_loss: 0.4862 - val_acc: 0.7767\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4624 - acc: 0.7918 - val_loss: 0.4805 - val_acc: 0.7898\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4576 - acc: 0.7975 - val_loss: 0.4752 - val_acc: 0.7882\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4521 - acc: 0.7975 - val_loss: 0.4705 - val_acc: 0.7882\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4475 - acc: 0.7993 - val_loss: 0.4669 - val_acc: 0.7898\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4437 - acc: 0.8000 - val_loss: 0.4649 - val_acc: 0.7882\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4385 - acc: 0.8073 - val_loss: 0.4612 - val_acc: 0.7898\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4344 - acc: 0.8099 - val_loss: 0.4587 - val_acc: 0.7898\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4317 - acc: 0.8106 - val_loss: 0.4573 - val_acc: 0.8013\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4289 - acc: 0.8130 - val_loss: 0.4544 - val_acc: 0.7964\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4260 - acc: 0.8117 - val_loss: 0.4532 - val_acc: 0.7997\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4253 - acc: 0.8135 - val_loss: 0.4524 - val_acc: 0.7947\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4215 - acc: 0.8166 - val_loss: 0.4512 - val_acc: 0.7964\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4189 - acc: 0.8161 - val_loss: 0.4499 - val_acc: 0.7980\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4174 - acc: 0.8168 - val_loss: 0.4493 - val_acc: 0.8013\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4139 - acc: 0.8185 - val_loss: 0.4483 - val_acc: 0.8046\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4121 - acc: 0.8201 - val_loss: 0.4479 - val_acc: 0.8062\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4097 - acc: 0.8185 - val_loss: 0.4468 - val_acc: 0.8079\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4073 - acc: 0.8232 - val_loss: 0.4461 - val_acc: 0.8095\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4067 - acc: 0.8234 - val_loss: 0.4447 - val_acc: 0.8062\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4044 - acc: 0.8221 - val_loss: 0.4445 - val_acc: 0.8112\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4032 - acc: 0.8205 - val_loss: 0.4447 - val_acc: 0.8112\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4031 - acc: 0.8269 - val_loss: 0.4425 - val_acc: 0.8095\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4011 - acc: 0.8212 - val_loss: 0.4426 - val_acc: 0.8128\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3988 - acc: 0.8269 - val_loss: 0.4426 - val_acc: 0.8112\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3974 - acc: 0.8248 - val_loss: 0.4417 - val_acc: 0.8128\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3947 - acc: 0.8281 - val_loss: 0.4418 - val_acc: 0.8128\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.3920 - acc: 0.8265 - val_loss: 0.4400 - val_acc: 0.8095\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.3917 - acc: 0.8283 - val_loss: 0.4402 - val_acc: 0.8095\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3891 - acc: 0.8285 - val_loss: 0.4397 - val_acc: 0.8046\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3880 - acc: 0.8283 - val_loss: 0.4409 - val_acc: 0.8128\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3879 - acc: 0.8303 - val_loss: 0.4396 - val_acc: 0.8079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3848 - acc: 0.8314 - val_loss: 0.4388 - val_acc: 0.8062\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3832 - acc: 0.8314 - val_loss: 0.4400 - val_acc: 0.8062\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3808 - acc: 0.8321 - val_loss: 0.4384 - val_acc: 0.8062\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3801 - acc: 0.8311 - val_loss: 0.4396 - val_acc: 0.8095\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3785 - acc: 0.8356 - val_loss: 0.4369 - val_acc: 0.8046\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3767 - acc: 0.8360 - val_loss: 0.4364 - val_acc: 0.8095\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3752 - acc: 0.8380 - val_loss: 0.4359 - val_acc: 0.8046\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3741 - acc: 0.8362 - val_loss: 0.4356 - val_acc: 0.8062\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3716 - acc: 0.8387 - val_loss: 0.4351 - val_acc: 0.8079\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3692 - acc: 0.8405 - val_loss: 0.4345 - val_acc: 0.8095\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3693 - acc: 0.8387 - val_loss: 0.4338 - val_acc: 0.8079\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3678 - acc: 0.8420 - val_loss: 0.4349 - val_acc: 0.8062\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3656 - acc: 0.8404 - val_loss: 0.4342 - val_acc: 0.8128\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3639 - acc: 0.8409 - val_loss: 0.4339 - val_acc: 0.8095\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3616 - acc: 0.8418 - val_loss: 0.4337 - val_acc: 0.8161\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3609 - acc: 0.8429 - val_loss: 0.4330 - val_acc: 0.8177\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3582 - acc: 0.8451 - val_loss: 0.4346 - val_acc: 0.8013\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3574 - acc: 0.8438 - val_loss: 0.4329 - val_acc: 0.8194\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3552 - acc: 0.8453 - val_loss: 0.4355 - val_acc: 0.8013\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3550 - acc: 0.8471 - val_loss: 0.4329 - val_acc: 0.8144\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3524 - acc: 0.8475 - val_loss: 0.4343 - val_acc: 0.8095\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3506 - acc: 0.8488 - val_loss: 0.4328 - val_acc: 0.8144\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3493 - acc: 0.8489 - val_loss: 0.4350 - val_acc: 0.8112\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3482 - acc: 0.8509 - val_loss: 0.4319 - val_acc: 0.8161\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3446 - acc: 0.8497 - val_loss: 0.4341 - val_acc: 0.8128\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3431 - acc: 0.8511 - val_loss: 0.4325 - val_acc: 0.8144\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3408 - acc: 0.8535 - val_loss: 0.4318 - val_acc: 0.8144\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3390 - acc: 0.8555 - val_loss: 0.4311 - val_acc: 0.8095\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.3377 - acc: 0.8540 - val_loss: 0.4372 - val_acc: 0.8194\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3362 - acc: 0.8560 - val_loss: 0.4330 - val_acc: 0.8144\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3329 - acc: 0.8571 - val_loss: 0.4356 - val_acc: 0.8194\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3325 - acc: 0.8571 - val_loss: 0.4315 - val_acc: 0.8112\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3316 - acc: 0.8568 - val_loss: 0.4345 - val_acc: 0.8194\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.01, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_72\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_73 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_206 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_134 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_207 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 1s 53ms/step - loss: 0.6626 - acc: 0.6307 - val_loss: 0.6456 - val_acc: 0.6749\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6414 - acc: 0.6533 - val_loss: 0.6296 - val_acc: 0.6716\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6258 - acc: 0.6687 - val_loss: 0.6174 - val_acc: 0.7209\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6110 - acc: 0.7011 - val_loss: 0.6069 - val_acc: 0.6995\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5985 - acc: 0.7114 - val_loss: 0.5982 - val_acc: 0.7110\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5868 - acc: 0.7159 - val_loss: 0.5892 - val_acc: 0.7126\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5772 - acc: 0.7227 - val_loss: 0.5804 - val_acc: 0.7291\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5679 - acc: 0.7314 - val_loss: 0.5721 - val_acc: 0.7356\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5580 - acc: 0.7396 - val_loss: 0.5643 - val_acc: 0.7422\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5503 - acc: 0.7455 - val_loss: 0.5571 - val_acc: 0.7455\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5421 - acc: 0.7519 - val_loss: 0.5503 - val_acc: 0.7488\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5349 - acc: 0.7552 - val_loss: 0.5438 - val_acc: 0.7553\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5279 - acc: 0.7628 - val_loss: 0.5378 - val_acc: 0.7570\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5217 - acc: 0.7716 - val_loss: 0.5327 - val_acc: 0.7570\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5163 - acc: 0.7723 - val_loss: 0.5279 - val_acc: 0.7619\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5111 - acc: 0.7727 - val_loss: 0.5242 - val_acc: 0.7570\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5058 - acc: 0.7756 - val_loss: 0.5192 - val_acc: 0.7586\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5016 - acc: 0.7792 - val_loss: 0.5150 - val_acc: 0.7635\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4970 - acc: 0.7816 - val_loss: 0.5111 - val_acc: 0.7652\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4929 - acc: 0.7842 - val_loss: 0.5076 - val_acc: 0.7619\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4887 - acc: 0.7834 - val_loss: 0.5047 - val_acc: 0.7586\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4858 - acc: 0.7853 - val_loss: 0.5012 - val_acc: 0.7652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4819 - acc: 0.7902 - val_loss: 0.4985 - val_acc: 0.7685\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4793 - acc: 0.7896 - val_loss: 0.4956 - val_acc: 0.7668\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4757 - acc: 0.7896 - val_loss: 0.4932 - val_acc: 0.7685\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4728 - acc: 0.7909 - val_loss: 0.4903 - val_acc: 0.7668\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4699 - acc: 0.7938 - val_loss: 0.4882 - val_acc: 0.7718\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4679 - acc: 0.7933 - val_loss: 0.4861 - val_acc: 0.7750\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4652 - acc: 0.7955 - val_loss: 0.4841 - val_acc: 0.7718\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4637 - acc: 0.7964 - val_loss: 0.4822 - val_acc: 0.7750\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4602 - acc: 0.7986 - val_loss: 0.4808 - val_acc: 0.7783\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4593 - acc: 0.7982 - val_loss: 0.4788 - val_acc: 0.7800\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4567 - acc: 0.7995 - val_loss: 0.4772 - val_acc: 0.7750\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4549 - acc: 0.8004 - val_loss: 0.4756 - val_acc: 0.7783\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4532 - acc: 0.8028 - val_loss: 0.4744 - val_acc: 0.7865\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4507 - acc: 0.8035 - val_loss: 0.4729 - val_acc: 0.7816\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4494 - acc: 0.8051 - val_loss: 0.4716 - val_acc: 0.7882\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4474 - acc: 0.8070 - val_loss: 0.4706 - val_acc: 0.7865\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4466 - acc: 0.8064 - val_loss: 0.4700 - val_acc: 0.7915\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4447 - acc: 0.8053 - val_loss: 0.4683 - val_acc: 0.7882\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4433 - acc: 0.8075 - val_loss: 0.4676 - val_acc: 0.7865\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4423 - acc: 0.8099 - val_loss: 0.4662 - val_acc: 0.7931\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4405 - acc: 0.8068 - val_loss: 0.4662 - val_acc: 0.7964\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4392 - acc: 0.8092 - val_loss: 0.4643 - val_acc: 0.7915\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4382 - acc: 0.8103 - val_loss: 0.4634 - val_acc: 0.7915\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4365 - acc: 0.8134 - val_loss: 0.4631 - val_acc: 0.7997\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4353 - acc: 0.8115 - val_loss: 0.4621 - val_acc: 0.7947\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4343 - acc: 0.8123 - val_loss: 0.4613 - val_acc: 0.7947\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4320 - acc: 0.8121 - val_loss: 0.4607 - val_acc: 0.7964\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4321 - acc: 0.8119 - val_loss: 0.4599 - val_acc: 0.7980\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4306 - acc: 0.8132 - val_loss: 0.4594 - val_acc: 0.7980\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4303 - acc: 0.8126 - val_loss: 0.4589 - val_acc: 0.7997\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4289 - acc: 0.8139 - val_loss: 0.4582 - val_acc: 0.7964\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4278 - acc: 0.8161 - val_loss: 0.4578 - val_acc: 0.7980\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4264 - acc: 0.8166 - val_loss: 0.4574 - val_acc: 0.7997\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4255 - acc: 0.8155 - val_loss: 0.4567 - val_acc: 0.7931\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4248 - acc: 0.8176 - val_loss: 0.4565 - val_acc: 0.7997\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4235 - acc: 0.8166 - val_loss: 0.4561 - val_acc: 0.7997\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4231 - acc: 0.8172 - val_loss: 0.4553 - val_acc: 0.7980\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4221 - acc: 0.8190 - val_loss: 0.4548 - val_acc: 0.7997\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4211 - acc: 0.8203 - val_loss: 0.4548 - val_acc: 0.8046\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4204 - acc: 0.8207 - val_loss: 0.4543 - val_acc: 0.8013\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4197 - acc: 0.8185 - val_loss: 0.4540 - val_acc: 0.7980\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4191 - acc: 0.8186 - val_loss: 0.4542 - val_acc: 0.8030\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4174 - acc: 0.8212 - val_loss: 0.4534 - val_acc: 0.7997\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4169 - acc: 0.8205 - val_loss: 0.4528 - val_acc: 0.8013\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4165 - acc: 0.8192 - val_loss: 0.4525 - val_acc: 0.8013\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4159 - acc: 0.8196 - val_loss: 0.4522 - val_acc: 0.8013\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4150 - acc: 0.8217 - val_loss: 0.4522 - val_acc: 0.8013\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4147 - acc: 0.8219 - val_loss: 0.4520 - val_acc: 0.8030\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4129 - acc: 0.8228 - val_loss: 0.4511 - val_acc: 0.8046\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4138 - acc: 0.8207 - val_loss: 0.4509 - val_acc: 0.8030\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4130 - acc: 0.8232 - val_loss: 0.4519 - val_acc: 0.8128\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4109 - acc: 0.8241 - val_loss: 0.4505 - val_acc: 0.8062\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4105 - acc: 0.8219 - val_loss: 0.4500 - val_acc: 0.8030\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4100 - acc: 0.8247 - val_loss: 0.4502 - val_acc: 0.8046\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4090 - acc: 0.8248 - val_loss: 0.4500 - val_acc: 0.8062\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4088 - acc: 0.8221 - val_loss: 0.4497 - val_acc: 0.8062\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4078 - acc: 0.8258 - val_loss: 0.4497 - val_acc: 0.8079\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4077 - acc: 0.8247 - val_loss: 0.4492 - val_acc: 0.8062\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4062 - acc: 0.8250 - val_loss: 0.4491 - val_acc: 0.8079\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4063 - acc: 0.8243 - val_loss: 0.4493 - val_acc: 0.8095\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4056 - acc: 0.8252 - val_loss: 0.4485 - val_acc: 0.8079\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4055 - acc: 0.8245 - val_loss: 0.4482 - val_acc: 0.8079\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4048 - acc: 0.8265 - val_loss: 0.4485 - val_acc: 0.8128\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4039 - acc: 0.8258 - val_loss: 0.4476 - val_acc: 0.8079\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4032 - acc: 0.8261 - val_loss: 0.4474 - val_acc: 0.8095\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4024 - acc: 0.8259 - val_loss: 0.4475 - val_acc: 0.8112\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4020 - acc: 0.8265 - val_loss: 0.4467 - val_acc: 0.8095\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4020 - acc: 0.8252 - val_loss: 0.4468 - val_acc: 0.8095\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4010 - acc: 0.8259 - val_loss: 0.4474 - val_acc: 0.8128\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4000 - acc: 0.8274 - val_loss: 0.4470 - val_acc: 0.8095\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3994 - acc: 0.8274 - val_loss: 0.4468 - val_acc: 0.8112\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3996 - acc: 0.8265 - val_loss: 0.4467 - val_acc: 0.8112\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3988 - acc: 0.8263 - val_loss: 0.4465 - val_acc: 0.8128\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3978 - acc: 0.8285 - val_loss: 0.4469 - val_acc: 0.8144\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3971 - acc: 0.8281 - val_loss: 0.4460 - val_acc: 0.8128\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3977 - acc: 0.8287 - val_loss: 0.4461 - val_acc: 0.8079\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3969 - acc: 0.8281 - val_loss: 0.4467 - val_acc: 0.8112\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3960 - acc: 0.8290 - val_loss: 0.4459 - val_acc: 0.8112\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3957 - acc: 0.8283 - val_loss: 0.4453 - val_acc: 0.8079\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3957 - acc: 0.8265 - val_loss: 0.4450 - val_acc: 0.8095\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3944 - acc: 0.8281 - val_loss: 0.4460 - val_acc: 0.8079\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3941 - acc: 0.8287 - val_loss: 0.4452 - val_acc: 0.8079\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3941 - acc: 0.8278 - val_loss: 0.4447 - val_acc: 0.8062\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3932 - acc: 0.8292 - val_loss: 0.4453 - val_acc: 0.8079\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3931 - acc: 0.8301 - val_loss: 0.4457 - val_acc: 0.8062\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3923 - acc: 0.8283 - val_loss: 0.4444 - val_acc: 0.8062\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3916 - acc: 0.8290 - val_loss: 0.4444 - val_acc: 0.8062\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3905 - acc: 0.8289 - val_loss: 0.4442 - val_acc: 0.8062\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3903 - acc: 0.8307 - val_loss: 0.4438 - val_acc: 0.8062\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3898 - acc: 0.8316 - val_loss: 0.4440 - val_acc: 0.8062\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3895 - acc: 0.8329 - val_loss: 0.4439 - val_acc: 0.8046\n",
      "Epoch 114/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3884 - acc: 0.8314 - val_loss: 0.4435 - val_acc: 0.8062\n",
      "Epoch 115/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3881 - acc: 0.8327 - val_loss: 0.4440 - val_acc: 0.8062\n",
      "Epoch 116/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3879 - acc: 0.8305 - val_loss: 0.4434 - val_acc: 0.8046\n",
      "Epoch 117/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3874 - acc: 0.8325 - val_loss: 0.4432 - val_acc: 0.8062\n",
      "Epoch 118/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3875 - acc: 0.8325 - val_loss: 0.4426 - val_acc: 0.8062\n",
      "Epoch 119/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3867 - acc: 0.8327 - val_loss: 0.4425 - val_acc: 0.8062\n",
      "Epoch 120/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3865 - acc: 0.8314 - val_loss: 0.4436 - val_acc: 0.8046\n",
      "Epoch 121/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3856 - acc: 0.8323 - val_loss: 0.4427 - val_acc: 0.8062\n",
      "Epoch 122/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3854 - acc: 0.8343 - val_loss: 0.4427 - val_acc: 0.8046\n",
      "Epoch 123/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3840 - acc: 0.8340 - val_loss: 0.4431 - val_acc: 0.8046\n",
      "Epoch 124/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3843 - acc: 0.8349 - val_loss: 0.4428 - val_acc: 0.8062\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.01, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_73\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_74 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_208 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_135 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_209 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_136 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_210 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 1s 44ms/step - loss: 0.7961 - acc: 0.4319 - val_loss: 0.7043 - val_acc: 0.4548\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6893 - acc: 0.5187 - val_loss: 0.6600 - val_acc: 0.5862\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6587 - acc: 0.5707 - val_loss: 0.6392 - val_acc: 0.5878\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6321 - acc: 0.6004 - val_loss: 0.6166 - val_acc: 0.6880\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6060 - acc: 0.7152 - val_loss: 0.6024 - val_acc: 0.7143\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.5883 - acc: 0.7090 - val_loss: 0.5898 - val_acc: 0.7094\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.5719 - acc: 0.7232 - val_loss: 0.5746 - val_acc: 0.7389\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.5570 - acc: 0.7415 - val_loss: 0.5618 - val_acc: 0.7504\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.5414 - acc: 0.7497 - val_loss: 0.5486 - val_acc: 0.7553\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.5266 - acc: 0.7594 - val_loss: 0.5352 - val_acc: 0.7521\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.5132 - acc: 0.7677 - val_loss: 0.5235 - val_acc: 0.7570\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.5003 - acc: 0.7823 - val_loss: 0.5129 - val_acc: 0.7652\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.4904 - acc: 0.7820 - val_loss: 0.5035 - val_acc: 0.7701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4802 - acc: 0.7893 - val_loss: 0.4952 - val_acc: 0.7685\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4715 - acc: 0.7905 - val_loss: 0.4878 - val_acc: 0.7685\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4637 - acc: 0.7958 - val_loss: 0.4811 - val_acc: 0.7750\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.4575 - acc: 0.7964 - val_loss: 0.4751 - val_acc: 0.7767\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.4511 - acc: 0.8042 - val_loss: 0.4695 - val_acc: 0.7800\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4445 - acc: 0.8053 - val_loss: 0.4655 - val_acc: 0.7865\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4394 - acc: 0.8113 - val_loss: 0.4615 - val_acc: 0.7865\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4353 - acc: 0.8137 - val_loss: 0.4582 - val_acc: 0.7915\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4308 - acc: 0.8148 - val_loss: 0.4555 - val_acc: 0.7964\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4269 - acc: 0.8183 - val_loss: 0.4538 - val_acc: 0.7980\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4234 - acc: 0.8186 - val_loss: 0.4520 - val_acc: 0.7997\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4205 - acc: 0.8166 - val_loss: 0.4505 - val_acc: 0.7964\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4172 - acc: 0.8221 - val_loss: 0.4492 - val_acc: 0.7980\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4129 - acc: 0.8270 - val_loss: 0.4473 - val_acc: 0.8013\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4112 - acc: 0.8252 - val_loss: 0.4477 - val_acc: 0.7997\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4090 - acc: 0.8258 - val_loss: 0.4458 - val_acc: 0.8013\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4066 - acc: 0.8238 - val_loss: 0.4462 - val_acc: 0.8062\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4040 - acc: 0.8289 - val_loss: 0.4446 - val_acc: 0.7997\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4017 - acc: 0.8270 - val_loss: 0.4444 - val_acc: 0.7997\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.3986 - acc: 0.8270 - val_loss: 0.4436 - val_acc: 0.8013\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3976 - acc: 0.8325 - val_loss: 0.4425 - val_acc: 0.8030\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3941 - acc: 0.8298 - val_loss: 0.4424 - val_acc: 0.8030\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.3941 - acc: 0.8314 - val_loss: 0.4406 - val_acc: 0.7997\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3901 - acc: 0.8320 - val_loss: 0.4411 - val_acc: 0.8062\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3884 - acc: 0.8318 - val_loss: 0.4420 - val_acc: 0.8095\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3866 - acc: 0.8345 - val_loss: 0.4402 - val_acc: 0.8062\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3846 - acc: 0.8309 - val_loss: 0.4405 - val_acc: 0.8046\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3829 - acc: 0.8367 - val_loss: 0.4385 - val_acc: 0.8062\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3794 - acc: 0.8356 - val_loss: 0.4415 - val_acc: 0.8062\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3795 - acc: 0.8345 - val_loss: 0.4389 - val_acc: 0.8030\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3764 - acc: 0.8378 - val_loss: 0.4395 - val_acc: 0.8046\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3742 - acc: 0.8373 - val_loss: 0.4385 - val_acc: 0.8062\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3717 - acc: 0.8407 - val_loss: 0.4386 - val_acc: 0.8079\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3709 - acc: 0.8394 - val_loss: 0.4392 - val_acc: 0.8030\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3702 - acc: 0.8415 - val_loss: 0.4370 - val_acc: 0.8095\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3685 - acc: 0.8415 - val_loss: 0.4399 - val_acc: 0.8046\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.3657 - acc: 0.8442 - val_loss: 0.4382 - val_acc: 0.8079\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.3629 - acc: 0.8422 - val_loss: 0.4391 - val_acc: 0.8030\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3614 - acc: 0.8442 - val_loss: 0.4357 - val_acc: 0.8062\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3585 - acc: 0.8467 - val_loss: 0.4373 - val_acc: 0.8062\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.3562 - acc: 0.8491 - val_loss: 0.4373 - val_acc: 0.8079\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.3548 - acc: 0.8466 - val_loss: 0.4374 - val_acc: 0.8079\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3525 - acc: 0.8478 - val_loss: 0.4375 - val_acc: 0.8095\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3512 - acc: 0.8493 - val_loss: 0.4383 - val_acc: 0.8079\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.01, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_74\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_75 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_211 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_137 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_212 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_138 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_213 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 1s 38ms/step - loss: 0.8331 - acc: 0.4320 - val_loss: 0.7631 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.7334 - acc: 0.4335 - val_loss: 0.6981 - val_acc: 0.4499\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6846 - acc: 0.5492 - val_loss: 0.6656 - val_acc: 0.6765\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6590 - acc: 0.6782 - val_loss: 0.6472 - val_acc: 0.6946\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6409 - acc: 0.7002 - val_loss: 0.6323 - val_acc: 0.7356\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6251 - acc: 0.7338 - val_loss: 0.6200 - val_acc: 0.7406\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6113 - acc: 0.7375 - val_loss: 0.6092 - val_acc: 0.7307\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5984 - acc: 0.7294 - val_loss: 0.5997 - val_acc: 0.7192\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5872 - acc: 0.7313 - val_loss: 0.5905 - val_acc: 0.7225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5779 - acc: 0.7340 - val_loss: 0.5816 - val_acc: 0.7373\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5678 - acc: 0.7446 - val_loss: 0.5736 - val_acc: 0.7455\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.5582 - acc: 0.7462 - val_loss: 0.5659 - val_acc: 0.7340\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5496 - acc: 0.7488 - val_loss: 0.5574 - val_acc: 0.7373\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5395 - acc: 0.7559 - val_loss: 0.5486 - val_acc: 0.7570\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5307 - acc: 0.7645 - val_loss: 0.5403 - val_acc: 0.7603\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5214 - acc: 0.7681 - val_loss: 0.5329 - val_acc: 0.7619\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5135 - acc: 0.7729 - val_loss: 0.5258 - val_acc: 0.7652\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5066 - acc: 0.7756 - val_loss: 0.5193 - val_acc: 0.7668\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5003 - acc: 0.7776 - val_loss: 0.5133 - val_acc: 0.7701\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4936 - acc: 0.7801 - val_loss: 0.5076 - val_acc: 0.7718\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4870 - acc: 0.7843 - val_loss: 0.5024 - val_acc: 0.7750\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4813 - acc: 0.7900 - val_loss: 0.4973 - val_acc: 0.7734\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4756 - acc: 0.7889 - val_loss: 0.4928 - val_acc: 0.7734\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4714 - acc: 0.7915 - val_loss: 0.4886 - val_acc: 0.7783\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4666 - acc: 0.7957 - val_loss: 0.4848 - val_acc: 0.7767\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4619 - acc: 0.7995 - val_loss: 0.4823 - val_acc: 0.7882\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4592 - acc: 0.8004 - val_loss: 0.4786 - val_acc: 0.7833\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4550 - acc: 0.8020 - val_loss: 0.4758 - val_acc: 0.7882\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4530 - acc: 0.8020 - val_loss: 0.4735 - val_acc: 0.7882\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4489 - acc: 0.8051 - val_loss: 0.4710 - val_acc: 0.7898\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4457 - acc: 0.8066 - val_loss: 0.4691 - val_acc: 0.7882\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4424 - acc: 0.8064 - val_loss: 0.4669 - val_acc: 0.7915\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4406 - acc: 0.8095 - val_loss: 0.4650 - val_acc: 0.7898\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4380 - acc: 0.8103 - val_loss: 0.4630 - val_acc: 0.7947\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4365 - acc: 0.8121 - val_loss: 0.4616 - val_acc: 0.7964\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4330 - acc: 0.8104 - val_loss: 0.4607 - val_acc: 0.7964\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4314 - acc: 0.8106 - val_loss: 0.4594 - val_acc: 0.7915\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4295 - acc: 0.8150 - val_loss: 0.4584 - val_acc: 0.7931\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4271 - acc: 0.8119 - val_loss: 0.4567 - val_acc: 0.7947\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4261 - acc: 0.8144 - val_loss: 0.4553 - val_acc: 0.7964\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4246 - acc: 0.8137 - val_loss: 0.4541 - val_acc: 0.7964\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4222 - acc: 0.8163 - val_loss: 0.4538 - val_acc: 0.8013\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4205 - acc: 0.8152 - val_loss: 0.4525 - val_acc: 0.8013\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4186 - acc: 0.8185 - val_loss: 0.4519 - val_acc: 0.8013\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4186 - acc: 0.8177 - val_loss: 0.4521 - val_acc: 0.8062\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4146 - acc: 0.8165 - val_loss: 0.4499 - val_acc: 0.8030\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4136 - acc: 0.8196 - val_loss: 0.4499 - val_acc: 0.8046\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4119 - acc: 0.8208 - val_loss: 0.4486 - val_acc: 0.8046\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4100 - acc: 0.8186 - val_loss: 0.4482 - val_acc: 0.8062\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4093 - acc: 0.8203 - val_loss: 0.4476 - val_acc: 0.8079\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4085 - acc: 0.8208 - val_loss: 0.4472 - val_acc: 0.8079\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4056 - acc: 0.8214 - val_loss: 0.4473 - val_acc: 0.8062\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4056 - acc: 0.8239 - val_loss: 0.4464 - val_acc: 0.8128\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4043 - acc: 0.8223 - val_loss: 0.4461 - val_acc: 0.8128\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4033 - acc: 0.8223 - val_loss: 0.4465 - val_acc: 0.8095\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4028 - acc: 0.8219 - val_loss: 0.4457 - val_acc: 0.8161\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4010 - acc: 0.8238 - val_loss: 0.4448 - val_acc: 0.8177\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3974 - acc: 0.8248 - val_loss: 0.4445 - val_acc: 0.8194\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3976 - acc: 0.8256 - val_loss: 0.4441 - val_acc: 0.8177\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3967 - acc: 0.8258 - val_loss: 0.4432 - val_acc: 0.8161\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3957 - acc: 0.8272 - val_loss: 0.4434 - val_acc: 0.8177\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3942 - acc: 0.8280 - val_loss: 0.4435 - val_acc: 0.8177\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3937 - acc: 0.8276 - val_loss: 0.4435 - val_acc: 0.8177\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3924 - acc: 0.8283 - val_loss: 0.4425 - val_acc: 0.8161\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3907 - acc: 0.8307 - val_loss: 0.4431 - val_acc: 0.8128\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3891 - acc: 0.8269 - val_loss: 0.4413 - val_acc: 0.8144\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3889 - acc: 0.8281 - val_loss: 0.4415 - val_acc: 0.8161\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3887 - acc: 0.8311 - val_loss: 0.4414 - val_acc: 0.8112\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3869 - acc: 0.8351 - val_loss: 0.4403 - val_acc: 0.8177\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3862 - acc: 0.8329 - val_loss: 0.4408 - val_acc: 0.8112\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3850 - acc: 0.8311 - val_loss: 0.4401 - val_acc: 0.8144\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3849 - acc: 0.8325 - val_loss: 0.4398 - val_acc: 0.8144\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3830 - acc: 0.8316 - val_loss: 0.4397 - val_acc: 0.8112\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3821 - acc: 0.8362 - val_loss: 0.4408 - val_acc: 0.8112\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3804 - acc: 0.8354 - val_loss: 0.4391 - val_acc: 0.8128\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3792 - acc: 0.8369 - val_loss: 0.4407 - val_acc: 0.8144\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3779 - acc: 0.8352 - val_loss: 0.4399 - val_acc: 0.8144\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3772 - acc: 0.8367 - val_loss: 0.4391 - val_acc: 0.8095\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3765 - acc: 0.8365 - val_loss: 0.4394 - val_acc: 0.8144\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3759 - acc: 0.8373 - val_loss: 0.4395 - val_acc: 0.8128\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3753 - acc: 0.8380 - val_loss: 0.4390 - val_acc: 0.8079\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3739 - acc: 0.8374 - val_loss: 0.4415 - val_acc: 0.8095\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3724 - acc: 0.8374 - val_loss: 0.4384 - val_acc: 0.8112\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3709 - acc: 0.8373 - val_loss: 0.4387 - val_acc: 0.8177\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3708 - acc: 0.8409 - val_loss: 0.4397 - val_acc: 0.8112\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3682 - acc: 0.8405 - val_loss: 0.4384 - val_acc: 0.8112\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3683 - acc: 0.8409 - val_loss: 0.4382 - val_acc: 0.8144\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3670 - acc: 0.8422 - val_loss: 0.4392 - val_acc: 0.8128\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3667 - acc: 0.8427 - val_loss: 0.4389 - val_acc: 0.8128\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3652 - acc: 0.8427 - val_loss: 0.4387 - val_acc: 0.8128\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3643 - acc: 0.8405 - val_loss: 0.4384 - val_acc: 0.8144\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3622 - acc: 0.8427 - val_loss: 0.4381 - val_acc: 0.8128\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3607 - acc: 0.8438 - val_loss: 0.4379 - val_acc: 0.8144\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3613 - acc: 0.8435 - val_loss: 0.4380 - val_acc: 0.8161\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3595 - acc: 0.8436 - val_loss: 0.4377 - val_acc: 0.8161\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3597 - acc: 0.8453 - val_loss: 0.4380 - val_acc: 0.8128\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3583 - acc: 0.8469 - val_loss: 0.4381 - val_acc: 0.8095\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3576 - acc: 0.8467 - val_loss: 0.4386 - val_acc: 0.8128\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3568 - acc: 0.8462 - val_loss: 0.4384 - val_acc: 0.8079\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3559 - acc: 0.8458 - val_loss: 0.4385 - val_acc: 0.8144\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.01, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_75\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_76 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_214 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_139 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_215 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_140 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_216 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.7448 - acc: 0.4335 - val_loss: 0.7135 - val_acc: 0.4187\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6949 - acc: 0.4935 - val_loss: 0.6759 - val_acc: 0.6158\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6673 - acc: 0.6648 - val_loss: 0.6562 - val_acc: 0.6552\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6534 - acc: 0.6294 - val_loss: 0.6442 - val_acc: 0.6207\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6418 - acc: 0.6180 - val_loss: 0.6336 - val_acc: 0.6404\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6303 - acc: 0.6550 - val_loss: 0.6231 - val_acc: 0.6995\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6183 - acc: 0.7059 - val_loss: 0.6136 - val_acc: 0.7406\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6060 - acc: 0.7260 - val_loss: 0.6051 - val_acc: 0.7323\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5959 - acc: 0.7241 - val_loss: 0.5962 - val_acc: 0.7356\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5853 - acc: 0.7207 - val_loss: 0.5867 - val_acc: 0.7422\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5753 - acc: 0.7327 - val_loss: 0.5770 - val_acc: 0.7521\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5638 - acc: 0.7411 - val_loss: 0.5676 - val_acc: 0.7586\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5549 - acc: 0.7455 - val_loss: 0.5594 - val_acc: 0.7504\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5451 - acc: 0.7510 - val_loss: 0.5509 - val_acc: 0.7455\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5343 - acc: 0.7561 - val_loss: 0.5420 - val_acc: 0.7603\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5255 - acc: 0.7632 - val_loss: 0.5334 - val_acc: 0.7652\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5165 - acc: 0.7677 - val_loss: 0.5253 - val_acc: 0.7619\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5082 - acc: 0.7683 - val_loss: 0.5182 - val_acc: 0.7586\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5008 - acc: 0.7716 - val_loss: 0.5111 - val_acc: 0.7619\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4927 - acc: 0.7829 - val_loss: 0.5052 - val_acc: 0.7701\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4862 - acc: 0.7842 - val_loss: 0.4997 - val_acc: 0.7652\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4813 - acc: 0.7862 - val_loss: 0.4953 - val_acc: 0.7668\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4760 - acc: 0.7898 - val_loss: 0.4913 - val_acc: 0.7701\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4712 - acc: 0.7898 - val_loss: 0.4870 - val_acc: 0.7718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4669 - acc: 0.7942 - val_loss: 0.4836 - val_acc: 0.7718\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4617 - acc: 0.7971 - val_loss: 0.4800 - val_acc: 0.7718\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4581 - acc: 0.7980 - val_loss: 0.4769 - val_acc: 0.7783\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4553 - acc: 0.8013 - val_loss: 0.4742 - val_acc: 0.7767\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4510 - acc: 0.8019 - val_loss: 0.4722 - val_acc: 0.7865\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4483 - acc: 0.8004 - val_loss: 0.4701 - val_acc: 0.7882\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4464 - acc: 0.8055 - val_loss: 0.4678 - val_acc: 0.7865\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4431 - acc: 0.8041 - val_loss: 0.4661 - val_acc: 0.7931\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4401 - acc: 0.8070 - val_loss: 0.4639 - val_acc: 0.7865\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4373 - acc: 0.8088 - val_loss: 0.4624 - val_acc: 0.7947\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4349 - acc: 0.8110 - val_loss: 0.4616 - val_acc: 0.7980\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4337 - acc: 0.8108 - val_loss: 0.4596 - val_acc: 0.7980\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4307 - acc: 0.8135 - val_loss: 0.4585 - val_acc: 0.7997\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4293 - acc: 0.8124 - val_loss: 0.4581 - val_acc: 0.7997\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4265 - acc: 0.8163 - val_loss: 0.4565 - val_acc: 0.8030\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4255 - acc: 0.8161 - val_loss: 0.4558 - val_acc: 0.8013\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4221 - acc: 0.8166 - val_loss: 0.4549 - val_acc: 0.8062\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4222 - acc: 0.8150 - val_loss: 0.4544 - val_acc: 0.8062\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4192 - acc: 0.8168 - val_loss: 0.4541 - val_acc: 0.8030\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4172 - acc: 0.8205 - val_loss: 0.4520 - val_acc: 0.8095\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4166 - acc: 0.8192 - val_loss: 0.4519 - val_acc: 0.8062\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4148 - acc: 0.8201 - val_loss: 0.4517 - val_acc: 0.8062\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4135 - acc: 0.8210 - val_loss: 0.4502 - val_acc: 0.8079\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4112 - acc: 0.8214 - val_loss: 0.4501 - val_acc: 0.8112\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4097 - acc: 0.8192 - val_loss: 0.4496 - val_acc: 0.8128\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4092 - acc: 0.8208 - val_loss: 0.4488 - val_acc: 0.8095\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4076 - acc: 0.8236 - val_loss: 0.4489 - val_acc: 0.8128\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4068 - acc: 0.8228 - val_loss: 0.4479 - val_acc: 0.8128\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4040 - acc: 0.8241 - val_loss: 0.4466 - val_acc: 0.8128\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4037 - acc: 0.8238 - val_loss: 0.4462 - val_acc: 0.8112\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4020 - acc: 0.8256 - val_loss: 0.4469 - val_acc: 0.8161\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4024 - acc: 0.8245 - val_loss: 0.4459 - val_acc: 0.8144\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3993 - acc: 0.8283 - val_loss: 0.4471 - val_acc: 0.8144\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3978 - acc: 0.8290 - val_loss: 0.4451 - val_acc: 0.8161\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3972 - acc: 0.8269 - val_loss: 0.4446 - val_acc: 0.8161\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3955 - acc: 0.8298 - val_loss: 0.4459 - val_acc: 0.8161\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3955 - acc: 0.8281 - val_loss: 0.4436 - val_acc: 0.8144\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3939 - acc: 0.8294 - val_loss: 0.4452 - val_acc: 0.8210\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3924 - acc: 0.8314 - val_loss: 0.4433 - val_acc: 0.8161\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3904 - acc: 0.8311 - val_loss: 0.4435 - val_acc: 0.8194\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3893 - acc: 0.8320 - val_loss: 0.4429 - val_acc: 0.8194\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3891 - acc: 0.8321 - val_loss: 0.4426 - val_acc: 0.8177\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3874 - acc: 0.8327 - val_loss: 0.4427 - val_acc: 0.8177\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3875 - acc: 0.8283 - val_loss: 0.4444 - val_acc: 0.8194\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3840 - acc: 0.8338 - val_loss: 0.4420 - val_acc: 0.8144\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3849 - acc: 0.8332 - val_loss: 0.4425 - val_acc: 0.8177\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3818 - acc: 0.8360 - val_loss: 0.4418 - val_acc: 0.8144\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3818 - acc: 0.8336 - val_loss: 0.4416 - val_acc: 0.8128\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3806 - acc: 0.8369 - val_loss: 0.4422 - val_acc: 0.8112\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3804 - acc: 0.8345 - val_loss: 0.4413 - val_acc: 0.8112\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3775 - acc: 0.8373 - val_loss: 0.4428 - val_acc: 0.8144\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3769 - acc: 0.8374 - val_loss: 0.4408 - val_acc: 0.8095\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3753 - acc: 0.8387 - val_loss: 0.4416 - val_acc: 0.8095\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3752 - acc: 0.8387 - val_loss: 0.4416 - val_acc: 0.8112\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3734 - acc: 0.8380 - val_loss: 0.4413 - val_acc: 0.8161\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3730 - acc: 0.8387 - val_loss: 0.4408 - val_acc: 0.8161\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3709 - acc: 0.8404 - val_loss: 0.4402 - val_acc: 0.8112\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3701 - acc: 0.8394 - val_loss: 0.4410 - val_acc: 0.8112\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3699 - acc: 0.8400 - val_loss: 0.4415 - val_acc: 0.8144\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3685 - acc: 0.8427 - val_loss: 0.4399 - val_acc: 0.8095\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3675 - acc: 0.8407 - val_loss: 0.4403 - val_acc: 0.8177\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3665 - acc: 0.8433 - val_loss: 0.4396 - val_acc: 0.8128\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3645 - acc: 0.8438 - val_loss: 0.4402 - val_acc: 0.8177\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3642 - acc: 0.8425 - val_loss: 0.4401 - val_acc: 0.8161\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3627 - acc: 0.8422 - val_loss: 0.4399 - val_acc: 0.8177\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3617 - acc: 0.8442 - val_loss: 0.4393 - val_acc: 0.8144\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3598 - acc: 0.8438 - val_loss: 0.4401 - val_acc: 0.8177\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3590 - acc: 0.8446 - val_loss: 0.4390 - val_acc: 0.8112\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3577 - acc: 0.8447 - val_loss: 0.4395 - val_acc: 0.8161\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3567 - acc: 0.8464 - val_loss: 0.4391 - val_acc: 0.8128\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3576 - acc: 0.8478 - val_loss: 0.4399 - val_acc: 0.8161\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3542 - acc: 0.8484 - val_loss: 0.4408 - val_acc: 0.8194\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3544 - acc: 0.8486 - val_loss: 0.4406 - val_acc: 0.8112\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.01, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_76\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_77 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_217 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_141 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_218 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_142 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_219 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.8001 - acc: 0.4311 - val_loss: 0.7401 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.7103 - acc: 0.4808 - val_loss: 0.6799 - val_acc: 0.6059\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6692 - acc: 0.6057 - val_loss: 0.6551 - val_acc: 0.5911\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6488 - acc: 0.5835 - val_loss: 0.6396 - val_acc: 0.5977\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6330 - acc: 0.6017 - val_loss: 0.6241 - val_acc: 0.6453\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6135 - acc: 0.6878 - val_loss: 0.6111 - val_acc: 0.7373\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5986 - acc: 0.7327 - val_loss: 0.6013 - val_acc: 0.7241\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5848 - acc: 0.7263 - val_loss: 0.5908 - val_acc: 0.7258\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.5730 - acc: 0.7298 - val_loss: 0.5797 - val_acc: 0.7307\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5610 - acc: 0.7375 - val_loss: 0.5686 - val_acc: 0.7422\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.5493 - acc: 0.7455 - val_loss: 0.5576 - val_acc: 0.7438\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5368 - acc: 0.7522 - val_loss: 0.5465 - val_acc: 0.7471\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.5248 - acc: 0.7610 - val_loss: 0.5357 - val_acc: 0.7570\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5140 - acc: 0.7639 - val_loss: 0.5255 - val_acc: 0.7718\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5031 - acc: 0.7699 - val_loss: 0.5158 - val_acc: 0.7750\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4933 - acc: 0.7774 - val_loss: 0.5071 - val_acc: 0.7734\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4844 - acc: 0.7833 - val_loss: 0.4997 - val_acc: 0.7734\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4765 - acc: 0.7884 - val_loss: 0.4931 - val_acc: 0.7750\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4694 - acc: 0.7880 - val_loss: 0.4874 - val_acc: 0.7800\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4637 - acc: 0.7929 - val_loss: 0.4828 - val_acc: 0.7849\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4583 - acc: 0.7938 - val_loss: 0.4777 - val_acc: 0.7750\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4536 - acc: 0.7966 - val_loss: 0.4741 - val_acc: 0.7865\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4498 - acc: 0.8035 - val_loss: 0.4695 - val_acc: 0.7865\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4448 - acc: 0.8017 - val_loss: 0.4663 - val_acc: 0.7882\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4398 - acc: 0.8079 - val_loss: 0.4641 - val_acc: 0.7898\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4366 - acc: 0.8079 - val_loss: 0.4614 - val_acc: 0.7898\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4330 - acc: 0.8095 - val_loss: 0.4596 - val_acc: 0.7915\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4306 - acc: 0.8126 - val_loss: 0.4573 - val_acc: 0.7931\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4278 - acc: 0.8126 - val_loss: 0.4556 - val_acc: 0.7931\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4249 - acc: 0.8141 - val_loss: 0.4540 - val_acc: 0.7931\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4226 - acc: 0.8146 - val_loss: 0.4523 - val_acc: 0.7931\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4204 - acc: 0.8154 - val_loss: 0.4510 - val_acc: 0.7931\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4169 - acc: 0.8179 - val_loss: 0.4500 - val_acc: 0.7947\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4145 - acc: 0.8192 - val_loss: 0.4517 - val_acc: 0.8013\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4122 - acc: 0.8227 - val_loss: 0.4493 - val_acc: 0.8046\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4098 - acc: 0.8214 - val_loss: 0.4485 - val_acc: 0.8030\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4091 - acc: 0.8221 - val_loss: 0.4468 - val_acc: 0.8079\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4066 - acc: 0.8219 - val_loss: 0.4471 - val_acc: 0.8062\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4053 - acc: 0.8241 - val_loss: 0.4467 - val_acc: 0.8062\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4029 - acc: 0.8219 - val_loss: 0.4456 - val_acc: 0.8079\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4012 - acc: 0.8267 - val_loss: 0.4454 - val_acc: 0.8112\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4008 - acc: 0.8232 - val_loss: 0.4438 - val_acc: 0.8079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3988 - acc: 0.8269 - val_loss: 0.4459 - val_acc: 0.8095\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3956 - acc: 0.8287 - val_loss: 0.4428 - val_acc: 0.8062\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3927 - acc: 0.8272 - val_loss: 0.4441 - val_acc: 0.8112\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3938 - acc: 0.8261 - val_loss: 0.4416 - val_acc: 0.7980\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3901 - acc: 0.8296 - val_loss: 0.4433 - val_acc: 0.8128\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3894 - acc: 0.8312 - val_loss: 0.4422 - val_acc: 0.8062\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3879 - acc: 0.8301 - val_loss: 0.4419 - val_acc: 0.8062\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3859 - acc: 0.8316 - val_loss: 0.4406 - val_acc: 0.8095\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3845 - acc: 0.8314 - val_loss: 0.4390 - val_acc: 0.8013\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3829 - acc: 0.8352 - val_loss: 0.4406 - val_acc: 0.8046\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3810 - acc: 0.8325 - val_loss: 0.4401 - val_acc: 0.8013\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3797 - acc: 0.8332 - val_loss: 0.4401 - val_acc: 0.8030\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3779 - acc: 0.8362 - val_loss: 0.4377 - val_acc: 0.8013\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3774 - acc: 0.8362 - val_loss: 0.4392 - val_acc: 0.8079\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3755 - acc: 0.8387 - val_loss: 0.4374 - val_acc: 0.8079\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3739 - acc: 0.8369 - val_loss: 0.4380 - val_acc: 0.8013\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3702 - acc: 0.8391 - val_loss: 0.4362 - val_acc: 0.8095\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3692 - acc: 0.8416 - val_loss: 0.4359 - val_acc: 0.8095\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3682 - acc: 0.8431 - val_loss: 0.4366 - val_acc: 0.8046\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3674 - acc: 0.8404 - val_loss: 0.4368 - val_acc: 0.8046\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3642 - acc: 0.8411 - val_loss: 0.4354 - val_acc: 0.8128\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.3640 - acc: 0.8394 - val_loss: 0.4365 - val_acc: 0.8079\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3623 - acc: 0.8420 - val_loss: 0.4347 - val_acc: 0.8112\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3592 - acc: 0.8435 - val_loss: 0.4357 - val_acc: 0.8128\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3571 - acc: 0.8473 - val_loss: 0.4354 - val_acc: 0.8144\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3558 - acc: 0.8469 - val_loss: 0.4345 - val_acc: 0.8112\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3572 - acc: 0.8442 - val_loss: 0.4376 - val_acc: 0.8062\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3531 - acc: 0.8488 - val_loss: 0.4334 - val_acc: 0.8161\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3509 - acc: 0.8480 - val_loss: 0.4336 - val_acc: 0.8177\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3485 - acc: 0.8497 - val_loss: 0.4329 - val_acc: 0.8144\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3465 - acc: 0.8517 - val_loss: 0.4354 - val_acc: 0.8161\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3448 - acc: 0.8515 - val_loss: 0.4335 - val_acc: 0.8177\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3439 - acc: 0.8537 - val_loss: 0.4320 - val_acc: 0.8144\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3426 - acc: 0.8535 - val_loss: 0.4353 - val_acc: 0.8227\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3396 - acc: 0.8562 - val_loss: 0.4335 - val_acc: 0.8227\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3388 - acc: 0.8535 - val_loss: 0.4319 - val_acc: 0.8112\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3371 - acc: 0.8553 - val_loss: 0.4349 - val_acc: 0.8243\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3340 - acc: 0.8588 - val_loss: 0.4333 - val_acc: 0.8144\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3329 - acc: 0.8581 - val_loss: 0.4340 - val_acc: 0.8112\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3301 - acc: 0.8610 - val_loss: 0.4316 - val_acc: 0.8079\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3286 - acc: 0.8590 - val_loss: 0.4358 - val_acc: 0.8210\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3277 - acc: 0.8610 - val_loss: 0.4338 - val_acc: 0.8128\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3254 - acc: 0.8624 - val_loss: 0.4329 - val_acc: 0.8128\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3232 - acc: 0.8624 - val_loss: 0.4337 - val_acc: 0.8144\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3217 - acc: 0.8621 - val_loss: 0.4333 - val_acc: 0.8161\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.1, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_77\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_78 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_220 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_143 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_221 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_144 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_222 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 1s 44ms/step - loss: 0.7378 - acc: 0.4501 - val_loss: 0.6791 - val_acc: 0.5632\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6706 - acc: 0.5747 - val_loss: 0.6562 - val_acc: 0.5846\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6481 - acc: 0.5811 - val_loss: 0.6311 - val_acc: 0.6273\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6194 - acc: 0.6741 - val_loss: 0.6128 - val_acc: 0.6913\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.5962 - acc: 0.7037 - val_loss: 0.6004 - val_acc: 0.6765\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.5814 - acc: 0.7041 - val_loss: 0.5815 - val_acc: 0.7192\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5616 - acc: 0.7351 - val_loss: 0.5668 - val_acc: 0.7471\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.5449 - acc: 0.7480 - val_loss: 0.5517 - val_acc: 0.7422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.5275 - acc: 0.7539 - val_loss: 0.5376 - val_acc: 0.7438\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.5137 - acc: 0.7666 - val_loss: 0.5250 - val_acc: 0.7537\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.5009 - acc: 0.7780 - val_loss: 0.5132 - val_acc: 0.7553\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4897 - acc: 0.7814 - val_loss: 0.5042 - val_acc: 0.7521\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.4814 - acc: 0.7842 - val_loss: 0.4968 - val_acc: 0.7685\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4699 - acc: 0.7940 - val_loss: 0.4895 - val_acc: 0.7652\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.4640 - acc: 0.7944 - val_loss: 0.4840 - val_acc: 0.7586\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.4566 - acc: 0.8011 - val_loss: 0.4802 - val_acc: 0.7783\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4517 - acc: 0.7975 - val_loss: 0.4731 - val_acc: 0.7734\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4462 - acc: 0.8042 - val_loss: 0.4694 - val_acc: 0.7833\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4406 - acc: 0.8082 - val_loss: 0.4654 - val_acc: 0.7898\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4377 - acc: 0.8039 - val_loss: 0.4628 - val_acc: 0.7898\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4318 - acc: 0.8119 - val_loss: 0.4606 - val_acc: 0.7931\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4261 - acc: 0.8110 - val_loss: 0.4587 - val_acc: 0.7964\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4252 - acc: 0.8132 - val_loss: 0.4567 - val_acc: 0.7997\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4202 - acc: 0.8172 - val_loss: 0.4545 - val_acc: 0.7997\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4191 - acc: 0.8168 - val_loss: 0.4529 - val_acc: 0.7980\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4178 - acc: 0.8190 - val_loss: 0.4532 - val_acc: 0.7997\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4131 - acc: 0.8227 - val_loss: 0.4514 - val_acc: 0.7964\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4110 - acc: 0.8243 - val_loss: 0.4516 - val_acc: 0.8030\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4105 - acc: 0.8223 - val_loss: 0.4483 - val_acc: 0.7997\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4050 - acc: 0.8228 - val_loss: 0.4477 - val_acc: 0.8030\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.4035 - acc: 0.8239 - val_loss: 0.4470 - val_acc: 0.8030\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4014 - acc: 0.8214 - val_loss: 0.4485 - val_acc: 0.8046\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3997 - acc: 0.8301 - val_loss: 0.4447 - val_acc: 0.8095\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.3970 - acc: 0.8270 - val_loss: 0.4447 - val_acc: 0.8062\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3919 - acc: 0.8307 - val_loss: 0.4439 - val_acc: 0.8079\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3939 - acc: 0.8280 - val_loss: 0.4452 - val_acc: 0.8128\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.3889 - acc: 0.8305 - val_loss: 0.4434 - val_acc: 0.7915\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3894 - acc: 0.8269 - val_loss: 0.4488 - val_acc: 0.8046\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3902 - acc: 0.8321 - val_loss: 0.4421 - val_acc: 0.7980\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3870 - acc: 0.8305 - val_loss: 0.4437 - val_acc: 0.8095\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.3827 - acc: 0.8354 - val_loss: 0.4408 - val_acc: 0.8046\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3792 - acc: 0.8349 - val_loss: 0.4421 - val_acc: 0.8095\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3801 - acc: 0.8352 - val_loss: 0.4405 - val_acc: 0.8046\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.3765 - acc: 0.8369 - val_loss: 0.4442 - val_acc: 0.8079\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3733 - acc: 0.8404 - val_loss: 0.4402 - val_acc: 0.8030\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3729 - acc: 0.8404 - val_loss: 0.4421 - val_acc: 0.8079\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.3698 - acc: 0.8398 - val_loss: 0.4395 - val_acc: 0.8095\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3667 - acc: 0.8435 - val_loss: 0.4381 - val_acc: 0.8079\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3649 - acc: 0.8415 - val_loss: 0.4389 - val_acc: 0.8079\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3632 - acc: 0.8404 - val_loss: 0.4392 - val_acc: 0.8079\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.3623 - acc: 0.8466 - val_loss: 0.4393 - val_acc: 0.8046\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3592 - acc: 0.8425 - val_loss: 0.4374 - val_acc: 0.8030\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3578 - acc: 0.8458 - val_loss: 0.4376 - val_acc: 0.8062\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.3538 - acc: 0.8484 - val_loss: 0.4400 - val_acc: 0.8046\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3507 - acc: 0.8520 - val_loss: 0.4379 - val_acc: 0.8095\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3507 - acc: 0.8511 - val_loss: 0.4376 - val_acc: 0.8046\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.3483 - acc: 0.8513 - val_loss: 0.4382 - val_acc: 0.8046\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_78\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_79 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_223 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_145 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_224 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_146 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_225 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.8065 - acc: 0.4350 - val_loss: 0.7405 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7188 - acc: 0.4678 - val_loss: 0.6807 - val_acc: 0.6059\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6754 - acc: 0.5931 - val_loss: 0.6562 - val_acc: 0.5895\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6542 - acc: 0.6006 - val_loss: 0.6421 - val_acc: 0.5895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6396 - acc: 0.6141 - val_loss: 0.6276 - val_acc: 0.6289\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6217 - acc: 0.6743 - val_loss: 0.6140 - val_acc: 0.7241\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6078 - acc: 0.7041 - val_loss: 0.6040 - val_acc: 0.7258\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5932 - acc: 0.7077 - val_loss: 0.5944 - val_acc: 0.7258\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.5840 - acc: 0.7114 - val_loss: 0.5839 - val_acc: 0.7258\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5691 - acc: 0.7269 - val_loss: 0.5737 - val_acc: 0.7422\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5608 - acc: 0.7327 - val_loss: 0.5634 - val_acc: 0.7455\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5501 - acc: 0.7404 - val_loss: 0.5531 - val_acc: 0.7373\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.5383 - acc: 0.7398 - val_loss: 0.5423 - val_acc: 0.7504\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5270 - acc: 0.7555 - val_loss: 0.5320 - val_acc: 0.7652\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5169 - acc: 0.7575 - val_loss: 0.5220 - val_acc: 0.7635\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5053 - acc: 0.7652 - val_loss: 0.5128 - val_acc: 0.7750\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4991 - acc: 0.7696 - val_loss: 0.5050 - val_acc: 0.7767\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4899 - acc: 0.7741 - val_loss: 0.4974 - val_acc: 0.7718\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4834 - acc: 0.7787 - val_loss: 0.4913 - val_acc: 0.7734\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4747 - acc: 0.7887 - val_loss: 0.4876 - val_acc: 0.7833\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4702 - acc: 0.7865 - val_loss: 0.4810 - val_acc: 0.7750\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4664 - acc: 0.7935 - val_loss: 0.4779 - val_acc: 0.7849\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4611 - acc: 0.7942 - val_loss: 0.4730 - val_acc: 0.7849\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4568 - acc: 0.7937 - val_loss: 0.4698 - val_acc: 0.7849\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4535 - acc: 0.7999 - val_loss: 0.4685 - val_acc: 0.7865\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4484 - acc: 0.7973 - val_loss: 0.4646 - val_acc: 0.7800\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4467 - acc: 0.7989 - val_loss: 0.4639 - val_acc: 0.7947\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4422 - acc: 0.8073 - val_loss: 0.4597 - val_acc: 0.7947\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4415 - acc: 0.8066 - val_loss: 0.4581 - val_acc: 0.7931\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4366 - acc: 0.8093 - val_loss: 0.4564 - val_acc: 0.7931\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4336 - acc: 0.8068 - val_loss: 0.4558 - val_acc: 0.7980\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4312 - acc: 0.8113 - val_loss: 0.4532 - val_acc: 0.7898\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4311 - acc: 0.8093 - val_loss: 0.4525 - val_acc: 0.7964\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4260 - acc: 0.8099 - val_loss: 0.4513 - val_acc: 0.7947\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4252 - acc: 0.8126 - val_loss: 0.4497 - val_acc: 0.7980\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4237 - acc: 0.8154 - val_loss: 0.4490 - val_acc: 0.7997\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4189 - acc: 0.8161 - val_loss: 0.4481 - val_acc: 0.7997\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4193 - acc: 0.8128 - val_loss: 0.4484 - val_acc: 0.8030\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4181 - acc: 0.8179 - val_loss: 0.4472 - val_acc: 0.8062\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4151 - acc: 0.8161 - val_loss: 0.4461 - val_acc: 0.8079\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4146 - acc: 0.8217 - val_loss: 0.4454 - val_acc: 0.8079\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4090 - acc: 0.8223 - val_loss: 0.4438 - val_acc: 0.8079\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4076 - acc: 0.8181 - val_loss: 0.4459 - val_acc: 0.8095\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4098 - acc: 0.8214 - val_loss: 0.4444 - val_acc: 0.8095\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4057 - acc: 0.8207 - val_loss: 0.4431 - val_acc: 0.8079\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4054 - acc: 0.8225 - val_loss: 0.4434 - val_acc: 0.8079\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4030 - acc: 0.8252 - val_loss: 0.4418 - val_acc: 0.8112\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4013 - acc: 0.8247 - val_loss: 0.4419 - val_acc: 0.8079\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4021 - acc: 0.8258 - val_loss: 0.4426 - val_acc: 0.8112\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3966 - acc: 0.8227 - val_loss: 0.4407 - val_acc: 0.8144\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3973 - acc: 0.8254 - val_loss: 0.4416 - val_acc: 0.8128\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3963 - acc: 0.8287 - val_loss: 0.4410 - val_acc: 0.8161\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.3927 - acc: 0.8280 - val_loss: 0.4411 - val_acc: 0.8144\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3900 - acc: 0.8318 - val_loss: 0.4391 - val_acc: 0.8112\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3912 - acc: 0.8296 - val_loss: 0.4379 - val_acc: 0.8112\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3903 - acc: 0.8250 - val_loss: 0.4392 - val_acc: 0.8079\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3865 - acc: 0.8327 - val_loss: 0.4370 - val_acc: 0.8112\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3863 - acc: 0.8289 - val_loss: 0.4371 - val_acc: 0.8112\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3845 - acc: 0.8325 - val_loss: 0.4382 - val_acc: 0.8128\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3818 - acc: 0.8323 - val_loss: 0.4384 - val_acc: 0.8112\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3809 - acc: 0.8342 - val_loss: 0.4379 - val_acc: 0.8079\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3777 - acc: 0.8365 - val_loss: 0.4366 - val_acc: 0.8046\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3781 - acc: 0.8360 - val_loss: 0.4367 - val_acc: 0.8030\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3727 - acc: 0.8376 - val_loss: 0.4384 - val_acc: 0.8112\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3738 - acc: 0.8367 - val_loss: 0.4367 - val_acc: 0.8013\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3720 - acc: 0.8387 - val_loss: 0.4380 - val_acc: 0.8095\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3737 - acc: 0.8404 - val_loss: 0.4354 - val_acc: 0.8062\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3707 - acc: 0.8356 - val_loss: 0.4355 - val_acc: 0.8128\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3669 - acc: 0.8429 - val_loss: 0.4372 - val_acc: 0.8079\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3668 - acc: 0.8394 - val_loss: 0.4346 - val_acc: 0.8079\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3614 - acc: 0.8422 - val_loss: 0.4361 - val_acc: 0.8079\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3607 - acc: 0.8435 - val_loss: 0.4347 - val_acc: 0.8030\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3621 - acc: 0.8449 - val_loss: 0.4347 - val_acc: 0.8079\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3591 - acc: 0.8458 - val_loss: 0.4336 - val_acc: 0.8128\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3582 - acc: 0.8464 - val_loss: 0.4346 - val_acc: 0.8095\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3547 - acc: 0.8451 - val_loss: 0.4345 - val_acc: 0.8112\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3530 - acc: 0.8531 - val_loss: 0.4333 - val_acc: 0.8161\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3543 - acc: 0.8489 - val_loss: 0.4349 - val_acc: 0.8095\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3487 - acc: 0.8493 - val_loss: 0.4343 - val_acc: 0.8128\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3494 - acc: 0.8451 - val_loss: 0.4375 - val_acc: 0.8046\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3484 - acc: 0.8489 - val_loss: 0.4348 - val_acc: 0.8128\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3458 - acc: 0.8495 - val_loss: 0.4358 - val_acc: 0.8112\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.1, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_79\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_80 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_226 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_147 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_227 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.6640 - acc: 0.6200 - val_loss: 0.6459 - val_acc: 0.6749\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6413 - acc: 0.6457 - val_loss: 0.6301 - val_acc: 0.6683\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6289 - acc: 0.6590 - val_loss: 0.6181 - val_acc: 0.7209\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6129 - acc: 0.6913 - val_loss: 0.6076 - val_acc: 0.7028\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6015 - acc: 0.7039 - val_loss: 0.5990 - val_acc: 0.7044\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5901 - acc: 0.7119 - val_loss: 0.5901 - val_acc: 0.7126\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5815 - acc: 0.7161 - val_loss: 0.5816 - val_acc: 0.7241\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5714 - acc: 0.7243 - val_loss: 0.5733 - val_acc: 0.7373\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5618 - acc: 0.7345 - val_loss: 0.5658 - val_acc: 0.7389\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5540 - acc: 0.7449 - val_loss: 0.5585 - val_acc: 0.7488\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5462 - acc: 0.7526 - val_loss: 0.5520 - val_acc: 0.7488\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5393 - acc: 0.7524 - val_loss: 0.5458 - val_acc: 0.7570\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5307 - acc: 0.7632 - val_loss: 0.5404 - val_acc: 0.7521\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5260 - acc: 0.7581 - val_loss: 0.5355 - val_acc: 0.7521\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5203 - acc: 0.7652 - val_loss: 0.5305 - val_acc: 0.7521\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5163 - acc: 0.7681 - val_loss: 0.5257 - val_acc: 0.7586\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5098 - acc: 0.7703 - val_loss: 0.5214 - val_acc: 0.7586\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5056 - acc: 0.7756 - val_loss: 0.5172 - val_acc: 0.7603\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5005 - acc: 0.7769 - val_loss: 0.5134 - val_acc: 0.7603\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4968 - acc: 0.7814 - val_loss: 0.5097 - val_acc: 0.7635\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4931 - acc: 0.7803 - val_loss: 0.5062 - val_acc: 0.7635\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.5006 - acc: 0.769 - 0s 12ms/step - loss: 0.4892 - acc: 0.7865 - val_loss: 0.5031 - val_acc: 0.7668\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4852 - acc: 0.7856 - val_loss: 0.5000 - val_acc: 0.7685\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4821 - acc: 0.7825 - val_loss: 0.4973 - val_acc: 0.7668\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4805 - acc: 0.7895 - val_loss: 0.4947 - val_acc: 0.7685\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4778 - acc: 0.7891 - val_loss: 0.4921 - val_acc: 0.7701\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4743 - acc: 0.7900 - val_loss: 0.4899 - val_acc: 0.7718\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4705 - acc: 0.7927 - val_loss: 0.4876 - val_acc: 0.7718\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4670 - acc: 0.7946 - val_loss: 0.4856 - val_acc: 0.7734\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4669 - acc: 0.7958 - val_loss: 0.4836 - val_acc: 0.7734\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4624 - acc: 0.7966 - val_loss: 0.4817 - val_acc: 0.7783\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4608 - acc: 0.7964 - val_loss: 0.4799 - val_acc: 0.7750\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4591 - acc: 0.8000 - val_loss: 0.4784 - val_acc: 0.7800\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4578 - acc: 0.7989 - val_loss: 0.4767 - val_acc: 0.7767\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4564 - acc: 0.8035 - val_loss: 0.4751 - val_acc: 0.7800\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4544 - acc: 0.8024 - val_loss: 0.4743 - val_acc: 0.7865\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4520 - acc: 0.8002 - val_loss: 0.4727 - val_acc: 0.7849\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4502 - acc: 0.8022 - val_loss: 0.4717 - val_acc: 0.7849\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4493 - acc: 0.8046 - val_loss: 0.4709 - val_acc: 0.7898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4477 - acc: 0.8041 - val_loss: 0.4699 - val_acc: 0.7898\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4474 - acc: 0.8062 - val_loss: 0.4687 - val_acc: 0.7898\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4466 - acc: 0.8079 - val_loss: 0.4680 - val_acc: 0.7947\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4443 - acc: 0.8088 - val_loss: 0.4667 - val_acc: 0.7898\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4422 - acc: 0.8042 - val_loss: 0.4662 - val_acc: 0.7898\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4416 - acc: 0.8081 - val_loss: 0.4650 - val_acc: 0.7931\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4403 - acc: 0.8090 - val_loss: 0.4645 - val_acc: 0.7980\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4377 - acc: 0.8104 - val_loss: 0.4631 - val_acc: 0.7915\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4385 - acc: 0.8092 - val_loss: 0.4620 - val_acc: 0.7980\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4352 - acc: 0.8101 - val_loss: 0.4613 - val_acc: 0.7980\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4345 - acc: 0.8115 - val_loss: 0.4608 - val_acc: 0.7997\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4337 - acc: 0.8119 - val_loss: 0.4604 - val_acc: 0.7980\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4337 - acc: 0.8154 - val_loss: 0.4598 - val_acc: 0.7980\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4315 - acc: 0.8134 - val_loss: 0.4593 - val_acc: 0.7980\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4309 - acc: 0.8124 - val_loss: 0.4587 - val_acc: 0.7980\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4297 - acc: 0.8144 - val_loss: 0.4578 - val_acc: 0.7980\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4291 - acc: 0.8117 - val_loss: 0.4571 - val_acc: 0.7980\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4278 - acc: 0.8132 - val_loss: 0.4566 - val_acc: 0.7980\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4269 - acc: 0.8143 - val_loss: 0.4561 - val_acc: 0.7980\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4255 - acc: 0.8146 - val_loss: 0.4558 - val_acc: 0.7964\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4253 - acc: 0.8137 - val_loss: 0.4560 - val_acc: 0.8046\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4254 - acc: 0.8130 - val_loss: 0.4553 - val_acc: 0.8046\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4241 - acc: 0.8188 - val_loss: 0.4543 - val_acc: 0.7980\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4210 - acc: 0.8170 - val_loss: 0.4538 - val_acc: 0.8013\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4211 - acc: 0.8197 - val_loss: 0.4540 - val_acc: 0.8062\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4192 - acc: 0.8228 - val_loss: 0.4535 - val_acc: 0.8030\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4188 - acc: 0.8199 - val_loss: 0.4529 - val_acc: 0.7964\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4197 - acc: 0.8179 - val_loss: 0.4524 - val_acc: 0.8030\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4182 - acc: 0.8188 - val_loss: 0.4527 - val_acc: 0.8046\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4167 - acc: 0.8216 - val_loss: 0.4523 - val_acc: 0.8046\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4182 - acc: 0.8203 - val_loss: 0.4511 - val_acc: 0.7997\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4183 - acc: 0.8179 - val_loss: 0.4508 - val_acc: 0.8030\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4168 - acc: 0.8207 - val_loss: 0.4517 - val_acc: 0.8095\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4150 - acc: 0.8199 - val_loss: 0.4505 - val_acc: 0.8062\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4149 - acc: 0.8186 - val_loss: 0.4503 - val_acc: 0.8013\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4135 - acc: 0.8205 - val_loss: 0.4496 - val_acc: 0.8062\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4147 - acc: 0.8203 - val_loss: 0.4509 - val_acc: 0.8128\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4142 - acc: 0.8228 - val_loss: 0.4487 - val_acc: 0.8062\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4109 - acc: 0.8228 - val_loss: 0.4484 - val_acc: 0.8062\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4113 - acc: 0.8219 - val_loss: 0.4483 - val_acc: 0.8095\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4098 - acc: 0.8214 - val_loss: 0.4484 - val_acc: 0.8046\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4086 - acc: 0.8227 - val_loss: 0.4483 - val_acc: 0.8062\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4085 - acc: 0.8230 - val_loss: 0.4480 - val_acc: 0.8079\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4084 - acc: 0.8212 - val_loss: 0.4487 - val_acc: 0.8112\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4087 - acc: 0.8227 - val_loss: 0.4479 - val_acc: 0.8112\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4063 - acc: 0.8252 - val_loss: 0.4476 - val_acc: 0.8112\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4056 - acc: 0.8212 - val_loss: 0.4475 - val_acc: 0.8079\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4050 - acc: 0.8239 - val_loss: 0.4472 - val_acc: 0.8128\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4048 - acc: 0.8259 - val_loss: 0.4469 - val_acc: 0.8144\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4057 - acc: 0.8238 - val_loss: 0.4470 - val_acc: 0.8144\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4045 - acc: 0.8259 - val_loss: 0.4466 - val_acc: 0.8144\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4033 - acc: 0.8274 - val_loss: 0.4463 - val_acc: 0.8144\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4039 - acc: 0.8265 - val_loss: 0.4463 - val_acc: 0.8128\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4025 - acc: 0.8278 - val_loss: 0.4461 - val_acc: 0.8177\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4023 - acc: 0.8243 - val_loss: 0.4456 - val_acc: 0.8144\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4028 - acc: 0.8263 - val_loss: 0.4459 - val_acc: 0.8161\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4013 - acc: 0.8248 - val_loss: 0.4461 - val_acc: 0.8128\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4015 - acc: 0.8270 - val_loss: 0.4455 - val_acc: 0.8095\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3985 - acc: 0.8289 - val_loss: 0.4458 - val_acc: 0.8128\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4003 - acc: 0.8263 - val_loss: 0.4457 - val_acc: 0.8128\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3975 - acc: 0.8243 - val_loss: 0.4452 - val_acc: 0.8079\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3985 - acc: 0.8270 - val_loss: 0.4452 - val_acc: 0.8112\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3980 - acc: 0.8289 - val_loss: 0.4450 - val_acc: 0.8112\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3963 - acc: 0.8294 - val_loss: 0.4445 - val_acc: 0.8095\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3979 - acc: 0.8265 - val_loss: 0.4449 - val_acc: 0.8128\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3963 - acc: 0.8283 - val_loss: 0.4446 - val_acc: 0.8144\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3951 - acc: 0.8272 - val_loss: 0.4440 - val_acc: 0.8128\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3956 - acc: 0.8278 - val_loss: 0.4439 - val_acc: 0.8095\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3949 - acc: 0.8323 - val_loss: 0.4438 - val_acc: 0.8128\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3949 - acc: 0.8278 - val_loss: 0.4436 - val_acc: 0.8095\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3936 - acc: 0.8305 - val_loss: 0.4445 - val_acc: 0.8079\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3936 - acc: 0.8280 - val_loss: 0.4439 - val_acc: 0.8095\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3931 - acc: 0.8290 - val_loss: 0.4435 - val_acc: 0.8112\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3922 - acc: 0.8287 - val_loss: 0.4436 - val_acc: 0.8128\n",
      "Epoch 114/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3932 - acc: 0.8320 - val_loss: 0.4438 - val_acc: 0.8079\n",
      "Epoch 115/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3908 - acc: 0.8311 - val_loss: 0.4432 - val_acc: 0.8079\n",
      "Epoch 116/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3912 - acc: 0.8314 - val_loss: 0.4430 - val_acc: 0.8112\n",
      "Epoch 117/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3917 - acc: 0.8300 - val_loss: 0.4430 - val_acc: 0.8079\n",
      "Epoch 118/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3905 - acc: 0.8314 - val_loss: 0.4435 - val_acc: 0.8062\n",
      "Epoch 119/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3892 - acc: 0.8296 - val_loss: 0.4428 - val_acc: 0.8079\n",
      "Epoch 120/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3907 - acc: 0.8294 - val_loss: 0.4433 - val_acc: 0.8095\n",
      "Epoch 121/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3886 - acc: 0.8340 - val_loss: 0.4428 - val_acc: 0.8062\n",
      "Epoch 122/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3882 - acc: 0.8298 - val_loss: 0.4428 - val_acc: 0.8062\n",
      "Epoch 123/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3892 - acc: 0.8321 - val_loss: 0.4434 - val_acc: 0.8079\n",
      "Epoch 124/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3871 - acc: 0.8329 - val_loss: 0.4426 - val_acc: 0.8062\n",
      "Epoch 125/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3861 - acc: 0.8316 - val_loss: 0.4426 - val_acc: 0.8046\n",
      "Epoch 126/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3875 - acc: 0.8331 - val_loss: 0.4432 - val_acc: 0.8079\n",
      "Epoch 127/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3875 - acc: 0.8332 - val_loss: 0.4424 - val_acc: 0.8079\n",
      "Epoch 128/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3856 - acc: 0.8334 - val_loss: 0.4421 - val_acc: 0.8062\n",
      "Epoch 129/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3856 - acc: 0.8345 - val_loss: 0.4421 - val_acc: 0.8079\n",
      "Epoch 130/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3846 - acc: 0.8314 - val_loss: 0.4416 - val_acc: 0.8030\n",
      "Epoch 131/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3848 - acc: 0.8343 - val_loss: 0.4416 - val_acc: 0.8030\n",
      "Epoch 132/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3837 - acc: 0.8342 - val_loss: 0.4423 - val_acc: 0.8079\n",
      "Epoch 133/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3824 - acc: 0.8331 - val_loss: 0.4417 - val_acc: 0.8062\n",
      "Epoch 134/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3832 - acc: 0.8343 - val_loss: 0.4423 - val_acc: 0.8079\n",
      "Epoch 135/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3815 - acc: 0.8354 - val_loss: 0.4417 - val_acc: 0.8062\n",
      "Epoch 136/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3817 - acc: 0.8340 - val_loss: 0.4418 - val_acc: 0.8079\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.1, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_80\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_81 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_228 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_148 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_229 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_149 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_230 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.7999 - acc: 0.4348 - val_loss: 0.7041 - val_acc: 0.4532\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6919 - acc: 0.5233 - val_loss: 0.6615 - val_acc: 0.5862\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6678 - acc: 0.5681 - val_loss: 0.6441 - val_acc: 0.5846\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6439 - acc: 0.5900 - val_loss: 0.6203 - val_acc: 0.6404\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.6138 - acc: 0.6760 - val_loss: 0.6064 - val_acc: 0.7110\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5967 - acc: 0.7041 - val_loss: 0.5965 - val_acc: 0.6864\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5801 - acc: 0.7095 - val_loss: 0.5799 - val_acc: 0.7225\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5630 - acc: 0.7327 - val_loss: 0.5662 - val_acc: 0.7488\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.5521 - acc: 0.7469 - val_loss: 0.5534 - val_acc: 0.7553\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.5354 - acc: 0.7515 - val_loss: 0.5418 - val_acc: 0.7389\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.5248 - acc: 0.7562 - val_loss: 0.5290 - val_acc: 0.7537\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.5110 - acc: 0.7703 - val_loss: 0.5183 - val_acc: 0.7652\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.5020 - acc: 0.7752 - val_loss: 0.5091 - val_acc: 0.7668\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4923 - acc: 0.7829 - val_loss: 0.5011 - val_acc: 0.7586\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4839 - acc: 0.7822 - val_loss: 0.4931 - val_acc: 0.7668\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4772 - acc: 0.7938 - val_loss: 0.4863 - val_acc: 0.7685\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4674 - acc: 0.7942 - val_loss: 0.4801 - val_acc: 0.7718\n",
      "Epoch 18/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4643 - acc: 0.7969 - val_loss: 0.4746 - val_acc: 0.7750\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4559 - acc: 0.7993 - val_loss: 0.4700 - val_acc: 0.7800\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4516 - acc: 0.8053 - val_loss: 0.4658 - val_acc: 0.7882\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4476 - acc: 0.8055 - val_loss: 0.4619 - val_acc: 0.7898\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4407 - acc: 0.8090 - val_loss: 0.4598 - val_acc: 0.7898\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4381 - acc: 0.8135 - val_loss: 0.4568 - val_acc: 0.7915\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4319 - acc: 0.8179 - val_loss: 0.4544 - val_acc: 0.7947\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4312 - acc: 0.8146 - val_loss: 0.4527 - val_acc: 0.7964\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4259 - acc: 0.8123 - val_loss: 0.4505 - val_acc: 0.7997\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4235 - acc: 0.8177 - val_loss: 0.4491 - val_acc: 0.7997\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4201 - acc: 0.8179 - val_loss: 0.4495 - val_acc: 0.8013\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4187 - acc: 0.8192 - val_loss: 0.4471 - val_acc: 0.7997\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4175 - acc: 0.8221 - val_loss: 0.4462 - val_acc: 0.7997\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4126 - acc: 0.8238 - val_loss: 0.4448 - val_acc: 0.8013\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4114 - acc: 0.8219 - val_loss: 0.4445 - val_acc: 0.7997\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4099 - acc: 0.8265 - val_loss: 0.4427 - val_acc: 0.7964\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4048 - acc: 0.8234 - val_loss: 0.4424 - val_acc: 0.8030\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4042 - acc: 0.8269 - val_loss: 0.4408 - val_acc: 0.8046\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4024 - acc: 0.8217 - val_loss: 0.4431 - val_acc: 0.8144\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4050 - acc: 0.8230 - val_loss: 0.4402 - val_acc: 0.8095\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3986 - acc: 0.8256 - val_loss: 0.4386 - val_acc: 0.8062\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.3981 - acc: 0.8300 - val_loss: 0.4387 - val_acc: 0.8062\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3972 - acc: 0.8329 - val_loss: 0.4388 - val_acc: 0.8046\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3986 - acc: 0.8287 - val_loss: 0.4406 - val_acc: 0.8128\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3918 - acc: 0.8276 - val_loss: 0.4386 - val_acc: 0.8079\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3894 - acc: 0.8327 - val_loss: 0.4411 - val_acc: 0.8112\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3869 - acc: 0.8334 - val_loss: 0.4371 - val_acc: 0.8095\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3843 - acc: 0.8345 - val_loss: 0.4390 - val_acc: 0.8112\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3838 - acc: 0.8362 - val_loss: 0.4377 - val_acc: 0.8079\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3833 - acc: 0.8378 - val_loss: 0.4363 - val_acc: 0.8030\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3818 - acc: 0.8365 - val_loss: 0.4372 - val_acc: 0.8095\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3791 - acc: 0.8365 - val_loss: 0.4357 - val_acc: 0.8013\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.3774 - acc: 0.8349 - val_loss: 0.4376 - val_acc: 0.8095\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3780 - acc: 0.8391 - val_loss: 0.4357 - val_acc: 0.8112\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3747 - acc: 0.8349 - val_loss: 0.4347 - val_acc: 0.8095\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.3705 - acc: 0.8404 - val_loss: 0.4373 - val_acc: 0.8079\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.3707 - acc: 0.8402 - val_loss: 0.4352 - val_acc: 0.8079\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3701 - acc: 0.8429 - val_loss: 0.4370 - val_acc: 0.8079\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3665 - acc: 0.8435 - val_loss: 0.4350 - val_acc: 0.8095\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3654 - acc: 0.8464 - val_loss: 0.4350 - val_acc: 0.8079\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.1, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_81\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_82 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_231 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_150 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_232 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_151 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_233 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 1s 34ms/step - loss: 0.8403 - acc: 0.4320 - val_loss: 0.7630 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.7416 - acc: 0.4452 - val_loss: 0.6981 - val_acc: 0.4696\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6931 - acc: 0.5169 - val_loss: 0.6671 - val_acc: 0.6437\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6665 - acc: 0.6207 - val_loss: 0.6508 - val_acc: 0.6338\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6498 - acc: 0.6415 - val_loss: 0.6369 - val_acc: 0.6732\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6358 - acc: 0.6749 - val_loss: 0.6240 - val_acc: 0.7422\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6203 - acc: 0.7024 - val_loss: 0.6130 - val_acc: 0.7455\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6077 - acc: 0.7088 - val_loss: 0.6040 - val_acc: 0.7274\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5976 - acc: 0.7026 - val_loss: 0.5946 - val_acc: 0.7209\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5878 - acc: 0.7148 - val_loss: 0.5850 - val_acc: 0.7258\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5767 - acc: 0.7276 - val_loss: 0.5761 - val_acc: 0.7488\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5714 - acc: 0.7269 - val_loss: 0.5675 - val_acc: 0.7488\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5609 - acc: 0.7395 - val_loss: 0.5592 - val_acc: 0.7471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5497 - acc: 0.7438 - val_loss: 0.5513 - val_acc: 0.7438\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.5416 - acc: 0.7519 - val_loss: 0.5436 - val_acc: 0.7619\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5358 - acc: 0.7546 - val_loss: 0.5360 - val_acc: 0.7652\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5240 - acc: 0.7626 - val_loss: 0.5286 - val_acc: 0.7635\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.5166 - acc: 0.7615 - val_loss: 0.5219 - val_acc: 0.7619\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5087 - acc: 0.7721 - val_loss: 0.5154 - val_acc: 0.7718\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5057 - acc: 0.7685 - val_loss: 0.5097 - val_acc: 0.7685\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5005 - acc: 0.7710 - val_loss: 0.5045 - val_acc: 0.7701\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4949 - acc: 0.7719 - val_loss: 0.4997 - val_acc: 0.7718\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4868 - acc: 0.7781 - val_loss: 0.4952 - val_acc: 0.7734\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4821 - acc: 0.7831 - val_loss: 0.4913 - val_acc: 0.7800\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4809 - acc: 0.7833 - val_loss: 0.4876 - val_acc: 0.7767\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4737 - acc: 0.7885 - val_loss: 0.4844 - val_acc: 0.7816\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4725 - acc: 0.7891 - val_loss: 0.4809 - val_acc: 0.7833\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4666 - acc: 0.7885 - val_loss: 0.4779 - val_acc: 0.7833\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4626 - acc: 0.7904 - val_loss: 0.4750 - val_acc: 0.7833\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4595 - acc: 0.8015 - val_loss: 0.4736 - val_acc: 0.7849\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4588 - acc: 0.7978 - val_loss: 0.4701 - val_acc: 0.7865\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4559 - acc: 0.7973 - val_loss: 0.4679 - val_acc: 0.7865\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4501 - acc: 0.7982 - val_loss: 0.4663 - val_acc: 0.7898\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4482 - acc: 0.8030 - val_loss: 0.4643 - val_acc: 0.7915\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4467 - acc: 0.8037 - val_loss: 0.4630 - val_acc: 0.7915\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4471 - acc: 0.8030 - val_loss: 0.4611 - val_acc: 0.7915\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4427 - acc: 0.8039 - val_loss: 0.4605 - val_acc: 0.7947\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4415 - acc: 0.8048 - val_loss: 0.4588 - val_acc: 0.7898\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4378 - acc: 0.8053 - val_loss: 0.4575 - val_acc: 0.7915\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4338 - acc: 0.8090 - val_loss: 0.4565 - val_acc: 0.7964\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4325 - acc: 0.8090 - val_loss: 0.4552 - val_acc: 0.7964\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4287 - acc: 0.8113 - val_loss: 0.4536 - val_acc: 0.7947\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4285 - acc: 0.8084 - val_loss: 0.4530 - val_acc: 0.7980\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4291 - acc: 0.8126 - val_loss: 0.4514 - val_acc: 0.7980\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4271 - acc: 0.8112 - val_loss: 0.4506 - val_acc: 0.7980\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4254 - acc: 0.8148 - val_loss: 0.4500 - val_acc: 0.7997\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4206 - acc: 0.8172 - val_loss: 0.4492 - val_acc: 0.8030\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4228 - acc: 0.8163 - val_loss: 0.4492 - val_acc: 0.8112\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4205 - acc: 0.8166 - val_loss: 0.4475 - val_acc: 0.8062\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4220 - acc: 0.8166 - val_loss: 0.4472 - val_acc: 0.8112\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4212 - acc: 0.8135 - val_loss: 0.4471 - val_acc: 0.8128\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4166 - acc: 0.8186 - val_loss: 0.4464 - val_acc: 0.8128\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4133 - acc: 0.8205 - val_loss: 0.4449 - val_acc: 0.8079\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4154 - acc: 0.8161 - val_loss: 0.4454 - val_acc: 0.8144\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4122 - acc: 0.8203 - val_loss: 0.4443 - val_acc: 0.8112\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4108 - acc: 0.8225 - val_loss: 0.4438 - val_acc: 0.8112\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4099 - acc: 0.8234 - val_loss: 0.4427 - val_acc: 0.8095\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4087 - acc: 0.8248 - val_loss: 0.4430 - val_acc: 0.8095\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4083 - acc: 0.8203 - val_loss: 0.4424 - val_acc: 0.8095\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4079 - acc: 0.8210 - val_loss: 0.4429 - val_acc: 0.8128\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4053 - acc: 0.8239 - val_loss: 0.4426 - val_acc: 0.8128\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4029 - acc: 0.8248 - val_loss: 0.4413 - val_acc: 0.8128\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4019 - acc: 0.8227 - val_loss: 0.4422 - val_acc: 0.8177\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4010 - acc: 0.8281 - val_loss: 0.4406 - val_acc: 0.8128\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4003 - acc: 0.8225 - val_loss: 0.4399 - val_acc: 0.8112\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3985 - acc: 0.8221 - val_loss: 0.4403 - val_acc: 0.8095\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3947 - acc: 0.8321 - val_loss: 0.4396 - val_acc: 0.8144\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3962 - acc: 0.8301 - val_loss: 0.4394 - val_acc: 0.8112\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3948 - acc: 0.8272 - val_loss: 0.4396 - val_acc: 0.8112\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3947 - acc: 0.8274 - val_loss: 0.4384 - val_acc: 0.8095\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3950 - acc: 0.8272 - val_loss: 0.4387 - val_acc: 0.8079\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3938 - acc: 0.8303 - val_loss: 0.4379 - val_acc: 0.8112\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3930 - acc: 0.8281 - val_loss: 0.4380 - val_acc: 0.8112\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3893 - acc: 0.8323 - val_loss: 0.4376 - val_acc: 0.8095\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3893 - acc: 0.8342 - val_loss: 0.4387 - val_acc: 0.8095\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3889 - acc: 0.8307 - val_loss: 0.4374 - val_acc: 0.8079\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3862 - acc: 0.8343 - val_loss: 0.4382 - val_acc: 0.8095\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3848 - acc: 0.8365 - val_loss: 0.4378 - val_acc: 0.8079\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3851 - acc: 0.8349 - val_loss: 0.4393 - val_acc: 0.8128\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3829 - acc: 0.8340 - val_loss: 0.4370 - val_acc: 0.8062\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3810 - acc: 0.8349 - val_loss: 0.4367 - val_acc: 0.8095\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3815 - acc: 0.8323 - val_loss: 0.4367 - val_acc: 0.8112\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3801 - acc: 0.8376 - val_loss: 0.4377 - val_acc: 0.8095\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3797 - acc: 0.8360 - val_loss: 0.4366 - val_acc: 0.8079\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3771 - acc: 0.8369 - val_loss: 0.4373 - val_acc: 0.8112\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3770 - acc: 0.8369 - val_loss: 0.4368 - val_acc: 0.8112\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3771 - acc: 0.8382 - val_loss: 0.4369 - val_acc: 0.8144\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3748 - acc: 0.8382 - val_loss: 0.4356 - val_acc: 0.8095\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3752 - acc: 0.8424 - val_loss: 0.4358 - val_acc: 0.8112\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3690 - acc: 0.8411 - val_loss: 0.4353 - val_acc: 0.8112\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3713 - acc: 0.8369 - val_loss: 0.4356 - val_acc: 0.8112\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3686 - acc: 0.8413 - val_loss: 0.4357 - val_acc: 0.8112\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3740 - acc: 0.8393 - val_loss: 0.4357 - val_acc: 0.8095\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3694 - acc: 0.8369 - val_loss: 0.4371 - val_acc: 0.8161\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3674 - acc: 0.8442 - val_loss: 0.4355 - val_acc: 0.8095\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.1, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_82\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_83 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_234 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_152 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_235 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_153 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_236 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.7469 - acc: 0.4351 - val_loss: 0.7134 - val_acc: 0.4204\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.7002 - acc: 0.4941 - val_loss: 0.6758 - val_acc: 0.6223\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6716 - acc: 0.6099 - val_loss: 0.6562 - val_acc: 0.6568\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6575 - acc: 0.6269 - val_loss: 0.6446 - val_acc: 0.6174\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6451 - acc: 0.6274 - val_loss: 0.6344 - val_acc: 0.6289\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6355 - acc: 0.6466 - val_loss: 0.6242 - val_acc: 0.6897\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6238 - acc: 0.6762 - val_loss: 0.6145 - val_acc: 0.7356\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6118 - acc: 0.7077 - val_loss: 0.6059 - val_acc: 0.7307\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6009 - acc: 0.7159 - val_loss: 0.5974 - val_acc: 0.7323\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5938 - acc: 0.7090 - val_loss: 0.5880 - val_acc: 0.7389\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5790 - acc: 0.7261 - val_loss: 0.5787 - val_acc: 0.7537\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5708 - acc: 0.7362 - val_loss: 0.5698 - val_acc: 0.7553\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5606 - acc: 0.7384 - val_loss: 0.5614 - val_acc: 0.7521\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5509 - acc: 0.7471 - val_loss: 0.5530 - val_acc: 0.7504\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5426 - acc: 0.7490 - val_loss: 0.5447 - val_acc: 0.7553\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5339 - acc: 0.7550 - val_loss: 0.5361 - val_acc: 0.7619\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5236 - acc: 0.7619 - val_loss: 0.5281 - val_acc: 0.7685\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.5170 - acc: 0.7657 - val_loss: 0.5210 - val_acc: 0.7635\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5098 - acc: 0.7705 - val_loss: 0.5140 - val_acc: 0.7668\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5025 - acc: 0.7745 - val_loss: 0.5083 - val_acc: 0.7701\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4953 - acc: 0.7765 - val_loss: 0.5027 - val_acc: 0.7668\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4901 - acc: 0.7803 - val_loss: 0.4978 - val_acc: 0.7668\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4804 - acc: 0.7854 - val_loss: 0.4936 - val_acc: 0.7734\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4783 - acc: 0.7880 - val_loss: 0.4893 - val_acc: 0.7718\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4743 - acc: 0.7898 - val_loss: 0.4857 - val_acc: 0.7685\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4698 - acc: 0.7929 - val_loss: 0.4827 - val_acc: 0.7718\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4692 - acc: 0.7931 - val_loss: 0.4797 - val_acc: 0.7800\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4636 - acc: 0.7951 - val_loss: 0.4764 - val_acc: 0.7734\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4600 - acc: 0.7953 - val_loss: 0.4742 - val_acc: 0.7750\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4570 - acc: 0.7973 - val_loss: 0.4719 - val_acc: 0.7865\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4547 - acc: 0.7968 - val_loss: 0.4693 - val_acc: 0.7816\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4513 - acc: 0.7980 - val_loss: 0.4673 - val_acc: 0.7882\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4492 - acc: 0.8050 - val_loss: 0.4660 - val_acc: 0.7915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4459 - acc: 0.8037 - val_loss: 0.4640 - val_acc: 0.7931\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4441 - acc: 0.8031 - val_loss: 0.4625 - val_acc: 0.7915\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4418 - acc: 0.8046 - val_loss: 0.4614 - val_acc: 0.7947\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4416 - acc: 0.8075 - val_loss: 0.4599 - val_acc: 0.7947\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4377 - acc: 0.8072 - val_loss: 0.4585 - val_acc: 0.7964\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4364 - acc: 0.8095 - val_loss: 0.4578 - val_acc: 0.7980\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4346 - acc: 0.8073 - val_loss: 0.4563 - val_acc: 0.7964\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4327 - acc: 0.8082 - val_loss: 0.4557 - val_acc: 0.7997\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4305 - acc: 0.8117 - val_loss: 0.4545 - val_acc: 0.8030\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4284 - acc: 0.8128 - val_loss: 0.4534 - val_acc: 0.8030\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4267 - acc: 0.8135 - val_loss: 0.4533 - val_acc: 0.8079\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4262 - acc: 0.8152 - val_loss: 0.4517 - val_acc: 0.8079\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4239 - acc: 0.8124 - val_loss: 0.4513 - val_acc: 0.8079\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4246 - acc: 0.8143 - val_loss: 0.4504 - val_acc: 0.8062\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4198 - acc: 0.8134 - val_loss: 0.4501 - val_acc: 0.8079\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4195 - acc: 0.8176 - val_loss: 0.4502 - val_acc: 0.8112\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4176 - acc: 0.8185 - val_loss: 0.4492 - val_acc: 0.8062\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4172 - acc: 0.8132 - val_loss: 0.4488 - val_acc: 0.8095\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4154 - acc: 0.8196 - val_loss: 0.4506 - val_acc: 0.8112\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4153 - acc: 0.8186 - val_loss: 0.4479 - val_acc: 0.8112\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4149 - acc: 0.8188 - val_loss: 0.4471 - val_acc: 0.8046\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4109 - acc: 0.8207 - val_loss: 0.4483 - val_acc: 0.8128\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4093 - acc: 0.8207 - val_loss: 0.4459 - val_acc: 0.8062\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4116 - acc: 0.8190 - val_loss: 0.4465 - val_acc: 0.8194\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4079 - acc: 0.8223 - val_loss: 0.4459 - val_acc: 0.8144\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4057 - acc: 0.8248 - val_loss: 0.4450 - val_acc: 0.8112\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4043 - acc: 0.8223 - val_loss: 0.4446 - val_acc: 0.8161\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4014 - acc: 0.8208 - val_loss: 0.4434 - val_acc: 0.8210\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4035 - acc: 0.8223 - val_loss: 0.4433 - val_acc: 0.8144\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4029 - acc: 0.8259 - val_loss: 0.4446 - val_acc: 0.8128\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3992 - acc: 0.8269 - val_loss: 0.4429 - val_acc: 0.8161\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4012 - acc: 0.8250 - val_loss: 0.4424 - val_acc: 0.8161\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3970 - acc: 0.8258 - val_loss: 0.4427 - val_acc: 0.8144\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3968 - acc: 0.8311 - val_loss: 0.4412 - val_acc: 0.8095\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3962 - acc: 0.8274 - val_loss: 0.4421 - val_acc: 0.8177\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3975 - acc: 0.8267 - val_loss: 0.4414 - val_acc: 0.8128\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3947 - acc: 0.8265 - val_loss: 0.4414 - val_acc: 0.8177\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3914 - acc: 0.8252 - val_loss: 0.4419 - val_acc: 0.8177\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3939 - acc: 0.8287 - val_loss: 0.4408 - val_acc: 0.8079\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3945 - acc: 0.8281 - val_loss: 0.4407 - val_acc: 0.8144\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3918 - acc: 0.8270 - val_loss: 0.4406 - val_acc: 0.8128\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3906 - acc: 0.8323 - val_loss: 0.4399 - val_acc: 0.8128\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3852 - acc: 0.8301 - val_loss: 0.4408 - val_acc: 0.8128\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3889 - acc: 0.8314 - val_loss: 0.4402 - val_acc: 0.8161\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3846 - acc: 0.8332 - val_loss: 0.4396 - val_acc: 0.8095\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3857 - acc: 0.8320 - val_loss: 0.4409 - val_acc: 0.8177\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3841 - acc: 0.8329 - val_loss: 0.4395 - val_acc: 0.8144\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3826 - acc: 0.8349 - val_loss: 0.4403 - val_acc: 0.8128\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3812 - acc: 0.8312 - val_loss: 0.4396 - val_acc: 0.8128\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3799 - acc: 0.8340 - val_loss: 0.4383 - val_acc: 0.8112\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3808 - acc: 0.8371 - val_loss: 0.4380 - val_acc: 0.8128\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3808 - acc: 0.8332 - val_loss: 0.4385 - val_acc: 0.8112\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3757 - acc: 0.8387 - val_loss: 0.4388 - val_acc: 0.8144\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3772 - acc: 0.8367 - val_loss: 0.4382 - val_acc: 0.8079\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3748 - acc: 0.8374 - val_loss: 0.4376 - val_acc: 0.8062\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3733 - acc: 0.8385 - val_loss: 0.4389 - val_acc: 0.8177\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3753 - acc: 0.8398 - val_loss: 0.4388 - val_acc: 0.8079\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3738 - acc: 0.8384 - val_loss: 0.4387 - val_acc: 0.8128\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3683 - acc: 0.8389 - val_loss: 0.4385 - val_acc: 0.8062\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3723 - acc: 0.8365 - val_loss: 0.4386 - val_acc: 0.8062\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.1, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_83\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_84 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_237 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_154 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_238 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_155 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_239 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 34ms/step - loss: 0.8080 - acc: 0.4335 - val_loss: 0.7404 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7164 - acc: 0.4687 - val_loss: 0.6803 - val_acc: 0.6043\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6724 - acc: 0.5975 - val_loss: 0.6565 - val_acc: 0.5895\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6543 - acc: 0.5973 - val_loss: 0.6428 - val_acc: 0.5878\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6427 - acc: 0.6046 - val_loss: 0.6278 - val_acc: 0.6223\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6228 - acc: 0.6641 - val_loss: 0.6136 - val_acc: 0.7225\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6052 - acc: 0.7072 - val_loss: 0.6030 - val_acc: 0.7373\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5950 - acc: 0.7101 - val_loss: 0.5936 - val_acc: 0.7126\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5831 - acc: 0.7057 - val_loss: 0.5829 - val_acc: 0.7241\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5708 - acc: 0.7209 - val_loss: 0.5724 - val_acc: 0.7471\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5575 - acc: 0.7367 - val_loss: 0.5622 - val_acc: 0.7488\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5480 - acc: 0.7433 - val_loss: 0.5522 - val_acc: 0.7471\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5370 - acc: 0.7557 - val_loss: 0.5419 - val_acc: 0.7504\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.5275 - acc: 0.7528 - val_loss: 0.5317 - val_acc: 0.7652\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5163 - acc: 0.7606 - val_loss: 0.5218 - val_acc: 0.7734\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5085 - acc: 0.7628 - val_loss: 0.5126 - val_acc: 0.7718\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4981 - acc: 0.7734 - val_loss: 0.5042 - val_acc: 0.7767\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4916 - acc: 0.7739 - val_loss: 0.4969 - val_acc: 0.7701\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4825 - acc: 0.7789 - val_loss: 0.4907 - val_acc: 0.7701\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4747 - acc: 0.7849 - val_loss: 0.4852 - val_acc: 0.7800\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4705 - acc: 0.7874 - val_loss: 0.4800 - val_acc: 0.7816\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4653 - acc: 0.7896 - val_loss: 0.4755 - val_acc: 0.7833\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4572 - acc: 0.7931 - val_loss: 0.4729 - val_acc: 0.7882\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4574 - acc: 0.7960 - val_loss: 0.4693 - val_acc: 0.7882\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4504 - acc: 0.8019 - val_loss: 0.4661 - val_acc: 0.7882\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4491 - acc: 0.8026 - val_loss: 0.4633 - val_acc: 0.7947\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4429 - acc: 0.8042 - val_loss: 0.4606 - val_acc: 0.7947\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4403 - acc: 0.8053 - val_loss: 0.4592 - val_acc: 0.7947\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4374 - acc: 0.8046 - val_loss: 0.4574 - val_acc: 0.7915\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4336 - acc: 0.8082 - val_loss: 0.4557 - val_acc: 0.7931\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4337 - acc: 0.8062 - val_loss: 0.4541 - val_acc: 0.7980\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4273 - acc: 0.8146 - val_loss: 0.4525 - val_acc: 0.8013\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4275 - acc: 0.8106 - val_loss: 0.4514 - val_acc: 0.8013\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4241 - acc: 0.8154 - val_loss: 0.4502 - val_acc: 0.8013\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4207 - acc: 0.8154 - val_loss: 0.4493 - val_acc: 0.8030\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4204 - acc: 0.8166 - val_loss: 0.4488 - val_acc: 0.8046\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4164 - acc: 0.8155 - val_loss: 0.4477 - val_acc: 0.8046\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4164 - acc: 0.8172 - val_loss: 0.4472 - val_acc: 0.8079\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4127 - acc: 0.8172 - val_loss: 0.4464 - val_acc: 0.8095\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4138 - acc: 0.8161 - val_loss: 0.4440 - val_acc: 0.8079\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4099 - acc: 0.8186 - val_loss: 0.4445 - val_acc: 0.8112\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4109 - acc: 0.8207 - val_loss: 0.4441 - val_acc: 0.8144\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4104 - acc: 0.8205 - val_loss: 0.4436 - val_acc: 0.8144\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4055 - acc: 0.8234 - val_loss: 0.4436 - val_acc: 0.8095\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4019 - acc: 0.8230 - val_loss: 0.4415 - val_acc: 0.8062\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4036 - acc: 0.8238 - val_loss: 0.4436 - val_acc: 0.8112\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3998 - acc: 0.8234 - val_loss: 0.4408 - val_acc: 0.8128\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3975 - acc: 0.8259 - val_loss: 0.4431 - val_acc: 0.8128\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3977 - acc: 0.8294 - val_loss: 0.4393 - val_acc: 0.8128\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3937 - acc: 0.8272 - val_loss: 0.4416 - val_acc: 0.8095\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3947 - acc: 0.8316 - val_loss: 0.4399 - val_acc: 0.8079\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3928 - acc: 0.8254 - val_loss: 0.4387 - val_acc: 0.8095\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3894 - acc: 0.8303 - val_loss: 0.4403 - val_acc: 0.8095\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3883 - acc: 0.8298 - val_loss: 0.4384 - val_acc: 0.8062\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3888 - acc: 0.8278 - val_loss: 0.4405 - val_acc: 0.8062\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3892 - acc: 0.8316 - val_loss: 0.4378 - val_acc: 0.8062\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3845 - acc: 0.8298 - val_loss: 0.4375 - val_acc: 0.8112\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3847 - acc: 0.8312 - val_loss: 0.4379 - val_acc: 0.8062\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3820 - acc: 0.8298 - val_loss: 0.4381 - val_acc: 0.7997\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3790 - acc: 0.8354 - val_loss: 0.4413 - val_acc: 0.8062\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3787 - acc: 0.8323 - val_loss: 0.4371 - val_acc: 0.8079\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3753 - acc: 0.8374 - val_loss: 0.4372 - val_acc: 0.8079\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3750 - acc: 0.8382 - val_loss: 0.4382 - val_acc: 0.8062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3742 - acc: 0.8385 - val_loss: 0.4353 - val_acc: 0.8079\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3718 - acc: 0.8400 - val_loss: 0.4378 - val_acc: 0.8046\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3715 - acc: 0.8402 - val_loss: 0.4372 - val_acc: 0.8046\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3689 - acc: 0.8389 - val_loss: 0.4357 - val_acc: 0.8062\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3714 - acc: 0.8376 - val_loss: 0.4373 - val_acc: 0.8095\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3682 - acc: 0.8387 - val_loss: 0.4356 - val_acc: 0.8112\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.15, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_84\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_85 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_240 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_156 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_241 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_157 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_242 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.7415 - acc: 0.4472 - val_loss: 0.6800 - val_acc: 0.5632\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6744 - acc: 0.5616 - val_loss: 0.6582 - val_acc: 0.5846\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6498 - acc: 0.5873 - val_loss: 0.6335 - val_acc: 0.6108\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6228 - acc: 0.6585 - val_loss: 0.6140 - val_acc: 0.7011\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6011 - acc: 0.7001 - val_loss: 0.6016 - val_acc: 0.6831\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5838 - acc: 0.7068 - val_loss: 0.5847 - val_acc: 0.7192\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5654 - acc: 0.7251 - val_loss: 0.5708 - val_acc: 0.7471\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5536 - acc: 0.7424 - val_loss: 0.5562 - val_acc: 0.7455\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5338 - acc: 0.7468 - val_loss: 0.5434 - val_acc: 0.7389\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5201 - acc: 0.7548 - val_loss: 0.5291 - val_acc: 0.7471\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5072 - acc: 0.7694 - val_loss: 0.5177 - val_acc: 0.7553\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4963 - acc: 0.7747 - val_loss: 0.5081 - val_acc: 0.7537\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4841 - acc: 0.7860 - val_loss: 0.5007 - val_acc: 0.7619\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4775 - acc: 0.7834 - val_loss: 0.4931 - val_acc: 0.7586\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4715 - acc: 0.7869 - val_loss: 0.4869 - val_acc: 0.7668\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4633 - acc: 0.7905 - val_loss: 0.4810 - val_acc: 0.7635\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4554 - acc: 0.7962 - val_loss: 0.4778 - val_acc: 0.7783\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4513 - acc: 0.8028 - val_loss: 0.4717 - val_acc: 0.7783\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4440 - acc: 0.8009 - val_loss: 0.4678 - val_acc: 0.7849\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 54ms/step - loss: 0.4390 - acc: 0.8084 - val_loss: 0.4641 - val_acc: 0.7865\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4366 - acc: 0.8093 - val_loss: 0.4625 - val_acc: 0.7898\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4351 - acc: 0.8057 - val_loss: 0.4599 - val_acc: 0.7898\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4301 - acc: 0.8099 - val_loss: 0.4591 - val_acc: 0.7915\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4278 - acc: 0.8143 - val_loss: 0.4550 - val_acc: 0.7980\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4251 - acc: 0.8112 - val_loss: 0.4545 - val_acc: 0.7980\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4213 - acc: 0.8163 - val_loss: 0.4540 - val_acc: 0.7980\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.4173 - acc: 0.8179 - val_loss: 0.4529 - val_acc: 0.7997\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4173 - acc: 0.8203 - val_loss: 0.4521 - val_acc: 0.8013\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.4126 - acc: 0.8217 - val_loss: 0.4506 - val_acc: 0.7980\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.4107 - acc: 0.8205 - val_loss: 0.4476 - val_acc: 0.7947\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4091 - acc: 0.8210 - val_loss: 0.4468 - val_acc: 0.7980\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4057 - acc: 0.8252 - val_loss: 0.4466 - val_acc: 0.7997\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4054 - acc: 0.8252 - val_loss: 0.4476 - val_acc: 0.8062\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4023 - acc: 0.8294 - val_loss: 0.4454 - val_acc: 0.7997\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4005 - acc: 0.8254 - val_loss: 0.4461 - val_acc: 0.8095\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.3961 - acc: 0.8285 - val_loss: 0.4447 - val_acc: 0.8095\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3936 - acc: 0.8283 - val_loss: 0.4444 - val_acc: 0.8079\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3935 - acc: 0.8301 - val_loss: 0.4449 - val_acc: 0.8079\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3902 - acc: 0.8356 - val_loss: 0.4422 - val_acc: 0.8095\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3883 - acc: 0.8272 - val_loss: 0.4424 - val_acc: 0.8062\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.3868 - acc: 0.8336 - val_loss: 0.4404 - val_acc: 0.8095\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3854 - acc: 0.8351 - val_loss: 0.4401 - val_acc: 0.8079\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.3820 - acc: 0.8365 - val_loss: 0.4402 - val_acc: 0.7980\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3817 - acc: 0.8371 - val_loss: 0.4427 - val_acc: 0.8079\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3778 - acc: 0.8391 - val_loss: 0.4387 - val_acc: 0.7997\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3800 - acc: 0.8352 - val_loss: 0.4415 - val_acc: 0.8095\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3706 - acc: 0.8391 - val_loss: 0.4385 - val_acc: 0.8030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3730 - acc: 0.8387 - val_loss: 0.4398 - val_acc: 0.8095\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3678 - acc: 0.8431 - val_loss: 0.4369 - val_acc: 0.8030\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.3686 - acc: 0.8371 - val_loss: 0.4387 - val_acc: 0.8062\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.3646 - acc: 0.8446 - val_loss: 0.4383 - val_acc: 0.8013\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3659 - acc: 0.8425 - val_loss: 0.4429 - val_acc: 0.8079\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3630 - acc: 0.8424 - val_loss: 0.4375 - val_acc: 0.8046\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3584 - acc: 0.8438 - val_loss: 0.4375 - val_acc: 0.8062\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.15, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_85\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_86 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_243 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_158 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_244 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_159 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_245 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.8128 - acc: 0.4379 - val_loss: 0.7404 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.7245 - acc: 0.4682 - val_loss: 0.6807 - val_acc: 0.6043\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6770 - acc: 0.5787 - val_loss: 0.6574 - val_acc: 0.5895\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6565 - acc: 0.6061 - val_loss: 0.6446 - val_acc: 0.5862\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6465 - acc: 0.6138 - val_loss: 0.6302 - val_acc: 0.6108\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6297 - acc: 0.6563 - val_loss: 0.6160 - val_acc: 0.6995\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6150 - acc: 0.6855 - val_loss: 0.6053 - val_acc: 0.7307\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6003 - acc: 0.7015 - val_loss: 0.5961 - val_acc: 0.7209\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5883 - acc: 0.6997 - val_loss: 0.5863 - val_acc: 0.7241\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5781 - acc: 0.7086 - val_loss: 0.5762 - val_acc: 0.7373\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5661 - acc: 0.7291 - val_loss: 0.5662 - val_acc: 0.7504\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5572 - acc: 0.7289 - val_loss: 0.5559 - val_acc: 0.7504\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5465 - acc: 0.7402 - val_loss: 0.5456 - val_acc: 0.7521\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5332 - acc: 0.7457 - val_loss: 0.5353 - val_acc: 0.7603\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5245 - acc: 0.7548 - val_loss: 0.5256 - val_acc: 0.7701\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5144 - acc: 0.7645 - val_loss: 0.5163 - val_acc: 0.7701\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.5036 - acc: 0.7707 - val_loss: 0.5086 - val_acc: 0.7767\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4963 - acc: 0.7745 - val_loss: 0.5008 - val_acc: 0.7767\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4856 - acc: 0.7776 - val_loss: 0.4944 - val_acc: 0.7718\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4861 - acc: 0.7783 - val_loss: 0.4893 - val_acc: 0.7750\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4790 - acc: 0.7816 - val_loss: 0.4836 - val_acc: 0.7800\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4729 - acc: 0.7823 - val_loss: 0.4796 - val_acc: 0.7833\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4670 - acc: 0.7900 - val_loss: 0.4749 - val_acc: 0.7833\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4616 - acc: 0.7937 - val_loss: 0.4726 - val_acc: 0.7865\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4576 - acc: 0.7955 - val_loss: 0.4691 - val_acc: 0.7898\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4572 - acc: 0.7949 - val_loss: 0.4665 - val_acc: 0.7898\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4490 - acc: 0.7988 - val_loss: 0.4653 - val_acc: 0.7915\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4478 - acc: 0.8024 - val_loss: 0.4612 - val_acc: 0.7915\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4464 - acc: 0.8041 - val_loss: 0.4596 - val_acc: 0.7947\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4422 - acc: 0.8050 - val_loss: 0.4566 - val_acc: 0.7947\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4381 - acc: 0.8050 - val_loss: 0.4547 - val_acc: 0.7915\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4355 - acc: 0.8066 - val_loss: 0.4549 - val_acc: 0.8013\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4338 - acc: 0.8103 - val_loss: 0.4531 - val_acc: 0.7947\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4325 - acc: 0.8103 - val_loss: 0.4518 - val_acc: 0.7931\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4238 - acc: 0.8135 - val_loss: 0.4513 - val_acc: 0.8030\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4243 - acc: 0.8117 - val_loss: 0.4485 - val_acc: 0.7947\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4240 - acc: 0.8137 - val_loss: 0.4494 - val_acc: 0.8013\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4247 - acc: 0.8123 - val_loss: 0.4475 - val_acc: 0.7997\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4213 - acc: 0.8154 - val_loss: 0.4469 - val_acc: 0.7997\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4208 - acc: 0.8130 - val_loss: 0.4481 - val_acc: 0.8079\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4157 - acc: 0.8177 - val_loss: 0.4455 - val_acc: 0.8079\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4155 - acc: 0.8172 - val_loss: 0.4452 - val_acc: 0.8046\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4136 - acc: 0.8196 - val_loss: 0.4449 - val_acc: 0.8079\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4119 - acc: 0.8194 - val_loss: 0.4427 - val_acc: 0.8095\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4133 - acc: 0.8177 - val_loss: 0.4421 - val_acc: 0.8079\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4145 - acc: 0.8210 - val_loss: 0.4440 - val_acc: 0.8112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4091 - acc: 0.8190 - val_loss: 0.4418 - val_acc: 0.8046\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4080 - acc: 0.8234 - val_loss: 0.4448 - val_acc: 0.8095\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4031 - acc: 0.8252 - val_loss: 0.4397 - val_acc: 0.8095\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4009 - acc: 0.8263 - val_loss: 0.4407 - val_acc: 0.8095\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4006 - acc: 0.8263 - val_loss: 0.4413 - val_acc: 0.8062\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3985 - acc: 0.8225 - val_loss: 0.4403 - val_acc: 0.8112\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.3977 - acc: 0.8252 - val_loss: 0.4392 - val_acc: 0.8046\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3950 - acc: 0.8289 - val_loss: 0.4385 - val_acc: 0.8079\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3936 - acc: 0.8276 - val_loss: 0.4386 - val_acc: 0.8046\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3915 - acc: 0.8285 - val_loss: 0.4394 - val_acc: 0.8030\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3896 - acc: 0.8290 - val_loss: 0.4384 - val_acc: 0.8046\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3903 - acc: 0.8269 - val_loss: 0.4379 - val_acc: 0.8013\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3906 - acc: 0.8283 - val_loss: 0.4375 - val_acc: 0.8030\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3878 - acc: 0.8312 - val_loss: 0.4386 - val_acc: 0.8030\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3860 - acc: 0.8294 - val_loss: 0.4366 - val_acc: 0.8030\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3826 - acc: 0.8343 - val_loss: 0.4372 - val_acc: 0.8046\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3844 - acc: 0.8318 - val_loss: 0.4368 - val_acc: 0.8046\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3823 - acc: 0.8338 - val_loss: 0.4356 - val_acc: 0.8046\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3808 - acc: 0.8376 - val_loss: 0.4359 - val_acc: 0.8046\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.3768 - acc: 0.8378 - val_loss: 0.4340 - val_acc: 0.8013\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3752 - acc: 0.8382 - val_loss: 0.4349 - val_acc: 0.8030\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3757 - acc: 0.8376 - val_loss: 0.4342 - val_acc: 0.8013\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3750 - acc: 0.8404 - val_loss: 0.4335 - val_acc: 0.8030\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3719 - acc: 0.8396 - val_loss: 0.4381 - val_acc: 0.8062\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3716 - acc: 0.8358 - val_loss: 0.4327 - val_acc: 0.8013\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3686 - acc: 0.8384 - val_loss: 0.4332 - val_acc: 0.8062\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3700 - acc: 0.8411 - val_loss: 0.4311 - val_acc: 0.8095\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3692 - acc: 0.8380 - val_loss: 0.4310 - val_acc: 0.8079\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3656 - acc: 0.8427 - val_loss: 0.4309 - val_acc: 0.8079\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3615 - acc: 0.8436 - val_loss: 0.4310 - val_acc: 0.8095\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3608 - acc: 0.8425 - val_loss: 0.4306 - val_acc: 0.8095\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3604 - acc: 0.8413 - val_loss: 0.4314 - val_acc: 0.8112\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3567 - acc: 0.8431 - val_loss: 0.4319 - val_acc: 0.8095\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3570 - acc: 0.8433 - val_loss: 0.4317 - val_acc: 0.8062\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3550 - acc: 0.8455 - val_loss: 0.4305 - val_acc: 0.8095\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3503 - acc: 0.8526 - val_loss: 0.4315 - val_acc: 0.8161\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3523 - acc: 0.8511 - val_loss: 0.4300 - val_acc: 0.8095\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3520 - acc: 0.8486 - val_loss: 0.4303 - val_acc: 0.8095\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3485 - acc: 0.8502 - val_loss: 0.4309 - val_acc: 0.8161\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3493 - acc: 0.8500 - val_loss: 0.4309 - val_acc: 0.8144\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3456 - acc: 0.8506 - val_loss: 0.4302 - val_acc: 0.8128\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3427 - acc: 0.8568 - val_loss: 0.4306 - val_acc: 0.8177\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.15, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_86\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_87 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_246 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_160 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_247 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.6675 - acc: 0.6072 - val_loss: 0.6458 - val_acc: 0.6782\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6445 - acc: 0.6344 - val_loss: 0.6299 - val_acc: 0.6700\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6294 - acc: 0.6510 - val_loss: 0.6179 - val_acc: 0.7143\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6153 - acc: 0.6802 - val_loss: 0.6078 - val_acc: 0.7011\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6007 - acc: 0.7064 - val_loss: 0.5990 - val_acc: 0.7077\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5924 - acc: 0.7115 - val_loss: 0.5899 - val_acc: 0.7225\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5805 - acc: 0.7216 - val_loss: 0.5813 - val_acc: 0.7291\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5706 - acc: 0.7307 - val_loss: 0.5735 - val_acc: 0.7356\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5647 - acc: 0.7378 - val_loss: 0.5662 - val_acc: 0.7373\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5548 - acc: 0.7393 - val_loss: 0.5593 - val_acc: 0.7422\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5496 - acc: 0.7429 - val_loss: 0.5524 - val_acc: 0.7471\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5410 - acc: 0.7511 - val_loss: 0.5460 - val_acc: 0.7537\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5336 - acc: 0.7586 - val_loss: 0.5400 - val_acc: 0.7570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5268 - acc: 0.7625 - val_loss: 0.5347 - val_acc: 0.7537\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5215 - acc: 0.7628 - val_loss: 0.5295 - val_acc: 0.7603\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5176 - acc: 0.7645 - val_loss: 0.5249 - val_acc: 0.7603\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5103 - acc: 0.7679 - val_loss: 0.5208 - val_acc: 0.7635\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5058 - acc: 0.7730 - val_loss: 0.5165 - val_acc: 0.7652\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5031 - acc: 0.7780 - val_loss: 0.5126 - val_acc: 0.7635\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4962 - acc: 0.7783 - val_loss: 0.5092 - val_acc: 0.7603\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4939 - acc: 0.7803 - val_loss: 0.5060 - val_acc: 0.7619\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4889 - acc: 0.7814 - val_loss: 0.5028 - val_acc: 0.7701\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4880 - acc: 0.7856 - val_loss: 0.4999 - val_acc: 0.7652\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4825 - acc: 0.7900 - val_loss: 0.4972 - val_acc: 0.7668\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4815 - acc: 0.7865 - val_loss: 0.4945 - val_acc: 0.7685\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4793 - acc: 0.7902 - val_loss: 0.4920 - val_acc: 0.7701\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4761 - acc: 0.7864 - val_loss: 0.4898 - val_acc: 0.7701\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4722 - acc: 0.7938 - val_loss: 0.4875 - val_acc: 0.7734\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4691 - acc: 0.7926 - val_loss: 0.4855 - val_acc: 0.7734\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4694 - acc: 0.7931 - val_loss: 0.4837 - val_acc: 0.7734\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4658 - acc: 0.7946 - val_loss: 0.4820 - val_acc: 0.7734\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4631 - acc: 0.8000 - val_loss: 0.4803 - val_acc: 0.7767\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4613 - acc: 0.7960 - val_loss: 0.4788 - val_acc: 0.7783\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4591 - acc: 0.7968 - val_loss: 0.4771 - val_acc: 0.7783\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4573 - acc: 0.8006 - val_loss: 0.4755 - val_acc: 0.7750\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4554 - acc: 0.8011 - val_loss: 0.4740 - val_acc: 0.7783\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4536 - acc: 0.7986 - val_loss: 0.4729 - val_acc: 0.7865\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4512 - acc: 0.8011 - val_loss: 0.4717 - val_acc: 0.7865\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4496 - acc: 0.8035 - val_loss: 0.4704 - val_acc: 0.7849\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4484 - acc: 0.8033 - val_loss: 0.4693 - val_acc: 0.7865\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4449 - acc: 0.8059 - val_loss: 0.4684 - val_acc: 0.7931\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4448 - acc: 0.8093 - val_loss: 0.4675 - val_acc: 0.7931\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4460 - acc: 0.8081 - val_loss: 0.4666 - val_acc: 0.7915\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4431 - acc: 0.8073 - val_loss: 0.4656 - val_acc: 0.7915\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4436 - acc: 0.8051 - val_loss: 0.4648 - val_acc: 0.7964\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4412 - acc: 0.8064 - val_loss: 0.4637 - val_acc: 0.7898\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4402 - acc: 0.8046 - val_loss: 0.4628 - val_acc: 0.7931\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4378 - acc: 0.8082 - val_loss: 0.4619 - val_acc: 0.7964\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4371 - acc: 0.8126 - val_loss: 0.4612 - val_acc: 0.7964\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4356 - acc: 0.8113 - val_loss: 0.4606 - val_acc: 0.7964\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4326 - acc: 0.8143 - val_loss: 0.4598 - val_acc: 0.7964\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4327 - acc: 0.8135 - val_loss: 0.4595 - val_acc: 0.7980\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4310 - acc: 0.8124 - val_loss: 0.4591 - val_acc: 0.7980\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4312 - acc: 0.8082 - val_loss: 0.4582 - val_acc: 0.7980\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4298 - acc: 0.8113 - val_loss: 0.4580 - val_acc: 0.7980\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4316 - acc: 0.8124 - val_loss: 0.4573 - val_acc: 0.7980\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4288 - acc: 0.8141 - val_loss: 0.4565 - val_acc: 0.7997\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4281 - acc: 0.8166 - val_loss: 0.4560 - val_acc: 0.7997\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4284 - acc: 0.8172 - val_loss: 0.4557 - val_acc: 0.8046\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4260 - acc: 0.8170 - val_loss: 0.4550 - val_acc: 0.7980\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4245 - acc: 0.8157 - val_loss: 0.4546 - val_acc: 0.7980\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4218 - acc: 0.8168 - val_loss: 0.4543 - val_acc: 0.8013\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4234 - acc: 0.8174 - val_loss: 0.4543 - val_acc: 0.8046\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4223 - acc: 0.8183 - val_loss: 0.4536 - val_acc: 0.8013\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4231 - acc: 0.8157 - val_loss: 0.4533 - val_acc: 0.7980\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4203 - acc: 0.8152 - val_loss: 0.4531 - val_acc: 0.8046\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4219 - acc: 0.8159 - val_loss: 0.4529 - val_acc: 0.8030\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4191 - acc: 0.8163 - val_loss: 0.4518 - val_acc: 0.8013\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4173 - acc: 0.8192 - val_loss: 0.4513 - val_acc: 0.8013\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4179 - acc: 0.8183 - val_loss: 0.4522 - val_acc: 0.8112\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4156 - acc: 0.8201 - val_loss: 0.4513 - val_acc: 0.8030\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4145 - acc: 0.8212 - val_loss: 0.4508 - val_acc: 0.8030\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4138 - acc: 0.8183 - val_loss: 0.4504 - val_acc: 0.8046\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4149 - acc: 0.8201 - val_loss: 0.4506 - val_acc: 0.8112\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4150 - acc: 0.8230 - val_loss: 0.4501 - val_acc: 0.8079\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4136 - acc: 0.8201 - val_loss: 0.4498 - val_acc: 0.8079\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4110 - acc: 0.8227 - val_loss: 0.4492 - val_acc: 0.8046\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4109 - acc: 0.8196 - val_loss: 0.4493 - val_acc: 0.8046\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4123 - acc: 0.8232 - val_loss: 0.4493 - val_acc: 0.8144\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4085 - acc: 0.8212 - val_loss: 0.4486 - val_acc: 0.8095\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4120 - acc: 0.8199 - val_loss: 0.4484 - val_acc: 0.8079\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4097 - acc: 0.8239 - val_loss: 0.4480 - val_acc: 0.8095\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4094 - acc: 0.8238 - val_loss: 0.4477 - val_acc: 0.8079\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4073 - acc: 0.8234 - val_loss: 0.4476 - val_acc: 0.8079\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4080 - acc: 0.8247 - val_loss: 0.4478 - val_acc: 0.8128\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4071 - acc: 0.8227 - val_loss: 0.4472 - val_acc: 0.8095\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4068 - acc: 0.8238 - val_loss: 0.4469 - val_acc: 0.8112\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4053 - acc: 0.8250 - val_loss: 0.4466 - val_acc: 0.8112\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4055 - acc: 0.8225 - val_loss: 0.4460 - val_acc: 0.8112\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4058 - acc: 0.8230 - val_loss: 0.4461 - val_acc: 0.8144\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4039 - acc: 0.8238 - val_loss: 0.4459 - val_acc: 0.8144\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4043 - acc: 0.8274 - val_loss: 0.4453 - val_acc: 0.8144\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4020 - acc: 0.8278 - val_loss: 0.4454 - val_acc: 0.8144\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4022 - acc: 0.8256 - val_loss: 0.4452 - val_acc: 0.8112\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4015 - acc: 0.8278 - val_loss: 0.4456 - val_acc: 0.8128\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4029 - acc: 0.8254 - val_loss: 0.4461 - val_acc: 0.8144\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4015 - acc: 0.8267 - val_loss: 0.4456 - val_acc: 0.8144\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4009 - acc: 0.8274 - val_loss: 0.4450 - val_acc: 0.8128\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4013 - acc: 0.8252 - val_loss: 0.4454 - val_acc: 0.8128\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3999 - acc: 0.8254 - val_loss: 0.4450 - val_acc: 0.8128\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3989 - acc: 0.8265 - val_loss: 0.4444 - val_acc: 0.8112\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3989 - acc: 0.8243 - val_loss: 0.4446 - val_acc: 0.8112\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3976 - acc: 0.8303 - val_loss: 0.4441 - val_acc: 0.8128\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3968 - acc: 0.8278 - val_loss: 0.4441 - val_acc: 0.8112\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3959 - acc: 0.8311 - val_loss: 0.4439 - val_acc: 0.8095\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3977 - acc: 0.8256 - val_loss: 0.4443 - val_acc: 0.8095\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3982 - acc: 0.8287 - val_loss: 0.4440 - val_acc: 0.8112\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3952 - acc: 0.8300 - val_loss: 0.4444 - val_acc: 0.8062\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3952 - acc: 0.8274 - val_loss: 0.4445 - val_acc: 0.8046\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3946 - acc: 0.8278 - val_loss: 0.4435 - val_acc: 0.8095\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3953 - acc: 0.8252 - val_loss: 0.4435 - val_acc: 0.8079\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3938 - acc: 0.8298 - val_loss: 0.4433 - val_acc: 0.8095\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3936 - acc: 0.8301 - val_loss: 0.4430 - val_acc: 0.8079\n",
      "Epoch 114/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3919 - acc: 0.8307 - val_loss: 0.4437 - val_acc: 0.8062\n",
      "Epoch 115/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3928 - acc: 0.8300 - val_loss: 0.4433 - val_acc: 0.8079\n",
      "Epoch 116/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3916 - acc: 0.8280 - val_loss: 0.4432 - val_acc: 0.8079\n",
      "Epoch 117/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3922 - acc: 0.8300 - val_loss: 0.4434 - val_acc: 0.8079\n",
      "Epoch 118/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3922 - acc: 0.8285 - val_loss: 0.4428 - val_acc: 0.8112\n",
      "Epoch 119/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3912 - acc: 0.8285 - val_loss: 0.4427 - val_acc: 0.8095\n",
      "Epoch 120/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3922 - acc: 0.8280 - val_loss: 0.4424 - val_acc: 0.8095\n",
      "Epoch 121/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3895 - acc: 0.8309 - val_loss: 0.4425 - val_acc: 0.8079\n",
      "Epoch 122/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3869 - acc: 0.8300 - val_loss: 0.4429 - val_acc: 0.8095\n",
      "Epoch 123/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3878 - acc: 0.8292 - val_loss: 0.4429 - val_acc: 0.8095\n",
      "Epoch 124/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3869 - acc: 0.8309 - val_loss: 0.4427 - val_acc: 0.8112\n",
      "Epoch 125/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3880 - acc: 0.8334 - val_loss: 0.4425 - val_acc: 0.8112\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.15, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_87\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_88 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_248 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_161 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_249 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_162 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_250 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.8040 - acc: 0.4320 - val_loss: 0.7050 - val_acc: 0.4548\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.6954 - acc: 0.5136 - val_loss: 0.6619 - val_acc: 0.5862\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6659 - acc: 0.5782 - val_loss: 0.6447 - val_acc: 0.5846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6428 - acc: 0.6008 - val_loss: 0.6220 - val_acc: 0.6305\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.6203 - acc: 0.6614 - val_loss: 0.6074 - val_acc: 0.7159\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.6001 - acc: 0.6970 - val_loss: 0.5984 - val_acc: 0.6765\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.5864 - acc: 0.6968 - val_loss: 0.5832 - val_acc: 0.7110\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.5713 - acc: 0.7232 - val_loss: 0.5691 - val_acc: 0.7488\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.5561 - acc: 0.7396 - val_loss: 0.5559 - val_acc: 0.7521\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.5416 - acc: 0.7493 - val_loss: 0.5436 - val_acc: 0.7488\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.5311 - acc: 0.7521 - val_loss: 0.5314 - val_acc: 0.7521\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.5171 - acc: 0.7621 - val_loss: 0.5203 - val_acc: 0.7603\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.5065 - acc: 0.7674 - val_loss: 0.5114 - val_acc: 0.7619\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4985 - acc: 0.7749 - val_loss: 0.5032 - val_acc: 0.7668\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4885 - acc: 0.7805 - val_loss: 0.4959 - val_acc: 0.7652\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4789 - acc: 0.7860 - val_loss: 0.4891 - val_acc: 0.7652\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4735 - acc: 0.7898 - val_loss: 0.4829 - val_acc: 0.7685\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4662 - acc: 0.7893 - val_loss: 0.4774 - val_acc: 0.7750\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4641 - acc: 0.7980 - val_loss: 0.4723 - val_acc: 0.7767\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4530 - acc: 0.7993 - val_loss: 0.4682 - val_acc: 0.7816\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4477 - acc: 0.8028 - val_loss: 0.4650 - val_acc: 0.7849\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4480 - acc: 0.8030 - val_loss: 0.4628 - val_acc: 0.7865\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.4448 - acc: 0.8050 - val_loss: 0.4590 - val_acc: 0.7915\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4374 - acc: 0.8117 - val_loss: 0.4580 - val_acc: 0.7882\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4360 - acc: 0.8132 - val_loss: 0.4557 - val_acc: 0.7898\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4355 - acc: 0.8110 - val_loss: 0.4540 - val_acc: 0.7931\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4279 - acc: 0.8146 - val_loss: 0.4513 - val_acc: 0.7964\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4260 - acc: 0.8150 - val_loss: 0.4499 - val_acc: 0.7964\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.4220 - acc: 0.8201 - val_loss: 0.4478 - val_acc: 0.7964\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4251 - acc: 0.8104 - val_loss: 0.4462 - val_acc: 0.7997\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4198 - acc: 0.8176 - val_loss: 0.4465 - val_acc: 0.8013\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4179 - acc: 0.8183 - val_loss: 0.4451 - val_acc: 0.8013\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4169 - acc: 0.8181 - val_loss: 0.4485 - val_acc: 0.8062\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4152 - acc: 0.8228 - val_loss: 0.4439 - val_acc: 0.8013\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4116 - acc: 0.8239 - val_loss: 0.4427 - val_acc: 0.8013\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4062 - acc: 0.8254 - val_loss: 0.4414 - val_acc: 0.8013\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4044 - acc: 0.8214 - val_loss: 0.4421 - val_acc: 0.7997\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4050 - acc: 0.8232 - val_loss: 0.4416 - val_acc: 0.8013\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4008 - acc: 0.8263 - val_loss: 0.4413 - val_acc: 0.8013\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3990 - acc: 0.8289 - val_loss: 0.4414 - val_acc: 0.8079\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3989 - acc: 0.8283 - val_loss: 0.4409 - val_acc: 0.8079\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.3967 - acc: 0.8290 - val_loss: 0.4398 - val_acc: 0.8095\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3943 - acc: 0.8307 - val_loss: 0.4391 - val_acc: 0.8112\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3956 - acc: 0.8289 - val_loss: 0.4391 - val_acc: 0.8128\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.3917 - acc: 0.8320 - val_loss: 0.4393 - val_acc: 0.8095\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.3904 - acc: 0.8321 - val_loss: 0.4403 - val_acc: 0.8128\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3899 - acc: 0.8314 - val_loss: 0.4373 - val_acc: 0.8095\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3905 - acc: 0.8283 - val_loss: 0.4378 - val_acc: 0.8112\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.3889 - acc: 0.8347 - val_loss: 0.4389 - val_acc: 0.8112\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3875 - acc: 0.8309 - val_loss: 0.4377 - val_acc: 0.8079\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3830 - acc: 0.8384 - val_loss: 0.4375 - val_acc: 0.8095\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.3803 - acc: 0.8374 - val_loss: 0.4364 - val_acc: 0.8079\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3806 - acc: 0.8362 - val_loss: 0.4382 - val_acc: 0.8112\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3764 - acc: 0.8384 - val_loss: 0.4365 - val_acc: 0.8079\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3735 - acc: 0.8440 - val_loss: 0.4378 - val_acc: 0.8079\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3740 - acc: 0.8374 - val_loss: 0.4376 - val_acc: 0.8062\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.3744 - acc: 0.8369 - val_loss: 0.4381 - val_acc: 0.8095\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.15, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_88\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_89 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_251 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_163 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_252 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_164 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_253 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.8481 - acc: 0.4322 - val_loss: 0.7632 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.7465 - acc: 0.4433 - val_loss: 0.6981 - val_acc: 0.4729\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6947 - acc: 0.5176 - val_loss: 0.6682 - val_acc: 0.6273\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6716 - acc: 0.5880 - val_loss: 0.6534 - val_acc: 0.6190\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6570 - acc: 0.6194 - val_loss: 0.6403 - val_acc: 0.6273\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6405 - acc: 0.6559 - val_loss: 0.6267 - val_acc: 0.7011\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6255 - acc: 0.6809 - val_loss: 0.6149 - val_acc: 0.7471\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6119 - acc: 0.7008 - val_loss: 0.6061 - val_acc: 0.7241\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6043 - acc: 0.7050 - val_loss: 0.5976 - val_acc: 0.7176\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5939 - acc: 0.7095 - val_loss: 0.5883 - val_acc: 0.7192\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5831 - acc: 0.7209 - val_loss: 0.5794 - val_acc: 0.7356\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5741 - acc: 0.7261 - val_loss: 0.5709 - val_acc: 0.7488\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5692 - acc: 0.7287 - val_loss: 0.5628 - val_acc: 0.7471\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5565 - acc: 0.7437 - val_loss: 0.5548 - val_acc: 0.7488\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5513 - acc: 0.7418 - val_loss: 0.5472 - val_acc: 0.7471\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5454 - acc: 0.7437 - val_loss: 0.5401 - val_acc: 0.7570\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5331 - acc: 0.7502 - val_loss: 0.5332 - val_acc: 0.7619\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5230 - acc: 0.7643 - val_loss: 0.5266 - val_acc: 0.7685\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.5190 - acc: 0.7594 - val_loss: 0.5197 - val_acc: 0.7734\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5160 - acc: 0.7612 - val_loss: 0.5134 - val_acc: 0.7685\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5060 - acc: 0.7699 - val_loss: 0.5080 - val_acc: 0.7685\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5019 - acc: 0.7694 - val_loss: 0.5031 - val_acc: 0.7685\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4948 - acc: 0.7739 - val_loss: 0.4987 - val_acc: 0.7734\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4913 - acc: 0.7770 - val_loss: 0.4944 - val_acc: 0.7800\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4905 - acc: 0.7789 - val_loss: 0.4903 - val_acc: 0.7800\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4829 - acc: 0.7825 - val_loss: 0.4865 - val_acc: 0.7800\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4829 - acc: 0.7803 - val_loss: 0.4828 - val_acc: 0.7816\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4760 - acc: 0.7871 - val_loss: 0.4802 - val_acc: 0.7865\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4724 - acc: 0.7909 - val_loss: 0.4766 - val_acc: 0.7849\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4666 - acc: 0.7947 - val_loss: 0.4742 - val_acc: 0.7882\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4630 - acc: 0.7946 - val_loss: 0.4718 - val_acc: 0.7882\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4639 - acc: 0.7978 - val_loss: 0.4694 - val_acc: 0.7882\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4579 - acc: 0.8011 - val_loss: 0.4673 - val_acc: 0.7882\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4521 - acc: 0.7984 - val_loss: 0.4657 - val_acc: 0.7865\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4536 - acc: 0.7978 - val_loss: 0.4633 - val_acc: 0.7882\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4499 - acc: 0.8004 - val_loss: 0.4612 - val_acc: 0.7915\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4543 - acc: 0.7999 - val_loss: 0.4605 - val_acc: 0.7980\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4479 - acc: 0.7999 - val_loss: 0.4589 - val_acc: 0.7947\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4456 - acc: 0.8030 - val_loss: 0.4579 - val_acc: 0.7980\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4419 - acc: 0.8053 - val_loss: 0.4562 - val_acc: 0.7964\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4413 - acc: 0.8075 - val_loss: 0.4557 - val_acc: 0.7980\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4394 - acc: 0.8119 - val_loss: 0.4544 - val_acc: 0.7997\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4370 - acc: 0.8110 - val_loss: 0.4524 - val_acc: 0.7915\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4355 - acc: 0.8077 - val_loss: 0.4521 - val_acc: 0.8013\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4346 - acc: 0.8106 - val_loss: 0.4509 - val_acc: 0.8013\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4313 - acc: 0.8066 - val_loss: 0.4493 - val_acc: 0.8013\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4313 - acc: 0.8146 - val_loss: 0.4490 - val_acc: 0.8030\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4262 - acc: 0.8126 - val_loss: 0.4480 - val_acc: 0.8030\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4266 - acc: 0.8112 - val_loss: 0.4470 - val_acc: 0.7997\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4238 - acc: 0.8113 - val_loss: 0.4465 - val_acc: 0.8046\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4238 - acc: 0.8148 - val_loss: 0.4469 - val_acc: 0.8030\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4215 - acc: 0.8154 - val_loss: 0.4451 - val_acc: 0.8062\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4225 - acc: 0.8108 - val_loss: 0.4448 - val_acc: 0.8046\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4219 - acc: 0.8146 - val_loss: 0.4437 - val_acc: 0.8062\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4219 - acc: 0.8159 - val_loss: 0.4434 - val_acc: 0.8079\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4165 - acc: 0.8150 - val_loss: 0.4431 - val_acc: 0.8128\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4186 - acc: 0.8201 - val_loss: 0.4426 - val_acc: 0.8095\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4123 - acc: 0.8243 - val_loss: 0.4446 - val_acc: 0.7980\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4155 - acc: 0.8168 - val_loss: 0.4413 - val_acc: 0.8079\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4103 - acc: 0.8197 - val_loss: 0.4414 - val_acc: 0.8095\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4110 - acc: 0.8252 - val_loss: 0.4414 - val_acc: 0.8128\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4108 - acc: 0.8219 - val_loss: 0.4408 - val_acc: 0.8128\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4113 - acc: 0.8196 - val_loss: 0.4392 - val_acc: 0.8128\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4056 - acc: 0.8265 - val_loss: 0.4399 - val_acc: 0.8095\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4102 - acc: 0.8208 - val_loss: 0.4390 - val_acc: 0.8128\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4036 - acc: 0.8256 - val_loss: 0.4387 - val_acc: 0.8161\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4026 - acc: 0.8258 - val_loss: 0.4387 - val_acc: 0.8144\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4054 - acc: 0.8243 - val_loss: 0.4385 - val_acc: 0.8161\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3984 - acc: 0.8283 - val_loss: 0.4377 - val_acc: 0.8144\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3997 - acc: 0.8269 - val_loss: 0.4396 - val_acc: 0.8095\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4010 - acc: 0.8259 - val_loss: 0.4382 - val_acc: 0.8128\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3979 - acc: 0.8263 - val_loss: 0.4384 - val_acc: 0.8112\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3990 - acc: 0.8283 - val_loss: 0.4376 - val_acc: 0.8128\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3980 - acc: 0.8254 - val_loss: 0.4372 - val_acc: 0.8144\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3936 - acc: 0.8309 - val_loss: 0.4379 - val_acc: 0.8079\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3946 - acc: 0.8287 - val_loss: 0.4364 - val_acc: 0.8112\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3954 - acc: 0.8301 - val_loss: 0.4372 - val_acc: 0.8079\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3961 - acc: 0.8338 - val_loss: 0.4362 - val_acc: 0.8095\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3936 - acc: 0.8265 - val_loss: 0.4359 - val_acc: 0.8112\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3868 - acc: 0.8323 - val_loss: 0.4375 - val_acc: 0.8079\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3892 - acc: 0.8342 - val_loss: 0.4355 - val_acc: 0.8095\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3882 - acc: 0.8354 - val_loss: 0.4359 - val_acc: 0.8112\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3880 - acc: 0.8329 - val_loss: 0.4360 - val_acc: 0.8112\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3870 - acc: 0.8305 - val_loss: 0.4359 - val_acc: 0.8095\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3840 - acc: 0.8362 - val_loss: 0.4358 - val_acc: 0.8079\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3852 - acc: 0.8327 - val_loss: 0.4358 - val_acc: 0.8079\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.15, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_89\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_90 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_254 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_165 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_255 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_166 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_256 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.7517 - acc: 0.4379 - val_loss: 0.7137 - val_acc: 0.4187\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.7029 - acc: 0.4952 - val_loss: 0.6763 - val_acc: 0.6190\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6725 - acc: 0.5941 - val_loss: 0.6568 - val_acc: 0.6552\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6593 - acc: 0.6223 - val_loss: 0.6455 - val_acc: 0.6174\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6477 - acc: 0.6305 - val_loss: 0.6359 - val_acc: 0.6207\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6353 - acc: 0.6506 - val_loss: 0.6257 - val_acc: 0.6782\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6268 - acc: 0.6672 - val_loss: 0.6158 - val_acc: 0.7258\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6143 - acc: 0.6940 - val_loss: 0.6069 - val_acc: 0.7422\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6069 - acc: 0.6979 - val_loss: 0.5985 - val_acc: 0.7373\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5956 - acc: 0.7165 - val_loss: 0.5894 - val_acc: 0.7340\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5857 - acc: 0.7170 - val_loss: 0.5800 - val_acc: 0.7455\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5760 - acc: 0.7271 - val_loss: 0.5711 - val_acc: 0.7586\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5640 - acc: 0.7402 - val_loss: 0.5630 - val_acc: 0.7537\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5561 - acc: 0.7431 - val_loss: 0.5551 - val_acc: 0.7471\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5491 - acc: 0.7448 - val_loss: 0.5471 - val_acc: 0.7471\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5373 - acc: 0.7480 - val_loss: 0.5392 - val_acc: 0.7586\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5279 - acc: 0.7584 - val_loss: 0.5318 - val_acc: 0.7685\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5201 - acc: 0.7630 - val_loss: 0.5250 - val_acc: 0.7668\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5151 - acc: 0.7630 - val_loss: 0.5182 - val_acc: 0.7619\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5074 - acc: 0.7708 - val_loss: 0.5120 - val_acc: 0.7652\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5029 - acc: 0.7734 - val_loss: 0.5062 - val_acc: 0.7652\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4953 - acc: 0.7774 - val_loss: 0.5010 - val_acc: 0.7701\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4890 - acc: 0.7736 - val_loss: 0.4964 - val_acc: 0.7701\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 0.4898 - acc: 0.7794 - val_loss: 0.4924 - val_acc: 0.7750\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4806 - acc: 0.7858 - val_loss: 0.4881 - val_acc: 0.7734\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4772 - acc: 0.7845 - val_loss: 0.4844 - val_acc: 0.7734\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4724 - acc: 0.7858 - val_loss: 0.4814 - val_acc: 0.7783\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4696 - acc: 0.7913 - val_loss: 0.4783 - val_acc: 0.7734\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4644 - acc: 0.7913 - val_loss: 0.4755 - val_acc: 0.7734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4622 - acc: 0.7929 - val_loss: 0.4732 - val_acc: 0.7800\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4593 - acc: 0.7960 - val_loss: 0.4708 - val_acc: 0.7865\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4557 - acc: 0.7995 - val_loss: 0.4687 - val_acc: 0.7849\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4519 - acc: 0.8002 - val_loss: 0.4665 - val_acc: 0.7882\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4525 - acc: 0.8048 - val_loss: 0.4654 - val_acc: 0.7947\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4475 - acc: 0.8000 - val_loss: 0.4628 - val_acc: 0.7915\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4480 - acc: 0.7978 - val_loss: 0.4613 - val_acc: 0.7898\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4424 - acc: 0.8035 - val_loss: 0.4615 - val_acc: 0.7947\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4434 - acc: 0.8035 - val_loss: 0.4590 - val_acc: 0.7915\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4400 - acc: 0.8064 - val_loss: 0.4581 - val_acc: 0.7964\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4398 - acc: 0.8055 - val_loss: 0.4579 - val_acc: 0.7980\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4373 - acc: 0.8090 - val_loss: 0.4560 - val_acc: 0.7997\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4374 - acc: 0.8095 - val_loss: 0.4550 - val_acc: 0.7947\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4332 - acc: 0.8121 - val_loss: 0.4559 - val_acc: 0.8013\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4310 - acc: 0.8108 - val_loss: 0.4536 - val_acc: 0.8046\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4286 - acc: 0.8097 - val_loss: 0.4524 - val_acc: 0.7964\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4270 - acc: 0.8139 - val_loss: 0.4520 - val_acc: 0.8112\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4261 - acc: 0.8176 - val_loss: 0.4516 - val_acc: 0.8095\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4262 - acc: 0.8124 - val_loss: 0.4507 - val_acc: 0.7997\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4253 - acc: 0.8159 - val_loss: 0.4506 - val_acc: 0.8095\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4225 - acc: 0.8152 - val_loss: 0.4503 - val_acc: 0.8112\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4217 - acc: 0.8148 - val_loss: 0.4487 - val_acc: 0.8128\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4158 - acc: 0.8165 - val_loss: 0.4479 - val_acc: 0.8128\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4163 - acc: 0.8170 - val_loss: 0.4471 - val_acc: 0.8161\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4168 - acc: 0.8170 - val_loss: 0.4469 - val_acc: 0.8144\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4179 - acc: 0.8214 - val_loss: 0.4463 - val_acc: 0.8177\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4150 - acc: 0.8212 - val_loss: 0.4458 - val_acc: 0.8128\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4131 - acc: 0.8236 - val_loss: 0.4458 - val_acc: 0.8210\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4125 - acc: 0.8179 - val_loss: 0.4457 - val_acc: 0.8161\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4116 - acc: 0.8225 - val_loss: 0.4451 - val_acc: 0.8177\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4103 - acc: 0.8236 - val_loss: 0.4443 - val_acc: 0.8210\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4090 - acc: 0.8243 - val_loss: 0.4436 - val_acc: 0.8194\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4035 - acc: 0.8276 - val_loss: 0.4453 - val_acc: 0.8144\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4051 - acc: 0.8252 - val_loss: 0.4433 - val_acc: 0.8161\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4052 - acc: 0.8274 - val_loss: 0.4431 - val_acc: 0.8194\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4043 - acc: 0.8239 - val_loss: 0.4427 - val_acc: 0.8161\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4006 - acc: 0.8239 - val_loss: 0.4427 - val_acc: 0.8161\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4002 - acc: 0.8236 - val_loss: 0.4428 - val_acc: 0.8161\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4003 - acc: 0.8280 - val_loss: 0.4425 - val_acc: 0.8161\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4006 - acc: 0.8252 - val_loss: 0.4421 - val_acc: 0.8161\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4010 - acc: 0.8265 - val_loss: 0.4417 - val_acc: 0.8161\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3982 - acc: 0.8292 - val_loss: 0.4405 - val_acc: 0.8144\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3965 - acc: 0.8290 - val_loss: 0.4399 - val_acc: 0.8177\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3932 - acc: 0.8261 - val_loss: 0.4415 - val_acc: 0.8177\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3968 - acc: 0.8312 - val_loss: 0.4401 - val_acc: 0.8177\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3954 - acc: 0.8298 - val_loss: 0.4404 - val_acc: 0.8144\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3920 - acc: 0.8318 - val_loss: 0.4411 - val_acc: 0.8161\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3920 - acc: 0.8294 - val_loss: 0.4400 - val_acc: 0.8079\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.15, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_90\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_91 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_257 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_167 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_258 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_168 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_259 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.8122 - acc: 0.4351 - val_loss: 0.7404 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7217 - acc: 0.4753 - val_loss: 0.6806 - val_acc: 0.6059\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6755 - acc: 0.5889 - val_loss: 0.6571 - val_acc: 0.5895\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6601 - acc: 0.6019 - val_loss: 0.6441 - val_acc: 0.5862\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6438 - acc: 0.6212 - val_loss: 0.6296 - val_acc: 0.6174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6279 - acc: 0.6608 - val_loss: 0.6156 - val_acc: 0.7061\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6112 - acc: 0.6964 - val_loss: 0.6048 - val_acc: 0.7340\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6029 - acc: 0.6942 - val_loss: 0.5953 - val_acc: 0.7258\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5876 - acc: 0.7021 - val_loss: 0.5854 - val_acc: 0.7241\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.5779 - acc: 0.7053 - val_loss: 0.5754 - val_acc: 0.7323\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5675 - acc: 0.7254 - val_loss: 0.5656 - val_acc: 0.7504\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5547 - acc: 0.7344 - val_loss: 0.5555 - val_acc: 0.7504\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5465 - acc: 0.7313 - val_loss: 0.5453 - val_acc: 0.7488\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.5327 - acc: 0.7458 - val_loss: 0.5352 - val_acc: 0.7586\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5211 - acc: 0.7533 - val_loss: 0.5255 - val_acc: 0.7652\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5154 - acc: 0.7659 - val_loss: 0.5164 - val_acc: 0.7718\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5076 - acc: 0.7652 - val_loss: 0.5081 - val_acc: 0.7800\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4963 - acc: 0.7696 - val_loss: 0.5002 - val_acc: 0.7685\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4884 - acc: 0.7796 - val_loss: 0.4938 - val_acc: 0.7718\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4825 - acc: 0.7864 - val_loss: 0.4882 - val_acc: 0.7783\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4778 - acc: 0.7843 - val_loss: 0.4832 - val_acc: 0.7800\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4704 - acc: 0.7896 - val_loss: 0.4790 - val_acc: 0.7816\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4659 - acc: 0.7916 - val_loss: 0.4764 - val_acc: 0.7833\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4602 - acc: 0.7964 - val_loss: 0.4721 - val_acc: 0.7800\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4571 - acc: 0.7922 - val_loss: 0.4701 - val_acc: 0.7865\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4575 - acc: 0.8015 - val_loss: 0.4683 - val_acc: 0.7882\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4521 - acc: 0.7988 - val_loss: 0.4636 - val_acc: 0.7882\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4441 - acc: 0.8048 - val_loss: 0.4628 - val_acc: 0.7931\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4434 - acc: 0.8037 - val_loss: 0.4597 - val_acc: 0.7915\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4407 - acc: 0.8004 - val_loss: 0.4591 - val_acc: 0.7931\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4368 - acc: 0.8070 - val_loss: 0.4583 - val_acc: 0.7947\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4368 - acc: 0.8057 - val_loss: 0.4559 - val_acc: 0.7947\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4283 - acc: 0.8113 - val_loss: 0.4548 - val_acc: 0.7947\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4292 - acc: 0.8132 - val_loss: 0.4536 - val_acc: 0.8013\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4274 - acc: 0.8126 - val_loss: 0.4530 - val_acc: 0.7964\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4265 - acc: 0.8146 - val_loss: 0.4517 - val_acc: 0.7980\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4210 - acc: 0.8148 - val_loss: 0.4510 - val_acc: 0.7980\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4222 - acc: 0.8146 - val_loss: 0.4504 - val_acc: 0.8046\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4197 - acc: 0.8168 - val_loss: 0.4492 - val_acc: 0.8030\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4208 - acc: 0.8137 - val_loss: 0.4487 - val_acc: 0.8062\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4169 - acc: 0.8159 - val_loss: 0.4481 - val_acc: 0.8079\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4147 - acc: 0.8207 - val_loss: 0.4461 - val_acc: 0.8112\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4149 - acc: 0.8132 - val_loss: 0.4455 - val_acc: 0.8112\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4077 - acc: 0.8201 - val_loss: 0.4452 - val_acc: 0.8112\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4088 - acc: 0.8212 - val_loss: 0.4444 - val_acc: 0.8112\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4067 - acc: 0.8190 - val_loss: 0.4457 - val_acc: 0.8095\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4043 - acc: 0.8199 - val_loss: 0.4447 - val_acc: 0.8112\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4054 - acc: 0.8232 - val_loss: 0.4433 - val_acc: 0.8095\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4014 - acc: 0.8254 - val_loss: 0.4462 - val_acc: 0.8095\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4044 - acc: 0.8254 - val_loss: 0.4427 - val_acc: 0.8112\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3982 - acc: 0.8248 - val_loss: 0.4422 - val_acc: 0.8095\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4005 - acc: 0.8256 - val_loss: 0.4422 - val_acc: 0.8112\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3959 - acc: 0.8272 - val_loss: 0.4416 - val_acc: 0.8112\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3928 - acc: 0.8294 - val_loss: 0.4394 - val_acc: 0.8095\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3939 - acc: 0.8219 - val_loss: 0.4400 - val_acc: 0.8079\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3928 - acc: 0.8318 - val_loss: 0.4401 - val_acc: 0.8095\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3908 - acc: 0.8281 - val_loss: 0.4381 - val_acc: 0.8095\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3886 - acc: 0.8258 - val_loss: 0.4403 - val_acc: 0.8062\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3854 - acc: 0.8376 - val_loss: 0.4373 - val_acc: 0.8030\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3864 - acc: 0.8312 - val_loss: 0.4370 - val_acc: 0.8095\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3829 - acc: 0.8323 - val_loss: 0.4369 - val_acc: 0.8112\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3829 - acc: 0.8334 - val_loss: 0.4361 - val_acc: 0.8095\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3831 - acc: 0.8290 - val_loss: 0.4374 - val_acc: 0.8046\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3813 - acc: 0.8352 - val_loss: 0.4392 - val_acc: 0.8030\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3777 - acc: 0.8320 - val_loss: 0.4368 - val_acc: 0.8013\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3783 - acc: 0.8358 - val_loss: 0.4392 - val_acc: 0.8062\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3761 - acc: 0.8316 - val_loss: 0.4362 - val_acc: 0.7997\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.2, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_91\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_92 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_260 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_169 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_261 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_170 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_262 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.7450 - acc: 0.4552 - val_loss: 0.6803 - val_acc: 0.5632\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6765 - acc: 0.5749 - val_loss: 0.6611 - val_acc: 0.5846\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6576 - acc: 0.5835 - val_loss: 0.6373 - val_acc: 0.5895\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6286 - acc: 0.6417 - val_loss: 0.6159 - val_acc: 0.7159\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6059 - acc: 0.6991 - val_loss: 0.6045 - val_acc: 0.6749\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5919 - acc: 0.6986 - val_loss: 0.5880 - val_acc: 0.7061\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5742 - acc: 0.7152 - val_loss: 0.5727 - val_acc: 0.7406\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5564 - acc: 0.7424 - val_loss: 0.5591 - val_acc: 0.7471\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5369 - acc: 0.7519 - val_loss: 0.5447 - val_acc: 0.7438\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5250 - acc: 0.7552 - val_loss: 0.5316 - val_acc: 0.7488\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5134 - acc: 0.7663 - val_loss: 0.5199 - val_acc: 0.7537\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4992 - acc: 0.7710 - val_loss: 0.5098 - val_acc: 0.7570\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4916 - acc: 0.7785 - val_loss: 0.5022 - val_acc: 0.7668\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4801 - acc: 0.7843 - val_loss: 0.4942 - val_acc: 0.7603\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4724 - acc: 0.7896 - val_loss: 0.4875 - val_acc: 0.7701\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4680 - acc: 0.7926 - val_loss: 0.4819 - val_acc: 0.7718\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4600 - acc: 0.7911 - val_loss: 0.4760 - val_acc: 0.7701\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4543 - acc: 0.7968 - val_loss: 0.4718 - val_acc: 0.7783\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4447 - acc: 0.8073 - val_loss: 0.4671 - val_acc: 0.7849\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4403 - acc: 0.8070 - val_loss: 0.4635 - val_acc: 0.7882\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4395 - acc: 0.8073 - val_loss: 0.4614 - val_acc: 0.7882\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4373 - acc: 0.8097 - val_loss: 0.4592 - val_acc: 0.7882\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4356 - acc: 0.8033 - val_loss: 0.4584 - val_acc: 0.7931\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4289 - acc: 0.8128 - val_loss: 0.4557 - val_acc: 0.7964\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.4259 - acc: 0.8152 - val_loss: 0.4522 - val_acc: 0.7997\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4234 - acc: 0.8132 - val_loss: 0.4541 - val_acc: 0.7980\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.4227 - acc: 0.8157 - val_loss: 0.4508 - val_acc: 0.7964\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4203 - acc: 0.8163 - val_loss: 0.4515 - val_acc: 0.7964\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4190 - acc: 0.8199 - val_loss: 0.4492 - val_acc: 0.7964\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4132 - acc: 0.8228 - val_loss: 0.4468 - val_acc: 0.8013\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4153 - acc: 0.8207 - val_loss: 0.4458 - val_acc: 0.7964\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4120 - acc: 0.8248 - val_loss: 0.4472 - val_acc: 0.8062\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4082 - acc: 0.8214 - val_loss: 0.4452 - val_acc: 0.8013\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4046 - acc: 0.8239 - val_loss: 0.4470 - val_acc: 0.8062\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4051 - acc: 0.8227 - val_loss: 0.4446 - val_acc: 0.8079\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4010 - acc: 0.8283 - val_loss: 0.4440 - val_acc: 0.8062\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.3995 - acc: 0.8272 - val_loss: 0.4427 - val_acc: 0.8046\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3964 - acc: 0.8267 - val_loss: 0.4429 - val_acc: 0.8062\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3978 - acc: 0.8287 - val_loss: 0.4409 - val_acc: 0.8062\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.3905 - acc: 0.8327 - val_loss: 0.4411 - val_acc: 0.8095\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3914 - acc: 0.8347 - val_loss: 0.4410 - val_acc: 0.8112\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3906 - acc: 0.8303 - val_loss: 0.4411 - val_acc: 0.8095\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3858 - acc: 0.8329 - val_loss: 0.4397 - val_acc: 0.8046\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3869 - acc: 0.8345 - val_loss: 0.4393 - val_acc: 0.8046\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3841 - acc: 0.8338 - val_loss: 0.4387 - val_acc: 0.8062\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3817 - acc: 0.8363 - val_loss: 0.4406 - val_acc: 0.8095\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3797 - acc: 0.8363 - val_loss: 0.4392 - val_acc: 0.8046\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3805 - acc: 0.8378 - val_loss: 0.4384 - val_acc: 0.8062\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3794 - acc: 0.8343 - val_loss: 0.4406 - val_acc: 0.8062\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.3764 - acc: 0.8396 - val_loss: 0.4382 - val_acc: 0.8112\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3744 - acc: 0.8345 - val_loss: 0.4387 - val_acc: 0.8079\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3731 - acc: 0.8413 - val_loss: 0.4371 - val_acc: 0.8095\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.3697 - acc: 0.8415 - val_loss: 0.4386 - val_acc: 0.8062\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3654 - acc: 0.8413 - val_loss: 0.4374 - val_acc: 0.8046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3651 - acc: 0.8422 - val_loss: 0.4384 - val_acc: 0.8062\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.3634 - acc: 0.8409 - val_loss: 0.4369 - val_acc: 0.8062\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.3599 - acc: 0.8442 - val_loss: 0.4394 - val_acc: 0.8079\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3567 - acc: 0.8460 - val_loss: 0.4392 - val_acc: 0.8095\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3572 - acc: 0.8447 - val_loss: 0.4363 - val_acc: 0.8062\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3585 - acc: 0.8456 - val_loss: 0.4359 - val_acc: 0.8079\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3511 - acc: 0.8508 - val_loss: 0.4372 - val_acc: 0.8079\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3497 - acc: 0.8509 - val_loss: 0.4376 - val_acc: 0.8095\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.3489 - acc: 0.8519 - val_loss: 0.4392 - val_acc: 0.8079\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3465 - acc: 0.8477 - val_loss: 0.4400 - val_acc: 0.8062\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3438 - acc: 0.8480 - val_loss: 0.4363 - val_acc: 0.8013\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.2, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_92\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_93 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_263 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_171 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_264 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_172 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_265 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.8199 - acc: 0.4412 - val_loss: 0.7412 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7234 - acc: 0.4851 - val_loss: 0.6816 - val_acc: 0.5993\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6850 - acc: 0.5607 - val_loss: 0.6585 - val_acc: 0.5895\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6659 - acc: 0.6015 - val_loss: 0.6468 - val_acc: 0.5862\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6501 - acc: 0.6125 - val_loss: 0.6335 - val_acc: 0.5961\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6344 - acc: 0.6402 - val_loss: 0.6194 - val_acc: 0.6667\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6186 - acc: 0.6796 - val_loss: 0.6083 - val_acc: 0.7422\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.6062 - acc: 0.6802 - val_loss: 0.5993 - val_acc: 0.7225\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5973 - acc: 0.6893 - val_loss: 0.5899 - val_acc: 0.7241\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5851 - acc: 0.7032 - val_loss: 0.5803 - val_acc: 0.7356\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5753 - acc: 0.7199 - val_loss: 0.5710 - val_acc: 0.7488\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5629 - acc: 0.7269 - val_loss: 0.5617 - val_acc: 0.7488\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5553 - acc: 0.7334 - val_loss: 0.5522 - val_acc: 0.7488\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5455 - acc: 0.7360 - val_loss: 0.5430 - val_acc: 0.7521\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.5362 - acc: 0.7464 - val_loss: 0.5336 - val_acc: 0.7619\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5248 - acc: 0.7552 - val_loss: 0.5244 - val_acc: 0.7734\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5133 - acc: 0.7654 - val_loss: 0.5154 - val_acc: 0.7701\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5034 - acc: 0.7670 - val_loss: 0.5076 - val_acc: 0.7783\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4980 - acc: 0.7730 - val_loss: 0.5003 - val_acc: 0.7718\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4932 - acc: 0.7745 - val_loss: 0.4939 - val_acc: 0.7718\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4846 - acc: 0.7783 - val_loss: 0.4891 - val_acc: 0.7783\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4794 - acc: 0.7814 - val_loss: 0.4842 - val_acc: 0.7800\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4708 - acc: 0.7860 - val_loss: 0.4800 - val_acc: 0.7865\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4706 - acc: 0.7856 - val_loss: 0.4764 - val_acc: 0.7898\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4661 - acc: 0.7889 - val_loss: 0.4725 - val_acc: 0.7849\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4586 - acc: 0.7958 - val_loss: 0.4703 - val_acc: 0.7849\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4593 - acc: 0.7920 - val_loss: 0.4672 - val_acc: 0.7833\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4513 - acc: 0.8009 - val_loss: 0.4639 - val_acc: 0.7898\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4525 - acc: 0.7988 - val_loss: 0.4642 - val_acc: 0.7915\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4453 - acc: 0.8004 - val_loss: 0.4620 - val_acc: 0.7915\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4413 - acc: 0.8035 - val_loss: 0.4594 - val_acc: 0.7931\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4440 - acc: 0.8004 - val_loss: 0.4599 - val_acc: 0.7980\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4383 - acc: 0.8051 - val_loss: 0.4554 - val_acc: 0.7915\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4406 - acc: 0.8039 - val_loss: 0.4550 - val_acc: 0.7980\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4329 - acc: 0.8101 - val_loss: 0.4534 - val_acc: 0.7947\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4328 - acc: 0.8113 - val_loss: 0.4533 - val_acc: 0.7980\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4267 - acc: 0.8110 - val_loss: 0.4520 - val_acc: 0.7980\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4272 - acc: 0.8101 - val_loss: 0.4513 - val_acc: 0.8046\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4263 - acc: 0.8130 - val_loss: 0.4491 - val_acc: 0.8013\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4247 - acc: 0.8112 - val_loss: 0.4490 - val_acc: 0.8046\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4203 - acc: 0.8166 - val_loss: 0.4487 - val_acc: 0.8062\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4203 - acc: 0.8143 - val_loss: 0.4466 - val_acc: 0.8079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4168 - acc: 0.8141 - val_loss: 0.4463 - val_acc: 0.8079\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4145 - acc: 0.8232 - val_loss: 0.4472 - val_acc: 0.8062\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4115 - acc: 0.8185 - val_loss: 0.4447 - val_acc: 0.8112\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4149 - acc: 0.8150 - val_loss: 0.4437 - val_acc: 0.8079\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4126 - acc: 0.8166 - val_loss: 0.4437 - val_acc: 0.8095\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4095 - acc: 0.8225 - val_loss: 0.4438 - val_acc: 0.8112\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4064 - acc: 0.8214 - val_loss: 0.4455 - val_acc: 0.8062\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4070 - acc: 0.8196 - val_loss: 0.4418 - val_acc: 0.7997\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4041 - acc: 0.8232 - val_loss: 0.4429 - val_acc: 0.8062\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4070 - acc: 0.8227 - val_loss: 0.4432 - val_acc: 0.8062\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4025 - acc: 0.8192 - val_loss: 0.4407 - val_acc: 0.8030\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4041 - acc: 0.8248 - val_loss: 0.4434 - val_acc: 0.8079\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3995 - acc: 0.8283 - val_loss: 0.4406 - val_acc: 0.8079\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3920 - acc: 0.8256 - val_loss: 0.4396 - val_acc: 0.8095\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3972 - acc: 0.8267 - val_loss: 0.4417 - val_acc: 0.8079\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3959 - acc: 0.8290 - val_loss: 0.4388 - val_acc: 0.8079\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3939 - acc: 0.8278 - val_loss: 0.4379 - val_acc: 0.8046\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3943 - acc: 0.8265 - val_loss: 0.4412 - val_acc: 0.8062\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3899 - acc: 0.8270 - val_loss: 0.4371 - val_acc: 0.8013\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3899 - acc: 0.8303 - val_loss: 0.4402 - val_acc: 0.8046\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3871 - acc: 0.8331 - val_loss: 0.4376 - val_acc: 0.8062\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3826 - acc: 0.8343 - val_loss: 0.4374 - val_acc: 0.8079\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3826 - acc: 0.8345 - val_loss: 0.4392 - val_acc: 0.8046\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3840 - acc: 0.8327 - val_loss: 0.4380 - val_acc: 0.8046\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.2, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_93\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_94 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_266 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_173 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_267 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.6675 - acc: 0.5950 - val_loss: 0.6457 - val_acc: 0.6749\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6459 - acc: 0.6282 - val_loss: 0.6299 - val_acc: 0.6667\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6310 - acc: 0.6451 - val_loss: 0.6180 - val_acc: 0.7192\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6158 - acc: 0.6805 - val_loss: 0.6077 - val_acc: 0.7011\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6038 - acc: 0.6926 - val_loss: 0.5996 - val_acc: 0.7077\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5944 - acc: 0.6984 - val_loss: 0.5908 - val_acc: 0.7126\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5844 - acc: 0.7086 - val_loss: 0.5817 - val_acc: 0.7274\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5732 - acc: 0.7283 - val_loss: 0.5734 - val_acc: 0.7422\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5670 - acc: 0.7313 - val_loss: 0.5663 - val_acc: 0.7389\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5584 - acc: 0.7387 - val_loss: 0.5596 - val_acc: 0.7406\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5501 - acc: 0.7462 - val_loss: 0.5534 - val_acc: 0.7471\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5402 - acc: 0.7511 - val_loss: 0.5467 - val_acc: 0.7521\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5355 - acc: 0.7542 - val_loss: 0.5407 - val_acc: 0.7603\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5292 - acc: 0.7628 - val_loss: 0.5353 - val_acc: 0.7586\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5228 - acc: 0.7617 - val_loss: 0.5309 - val_acc: 0.7586\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5185 - acc: 0.7628 - val_loss: 0.5261 - val_acc: 0.7586\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5132 - acc: 0.7688 - val_loss: 0.5217 - val_acc: 0.7603\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5074 - acc: 0.7738 - val_loss: 0.5181 - val_acc: 0.7603\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5063 - acc: 0.7743 - val_loss: 0.5147 - val_acc: 0.7619\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5013 - acc: 0.7749 - val_loss: 0.5110 - val_acc: 0.7603\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5000 - acc: 0.7770 - val_loss: 0.5076 - val_acc: 0.7635\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4923 - acc: 0.7816 - val_loss: 0.5044 - val_acc: 0.7635\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4903 - acc: 0.7783 - val_loss: 0.5014 - val_acc: 0.7619\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4862 - acc: 0.7836 - val_loss: 0.4984 - val_acc: 0.7668\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4839 - acc: 0.7849 - val_loss: 0.4957 - val_acc: 0.7685\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4800 - acc: 0.7893 - val_loss: 0.4935 - val_acc: 0.7718\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4777 - acc: 0.7882 - val_loss: 0.4913 - val_acc: 0.7685\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4723 - acc: 0.7889 - val_loss: 0.4890 - val_acc: 0.7734\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4738 - acc: 0.7922 - val_loss: 0.4872 - val_acc: 0.7750\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4692 - acc: 0.7973 - val_loss: 0.4849 - val_acc: 0.7718\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4682 - acc: 0.7958 - val_loss: 0.4830 - val_acc: 0.7734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4659 - acc: 0.7951 - val_loss: 0.4814 - val_acc: 0.7750\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4627 - acc: 0.7942 - val_loss: 0.4797 - val_acc: 0.7734\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4637 - acc: 0.7946 - val_loss: 0.4781 - val_acc: 0.7750\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4589 - acc: 0.7951 - val_loss: 0.4766 - val_acc: 0.7800\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4584 - acc: 0.7997 - val_loss: 0.4753 - val_acc: 0.7750\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4555 - acc: 0.8026 - val_loss: 0.4741 - val_acc: 0.7816\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4539 - acc: 0.8015 - val_loss: 0.4727 - val_acc: 0.7833\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4534 - acc: 0.8022 - val_loss: 0.4717 - val_acc: 0.7833\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4491 - acc: 0.8039 - val_loss: 0.4706 - val_acc: 0.7849\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4507 - acc: 0.8017 - val_loss: 0.4695 - val_acc: 0.7849\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4503 - acc: 0.8059 - val_loss: 0.4686 - val_acc: 0.7865\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4473 - acc: 0.8059 - val_loss: 0.4676 - val_acc: 0.7865\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4460 - acc: 0.8066 - val_loss: 0.4666 - val_acc: 0.7882\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4437 - acc: 0.8066 - val_loss: 0.4657 - val_acc: 0.7898\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4420 - acc: 0.8070 - val_loss: 0.4651 - val_acc: 0.7947\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4409 - acc: 0.8084 - val_loss: 0.4641 - val_acc: 0.7882\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4440 - acc: 0.8028 - val_loss: 0.4632 - val_acc: 0.7915\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4418 - acc: 0.8110 - val_loss: 0.4632 - val_acc: 0.7980\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4360 - acc: 0.8108 - val_loss: 0.4619 - val_acc: 0.7947\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4383 - acc: 0.8086 - val_loss: 0.4611 - val_acc: 0.7964\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4345 - acc: 0.8130 - val_loss: 0.4606 - val_acc: 0.7964\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4341 - acc: 0.8086 - val_loss: 0.4598 - val_acc: 0.7980\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4341 - acc: 0.8126 - val_loss: 0.4590 - val_acc: 0.7997\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4314 - acc: 0.8135 - val_loss: 0.4586 - val_acc: 0.7980\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4314 - acc: 0.8152 - val_loss: 0.4580 - val_acc: 0.8013\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4303 - acc: 0.8112 - val_loss: 0.4574 - val_acc: 0.7980\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4310 - acc: 0.8110 - val_loss: 0.4572 - val_acc: 0.7980\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4307 - acc: 0.8144 - val_loss: 0.4568 - val_acc: 0.7980\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4270 - acc: 0.8170 - val_loss: 0.4561 - val_acc: 0.7980\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4278 - acc: 0.8152 - val_loss: 0.4555 - val_acc: 0.7997\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4253 - acc: 0.8161 - val_loss: 0.4555 - val_acc: 0.8062\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4260 - acc: 0.8152 - val_loss: 0.4546 - val_acc: 0.7997\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4249 - acc: 0.8183 - val_loss: 0.4542 - val_acc: 0.8013\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4230 - acc: 0.8174 - val_loss: 0.4539 - val_acc: 0.8013\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4227 - acc: 0.8159 - val_loss: 0.4534 - val_acc: 0.8030\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4229 - acc: 0.8205 - val_loss: 0.4530 - val_acc: 0.8030\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4204 - acc: 0.8128 - val_loss: 0.4525 - val_acc: 0.8030\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4208 - acc: 0.8199 - val_loss: 0.4522 - val_acc: 0.8030\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4193 - acc: 0.8183 - val_loss: 0.4519 - val_acc: 0.8030\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4193 - acc: 0.8174 - val_loss: 0.4516 - val_acc: 0.8062\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4169 - acc: 0.8177 - val_loss: 0.4516 - val_acc: 0.8030\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4170 - acc: 0.8181 - val_loss: 0.4514 - val_acc: 0.8062\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4173 - acc: 0.8217 - val_loss: 0.4507 - val_acc: 0.8030\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4153 - acc: 0.8223 - val_loss: 0.4502 - val_acc: 0.8030\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4152 - acc: 0.8232 - val_loss: 0.4498 - val_acc: 0.8046\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4155 - acc: 0.8223 - val_loss: 0.4496 - val_acc: 0.8046\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4160 - acc: 0.8190 - val_loss: 0.4495 - val_acc: 0.8046\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4130 - acc: 0.8238 - val_loss: 0.4496 - val_acc: 0.8079\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4122 - acc: 0.8210 - val_loss: 0.4490 - val_acc: 0.8062\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4133 - acc: 0.8210 - val_loss: 0.4489 - val_acc: 0.8046\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4099 - acc: 0.8216 - val_loss: 0.4494 - val_acc: 0.8079\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4112 - acc: 0.8223 - val_loss: 0.4493 - val_acc: 0.8112\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4092 - acc: 0.8228 - val_loss: 0.4482 - val_acc: 0.8079\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4089 - acc: 0.8217 - val_loss: 0.4480 - val_acc: 0.8112\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4092 - acc: 0.8228 - val_loss: 0.4475 - val_acc: 0.8128\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4068 - acc: 0.8250 - val_loss: 0.4471 - val_acc: 0.8112\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4077 - acc: 0.8261 - val_loss: 0.4472 - val_acc: 0.8095\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4090 - acc: 0.8227 - val_loss: 0.4471 - val_acc: 0.8095\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4058 - acc: 0.8216 - val_loss: 0.4468 - val_acc: 0.8112\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4065 - acc: 0.8217 - val_loss: 0.4466 - val_acc: 0.8112\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4033 - acc: 0.8261 - val_loss: 0.4466 - val_acc: 0.8144\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4055 - acc: 0.8274 - val_loss: 0.4467 - val_acc: 0.8144\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4024 - acc: 0.8245 - val_loss: 0.4459 - val_acc: 0.8128\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4047 - acc: 0.8254 - val_loss: 0.4456 - val_acc: 0.8128\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4036 - acc: 0.8270 - val_loss: 0.4453 - val_acc: 0.8161\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4028 - acc: 0.8234 - val_loss: 0.4449 - val_acc: 0.8128\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4028 - acc: 0.8245 - val_loss: 0.4454 - val_acc: 0.8144\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4027 - acc: 0.8245 - val_loss: 0.4456 - val_acc: 0.8128\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4012 - acc: 0.8278 - val_loss: 0.4452 - val_acc: 0.8128\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4017 - acc: 0.8241 - val_loss: 0.4449 - val_acc: 0.8112\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4008 - acc: 0.8241 - val_loss: 0.4443 - val_acc: 0.8161\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4020 - acc: 0.8287 - val_loss: 0.4443 - val_acc: 0.8128\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3971 - acc: 0.8247 - val_loss: 0.4441 - val_acc: 0.8112\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3978 - acc: 0.8278 - val_loss: 0.4444 - val_acc: 0.8112\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3982 - acc: 0.8274 - val_loss: 0.4443 - val_acc: 0.8095\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3991 - acc: 0.8245 - val_loss: 0.4440 - val_acc: 0.8128\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3970 - acc: 0.8263 - val_loss: 0.4442 - val_acc: 0.8095\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3965 - acc: 0.8265 - val_loss: 0.4440 - val_acc: 0.8112\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3974 - acc: 0.8289 - val_loss: 0.4442 - val_acc: 0.8095\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3976 - acc: 0.8289 - val_loss: 0.4441 - val_acc: 0.8128\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3935 - acc: 0.8287 - val_loss: 0.4442 - val_acc: 0.8079\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3944 - acc: 0.8290 - val_loss: 0.4444 - val_acc: 0.8095\n",
      "Epoch 114/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3943 - acc: 0.8287 - val_loss: 0.4430 - val_acc: 0.8079\n",
      "Epoch 115/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3935 - acc: 0.8300 - val_loss: 0.4431 - val_acc: 0.8095\n",
      "Epoch 116/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3918 - acc: 0.8258 - val_loss: 0.4433 - val_acc: 0.8095\n",
      "Epoch 117/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3953 - acc: 0.8281 - val_loss: 0.4431 - val_acc: 0.8112\n",
      "Epoch 118/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3921 - acc: 0.8318 - val_loss: 0.4431 - val_acc: 0.8128\n",
      "Epoch 119/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3913 - acc: 0.8311 - val_loss: 0.4436 - val_acc: 0.8079\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.2, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_94\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_95 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_268 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_174 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_269 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_175 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_270 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.8102 - acc: 0.4355 - val_loss: 0.7061 - val_acc: 0.4565\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.7003 - acc: 0.5092 - val_loss: 0.6624 - val_acc: 0.5862\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.6706 - acc: 0.5778 - val_loss: 0.6456 - val_acc: 0.5846\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.6482 - acc: 0.5959 - val_loss: 0.6240 - val_acc: 0.6125\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.6218 - acc: 0.6504 - val_loss: 0.6072 - val_acc: 0.7291\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.6042 - acc: 0.6915 - val_loss: 0.5987 - val_acc: 0.6831\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.5909 - acc: 0.7008 - val_loss: 0.5853 - val_acc: 0.7061\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.5774 - acc: 0.7145 - val_loss: 0.5705 - val_acc: 0.7471\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.5632 - acc: 0.7336 - val_loss: 0.5587 - val_acc: 0.7537\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.5490 - acc: 0.7396 - val_loss: 0.5467 - val_acc: 0.7455\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.5372 - acc: 0.7464 - val_loss: 0.5344 - val_acc: 0.7537\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.5226 - acc: 0.7566 - val_loss: 0.5236 - val_acc: 0.7537\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.5138 - acc: 0.7648 - val_loss: 0.5142 - val_acc: 0.7603\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.5058 - acc: 0.7679 - val_loss: 0.5060 - val_acc: 0.7586\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4917 - acc: 0.7767 - val_loss: 0.4983 - val_acc: 0.7635\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4862 - acc: 0.7796 - val_loss: 0.4925 - val_acc: 0.7685\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4793 - acc: 0.7895 - val_loss: 0.4855 - val_acc: 0.7635\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4726 - acc: 0.7898 - val_loss: 0.4797 - val_acc: 0.7685\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4674 - acc: 0.7937 - val_loss: 0.4757 - val_acc: 0.7833\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4645 - acc: 0.7973 - val_loss: 0.4713 - val_acc: 0.7734\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.4580 - acc: 0.8000 - val_loss: 0.4676 - val_acc: 0.7849\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4544 - acc: 0.7991 - val_loss: 0.4634 - val_acc: 0.7816\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.4499 - acc: 0.8000 - val_loss: 0.4611 - val_acc: 0.7865\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.4484 - acc: 0.8055 - val_loss: 0.4580 - val_acc: 0.7882\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4439 - acc: 0.8055 - val_loss: 0.4562 - val_acc: 0.7865\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4367 - acc: 0.8088 - val_loss: 0.4550 - val_acc: 0.7898\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4344 - acc: 0.8139 - val_loss: 0.4524 - val_acc: 0.7980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4354 - acc: 0.8115 - val_loss: 0.4520 - val_acc: 0.7931\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4296 - acc: 0.8143 - val_loss: 0.4501 - val_acc: 0.7980\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4256 - acc: 0.8135 - val_loss: 0.4492 - val_acc: 0.7997\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4234 - acc: 0.8157 - val_loss: 0.4480 - val_acc: 0.7980\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4250 - acc: 0.8157 - val_loss: 0.4466 - val_acc: 0.7980\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4173 - acc: 0.8208 - val_loss: 0.4454 - val_acc: 0.7947\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4188 - acc: 0.8197 - val_loss: 0.4440 - val_acc: 0.7947\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4160 - acc: 0.8199 - val_loss: 0.4427 - val_acc: 0.7997\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4158 - acc: 0.8196 - val_loss: 0.4426 - val_acc: 0.7997\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4112 - acc: 0.8241 - val_loss: 0.4429 - val_acc: 0.8013\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4097 - acc: 0.8212 - val_loss: 0.4430 - val_acc: 0.8013\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4059 - acc: 0.8208 - val_loss: 0.4418 - val_acc: 0.8030\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.4045 - acc: 0.8225 - val_loss: 0.4407 - val_acc: 0.7997\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4053 - acc: 0.8234 - val_loss: 0.4418 - val_acc: 0.8095\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4048 - acc: 0.8252 - val_loss: 0.4410 - val_acc: 0.8062\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3982 - acc: 0.8283 - val_loss: 0.4416 - val_acc: 0.8095\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4006 - acc: 0.8261 - val_loss: 0.4400 - val_acc: 0.8062\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3979 - acc: 0.8294 - val_loss: 0.4388 - val_acc: 0.8079\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3945 - acc: 0.8303 - val_loss: 0.4382 - val_acc: 0.8095\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3929 - acc: 0.8314 - val_loss: 0.4375 - val_acc: 0.8046\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3926 - acc: 0.8298 - val_loss: 0.4370 - val_acc: 0.8112\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3905 - acc: 0.8316 - val_loss: 0.4367 - val_acc: 0.8128\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3921 - acc: 0.8336 - val_loss: 0.4368 - val_acc: 0.8046\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3853 - acc: 0.8311 - val_loss: 0.4392 - val_acc: 0.8095\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3834 - acc: 0.8369 - val_loss: 0.4364 - val_acc: 0.8062\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3808 - acc: 0.8362 - val_loss: 0.4360 - val_acc: 0.8079\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3796 - acc: 0.8376 - val_loss: 0.4340 - val_acc: 0.8112\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3866 - acc: 0.8318 - val_loss: 0.4345 - val_acc: 0.8095\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3790 - acc: 0.8376 - val_loss: 0.4341 - val_acc: 0.8112\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3773 - acc: 0.8349 - val_loss: 0.4349 - val_acc: 0.8095\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3784 - acc: 0.8363 - val_loss: 0.4345 - val_acc: 0.8062\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3730 - acc: 0.8405 - val_loss: 0.4339 - val_acc: 0.8112\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3754 - acc: 0.8363 - val_loss: 0.4341 - val_acc: 0.8095\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3725 - acc: 0.8376 - val_loss: 0.4343 - val_acc: 0.8062\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3710 - acc: 0.8418 - val_loss: 0.4373 - val_acc: 0.8079\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3650 - acc: 0.8442 - val_loss: 0.4342 - val_acc: 0.8128\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3696 - acc: 0.8433 - val_loss: 0.4360 - val_acc: 0.8095\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.2, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_95\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_96 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_271 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_176 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_272 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_177 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_273 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.8506 - acc: 0.4326 - val_loss: 0.7632 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.7515 - acc: 0.4512 - val_loss: 0.6984 - val_acc: 0.4729\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6985 - acc: 0.5047 - val_loss: 0.6687 - val_acc: 0.6273\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6732 - acc: 0.5895 - val_loss: 0.6541 - val_acc: 0.6174\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6606 - acc: 0.6152 - val_loss: 0.6417 - val_acc: 0.6240\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6457 - acc: 0.6409 - val_loss: 0.6290 - val_acc: 0.6814\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6307 - acc: 0.6674 - val_loss: 0.6173 - val_acc: 0.7488\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6191 - acc: 0.6845 - val_loss: 0.6075 - val_acc: 0.7422\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6075 - acc: 0.6933 - val_loss: 0.5989 - val_acc: 0.7258\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5992 - acc: 0.7008 - val_loss: 0.5901 - val_acc: 0.7176\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5920 - acc: 0.7088 - val_loss: 0.5816 - val_acc: 0.7291\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5796 - acc: 0.7183 - val_loss: 0.5734 - val_acc: 0.7389\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5753 - acc: 0.7185 - val_loss: 0.5653 - val_acc: 0.7521\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5618 - acc: 0.7354 - val_loss: 0.5576 - val_acc: 0.7455\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5588 - acc: 0.7364 - val_loss: 0.5503 - val_acc: 0.7455\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5516 - acc: 0.7395 - val_loss: 0.5432 - val_acc: 0.7521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5437 - acc: 0.7448 - val_loss: 0.5362 - val_acc: 0.7635\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5319 - acc: 0.7482 - val_loss: 0.5294 - val_acc: 0.7619\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5304 - acc: 0.7539 - val_loss: 0.5233 - val_acc: 0.7619\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.5272 - acc: 0.7583 - val_loss: 0.5177 - val_acc: 0.7701\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5144 - acc: 0.7637 - val_loss: 0.5126 - val_acc: 0.7718\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5136 - acc: 0.7670 - val_loss: 0.5072 - val_acc: 0.7734\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5039 - acc: 0.7708 - val_loss: 0.5025 - val_acc: 0.7701\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5018 - acc: 0.7718 - val_loss: 0.4982 - val_acc: 0.7734\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4975 - acc: 0.7708 - val_loss: 0.4944 - val_acc: 0.7718\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4946 - acc: 0.7798 - val_loss: 0.4914 - val_acc: 0.7750\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4896 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7767\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4839 - acc: 0.7836 - val_loss: 0.4850 - val_acc: 0.7767\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4785 - acc: 0.7874 - val_loss: 0.4816 - val_acc: 0.7833\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4757 - acc: 0.7893 - val_loss: 0.4797 - val_acc: 0.7833\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4755 - acc: 0.7865 - val_loss: 0.4766 - val_acc: 0.7816\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4729 - acc: 0.7871 - val_loss: 0.4739 - val_acc: 0.7849\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4687 - acc: 0.7926 - val_loss: 0.4722 - val_acc: 0.7833\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4654 - acc: 0.7955 - val_loss: 0.4696 - val_acc: 0.7849\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4633 - acc: 0.7949 - val_loss: 0.4673 - val_acc: 0.7898\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4620 - acc: 0.7907 - val_loss: 0.4658 - val_acc: 0.7816\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4553 - acc: 0.7984 - val_loss: 0.4640 - val_acc: 0.7833\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4524 - acc: 0.7957 - val_loss: 0.4620 - val_acc: 0.7882\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4549 - acc: 0.8028 - val_loss: 0.4607 - val_acc: 0.7882\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4521 - acc: 0.8041 - val_loss: 0.4592 - val_acc: 0.7882\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4490 - acc: 0.7982 - val_loss: 0.4582 - val_acc: 0.7915\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4491 - acc: 0.8031 - val_loss: 0.4579 - val_acc: 0.7898\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4449 - acc: 0.8062 - val_loss: 0.4568 - val_acc: 0.7931\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4439 - acc: 0.8079 - val_loss: 0.4560 - val_acc: 0.7947\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4370 - acc: 0.8068 - val_loss: 0.4543 - val_acc: 0.7964\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4380 - acc: 0.8088 - val_loss: 0.4531 - val_acc: 0.7980\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4364 - acc: 0.8088 - val_loss: 0.4519 - val_acc: 0.7980\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4356 - acc: 0.8084 - val_loss: 0.4516 - val_acc: 0.8046\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4349 - acc: 0.8143 - val_loss: 0.4501 - val_acc: 0.8030\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4315 - acc: 0.8148 - val_loss: 0.4486 - val_acc: 0.7997\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4268 - acc: 0.8134 - val_loss: 0.4483 - val_acc: 0.8079\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4304 - acc: 0.8121 - val_loss: 0.4481 - val_acc: 0.8095\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4266 - acc: 0.8128 - val_loss: 0.4468 - val_acc: 0.8046\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4279 - acc: 0.8095 - val_loss: 0.4464 - val_acc: 0.8079\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4305 - acc: 0.8134 - val_loss: 0.4465 - val_acc: 0.8062\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4247 - acc: 0.8135 - val_loss: 0.4451 - val_acc: 0.8095\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4223 - acc: 0.8150 - val_loss: 0.4464 - val_acc: 0.8046\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4231 - acc: 0.8139 - val_loss: 0.4448 - val_acc: 0.8128\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4179 - acc: 0.8165 - val_loss: 0.4442 - val_acc: 0.8079\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4206 - acc: 0.8225 - val_loss: 0.4443 - val_acc: 0.8095\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4151 - acc: 0.8221 - val_loss: 0.4446 - val_acc: 0.8095\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4154 - acc: 0.8196 - val_loss: 0.4426 - val_acc: 0.8095\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4159 - acc: 0.8165 - val_loss: 0.4426 - val_acc: 0.8128\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4122 - acc: 0.8227 - val_loss: 0.4439 - val_acc: 0.8079\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4142 - acc: 0.8212 - val_loss: 0.4416 - val_acc: 0.8079\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4121 - acc: 0.8190 - val_loss: 0.4420 - val_acc: 0.8095\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4139 - acc: 0.8203 - val_loss: 0.4417 - val_acc: 0.8112\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4119 - acc: 0.8203 - val_loss: 0.4405 - val_acc: 0.8112\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4074 - acc: 0.8227 - val_loss: 0.4393 - val_acc: 0.8128\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4070 - acc: 0.8247 - val_loss: 0.4405 - val_acc: 0.8144\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4065 - acc: 0.8210 - val_loss: 0.4388 - val_acc: 0.8161\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4102 - acc: 0.8214 - val_loss: 0.4395 - val_acc: 0.8112\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4067 - acc: 0.8234 - val_loss: 0.4394 - val_acc: 0.8112\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4070 - acc: 0.8261 - val_loss: 0.4386 - val_acc: 0.8112\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4025 - acc: 0.8258 - val_loss: 0.4383 - val_acc: 0.8128\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3999 - acc: 0.8245 - val_loss: 0.4382 - val_acc: 0.8112\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4023 - acc: 0.8254 - val_loss: 0.4381 - val_acc: 0.8128\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4017 - acc: 0.8263 - val_loss: 0.4376 - val_acc: 0.8128\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3994 - acc: 0.8261 - val_loss: 0.4378 - val_acc: 0.8128\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3971 - acc: 0.8296 - val_loss: 0.4374 - val_acc: 0.8112\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4030 - acc: 0.8254 - val_loss: 0.4371 - val_acc: 0.8112\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3944 - acc: 0.8301 - val_loss: 0.4376 - val_acc: 0.8079\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3961 - acc: 0.8292 - val_loss: 0.4371 - val_acc: 0.8095\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3963 - acc: 0.8314 - val_loss: 0.4383 - val_acc: 0.8095\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3939 - acc: 0.8298 - val_loss: 0.4366 - val_acc: 0.8079\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3933 - acc: 0.8301 - val_loss: 0.4362 - val_acc: 0.8046\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3894 - acc: 0.8356 - val_loss: 0.4370 - val_acc: 0.8112\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3923 - acc: 0.8305 - val_loss: 0.4350 - val_acc: 0.8079\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3897 - acc: 0.8318 - val_loss: 0.4360 - val_acc: 0.8046\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3907 - acc: 0.8336 - val_loss: 0.4373 - val_acc: 0.8112\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3844 - acc: 0.8349 - val_loss: 0.4367 - val_acc: 0.8046\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3916 - acc: 0.8298 - val_loss: 0.4361 - val_acc: 0.8062\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3894 - acc: 0.8358 - val_loss: 0.4372 - val_acc: 0.8062\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.2, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_96\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_97 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_274 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_178 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_275 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_179 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_276 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.7537 - acc: 0.4406 - val_loss: 0.7141 - val_acc: 0.4187\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.7052 - acc: 0.4970 - val_loss: 0.6768 - val_acc: 0.6125\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6781 - acc: 0.5789 - val_loss: 0.6573 - val_acc: 0.6486\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6635 - acc: 0.6088 - val_loss: 0.6463 - val_acc: 0.6125\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6504 - acc: 0.6284 - val_loss: 0.6371 - val_acc: 0.6125\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6422 - acc: 0.6404 - val_loss: 0.6273 - val_acc: 0.6486\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6285 - acc: 0.6590 - val_loss: 0.6174 - val_acc: 0.7044\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6193 - acc: 0.6804 - val_loss: 0.6082 - val_acc: 0.7488\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6087 - acc: 0.7015 - val_loss: 0.5995 - val_acc: 0.7340\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5975 - acc: 0.7013 - val_loss: 0.5905 - val_acc: 0.7373\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5859 - acc: 0.7123 - val_loss: 0.5811 - val_acc: 0.7455\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5794 - acc: 0.7181 - val_loss: 0.5725 - val_acc: 0.7521\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5689 - acc: 0.7267 - val_loss: 0.5644 - val_acc: 0.7570\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5594 - acc: 0.7354 - val_loss: 0.5565 - val_acc: 0.7553\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5555 - acc: 0.7344 - val_loss: 0.5488 - val_acc: 0.7537\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5427 - acc: 0.7469 - val_loss: 0.5413 - val_acc: 0.7586\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5365 - acc: 0.7484 - val_loss: 0.5341 - val_acc: 0.7635\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5260 - acc: 0.7572 - val_loss: 0.5271 - val_acc: 0.7701\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5177 - acc: 0.7630 - val_loss: 0.5208 - val_acc: 0.7668\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5104 - acc: 0.7643 - val_loss: 0.5148 - val_acc: 0.7635\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5097 - acc: 0.7654 - val_loss: 0.5090 - val_acc: 0.7668\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5028 - acc: 0.7718 - val_loss: 0.5038 - val_acc: 0.7685\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4979 - acc: 0.7745 - val_loss: 0.4988 - val_acc: 0.7668\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4912 - acc: 0.7829 - val_loss: 0.4948 - val_acc: 0.7718\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4842 - acc: 0.7853 - val_loss: 0.4911 - val_acc: 0.7734\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4829 - acc: 0.7840 - val_loss: 0.4878 - val_acc: 0.7734\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4770 - acc: 0.7889 - val_loss: 0.4843 - val_acc: 0.7734\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4741 - acc: 0.7847 - val_loss: 0.4818 - val_acc: 0.7718\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4690 - acc: 0.7915 - val_loss: 0.4794 - val_acc: 0.7701\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4702 - acc: 0.7937 - val_loss: 0.4769 - val_acc: 0.7701\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4637 - acc: 0.7918 - val_loss: 0.4746 - val_acc: 0.7767\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4610 - acc: 0.7911 - val_loss: 0.4719 - val_acc: 0.7816\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4599 - acc: 0.7947 - val_loss: 0.4694 - val_acc: 0.7865\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4577 - acc: 0.7980 - val_loss: 0.4686 - val_acc: 0.7915\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4550 - acc: 0.7993 - val_loss: 0.4660 - val_acc: 0.7882\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4544 - acc: 0.7955 - val_loss: 0.4643 - val_acc: 0.7915\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4523 - acc: 0.8006 - val_loss: 0.4638 - val_acc: 0.7931\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4483 - acc: 0.8019 - val_loss: 0.4615 - val_acc: 0.7898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4466 - acc: 0.8051 - val_loss: 0.4600 - val_acc: 0.7898\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4456 - acc: 0.8017 - val_loss: 0.4588 - val_acc: 0.7915\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4446 - acc: 0.8041 - val_loss: 0.4583 - val_acc: 0.7980\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4400 - acc: 0.8095 - val_loss: 0.4571 - val_acc: 0.7980\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4370 - acc: 0.8108 - val_loss: 0.4551 - val_acc: 0.7947\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4394 - acc: 0.8057 - val_loss: 0.4545 - val_acc: 0.7980\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4341 - acc: 0.8099 - val_loss: 0.4533 - val_acc: 0.7980\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4328 - acc: 0.8090 - val_loss: 0.4532 - val_acc: 0.8013\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4322 - acc: 0.8137 - val_loss: 0.4512 - val_acc: 0.7997\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4279 - acc: 0.8119 - val_loss: 0.4511 - val_acc: 0.8030\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4285 - acc: 0.8174 - val_loss: 0.4510 - val_acc: 0.8046\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4291 - acc: 0.8126 - val_loss: 0.4500 - val_acc: 0.8046\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4275 - acc: 0.8148 - val_loss: 0.4494 - val_acc: 0.8079\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4270 - acc: 0.8117 - val_loss: 0.4477 - val_acc: 0.8079\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4244 - acc: 0.8135 - val_loss: 0.4474 - val_acc: 0.8112\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4230 - acc: 0.8181 - val_loss: 0.4475 - val_acc: 0.8095\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4213 - acc: 0.8159 - val_loss: 0.4465 - val_acc: 0.8112\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4175 - acc: 0.8199 - val_loss: 0.4460 - val_acc: 0.8095\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4185 - acc: 0.8185 - val_loss: 0.4464 - val_acc: 0.8112\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4154 - acc: 0.8176 - val_loss: 0.4455 - val_acc: 0.8128\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4142 - acc: 0.8212 - val_loss: 0.4448 - val_acc: 0.8112\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4133 - acc: 0.8170 - val_loss: 0.4451 - val_acc: 0.8144\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4141 - acc: 0.8219 - val_loss: 0.4445 - val_acc: 0.8161\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4122 - acc: 0.8208 - val_loss: 0.4444 - val_acc: 0.8112\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4112 - acc: 0.8241 - val_loss: 0.4441 - val_acc: 0.8128\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4135 - acc: 0.8188 - val_loss: 0.4439 - val_acc: 0.8144\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4112 - acc: 0.8238 - val_loss: 0.4439 - val_acc: 0.8112\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4092 - acc: 0.8188 - val_loss: 0.4429 - val_acc: 0.8079\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4057 - acc: 0.8270 - val_loss: 0.4449 - val_acc: 0.8161\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4102 - acc: 0.8232 - val_loss: 0.4420 - val_acc: 0.8112\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4033 - acc: 0.8230 - val_loss: 0.4420 - val_acc: 0.8128\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4049 - acc: 0.8292 - val_loss: 0.4416 - val_acc: 0.8144\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4043 - acc: 0.8263 - val_loss: 0.4405 - val_acc: 0.8144\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4018 - acc: 0.8236 - val_loss: 0.4401 - val_acc: 0.8144\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3980 - acc: 0.8254 - val_loss: 0.4411 - val_acc: 0.8161\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4017 - acc: 0.8247 - val_loss: 0.4409 - val_acc: 0.8144\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3990 - acc: 0.8263 - val_loss: 0.4405 - val_acc: 0.8144\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3999 - acc: 0.8298 - val_loss: 0.4415 - val_acc: 0.8144\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3970 - acc: 0.8223 - val_loss: 0.4421 - val_acc: 0.8128\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.2, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_97\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_98 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_277 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_180 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_278 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_181 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_279 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.8189 - acc: 0.4393 - val_loss: 0.7405 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.7223 - acc: 0.4846 - val_loss: 0.6812 - val_acc: 0.5961\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6795 - acc: 0.5723 - val_loss: 0.6585 - val_acc: 0.5878\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6640 - acc: 0.6105 - val_loss: 0.6464 - val_acc: 0.5862\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6514 - acc: 0.6181 - val_loss: 0.6327 - val_acc: 0.5977\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6351 - acc: 0.6450 - val_loss: 0.6187 - val_acc: 0.6617\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6175 - acc: 0.6824 - val_loss: 0.6074 - val_acc: 0.7406\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.6049 - acc: 0.6900 - val_loss: 0.5982 - val_acc: 0.7274\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5928 - acc: 0.6973 - val_loss: 0.5892 - val_acc: 0.7192\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5858 - acc: 0.6977 - val_loss: 0.5794 - val_acc: 0.7373\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5732 - acc: 0.7094 - val_loss: 0.5697 - val_acc: 0.7455\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5609 - acc: 0.7252 - val_loss: 0.5599 - val_acc: 0.7570\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.5514 - acc: 0.7323 - val_loss: 0.5498 - val_acc: 0.7553\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5445 - acc: 0.7373 - val_loss: 0.5399 - val_acc: 0.7438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.5323 - acc: 0.7449 - val_loss: 0.5296 - val_acc: 0.7603\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.5261 - acc: 0.7493 - val_loss: 0.5210 - val_acc: 0.7767\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.5129 - acc: 0.7606 - val_loss: 0.5124 - val_acc: 0.7734\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5027 - acc: 0.7634 - val_loss: 0.5048 - val_acc: 0.7783\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4965 - acc: 0.7681 - val_loss: 0.4997 - val_acc: 0.7783\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4951 - acc: 0.7696 - val_loss: 0.4929 - val_acc: 0.7734\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4866 - acc: 0.7756 - val_loss: 0.4882 - val_acc: 0.7783\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4772 - acc: 0.7836 - val_loss: 0.4836 - val_acc: 0.7816\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4755 - acc: 0.7856 - val_loss: 0.4786 - val_acc: 0.7849\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4670 - acc: 0.7929 - val_loss: 0.4752 - val_acc: 0.7849\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4637 - acc: 0.7947 - val_loss: 0.4705 - val_acc: 0.7833\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4628 - acc: 0.7880 - val_loss: 0.4680 - val_acc: 0.7849\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4527 - acc: 0.8006 - val_loss: 0.4661 - val_acc: 0.7865\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4522 - acc: 0.7991 - val_loss: 0.4641 - val_acc: 0.7882\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4501 - acc: 0.8077 - val_loss: 0.4609 - val_acc: 0.7898\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4475 - acc: 0.8011 - val_loss: 0.4613 - val_acc: 0.7915\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4446 - acc: 0.8048 - val_loss: 0.4582 - val_acc: 0.7915\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4437 - acc: 0.8030 - val_loss: 0.4568 - val_acc: 0.7931\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4383 - acc: 0.8041 - val_loss: 0.4558 - val_acc: 0.7964\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4354 - acc: 0.8073 - val_loss: 0.4534 - val_acc: 0.7947\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4324 - acc: 0.8106 - val_loss: 0.4546 - val_acc: 0.7997\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4308 - acc: 0.8121 - val_loss: 0.4534 - val_acc: 0.7931\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4292 - acc: 0.8064 - val_loss: 0.4532 - val_acc: 0.7964\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4276 - acc: 0.8121 - val_loss: 0.4527 - val_acc: 0.8046\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4259 - acc: 0.8082 - val_loss: 0.4492 - val_acc: 0.8030\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4234 - acc: 0.8143 - val_loss: 0.4484 - val_acc: 0.8030\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4199 - acc: 0.8157 - val_loss: 0.4499 - val_acc: 0.8062\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4232 - acc: 0.8165 - val_loss: 0.4471 - val_acc: 0.8112\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4175 - acc: 0.8207 - val_loss: 0.4470 - val_acc: 0.8095\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4177 - acc: 0.8177 - val_loss: 0.4487 - val_acc: 0.8062\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4153 - acc: 0.8163 - val_loss: 0.4446 - val_acc: 0.8079\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4114 - acc: 0.8185 - val_loss: 0.4472 - val_acc: 0.8062\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4130 - acc: 0.8216 - val_loss: 0.4439 - val_acc: 0.8112\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4112 - acc: 0.8228 - val_loss: 0.4437 - val_acc: 0.8095\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4061 - acc: 0.8192 - val_loss: 0.4432 - val_acc: 0.8095\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4049 - acc: 0.8219 - val_loss: 0.4424 - val_acc: 0.8112\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4045 - acc: 0.8250 - val_loss: 0.4442 - val_acc: 0.8079\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4036 - acc: 0.8221 - val_loss: 0.4411 - val_acc: 0.8128\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4004 - acc: 0.8221 - val_loss: 0.4415 - val_acc: 0.8112\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4020 - acc: 0.8254 - val_loss: 0.4414 - val_acc: 0.8095\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3961 - acc: 0.8272 - val_loss: 0.4401 - val_acc: 0.8112\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3989 - acc: 0.8238 - val_loss: 0.4409 - val_acc: 0.8095\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3964 - acc: 0.8236 - val_loss: 0.4412 - val_acc: 0.8079\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3951 - acc: 0.8263 - val_loss: 0.4386 - val_acc: 0.8079\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3900 - acc: 0.8292 - val_loss: 0.4411 - val_acc: 0.8079\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3924 - acc: 0.8309 - val_loss: 0.4398 - val_acc: 0.8062\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3925 - acc: 0.8261 - val_loss: 0.4390 - val_acc: 0.8079\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3888 - acc: 0.8314 - val_loss: 0.4393 - val_acc: 0.8062\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3843 - acc: 0.8352 - val_loss: 0.4376 - val_acc: 0.7997\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3871 - acc: 0.8311 - val_loss: 0.4401 - val_acc: 0.8062\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3870 - acc: 0.8316 - val_loss: 0.4366 - val_acc: 0.7980\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3868 - acc: 0.8303 - val_loss: 0.4391 - val_acc: 0.8079\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3846 - acc: 0.8318 - val_loss: 0.4368 - val_acc: 0.8013\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3854 - acc: 0.8334 - val_loss: 0.4367 - val_acc: 0.8062\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3819 - acc: 0.8354 - val_loss: 0.4373 - val_acc: 0.8112\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3779 - acc: 0.8389 - val_loss: 0.4350 - val_acc: 0.8046\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3803 - acc: 0.8325 - val_loss: 0.4397 - val_acc: 0.8079\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3769 - acc: 0.8394 - val_loss: 0.4360 - val_acc: 0.7997\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3750 - acc: 0.8369 - val_loss: 0.4370 - val_acc: 0.8079\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3699 - acc: 0.8425 - val_loss: 0.4364 - val_acc: 0.8013\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3719 - acc: 0.8391 - val_loss: 0.4377 - val_acc: 0.8079\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.25, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_98\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_99 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_280 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_182 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_281 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_183 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_282 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 48ms/step - loss: 0.7481 - acc: 0.4579 - val_loss: 0.6807 - val_acc: 0.5599\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6791 - acc: 0.5627 - val_loss: 0.6621 - val_acc: 0.5846\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6591 - acc: 0.5842 - val_loss: 0.6397 - val_acc: 0.5846\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6312 - acc: 0.6346 - val_loss: 0.6164 - val_acc: 0.7159\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.6090 - acc: 0.6897 - val_loss: 0.6034 - val_acc: 0.6929\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.5946 - acc: 0.6975 - val_loss: 0.5887 - val_acc: 0.7028\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.5763 - acc: 0.7156 - val_loss: 0.5734 - val_acc: 0.7389\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.5596 - acc: 0.7364 - val_loss: 0.5604 - val_acc: 0.7471\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.5460 - acc: 0.7433 - val_loss: 0.5467 - val_acc: 0.7422\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.5295 - acc: 0.7504 - val_loss: 0.5339 - val_acc: 0.7471\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.5195 - acc: 0.7617 - val_loss: 0.5224 - val_acc: 0.7537\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.5032 - acc: 0.7783 - val_loss: 0.5124 - val_acc: 0.7521\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4945 - acc: 0.7734 - val_loss: 0.5043 - val_acc: 0.7586\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4887 - acc: 0.7772 - val_loss: 0.4976 - val_acc: 0.7652\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.4789 - acc: 0.7843 - val_loss: 0.4905 - val_acc: 0.7701\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4718 - acc: 0.7882 - val_loss: 0.4847 - val_acc: 0.7718\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4663 - acc: 0.7900 - val_loss: 0.4803 - val_acc: 0.7816\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4620 - acc: 0.7907 - val_loss: 0.4744 - val_acc: 0.7750\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4542 - acc: 0.7953 - val_loss: 0.4716 - val_acc: 0.7849\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4490 - acc: 0.7982 - val_loss: 0.4661 - val_acc: 0.7849\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4503 - acc: 0.7988 - val_loss: 0.4659 - val_acc: 0.7915\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4422 - acc: 0.8077 - val_loss: 0.4613 - val_acc: 0.7915\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4410 - acc: 0.8020 - val_loss: 0.4586 - val_acc: 0.7882\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4318 - acc: 0.8117 - val_loss: 0.4607 - val_acc: 0.7964\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.4337 - acc: 0.8103 - val_loss: 0.4547 - val_acc: 0.7898\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4290 - acc: 0.8106 - val_loss: 0.4529 - val_acc: 0.7980\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4253 - acc: 0.8139 - val_loss: 0.4524 - val_acc: 0.7964\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4213 - acc: 0.8163 - val_loss: 0.4506 - val_acc: 0.8013\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4196 - acc: 0.8161 - val_loss: 0.4506 - val_acc: 0.7980\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4168 - acc: 0.8205 - val_loss: 0.4503 - val_acc: 0.7964\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.4144 - acc: 0.8194 - val_loss: 0.4486 - val_acc: 0.7947\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.4137 - acc: 0.8188 - val_loss: 0.4457 - val_acc: 0.7997\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4098 - acc: 0.8217 - val_loss: 0.4444 - val_acc: 0.7980\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 0.4106 - acc: 0.8170 - val_loss: 0.4440 - val_acc: 0.8013\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4074 - acc: 0.8250 - val_loss: 0.4463 - val_acc: 0.8046\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4086 - acc: 0.8216 - val_loss: 0.4435 - val_acc: 0.8079\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4051 - acc: 0.8245 - val_loss: 0.4438 - val_acc: 0.8062\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.3995 - acc: 0.8247 - val_loss: 0.4403 - val_acc: 0.8062\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.3986 - acc: 0.8274 - val_loss: 0.4431 - val_acc: 0.8062\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3985 - acc: 0.8269 - val_loss: 0.4405 - val_acc: 0.8079\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.3954 - acc: 0.8305 - val_loss: 0.4420 - val_acc: 0.8079\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3920 - acc: 0.8354 - val_loss: 0.4390 - val_acc: 0.8030\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3901 - acc: 0.8371 - val_loss: 0.4406 - val_acc: 0.8112\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3911 - acc: 0.8311 - val_loss: 0.4383 - val_acc: 0.8062\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3895 - acc: 0.8329 - val_loss: 0.4375 - val_acc: 0.8095\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3883 - acc: 0.8331 - val_loss: 0.4367 - val_acc: 0.8079\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.3861 - acc: 0.8311 - val_loss: 0.4364 - val_acc: 0.8079\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.3825 - acc: 0.8340 - val_loss: 0.4382 - val_acc: 0.8079\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3775 - acc: 0.8387 - val_loss: 0.4361 - val_acc: 0.8079\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3781 - acc: 0.8396 - val_loss: 0.4396 - val_acc: 0.8095\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3777 - acc: 0.8380 - val_loss: 0.4377 - val_acc: 0.8062\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3787 - acc: 0.8360 - val_loss: 0.4373 - val_acc: 0.8062\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3744 - acc: 0.8402 - val_loss: 0.4370 - val_acc: 0.8079\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3695 - acc: 0.8418 - val_loss: 0.4348 - val_acc: 0.8030\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.3653 - acc: 0.8415 - val_loss: 0.4369 - val_acc: 0.8095\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.3668 - acc: 0.8427 - val_loss: 0.4350 - val_acc: 0.8046\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3646 - acc: 0.8442 - val_loss: 0.4339 - val_acc: 0.8062\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.3629 - acc: 0.8491 - val_loss: 0.4365 - val_acc: 0.8030\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.3615 - acc: 0.8436 - val_loss: 0.4360 - val_acc: 0.8046\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3611 - acc: 0.8471 - val_loss: 0.4375 - val_acc: 0.8046\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3604 - acc: 0.8435 - val_loss: 0.4348 - val_acc: 0.8046\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.3564 - acc: 0.8469 - val_loss: 0.4348 - val_acc: 0.8046\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.25, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_99\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_100 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_283 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_184 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_284 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_185 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_285 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.8247 - acc: 0.4446 - val_loss: 0.7412 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.7361 - acc: 0.4694 - val_loss: 0.6819 - val_acc: 0.5928\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6849 - acc: 0.5601 - val_loss: 0.6599 - val_acc: 0.5846\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6641 - acc: 0.6147 - val_loss: 0.6490 - val_acc: 0.5862\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6548 - acc: 0.6088 - val_loss: 0.6359 - val_acc: 0.5878\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6422 - acc: 0.6389 - val_loss: 0.6219 - val_acc: 0.6404\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6302 - acc: 0.6610 - val_loss: 0.6106 - val_acc: 0.7356\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6091 - acc: 0.6776 - val_loss: 0.6016 - val_acc: 0.7323\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.6020 - acc: 0.6763 - val_loss: 0.5926 - val_acc: 0.7241\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5874 - acc: 0.6984 - val_loss: 0.5831 - val_acc: 0.7389\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.5754 - acc: 0.7134 - val_loss: 0.5742 - val_acc: 0.7488\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5727 - acc: 0.7170 - val_loss: 0.5650 - val_acc: 0.7438\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.5581 - acc: 0.7349 - val_loss: 0.5555 - val_acc: 0.7455\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5518 - acc: 0.7386 - val_loss: 0.5458 - val_acc: 0.7455\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5440 - acc: 0.7418 - val_loss: 0.5364 - val_acc: 0.7603\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5310 - acc: 0.7457 - val_loss: 0.5279 - val_acc: 0.7767\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5248 - acc: 0.7548 - val_loss: 0.5189 - val_acc: 0.7668\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5140 - acc: 0.7645 - val_loss: 0.5116 - val_acc: 0.7718\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5056 - acc: 0.7681 - val_loss: 0.5060 - val_acc: 0.7750\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4996 - acc: 0.7690 - val_loss: 0.4990 - val_acc: 0.7783\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4891 - acc: 0.7772 - val_loss: 0.4938 - val_acc: 0.7718\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4881 - acc: 0.7772 - val_loss: 0.4892 - val_acc: 0.7750\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4848 - acc: 0.7801 - val_loss: 0.4839 - val_acc: 0.7783\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4745 - acc: 0.7836 - val_loss: 0.4801 - val_acc: 0.7833\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4732 - acc: 0.7876 - val_loss: 0.4770 - val_acc: 0.7833\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4685 - acc: 0.7871 - val_loss: 0.4733 - val_acc: 0.7816\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4667 - acc: 0.7891 - val_loss: 0.4723 - val_acc: 0.7833\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4626 - acc: 0.7931 - val_loss: 0.4687 - val_acc: 0.7833\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4564 - acc: 0.7978 - val_loss: 0.4673 - val_acc: 0.7816\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4539 - acc: 0.7946 - val_loss: 0.4640 - val_acc: 0.7865\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4486 - acc: 0.8000 - val_loss: 0.4627 - val_acc: 0.7882\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4475 - acc: 0.8015 - val_loss: 0.4609 - val_acc: 0.7898\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4416 - acc: 0.8064 - val_loss: 0.4605 - val_acc: 0.7898\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4401 - acc: 0.8055 - val_loss: 0.4574 - val_acc: 0.7915\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4398 - acc: 0.8039 - val_loss: 0.4588 - val_acc: 0.7898\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4347 - acc: 0.8082 - val_loss: 0.4557 - val_acc: 0.7898\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4344 - acc: 0.8070 - val_loss: 0.4543 - val_acc: 0.7915\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4302 - acc: 0.8128 - val_loss: 0.4530 - val_acc: 0.7947\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4302 - acc: 0.8095 - val_loss: 0.4521 - val_acc: 0.7980\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4279 - acc: 0.8115 - val_loss: 0.4512 - val_acc: 0.8030\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4265 - acc: 0.8077 - val_loss: 0.4494 - val_acc: 0.7997\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4262 - acc: 0.8101 - val_loss: 0.4497 - val_acc: 0.8046\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4214 - acc: 0.8137 - val_loss: 0.4473 - val_acc: 0.8046\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4239 - acc: 0.8135 - val_loss: 0.4494 - val_acc: 0.8079\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4190 - acc: 0.8210 - val_loss: 0.4463 - val_acc: 0.8013\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4189 - acc: 0.8161 - val_loss: 0.4471 - val_acc: 0.8062\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4224 - acc: 0.8117 - val_loss: 0.4470 - val_acc: 0.8079\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4115 - acc: 0.8201 - val_loss: 0.4441 - val_acc: 0.8046\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4151 - acc: 0.8177 - val_loss: 0.4481 - val_acc: 0.8046\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4107 - acc: 0.8217 - val_loss: 0.4445 - val_acc: 0.8079\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4087 - acc: 0.8183 - val_loss: 0.4440 - val_acc: 0.8079\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4108 - acc: 0.8227 - val_loss: 0.4449 - val_acc: 0.8079\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4046 - acc: 0.8217 - val_loss: 0.4440 - val_acc: 0.8095\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4092 - acc: 0.8207 - val_loss: 0.4429 - val_acc: 0.8095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4057 - acc: 0.8197 - val_loss: 0.4428 - val_acc: 0.8079\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4029 - acc: 0.8258 - val_loss: 0.4409 - val_acc: 0.8079\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4007 - acc: 0.8230 - val_loss: 0.4423 - val_acc: 0.8128\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4019 - acc: 0.8269 - val_loss: 0.4439 - val_acc: 0.8095\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3986 - acc: 0.8270 - val_loss: 0.4414 - val_acc: 0.8079\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3944 - acc: 0.8245 - val_loss: 0.4413 - val_acc: 0.8161\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3943 - acc: 0.8290 - val_loss: 0.4410 - val_acc: 0.8144\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.25, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_100\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_101 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_286 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_186 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_287 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.6679 - acc: 0.5933 - val_loss: 0.6458 - val_acc: 0.6765\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6498 - acc: 0.6245 - val_loss: 0.6301 - val_acc: 0.6700\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6311 - acc: 0.6468 - val_loss: 0.6181 - val_acc: 0.7209\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6178 - acc: 0.6767 - val_loss: 0.6077 - val_acc: 0.7044\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6058 - acc: 0.6904 - val_loss: 0.5995 - val_acc: 0.7094\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5946 - acc: 0.6999 - val_loss: 0.5909 - val_acc: 0.7094\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5863 - acc: 0.7088 - val_loss: 0.5821 - val_acc: 0.7258\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5724 - acc: 0.7249 - val_loss: 0.5739 - val_acc: 0.7356\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5669 - acc: 0.7303 - val_loss: 0.5665 - val_acc: 0.7438\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5608 - acc: 0.7311 - val_loss: 0.5596 - val_acc: 0.7438\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5517 - acc: 0.7418 - val_loss: 0.5534 - val_acc: 0.7471\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5452 - acc: 0.7462 - val_loss: 0.5471 - val_acc: 0.7537\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5381 - acc: 0.7561 - val_loss: 0.5412 - val_acc: 0.7586\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5321 - acc: 0.7544 - val_loss: 0.5358 - val_acc: 0.7586\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5273 - acc: 0.7617 - val_loss: 0.5307 - val_acc: 0.7570\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5207 - acc: 0.7639 - val_loss: 0.5265 - val_acc: 0.7570\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5164 - acc: 0.7666 - val_loss: 0.5217 - val_acc: 0.7570\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5102 - acc: 0.7712 - val_loss: 0.5179 - val_acc: 0.7635\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5068 - acc: 0.7701 - val_loss: 0.5141 - val_acc: 0.7652\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5026 - acc: 0.7792 - val_loss: 0.5108 - val_acc: 0.7619\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4981 - acc: 0.7791 - val_loss: 0.5076 - val_acc: 0.7586\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4944 - acc: 0.7752 - val_loss: 0.5043 - val_acc: 0.7652\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4896 - acc: 0.7827 - val_loss: 0.5012 - val_acc: 0.7668\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4885 - acc: 0.7818 - val_loss: 0.4984 - val_acc: 0.7652\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4835 - acc: 0.7842 - val_loss: 0.4957 - val_acc: 0.7701\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4818 - acc: 0.7878 - val_loss: 0.4934 - val_acc: 0.7734\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4782 - acc: 0.7898 - val_loss: 0.4913 - val_acc: 0.7718\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4770 - acc: 0.7900 - val_loss: 0.4893 - val_acc: 0.7701\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4755 - acc: 0.7913 - val_loss: 0.4873 - val_acc: 0.7734\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4712 - acc: 0.7924 - val_loss: 0.4856 - val_acc: 0.7734\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4687 - acc: 0.7929 - val_loss: 0.4838 - val_acc: 0.7767\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4685 - acc: 0.7951 - val_loss: 0.4820 - val_acc: 0.7750\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4669 - acc: 0.7929 - val_loss: 0.4803 - val_acc: 0.7750\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4646 - acc: 0.7982 - val_loss: 0.4788 - val_acc: 0.7800\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4633 - acc: 0.7944 - val_loss: 0.4772 - val_acc: 0.7833\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4585 - acc: 0.8000 - val_loss: 0.4762 - val_acc: 0.7816\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4597 - acc: 0.8002 - val_loss: 0.4749 - val_acc: 0.7833\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4573 - acc: 0.7960 - val_loss: 0.4736 - val_acc: 0.7833\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4576 - acc: 0.7973 - val_loss: 0.4725 - val_acc: 0.7849\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4537 - acc: 0.8013 - val_loss: 0.4714 - val_acc: 0.7865\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4531 - acc: 0.7997 - val_loss: 0.4703 - val_acc: 0.7865\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4497 - acc: 0.8046 - val_loss: 0.4693 - val_acc: 0.7865\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4511 - acc: 0.8002 - val_loss: 0.4682 - val_acc: 0.7882\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4489 - acc: 0.8028 - val_loss: 0.4674 - val_acc: 0.7882\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4475 - acc: 0.8070 - val_loss: 0.4665 - val_acc: 0.7915\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4463 - acc: 0.8033 - val_loss: 0.4661 - val_acc: 0.7947\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4450 - acc: 0.8039 - val_loss: 0.4648 - val_acc: 0.7915\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4407 - acc: 0.8053 - val_loss: 0.4641 - val_acc: 0.7915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4406 - acc: 0.8103 - val_loss: 0.4631 - val_acc: 0.7947\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4397 - acc: 0.8084 - val_loss: 0.4630 - val_acc: 0.7997\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4406 - acc: 0.8082 - val_loss: 0.4619 - val_acc: 0.7947\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4397 - acc: 0.8050 - val_loss: 0.4615 - val_acc: 0.7898\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4375 - acc: 0.8126 - val_loss: 0.4608 - val_acc: 0.7997\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4356 - acc: 0.8128 - val_loss: 0.4604 - val_acc: 0.7997\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4351 - acc: 0.8112 - val_loss: 0.4595 - val_acc: 0.7964\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4340 - acc: 0.8106 - val_loss: 0.4588 - val_acc: 0.7997\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4333 - acc: 0.8126 - val_loss: 0.4592 - val_acc: 0.8030\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4331 - acc: 0.8097 - val_loss: 0.4578 - val_acc: 0.7964\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4290 - acc: 0.8119 - val_loss: 0.4572 - val_acc: 0.7947\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4284 - acc: 0.8137 - val_loss: 0.4571 - val_acc: 0.7964\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4293 - acc: 0.8130 - val_loss: 0.4567 - val_acc: 0.7980\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4292 - acc: 0.8154 - val_loss: 0.4561 - val_acc: 0.7964\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4250 - acc: 0.8166 - val_loss: 0.4557 - val_acc: 0.7964\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4257 - acc: 0.8152 - val_loss: 0.4552 - val_acc: 0.7980\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4230 - acc: 0.8176 - val_loss: 0.4547 - val_acc: 0.8030\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4242 - acc: 0.8185 - val_loss: 0.4539 - val_acc: 0.7980\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4264 - acc: 0.8176 - val_loss: 0.4535 - val_acc: 0.7964\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4230 - acc: 0.8144 - val_loss: 0.4533 - val_acc: 0.7980\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4238 - acc: 0.8196 - val_loss: 0.4533 - val_acc: 0.8046\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4216 - acc: 0.8168 - val_loss: 0.4528 - val_acc: 0.8013\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4212 - acc: 0.8168 - val_loss: 0.4524 - val_acc: 0.7997\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4198 - acc: 0.8152 - val_loss: 0.4518 - val_acc: 0.8013\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4208 - acc: 0.8181 - val_loss: 0.4515 - val_acc: 0.8013\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4209 - acc: 0.8172 - val_loss: 0.4513 - val_acc: 0.8013\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4176 - acc: 0.8248 - val_loss: 0.4513 - val_acc: 0.8030\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4172 - acc: 0.8219 - val_loss: 0.4504 - val_acc: 0.8046\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4189 - acc: 0.8144 - val_loss: 0.4503 - val_acc: 0.8046\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4170 - acc: 0.8194 - val_loss: 0.4497 - val_acc: 0.8062\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4150 - acc: 0.8194 - val_loss: 0.4499 - val_acc: 0.8079\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4136 - acc: 0.8228 - val_loss: 0.4493 - val_acc: 0.8046\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4154 - acc: 0.8248 - val_loss: 0.4490 - val_acc: 0.8062\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4147 - acc: 0.8205 - val_loss: 0.4489 - val_acc: 0.8046\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4139 - acc: 0.8214 - val_loss: 0.4494 - val_acc: 0.8095\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4097 - acc: 0.8241 - val_loss: 0.4488 - val_acc: 0.8062\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4121 - acc: 0.8186 - val_loss: 0.4486 - val_acc: 0.8046\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4121 - acc: 0.8207 - val_loss: 0.4488 - val_acc: 0.8079\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4122 - acc: 0.8212 - val_loss: 0.4482 - val_acc: 0.8095\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4086 - acc: 0.8256 - val_loss: 0.4474 - val_acc: 0.8112\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4086 - acc: 0.8227 - val_loss: 0.4473 - val_acc: 0.8128\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4082 - acc: 0.8219 - val_loss: 0.4473 - val_acc: 0.8112\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4093 - acc: 0.8241 - val_loss: 0.4470 - val_acc: 0.8128\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4063 - acc: 0.8261 - val_loss: 0.4465 - val_acc: 0.8128\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4059 - acc: 0.8270 - val_loss: 0.4464 - val_acc: 0.8112\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4060 - acc: 0.8230 - val_loss: 0.4466 - val_acc: 0.8144\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4069 - acc: 0.8234 - val_loss: 0.4462 - val_acc: 0.8079\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4047 - acc: 0.8238 - val_loss: 0.4461 - val_acc: 0.8095\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4048 - acc: 0.8230 - val_loss: 0.4460 - val_acc: 0.8095\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4049 - acc: 0.8223 - val_loss: 0.4457 - val_acc: 0.8095\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4032 - acc: 0.8267 - val_loss: 0.4458 - val_acc: 0.8161\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4032 - acc: 0.8259 - val_loss: 0.4452 - val_acc: 0.8095\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4023 - acc: 0.8256 - val_loss: 0.4450 - val_acc: 0.8095\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4022 - acc: 0.8290 - val_loss: 0.4449 - val_acc: 0.8144\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4020 - acc: 0.8254 - val_loss: 0.4451 - val_acc: 0.8128\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4019 - acc: 0.8239 - val_loss: 0.4448 - val_acc: 0.8128\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4020 - acc: 0.8296 - val_loss: 0.4445 - val_acc: 0.8095\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4002 - acc: 0.8245 - val_loss: 0.4445 - val_acc: 0.8128\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3990 - acc: 0.8281 - val_loss: 0.4442 - val_acc: 0.8062\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3997 - acc: 0.8316 - val_loss: 0.4445 - val_acc: 0.8112\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3996 - acc: 0.8252 - val_loss: 0.4445 - val_acc: 0.8112\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3978 - acc: 0.8311 - val_loss: 0.4440 - val_acc: 0.8095\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3978 - acc: 0.8270 - val_loss: 0.4439 - val_acc: 0.8079\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3974 - acc: 0.8256 - val_loss: 0.4443 - val_acc: 0.8128\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3962 - acc: 0.8294 - val_loss: 0.4438 - val_acc: 0.8095\n",
      "Epoch 114/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3958 - acc: 0.8276 - val_loss: 0.4440 - val_acc: 0.8112\n",
      "Epoch 115/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3966 - acc: 0.8300 - val_loss: 0.4437 - val_acc: 0.8112\n",
      "Epoch 116/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3944 - acc: 0.8248 - val_loss: 0.4433 - val_acc: 0.8095\n",
      "Epoch 117/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3964 - acc: 0.8276 - val_loss: 0.4434 - val_acc: 0.8079\n",
      "Epoch 118/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3956 - acc: 0.8305 - val_loss: 0.4432 - val_acc: 0.8079\n",
      "Epoch 119/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3938 - acc: 0.8305 - val_loss: 0.4434 - val_acc: 0.8112\n",
      "Epoch 120/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3945 - acc: 0.8296 - val_loss: 0.4432 - val_acc: 0.8095\n",
      "Epoch 121/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3920 - acc: 0.8316 - val_loss: 0.4432 - val_acc: 0.8095\n",
      "Epoch 122/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3936 - acc: 0.8307 - val_loss: 0.4431 - val_acc: 0.8095\n",
      "Epoch 123/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3936 - acc: 0.8316 - val_loss: 0.4432 - val_acc: 0.8112\n",
      "Epoch 124/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3906 - acc: 0.8309 - val_loss: 0.4430 - val_acc: 0.8095\n",
      "Epoch 125/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3919 - acc: 0.8303 - val_loss: 0.4429 - val_acc: 0.8095\n",
      "Epoch 126/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3907 - acc: 0.8301 - val_loss: 0.4431 - val_acc: 0.8079\n",
      "Epoch 127/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3909 - acc: 0.8301 - val_loss: 0.4427 - val_acc: 0.8079\n",
      "Epoch 128/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3908 - acc: 0.8320 - val_loss: 0.4422 - val_acc: 0.8095\n",
      "Epoch 129/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3908 - acc: 0.8309 - val_loss: 0.4423 - val_acc: 0.8095\n",
      "Epoch 130/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3879 - acc: 0.8342 - val_loss: 0.4427 - val_acc: 0.8062\n",
      "Epoch 131/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3889 - acc: 0.8311 - val_loss: 0.4423 - val_acc: 0.8079\n",
      "Epoch 132/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3870 - acc: 0.8307 - val_loss: 0.4421 - val_acc: 0.8095\n",
      "Epoch 133/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3873 - acc: 0.8312 - val_loss: 0.4421 - val_acc: 0.8095\n",
      "Epoch 134/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3867 - acc: 0.8345 - val_loss: 0.4427 - val_acc: 0.8079\n",
      "Epoch 135/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3875 - acc: 0.8345 - val_loss: 0.4423 - val_acc: 0.8079\n",
      "Epoch 136/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3856 - acc: 0.8321 - val_loss: 0.4421 - val_acc: 0.8062\n",
      "Epoch 137/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3854 - acc: 0.8351 - val_loss: 0.4423 - val_acc: 0.8046\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.25, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_101\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_102 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_288 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_187 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_289 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_188 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_290 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.8103 - acc: 0.4302 - val_loss: 0.7058 - val_acc: 0.4548\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.7038 - acc: 0.5032 - val_loss: 0.6647 - val_acc: 0.5878\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6756 - acc: 0.5733 - val_loss: 0.6498 - val_acc: 0.5846\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6539 - acc: 0.5955 - val_loss: 0.6282 - val_acc: 0.5961\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6295 - acc: 0.6411 - val_loss: 0.6123 - val_acc: 0.7241\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6098 - acc: 0.6804 - val_loss: 0.6026 - val_acc: 0.6929\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6002 - acc: 0.6853 - val_loss: 0.5915 - val_acc: 0.6847\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5858 - acc: 0.7032 - val_loss: 0.5778 - val_acc: 0.7258\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5706 - acc: 0.7219 - val_loss: 0.5655 - val_acc: 0.7422\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5610 - acc: 0.7373 - val_loss: 0.5535 - val_acc: 0.7471\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.5431 - acc: 0.7449 - val_loss: 0.5419 - val_acc: 0.7504\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5334 - acc: 0.7533 - val_loss: 0.5307 - val_acc: 0.7504\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5230 - acc: 0.7612 - val_loss: 0.5204 - val_acc: 0.7586\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5155 - acc: 0.7626 - val_loss: 0.5122 - val_acc: 0.7553\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.5029 - acc: 0.7694 - val_loss: 0.5045 - val_acc: 0.7603\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4940 - acc: 0.7756 - val_loss: 0.4972 - val_acc: 0.7685\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4905 - acc: 0.7743 - val_loss: 0.4907 - val_acc: 0.7668\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4821 - acc: 0.7823 - val_loss: 0.4844 - val_acc: 0.7718\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.4772 - acc: 0.7860 - val_loss: 0.4789 - val_acc: 0.7718\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4696 - acc: 0.7924 - val_loss: 0.4747 - val_acc: 0.7750\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4639 - acc: 0.7988 - val_loss: 0.4703 - val_acc: 0.7833\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4603 - acc: 0.7988 - val_loss: 0.4667 - val_acc: 0.7833\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4564 - acc: 0.8000 - val_loss: 0.4635 - val_acc: 0.7816\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4543 - acc: 0.7991 - val_loss: 0.4606 - val_acc: 0.7833\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4450 - acc: 0.8033 - val_loss: 0.4583 - val_acc: 0.7865\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4436 - acc: 0.8053 - val_loss: 0.4560 - val_acc: 0.7898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4394 - acc: 0.8057 - val_loss: 0.4542 - val_acc: 0.7931\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4389 - acc: 0.8075 - val_loss: 0.4523 - val_acc: 0.7947\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4337 - acc: 0.8155 - val_loss: 0.4502 - val_acc: 0.7964\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.4317 - acc: 0.8121 - val_loss: 0.4487 - val_acc: 0.7997\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4289 - acc: 0.8123 - val_loss: 0.4482 - val_acc: 0.7947\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4253 - acc: 0.8221 - val_loss: 0.4474 - val_acc: 0.8046\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4246 - acc: 0.8144 - val_loss: 0.4468 - val_acc: 0.7964\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4186 - acc: 0.8203 - val_loss: 0.4457 - val_acc: 0.7964\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4184 - acc: 0.8203 - val_loss: 0.4442 - val_acc: 0.8013\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4126 - acc: 0.8241 - val_loss: 0.4436 - val_acc: 0.8013\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4144 - acc: 0.8217 - val_loss: 0.4430 - val_acc: 0.7980\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.4146 - acc: 0.8172 - val_loss: 0.4432 - val_acc: 0.8079\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4109 - acc: 0.8208 - val_loss: 0.4421 - val_acc: 0.7997\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4104 - acc: 0.8194 - val_loss: 0.4425 - val_acc: 0.8062\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.4083 - acc: 0.8223 - val_loss: 0.4409 - val_acc: 0.8062\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4072 - acc: 0.8228 - val_loss: 0.4390 - val_acc: 0.8013\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4049 - acc: 0.8247 - val_loss: 0.4418 - val_acc: 0.8095\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4044 - acc: 0.8267 - val_loss: 0.4391 - val_acc: 0.8112\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4013 - acc: 0.8274 - val_loss: 0.4392 - val_acc: 0.8095\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3973 - acc: 0.8316 - val_loss: 0.4406 - val_acc: 0.8161\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3949 - acc: 0.8307 - val_loss: 0.4372 - val_acc: 0.8112\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3955 - acc: 0.8278 - val_loss: 0.4364 - val_acc: 0.8095\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3937 - acc: 0.8283 - val_loss: 0.4371 - val_acc: 0.8112\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.3927 - acc: 0.8303 - val_loss: 0.4373 - val_acc: 0.8079\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3918 - acc: 0.8347 - val_loss: 0.4367 - val_acc: 0.8095\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3891 - acc: 0.8340 - val_loss: 0.4359 - val_acc: 0.8062\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3871 - acc: 0.8307 - val_loss: 0.4361 - val_acc: 0.8095\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3860 - acc: 0.8332 - val_loss: 0.4352 - val_acc: 0.8062\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3885 - acc: 0.8338 - val_loss: 0.4345 - val_acc: 0.8062\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3838 - acc: 0.8389 - val_loss: 0.4345 - val_acc: 0.8062\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3831 - acc: 0.8376 - val_loss: 0.4351 - val_acc: 0.8062\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3781 - acc: 0.8387 - val_loss: 0.4366 - val_acc: 0.8079\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3795 - acc: 0.8416 - val_loss: 0.4351 - val_acc: 0.8062\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.3761 - acc: 0.8387 - val_loss: 0.4360 - val_acc: 0.8062\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3765 - acc: 0.8378 - val_loss: 0.4343 - val_acc: 0.8046\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3711 - acc: 0.8429 - val_loss: 0.4354 - val_acc: 0.8062\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3737 - acc: 0.8442 - val_loss: 0.4350 - val_acc: 0.8062\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3741 - acc: 0.8400 - val_loss: 0.4355 - val_acc: 0.8046\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3713 - acc: 0.8415 - val_loss: 0.4356 - val_acc: 0.8046\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3654 - acc: 0.8455 - val_loss: 0.4355 - val_acc: 0.8062\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.25, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_102\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_103 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_291 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_189 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_292 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_190 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_293 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.8564 - acc: 0.4348 - val_loss: 0.7630 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.7572 - acc: 0.4583 - val_loss: 0.6991 - val_acc: 0.4713\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.7009 - acc: 0.5099 - val_loss: 0.6703 - val_acc: 0.6174\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6781 - acc: 0.5676 - val_loss: 0.6570 - val_acc: 0.6059\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6666 - acc: 0.5982 - val_loss: 0.6468 - val_acc: 0.6026\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6541 - acc: 0.6178 - val_loss: 0.6365 - val_acc: 0.6223\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6388 - acc: 0.6424 - val_loss: 0.6251 - val_acc: 0.7011\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6308 - acc: 0.6596 - val_loss: 0.6141 - val_acc: 0.7488\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6219 - acc: 0.6718 - val_loss: 0.6050 - val_acc: 0.7471\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6075 - acc: 0.6915 - val_loss: 0.5963 - val_acc: 0.7323\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6000 - acc: 0.6973 - val_loss: 0.5880 - val_acc: 0.7356\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5886 - acc: 0.7086 - val_loss: 0.5799 - val_acc: 0.7422\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5814 - acc: 0.7150 - val_loss: 0.5720 - val_acc: 0.7373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5735 - acc: 0.7183 - val_loss: 0.5638 - val_acc: 0.7488\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5648 - acc: 0.7283 - val_loss: 0.5555 - val_acc: 0.7504\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5566 - acc: 0.7329 - val_loss: 0.5478 - val_acc: 0.7521\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5535 - acc: 0.7347 - val_loss: 0.5406 - val_acc: 0.7570\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5467 - acc: 0.7446 - val_loss: 0.5337 - val_acc: 0.7652\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5388 - acc: 0.7466 - val_loss: 0.5271 - val_acc: 0.7652\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5321 - acc: 0.7546 - val_loss: 0.5212 - val_acc: 0.7652\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5253 - acc: 0.7555 - val_loss: 0.5158 - val_acc: 0.7668\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5173 - acc: 0.7603 - val_loss: 0.5108 - val_acc: 0.7734\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5095 - acc: 0.7656 - val_loss: 0.5065 - val_acc: 0.7718\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5130 - acc: 0.7639 - val_loss: 0.5016 - val_acc: 0.7718\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.5073 - acc: 0.7707 - val_loss: 0.4976 - val_acc: 0.7718\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4992 - acc: 0.7732 - val_loss: 0.4939 - val_acc: 0.7734\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4938 - acc: 0.7767 - val_loss: 0.4904 - val_acc: 0.7767\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4926 - acc: 0.7776 - val_loss: 0.4863 - val_acc: 0.7750\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4908 - acc: 0.7792 - val_loss: 0.4832 - val_acc: 0.7816\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4853 - acc: 0.7763 - val_loss: 0.4811 - val_acc: 0.7800\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4824 - acc: 0.7851 - val_loss: 0.4785 - val_acc: 0.7816\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4765 - acc: 0.7851 - val_loss: 0.4757 - val_acc: 0.7783\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4771 - acc: 0.7825 - val_loss: 0.4731 - val_acc: 0.7816\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4744 - acc: 0.7891 - val_loss: 0.4712 - val_acc: 0.7833\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4695 - acc: 0.7904 - val_loss: 0.4686 - val_acc: 0.7865\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4636 - acc: 0.7951 - val_loss: 0.4671 - val_acc: 0.7882\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4625 - acc: 0.7951 - val_loss: 0.4665 - val_acc: 0.7833\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4635 - acc: 0.7955 - val_loss: 0.4637 - val_acc: 0.7849\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4588 - acc: 0.7977 - val_loss: 0.4620 - val_acc: 0.7849\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4526 - acc: 0.8022 - val_loss: 0.4616 - val_acc: 0.7898\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4558 - acc: 0.7955 - val_loss: 0.4594 - val_acc: 0.7865\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4565 - acc: 0.8006 - val_loss: 0.4578 - val_acc: 0.7882\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4495 - acc: 0.8002 - val_loss: 0.4569 - val_acc: 0.7898\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4488 - acc: 0.8006 - val_loss: 0.4575 - val_acc: 0.7931\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4448 - acc: 0.8079 - val_loss: 0.4548 - val_acc: 0.7931\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4423 - acc: 0.8055 - val_loss: 0.4537 - val_acc: 0.7947\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4456 - acc: 0.8066 - val_loss: 0.4540 - val_acc: 0.7980\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4375 - acc: 0.8112 - val_loss: 0.4525 - val_acc: 0.7980\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4373 - acc: 0.8084 - val_loss: 0.4512 - val_acc: 0.7980\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4352 - acc: 0.8061 - val_loss: 0.4507 - val_acc: 0.7997\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4372 - acc: 0.8104 - val_loss: 0.4505 - val_acc: 0.8013\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4364 - acc: 0.8068 - val_loss: 0.4502 - val_acc: 0.7997\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4327 - acc: 0.8132 - val_loss: 0.4501 - val_acc: 0.7964\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4322 - acc: 0.8143 - val_loss: 0.4480 - val_acc: 0.8013\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4295 - acc: 0.8150 - val_loss: 0.4473 - val_acc: 0.7997\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4278 - acc: 0.8134 - val_loss: 0.4471 - val_acc: 0.7997\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4314 - acc: 0.8146 - val_loss: 0.4468 - val_acc: 0.7997\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4233 - acc: 0.8186 - val_loss: 0.4453 - val_acc: 0.8013\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4287 - acc: 0.8104 - val_loss: 0.4452 - val_acc: 0.8062\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4240 - acc: 0.8150 - val_loss: 0.4454 - val_acc: 0.7997\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4264 - acc: 0.8163 - val_loss: 0.4434 - val_acc: 0.8079\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4232 - acc: 0.8152 - val_loss: 0.4427 - val_acc: 0.8079\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4230 - acc: 0.8143 - val_loss: 0.4423 - val_acc: 0.8079\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4222 - acc: 0.8192 - val_loss: 0.4431 - val_acc: 0.8030\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4246 - acc: 0.8157 - val_loss: 0.4420 - val_acc: 0.8095\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4175 - acc: 0.8190 - val_loss: 0.4416 - val_acc: 0.8095\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4190 - acc: 0.8186 - val_loss: 0.4419 - val_acc: 0.8112\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4130 - acc: 0.8197 - val_loss: 0.4406 - val_acc: 0.8112\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4143 - acc: 0.8201 - val_loss: 0.4396 - val_acc: 0.8079\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4106 - acc: 0.8227 - val_loss: 0.4406 - val_acc: 0.8095\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4105 - acc: 0.8196 - val_loss: 0.4398 - val_acc: 0.8112\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4141 - acc: 0.8216 - val_loss: 0.4394 - val_acc: 0.8095\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4083 - acc: 0.8243 - val_loss: 0.4398 - val_acc: 0.8095\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4087 - acc: 0.8265 - val_loss: 0.4396 - val_acc: 0.8095\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4072 - acc: 0.8236 - val_loss: 0.4378 - val_acc: 0.8128\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4105 - acc: 0.8223 - val_loss: 0.4376 - val_acc: 0.8128\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4060 - acc: 0.8263 - val_loss: 0.4378 - val_acc: 0.8095\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4063 - acc: 0.8238 - val_loss: 0.4373 - val_acc: 0.8128\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4059 - acc: 0.8276 - val_loss: 0.4375 - val_acc: 0.8095\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4013 - acc: 0.8292 - val_loss: 0.4379 - val_acc: 0.8079\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4047 - acc: 0.8248 - val_loss: 0.4377 - val_acc: 0.8095\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4023 - acc: 0.8300 - val_loss: 0.4374 - val_acc: 0.8095\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4033 - acc: 0.8230 - val_loss: 0.4366 - val_acc: 0.8112\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4015 - acc: 0.8281 - val_loss: 0.4369 - val_acc: 0.8079\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3977 - acc: 0.8329 - val_loss: 0.4370 - val_acc: 0.8079\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3974 - acc: 0.8311 - val_loss: 0.4364 - val_acc: 0.8095\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3992 - acc: 0.8289 - val_loss: 0.4368 - val_acc: 0.8095\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3966 - acc: 0.8303 - val_loss: 0.4363 - val_acc: 0.8095\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3961 - acc: 0.8307 - val_loss: 0.4356 - val_acc: 0.8095\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3945 - acc: 0.8298 - val_loss: 0.4361 - val_acc: 0.8095\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3951 - acc: 0.8318 - val_loss: 0.4361 - val_acc: 0.8112\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3981 - acc: 0.8248 - val_loss: 0.4348 - val_acc: 0.8095\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3935 - acc: 0.8290 - val_loss: 0.4344 - val_acc: 0.8095\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3923 - acc: 0.8309 - val_loss: 0.4349 - val_acc: 0.8112\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3916 - acc: 0.8321 - val_loss: 0.4343 - val_acc: 0.8095\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3934 - acc: 0.8239 - val_loss: 0.4343 - val_acc: 0.8079\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3925 - acc: 0.8336 - val_loss: 0.4352 - val_acc: 0.8112\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3864 - acc: 0.8340 - val_loss: 0.4339 - val_acc: 0.8079\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3875 - acc: 0.8343 - val_loss: 0.4340 - val_acc: 0.8030\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3852 - acc: 0.8385 - val_loss: 0.4351 - val_acc: 0.8112\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3831 - acc: 0.8367 - val_loss: 0.4345 - val_acc: 0.8095\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3814 - acc: 0.8331 - val_loss: 0.4342 - val_acc: 0.8079\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3830 - acc: 0.8338 - val_loss: 0.4349 - val_acc: 0.8095\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.25, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_103\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_104 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_294 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_191 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_295 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_192 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_296 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.7581 - acc: 0.4441 - val_loss: 0.7140 - val_acc: 0.4204\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.7076 - acc: 0.4910 - val_loss: 0.6766 - val_acc: 0.6158\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6809 - acc: 0.5669 - val_loss: 0.6574 - val_acc: 0.6486\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6640 - acc: 0.5951 - val_loss: 0.6468 - val_acc: 0.5993\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6541 - acc: 0.6158 - val_loss: 0.6383 - val_acc: 0.5993\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6500 - acc: 0.6185 - val_loss: 0.6288 - val_acc: 0.6273\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6346 - acc: 0.6504 - val_loss: 0.6190 - val_acc: 0.6946\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6208 - acc: 0.6802 - val_loss: 0.6100 - val_acc: 0.7406\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6127 - acc: 0.6902 - val_loss: 0.6018 - val_acc: 0.7389\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6034 - acc: 0.7011 - val_loss: 0.5935 - val_acc: 0.7389\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5940 - acc: 0.7099 - val_loss: 0.5851 - val_acc: 0.7455\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5833 - acc: 0.7147 - val_loss: 0.5767 - val_acc: 0.7521\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5772 - acc: 0.7205 - val_loss: 0.5686 - val_acc: 0.7619\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5713 - acc: 0.7303 - val_loss: 0.5610 - val_acc: 0.7537\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5572 - acc: 0.7367 - val_loss: 0.5536 - val_acc: 0.7553\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5506 - acc: 0.7420 - val_loss: 0.5464 - val_acc: 0.7521\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5451 - acc: 0.7455 - val_loss: 0.5393 - val_acc: 0.7603\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5360 - acc: 0.7468 - val_loss: 0.5323 - val_acc: 0.7635\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5271 - acc: 0.7594 - val_loss: 0.5260 - val_acc: 0.7668\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5254 - acc: 0.7559 - val_loss: 0.5198 - val_acc: 0.7685\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5154 - acc: 0.7617 - val_loss: 0.5141 - val_acc: 0.7668\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5117 - acc: 0.7584 - val_loss: 0.5088 - val_acc: 0.7685\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5073 - acc: 0.7670 - val_loss: 0.5043 - val_acc: 0.7652\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5033 - acc: 0.7692 - val_loss: 0.5000 - val_acc: 0.7668\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4958 - acc: 0.7758 - val_loss: 0.4963 - val_acc: 0.7701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4912 - acc: 0.7749 - val_loss: 0.4928 - val_acc: 0.7718\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4890 - acc: 0.7794 - val_loss: 0.4902 - val_acc: 0.7750\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4817 - acc: 0.7831 - val_loss: 0.4858 - val_acc: 0.7734\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4804 - acc: 0.7854 - val_loss: 0.4829 - val_acc: 0.7750\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4779 - acc: 0.7896 - val_loss: 0.4807 - val_acc: 0.7750\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4697 - acc: 0.7895 - val_loss: 0.4786 - val_acc: 0.7816\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4671 - acc: 0.7955 - val_loss: 0.4751 - val_acc: 0.7734\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4680 - acc: 0.7902 - val_loss: 0.4731 - val_acc: 0.7734\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4650 - acc: 0.7862 - val_loss: 0.4709 - val_acc: 0.7767\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4626 - acc: 0.7947 - val_loss: 0.4698 - val_acc: 0.7833\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4614 - acc: 0.7966 - val_loss: 0.4674 - val_acc: 0.7816\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4592 - acc: 0.7931 - val_loss: 0.4661 - val_acc: 0.7849\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4534 - acc: 0.7980 - val_loss: 0.4649 - val_acc: 0.7882\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4535 - acc: 0.8031 - val_loss: 0.4630 - val_acc: 0.7882\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4514 - acc: 0.8002 - val_loss: 0.4614 - val_acc: 0.7898\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4490 - acc: 0.7977 - val_loss: 0.4602 - val_acc: 0.7898\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4474 - acc: 0.8022 - val_loss: 0.4595 - val_acc: 0.7931\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4473 - acc: 0.8020 - val_loss: 0.4578 - val_acc: 0.7931\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4428 - acc: 0.8042 - val_loss: 0.4574 - val_acc: 0.7931\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4451 - acc: 0.8050 - val_loss: 0.4575 - val_acc: 0.7947\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4408 - acc: 0.8062 - val_loss: 0.4557 - val_acc: 0.7980\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4372 - acc: 0.8081 - val_loss: 0.4544 - val_acc: 0.7947\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4388 - acc: 0.8057 - val_loss: 0.4537 - val_acc: 0.7997\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4356 - acc: 0.8095 - val_loss: 0.4527 - val_acc: 0.8013\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4302 - acc: 0.8135 - val_loss: 0.4517 - val_acc: 0.8030\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4368 - acc: 0.8106 - val_loss: 0.4513 - val_acc: 0.8013\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4322 - acc: 0.8141 - val_loss: 0.4505 - val_acc: 0.8046\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4302 - acc: 0.8124 - val_loss: 0.4496 - val_acc: 0.8095\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4282 - acc: 0.8130 - val_loss: 0.4491 - val_acc: 0.8095\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4267 - acc: 0.8134 - val_loss: 0.4491 - val_acc: 0.8030\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4246 - acc: 0.8144 - val_loss: 0.4482 - val_acc: 0.8079\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4248 - acc: 0.8139 - val_loss: 0.4474 - val_acc: 0.8079\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4203 - acc: 0.8168 - val_loss: 0.4471 - val_acc: 0.8095\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4226 - acc: 0.8163 - val_loss: 0.4470 - val_acc: 0.8095\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4225 - acc: 0.8181 - val_loss: 0.4464 - val_acc: 0.8112\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4204 - acc: 0.8154 - val_loss: 0.4456 - val_acc: 0.8095\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4174 - acc: 0.8219 - val_loss: 0.4457 - val_acc: 0.8144\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4152 - acc: 0.8194 - val_loss: 0.4457 - val_acc: 0.8095\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4157 - acc: 0.8205 - val_loss: 0.4461 - val_acc: 0.8144\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4156 - acc: 0.8227 - val_loss: 0.4447 - val_acc: 0.8112\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4126 - acc: 0.8221 - val_loss: 0.4439 - val_acc: 0.8128\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4124 - acc: 0.8188 - val_loss: 0.4439 - val_acc: 0.8161\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4119 - acc: 0.8181 - val_loss: 0.4428 - val_acc: 0.8112\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4137 - acc: 0.8221 - val_loss: 0.4426 - val_acc: 0.8194\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4092 - acc: 0.8221 - val_loss: 0.4428 - val_acc: 0.8161\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4059 - acc: 0.8236 - val_loss: 0.4419 - val_acc: 0.8177\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4094 - acc: 0.8210 - val_loss: 0.4412 - val_acc: 0.8177\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4075 - acc: 0.8248 - val_loss: 0.4416 - val_acc: 0.8177\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4076 - acc: 0.8241 - val_loss: 0.4409 - val_acc: 0.8144\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4039 - acc: 0.8245 - val_loss: 0.4406 - val_acc: 0.8161\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4067 - acc: 0.8267 - val_loss: 0.4420 - val_acc: 0.8144\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3990 - acc: 0.8256 - val_loss: 0.4397 - val_acc: 0.8095\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4017 - acc: 0.8250 - val_loss: 0.4401 - val_acc: 0.8128\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4013 - acc: 0.8234 - val_loss: 0.4402 - val_acc: 0.8161\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3987 - acc: 0.8287 - val_loss: 0.4401 - val_acc: 0.8177\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4000 - acc: 0.8248 - val_loss: 0.4397 - val_acc: 0.8177\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3986 - acc: 0.8274 - val_loss: 0.4398 - val_acc: 0.8161\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3948 - acc: 0.8307 - val_loss: 0.4390 - val_acc: 0.8177\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3952 - acc: 0.8283 - val_loss: 0.4393 - val_acc: 0.8161\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3947 - acc: 0.8309 - val_loss: 0.4395 - val_acc: 0.8144\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3944 - acc: 0.8352 - val_loss: 0.4383 - val_acc: 0.8161\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3939 - acc: 0.8281 - val_loss: 0.4377 - val_acc: 0.8095\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3924 - acc: 0.8334 - val_loss: 0.4398 - val_acc: 0.8177\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3939 - acc: 0.8285 - val_loss: 0.4374 - val_acc: 0.8128\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3906 - acc: 0.8296 - val_loss: 0.4370 - val_acc: 0.8144\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3891 - acc: 0.8247 - val_loss: 0.4374 - val_acc: 0.8177\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3869 - acc: 0.8325 - val_loss: 0.4367 - val_acc: 0.8112\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3856 - acc: 0.8336 - val_loss: 0.4378 - val_acc: 0.8177\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3882 - acc: 0.8318 - val_loss: 0.4375 - val_acc: 0.8177\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3839 - acc: 0.8342 - val_loss: 0.4374 - val_acc: 0.8144\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3849 - acc: 0.8320 - val_loss: 0.4373 - val_acc: 0.8161\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3837 - acc: 0.8323 - val_loss: 0.4376 - val_acc: 0.8144\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.25, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_104\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_105 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_297 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_193 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_298 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_194 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_299 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.8339 - acc: 0.4320 - val_loss: 0.7419 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.7322 - acc: 0.4764 - val_loss: 0.6821 - val_acc: 0.5977\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6822 - acc: 0.5665 - val_loss: 0.6588 - val_acc: 0.5878\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6663 - acc: 0.6070 - val_loss: 0.6472 - val_acc: 0.5862\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6533 - acc: 0.6145 - val_loss: 0.6341 - val_acc: 0.5911\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6393 - acc: 0.6391 - val_loss: 0.6204 - val_acc: 0.6502\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6204 - acc: 0.6723 - val_loss: 0.6093 - val_acc: 0.7373\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6130 - acc: 0.6809 - val_loss: 0.6001 - val_acc: 0.7274\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.6023 - acc: 0.6829 - val_loss: 0.5913 - val_acc: 0.7192\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.5907 - acc: 0.6886 - val_loss: 0.5819 - val_acc: 0.7340\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5791 - acc: 0.7061 - val_loss: 0.5725 - val_acc: 0.7455\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5641 - acc: 0.7205 - val_loss: 0.5632 - val_acc: 0.7488\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5579 - acc: 0.7282 - val_loss: 0.5536 - val_acc: 0.7438\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5469 - acc: 0.7356 - val_loss: 0.5442 - val_acc: 0.7553\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.5364 - acc: 0.7438 - val_loss: 0.5345 - val_acc: 0.7553\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5321 - acc: 0.7444 - val_loss: 0.5256 - val_acc: 0.7685\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5217 - acc: 0.7564 - val_loss: 0.5170 - val_acc: 0.7734\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.5114 - acc: 0.7652 - val_loss: 0.5094 - val_acc: 0.7767\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5043 - acc: 0.7661 - val_loss: 0.5032 - val_acc: 0.7783\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4960 - acc: 0.7712 - val_loss: 0.4964 - val_acc: 0.7767\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4885 - acc: 0.7739 - val_loss: 0.4909 - val_acc: 0.7783\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4815 - acc: 0.7809 - val_loss: 0.4861 - val_acc: 0.7849\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4771 - acc: 0.7874 - val_loss: 0.4816 - val_acc: 0.7865\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4758 - acc: 0.7909 - val_loss: 0.4807 - val_acc: 0.7865\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4721 - acc: 0.7871 - val_loss: 0.4756 - val_acc: 0.7865\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4672 - acc: 0.7885 - val_loss: 0.4716 - val_acc: 0.7849\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4664 - acc: 0.7898 - val_loss: 0.4708 - val_acc: 0.7849\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4537 - acc: 0.7962 - val_loss: 0.4664 - val_acc: 0.7849\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4581 - acc: 0.7951 - val_loss: 0.4657 - val_acc: 0.7833\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4488 - acc: 0.8037 - val_loss: 0.4639 - val_acc: 0.7849\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4497 - acc: 0.7984 - val_loss: 0.4614 - val_acc: 0.7882\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4426 - acc: 0.8051 - val_loss: 0.4600 - val_acc: 0.7915\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4430 - acc: 0.8073 - val_loss: 0.4583 - val_acc: 0.7931\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4430 - acc: 0.8081 - val_loss: 0.4569 - val_acc: 0.7947\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4387 - acc: 0.8101 - val_loss: 0.4548 - val_acc: 0.7947\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4393 - acc: 0.8055 - val_loss: 0.4559 - val_acc: 0.8013\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4347 - acc: 0.8082 - val_loss: 0.4515 - val_acc: 0.7931\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4303 - acc: 0.8104 - val_loss: 0.4521 - val_acc: 0.8030\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4306 - acc: 0.8113 - val_loss: 0.4517 - val_acc: 0.8046\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4276 - acc: 0.8130 - val_loss: 0.4492 - val_acc: 0.7980\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4272 - acc: 0.8119 - val_loss: 0.4496 - val_acc: 0.8095\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4249 - acc: 0.8134 - val_loss: 0.4476 - val_acc: 0.8062\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4202 - acc: 0.8190 - val_loss: 0.4472 - val_acc: 0.8079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4210 - acc: 0.8177 - val_loss: 0.4461 - val_acc: 0.8062\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4208 - acc: 0.8161 - val_loss: 0.4457 - val_acc: 0.8095\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4183 - acc: 0.8166 - val_loss: 0.4450 - val_acc: 0.8112\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4185 - acc: 0.8134 - val_loss: 0.4446 - val_acc: 0.8112\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4146 - acc: 0.8188 - val_loss: 0.4438 - val_acc: 0.8046\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4109 - acc: 0.8199 - val_loss: 0.4419 - val_acc: 0.8128\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4116 - acc: 0.8163 - val_loss: 0.4427 - val_acc: 0.8095\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4110 - acc: 0.8212 - val_loss: 0.4430 - val_acc: 0.8062\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4050 - acc: 0.8221 - val_loss: 0.4425 - val_acc: 0.8079\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4054 - acc: 0.8241 - val_loss: 0.4417 - val_acc: 0.8144\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4073 - acc: 0.8214 - val_loss: 0.4407 - val_acc: 0.8161\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4038 - acc: 0.8217 - val_loss: 0.4392 - val_acc: 0.8112\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4036 - acc: 0.8199 - val_loss: 0.4393 - val_acc: 0.8112\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4048 - acc: 0.8217 - val_loss: 0.4390 - val_acc: 0.8128\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3990 - acc: 0.8205 - val_loss: 0.4384 - val_acc: 0.8128\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3990 - acc: 0.8269 - val_loss: 0.4364 - val_acc: 0.8046\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3975 - acc: 0.8248 - val_loss: 0.4377 - val_acc: 0.8062\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3970 - acc: 0.8270 - val_loss: 0.4398 - val_acc: 0.8079\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3943 - acc: 0.8303 - val_loss: 0.4365 - val_acc: 0.8095\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3956 - acc: 0.8285 - val_loss: 0.4372 - val_acc: 0.8112\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3919 - acc: 0.8283 - val_loss: 0.4349 - val_acc: 0.8079\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3901 - acc: 0.8280 - val_loss: 0.4370 - val_acc: 0.8079\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3909 - acc: 0.8285 - val_loss: 0.4365 - val_acc: 0.8079\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3871 - acc: 0.8316 - val_loss: 0.4372 - val_acc: 0.8046\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3861 - acc: 0.8307 - val_loss: 0.4386 - val_acc: 0.8062\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3853 - acc: 0.8340 - val_loss: 0.4345 - val_acc: 0.8013\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3833 - acc: 0.8323 - val_loss: 0.4376 - val_acc: 0.8062\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3834 - acc: 0.8363 - val_loss: 0.4365 - val_acc: 0.8062\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3849 - acc: 0.8316 - val_loss: 0.4347 - val_acc: 0.8030\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3788 - acc: 0.8349 - val_loss: 0.4399 - val_acc: 0.8079\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3783 - acc: 0.8371 - val_loss: 0.4331 - val_acc: 0.8046\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3789 - acc: 0.8376 - val_loss: 0.4356 - val_acc: 0.8079\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3774 - acc: 0.8349 - val_loss: 0.4342 - val_acc: 0.8095\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3747 - acc: 0.8380 - val_loss: 0.4346 - val_acc: 0.8046\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3722 - acc: 0.8398 - val_loss: 0.4342 - val_acc: 0.8062\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.3687 - acc: 0.8400 - val_loss: 0.4344 - val_acc: 0.8062\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.3, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_105\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_106 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_300 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_195 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_301 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_196 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_302 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 1s 39ms/step - loss: 0.7542 - acc: 0.4506 - val_loss: 0.6811 - val_acc: 0.5616\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6788 - acc: 0.5570 - val_loss: 0.6621 - val_acc: 0.5846\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6622 - acc: 0.5862 - val_loss: 0.6418 - val_acc: 0.5846\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.6340 - acc: 0.6280 - val_loss: 0.6188 - val_acc: 0.7094\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.6131 - acc: 0.6869 - val_loss: 0.6056 - val_acc: 0.6929\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.5992 - acc: 0.6909 - val_loss: 0.5917 - val_acc: 0.6995\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.5829 - acc: 0.7052 - val_loss: 0.5763 - val_acc: 0.7340\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.5627 - acc: 0.7285 - val_loss: 0.5639 - val_acc: 0.7488\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.5515 - acc: 0.7413 - val_loss: 0.5504 - val_acc: 0.7488\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.5366 - acc: 0.7490 - val_loss: 0.5380 - val_acc: 0.7455\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.5233 - acc: 0.7583 - val_loss: 0.5265 - val_acc: 0.7504\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.5144 - acc: 0.7606 - val_loss: 0.5162 - val_acc: 0.7570\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.5025 - acc: 0.7707 - val_loss: 0.5078 - val_acc: 0.7603\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4952 - acc: 0.7723 - val_loss: 0.4999 - val_acc: 0.7635\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4841 - acc: 0.7794 - val_loss: 0.4934 - val_acc: 0.7668\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4817 - acc: 0.7843 - val_loss: 0.4863 - val_acc: 0.7668\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4739 - acc: 0.7847 - val_loss: 0.4807 - val_acc: 0.7750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4644 - acc: 0.7905 - val_loss: 0.4787 - val_acc: 0.7849\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4607 - acc: 0.7924 - val_loss: 0.4714 - val_acc: 0.7718\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4535 - acc: 0.7953 - val_loss: 0.4706 - val_acc: 0.7816\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4521 - acc: 0.7953 - val_loss: 0.4651 - val_acc: 0.7865\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4477 - acc: 0.8009 - val_loss: 0.4625 - val_acc: 0.7849\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4469 - acc: 0.8033 - val_loss: 0.4625 - val_acc: 0.7898\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4420 - acc: 0.8037 - val_loss: 0.4578 - val_acc: 0.7898\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4360 - acc: 0.8077 - val_loss: 0.4566 - val_acc: 0.7915\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4338 - acc: 0.8112 - val_loss: 0.4570 - val_acc: 0.7947\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.4344 - acc: 0.8121 - val_loss: 0.4535 - val_acc: 0.7964\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4278 - acc: 0.8115 - val_loss: 0.4524 - val_acc: 0.7964\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4254 - acc: 0.8115 - val_loss: 0.4522 - val_acc: 0.7964\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4199 - acc: 0.8150 - val_loss: 0.4510 - val_acc: 0.7980\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.4247 - acc: 0.8132 - val_loss: 0.4498 - val_acc: 0.7931\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4186 - acc: 0.8194 - val_loss: 0.4497 - val_acc: 0.7997\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4160 - acc: 0.8190 - val_loss: 0.4465 - val_acc: 0.8030\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.4162 - acc: 0.8219 - val_loss: 0.4462 - val_acc: 0.8030\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4114 - acc: 0.8217 - val_loss: 0.4481 - val_acc: 0.8046\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4094 - acc: 0.8225 - val_loss: 0.4455 - val_acc: 0.8079\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4070 - acc: 0.8243 - val_loss: 0.4430 - val_acc: 0.8046\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4077 - acc: 0.8227 - val_loss: 0.4444 - val_acc: 0.8095\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4012 - acc: 0.8259 - val_loss: 0.4430 - val_acc: 0.8046\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4021 - acc: 0.8234 - val_loss: 0.4435 - val_acc: 0.8079\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3997 - acc: 0.8258 - val_loss: 0.4424 - val_acc: 0.8079\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3989 - acc: 0.8292 - val_loss: 0.4425 - val_acc: 0.8062\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3970 - acc: 0.8289 - val_loss: 0.4405 - val_acc: 0.8079\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3953 - acc: 0.8287 - val_loss: 0.4416 - val_acc: 0.8062\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3932 - acc: 0.8280 - val_loss: 0.4397 - val_acc: 0.8079\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3914 - acc: 0.8285 - val_loss: 0.4377 - val_acc: 0.8062\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.3893 - acc: 0.8296 - val_loss: 0.4415 - val_acc: 0.8079\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3890 - acc: 0.8340 - val_loss: 0.4391 - val_acc: 0.8046\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3856 - acc: 0.8325 - val_loss: 0.4407 - val_acc: 0.8062\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.3828 - acc: 0.8314 - val_loss: 0.4372 - val_acc: 0.8079\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.3859 - acc: 0.8336 - val_loss: 0.4374 - val_acc: 0.8046\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3786 - acc: 0.8373 - val_loss: 0.4376 - val_acc: 0.8079\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.3809 - acc: 0.8373 - val_loss: 0.4398 - val_acc: 0.8079\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.3831 - acc: 0.8404 - val_loss: 0.4392 - val_acc: 0.8046\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3772 - acc: 0.8387 - val_loss: 0.4378 - val_acc: 0.8013\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.3, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_106\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_107 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_303 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_197 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_304 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_198 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_305 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.8305 - acc: 0.4443 - val_loss: 0.7421 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7410 - acc: 0.4735 - val_loss: 0.6828 - val_acc: 0.5846\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6898 - acc: 0.5514 - val_loss: 0.6605 - val_acc: 0.5862\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6747 - acc: 0.5931 - val_loss: 0.6497 - val_acc: 0.5862\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6662 - acc: 0.6061 - val_loss: 0.6380 - val_acc: 0.5878\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6451 - acc: 0.6300 - val_loss: 0.6250 - val_acc: 0.6273\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6277 - acc: 0.6546 - val_loss: 0.6139 - val_acc: 0.7159\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.6180 - acc: 0.6723 - val_loss: 0.6049 - val_acc: 0.7356\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.6080 - acc: 0.6683 - val_loss: 0.5965 - val_acc: 0.7291\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5966 - acc: 0.6853 - val_loss: 0.5880 - val_acc: 0.7356\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5864 - acc: 0.6944 - val_loss: 0.5795 - val_acc: 0.7438\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5752 - acc: 0.7101 - val_loss: 0.5708 - val_acc: 0.7422\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5707 - acc: 0.7148 - val_loss: 0.5619 - val_acc: 0.7504\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5575 - acc: 0.7269 - val_loss: 0.5529 - val_acc: 0.7537\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.5488 - acc: 0.7382 - val_loss: 0.5436 - val_acc: 0.7570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5396 - acc: 0.7449 - val_loss: 0.5346 - val_acc: 0.7619\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5335 - acc: 0.7513 - val_loss: 0.5260 - val_acc: 0.7701\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5204 - acc: 0.7530 - val_loss: 0.5182 - val_acc: 0.7800\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.5139 - acc: 0.7621 - val_loss: 0.5110 - val_acc: 0.7783\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5062 - acc: 0.7685 - val_loss: 0.5048 - val_acc: 0.7783\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4997 - acc: 0.7698 - val_loss: 0.4997 - val_acc: 0.7734\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4901 - acc: 0.7743 - val_loss: 0.4937 - val_acc: 0.7767\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4892 - acc: 0.7739 - val_loss: 0.4891 - val_acc: 0.7800\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4821 - acc: 0.7825 - val_loss: 0.4856 - val_acc: 0.7816\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4775 - acc: 0.7845 - val_loss: 0.4803 - val_acc: 0.7833\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4735 - acc: 0.7809 - val_loss: 0.4764 - val_acc: 0.7833\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4699 - acc: 0.7904 - val_loss: 0.4743 - val_acc: 0.7833\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4671 - acc: 0.7900 - val_loss: 0.4704 - val_acc: 0.7849\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4607 - acc: 0.7946 - val_loss: 0.4692 - val_acc: 0.7833\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4599 - acc: 0.7958 - val_loss: 0.4659 - val_acc: 0.7865\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4574 - acc: 0.7927 - val_loss: 0.4644 - val_acc: 0.7882\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4545 - acc: 0.7935 - val_loss: 0.4630 - val_acc: 0.7898\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4490 - acc: 0.8030 - val_loss: 0.4618 - val_acc: 0.7898\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4482 - acc: 0.8039 - val_loss: 0.4578 - val_acc: 0.7882\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4436 - acc: 0.8044 - val_loss: 0.4565 - val_acc: 0.7898\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4450 - acc: 0.8009 - val_loss: 0.4591 - val_acc: 0.7980\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4367 - acc: 0.8072 - val_loss: 0.4541 - val_acc: 0.7915\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4369 - acc: 0.8070 - val_loss: 0.4531 - val_acc: 0.7964\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4355 - acc: 0.8104 - val_loss: 0.4536 - val_acc: 0.8030\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4325 - acc: 0.8103 - val_loss: 0.4519 - val_acc: 0.8030\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4341 - acc: 0.8099 - val_loss: 0.4490 - val_acc: 0.8013\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4303 - acc: 0.8113 - val_loss: 0.4503 - val_acc: 0.8030\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4283 - acc: 0.8110 - val_loss: 0.4491 - val_acc: 0.8030\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4259 - acc: 0.8124 - val_loss: 0.4476 - val_acc: 0.7997\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4252 - acc: 0.8128 - val_loss: 0.4489 - val_acc: 0.8046\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4242 - acc: 0.8141 - val_loss: 0.4463 - val_acc: 0.8013\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4190 - acc: 0.8181 - val_loss: 0.4457 - val_acc: 0.8062\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4194 - acc: 0.8128 - val_loss: 0.4454 - val_acc: 0.8062\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4157 - acc: 0.8176 - val_loss: 0.4452 - val_acc: 0.8062\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4149 - acc: 0.8154 - val_loss: 0.4443 - val_acc: 0.8062\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4135 - acc: 0.8179 - val_loss: 0.4455 - val_acc: 0.8062\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4118 - acc: 0.8186 - val_loss: 0.4423 - val_acc: 0.8079\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4125 - acc: 0.8203 - val_loss: 0.4421 - val_acc: 0.8079\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4102 - acc: 0.8221 - val_loss: 0.4410 - val_acc: 0.8095\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4098 - acc: 0.8212 - val_loss: 0.4401 - val_acc: 0.8095\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4073 - acc: 0.8234 - val_loss: 0.4449 - val_acc: 0.8062\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4086 - acc: 0.8214 - val_loss: 0.4396 - val_acc: 0.8112\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4048 - acc: 0.8247 - val_loss: 0.4399 - val_acc: 0.8095\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4063 - acc: 0.8203 - val_loss: 0.4391 - val_acc: 0.8079\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4031 - acc: 0.8230 - val_loss: 0.4375 - val_acc: 0.8079\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3972 - acc: 0.8283 - val_loss: 0.4399 - val_acc: 0.8079\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3993 - acc: 0.8256 - val_loss: 0.4392 - val_acc: 0.8079\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3994 - acc: 0.8239 - val_loss: 0.4382 - val_acc: 0.8112\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3990 - acc: 0.8259 - val_loss: 0.4401 - val_acc: 0.8079\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3973 - acc: 0.8270 - val_loss: 0.4368 - val_acc: 0.8128\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3942 - acc: 0.8334 - val_loss: 0.4371 - val_acc: 0.8095\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3940 - acc: 0.8309 - val_loss: 0.4380 - val_acc: 0.8062\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3927 - acc: 0.8280 - val_loss: 0.4369 - val_acc: 0.8062\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3935 - acc: 0.8285 - val_loss: 0.4369 - val_acc: 0.8046\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3910 - acc: 0.8296 - val_loss: 0.4338 - val_acc: 0.8062\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3903 - acc: 0.8316 - val_loss: 0.4380 - val_acc: 0.8062\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3885 - acc: 0.8329 - val_loss: 0.4360 - val_acc: 0.8046\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3836 - acc: 0.8323 - val_loss: 0.4365 - val_acc: 0.8046\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3844 - acc: 0.8363 - val_loss: 0.4379 - val_acc: 0.8062\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3826 - acc: 0.8331 - val_loss: 0.4340 - val_acc: 0.8030\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.3, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_107\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_108 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_306 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_199 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_307 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.6707 - acc: 0.5847 - val_loss: 0.6462 - val_acc: 0.6700\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6480 - acc: 0.6238 - val_loss: 0.6305 - val_acc: 0.6683\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6346 - acc: 0.6347 - val_loss: 0.6187 - val_acc: 0.7209\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6202 - acc: 0.6639 - val_loss: 0.6086 - val_acc: 0.7044\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6090 - acc: 0.6763 - val_loss: 0.6001 - val_acc: 0.7044\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5968 - acc: 0.7022 - val_loss: 0.5917 - val_acc: 0.7110\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5858 - acc: 0.7134 - val_loss: 0.5828 - val_acc: 0.7258\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5777 - acc: 0.7134 - val_loss: 0.5748 - val_acc: 0.7422\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5702 - acc: 0.7285 - val_loss: 0.5677 - val_acc: 0.7455\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5617 - acc: 0.7380 - val_loss: 0.5615 - val_acc: 0.7422\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5535 - acc: 0.7406 - val_loss: 0.5549 - val_acc: 0.7488\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5462 - acc: 0.7475 - val_loss: 0.5487 - val_acc: 0.7488\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5415 - acc: 0.7522 - val_loss: 0.5430 - val_acc: 0.7586\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5356 - acc: 0.7559 - val_loss: 0.5378 - val_acc: 0.7603\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5268 - acc: 0.7610 - val_loss: 0.5332 - val_acc: 0.7553\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5263 - acc: 0.7579 - val_loss: 0.5286 - val_acc: 0.7570\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5188 - acc: 0.7615 - val_loss: 0.5243 - val_acc: 0.7586\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5158 - acc: 0.7668 - val_loss: 0.5203 - val_acc: 0.7619\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5076 - acc: 0.7663 - val_loss: 0.5166 - val_acc: 0.7619\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5026 - acc: 0.7760 - val_loss: 0.5131 - val_acc: 0.7603\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5022 - acc: 0.7747 - val_loss: 0.5096 - val_acc: 0.7603\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4984 - acc: 0.7754 - val_loss: 0.5062 - val_acc: 0.7652\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4944 - acc: 0.7756 - val_loss: 0.5031 - val_acc: 0.7635\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4895 - acc: 0.7827 - val_loss: 0.5002 - val_acc: 0.7668\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4872 - acc: 0.7854 - val_loss: 0.4976 - val_acc: 0.7685\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4829 - acc: 0.7889 - val_loss: 0.4950 - val_acc: 0.7685\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4819 - acc: 0.7838 - val_loss: 0.4927 - val_acc: 0.7701\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4811 - acc: 0.7871 - val_loss: 0.4906 - val_acc: 0.7668\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4784 - acc: 0.7878 - val_loss: 0.4885 - val_acc: 0.7701\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4741 - acc: 0.7927 - val_loss: 0.4865 - val_acc: 0.7734\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4710 - acc: 0.7962 - val_loss: 0.4846 - val_acc: 0.7718\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4698 - acc: 0.7924 - val_loss: 0.4830 - val_acc: 0.7734\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4695 - acc: 0.7940 - val_loss: 0.4814 - val_acc: 0.7767\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4650 - acc: 0.7962 - val_loss: 0.4799 - val_acc: 0.7750\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4643 - acc: 0.7951 - val_loss: 0.4789 - val_acc: 0.7800\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4614 - acc: 0.7955 - val_loss: 0.4774 - val_acc: 0.7767\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4609 - acc: 0.7942 - val_loss: 0.4761 - val_acc: 0.7783\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4573 - acc: 0.7988 - val_loss: 0.4747 - val_acc: 0.7783\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4569 - acc: 0.7971 - val_loss: 0.4734 - val_acc: 0.7783\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4538 - acc: 0.8011 - val_loss: 0.4723 - val_acc: 0.7833\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.4545 - acc: 0.8004 - val_loss: 0.4714 - val_acc: 0.7833\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4510 - acc: 0.8013 - val_loss: 0.4703 - val_acc: 0.7865\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4518 - acc: 0.8026 - val_loss: 0.4694 - val_acc: 0.7865\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4524 - acc: 0.7999 - val_loss: 0.4685 - val_acc: 0.7898\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4475 - acc: 0.8020 - val_loss: 0.4674 - val_acc: 0.7882\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4472 - acc: 0.8033 - val_loss: 0.4663 - val_acc: 0.7865\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4448 - acc: 0.8086 - val_loss: 0.4657 - val_acc: 0.7915\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4453 - acc: 0.8041 - val_loss: 0.4651 - val_acc: 0.7931\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4446 - acc: 0.8055 - val_loss: 0.4641 - val_acc: 0.7898\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4453 - acc: 0.8053 - val_loss: 0.4633 - val_acc: 0.7931\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4391 - acc: 0.8092 - val_loss: 0.4627 - val_acc: 0.7947\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4377 - acc: 0.8104 - val_loss: 0.4620 - val_acc: 0.7931\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4368 - acc: 0.8099 - val_loss: 0.4615 - val_acc: 0.7947\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4356 - acc: 0.8092 - val_loss: 0.4610 - val_acc: 0.7980\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4350 - acc: 0.8073 - val_loss: 0.4601 - val_acc: 0.7947\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4358 - acc: 0.8106 - val_loss: 0.4595 - val_acc: 0.7964\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4331 - acc: 0.8134 - val_loss: 0.4591 - val_acc: 0.7980\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4312 - acc: 0.8135 - val_loss: 0.4581 - val_acc: 0.7964\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4313 - acc: 0.8128 - val_loss: 0.4576 - val_acc: 0.7980\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4322 - acc: 0.8115 - val_loss: 0.4572 - val_acc: 0.7980\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4319 - acc: 0.8132 - val_loss: 0.4566 - val_acc: 0.7964\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4280 - acc: 0.8150 - val_loss: 0.4560 - val_acc: 0.7964\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4300 - acc: 0.8150 - val_loss: 0.4557 - val_acc: 0.7964\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4293 - acc: 0.8165 - val_loss: 0.4558 - val_acc: 0.8030\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4295 - acc: 0.8155 - val_loss: 0.4553 - val_acc: 0.7964\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4301 - acc: 0.8152 - val_loss: 0.4549 - val_acc: 0.7980\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4253 - acc: 0.8146 - val_loss: 0.4543 - val_acc: 0.7964\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4250 - acc: 0.8194 - val_loss: 0.4538 - val_acc: 0.8046\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4193 - acc: 0.8212 - val_loss: 0.4533 - val_acc: 0.8030\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4226 - acc: 0.8166 - val_loss: 0.4530 - val_acc: 0.8030\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4198 - acc: 0.8170 - val_loss: 0.4530 - val_acc: 0.8013\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4208 - acc: 0.8157 - val_loss: 0.4528 - val_acc: 0.8030\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4186 - acc: 0.8165 - val_loss: 0.4524 - val_acc: 0.8013\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4215 - acc: 0.8155 - val_loss: 0.4520 - val_acc: 0.8030\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4221 - acc: 0.8183 - val_loss: 0.4520 - val_acc: 0.8079\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4162 - acc: 0.8203 - val_loss: 0.4511 - val_acc: 0.8046\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4199 - acc: 0.8194 - val_loss: 0.4506 - val_acc: 0.8046\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4174 - acc: 0.8201 - val_loss: 0.4504 - val_acc: 0.8046\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4190 - acc: 0.8188 - val_loss: 0.4508 - val_acc: 0.8095\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4185 - acc: 0.8214 - val_loss: 0.4498 - val_acc: 0.8079\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4159 - acc: 0.8154 - val_loss: 0.4495 - val_acc: 0.8030\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4140 - acc: 0.8208 - val_loss: 0.4494 - val_acc: 0.8046\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4166 - acc: 0.8176 - val_loss: 0.4492 - val_acc: 0.8046\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4145 - acc: 0.8188 - val_loss: 0.4490 - val_acc: 0.8062\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4106 - acc: 0.8199 - val_loss: 0.4488 - val_acc: 0.8079\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4121 - acc: 0.8225 - val_loss: 0.4488 - val_acc: 0.8095\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4136 - acc: 0.8152 - val_loss: 0.4484 - val_acc: 0.8079\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4120 - acc: 0.8216 - val_loss: 0.4480 - val_acc: 0.8095\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4118 - acc: 0.8232 - val_loss: 0.4482 - val_acc: 0.8095\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4102 - acc: 0.8205 - val_loss: 0.4477 - val_acc: 0.8095\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4116 - acc: 0.8203 - val_loss: 0.4477 - val_acc: 0.8079\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4118 - acc: 0.8216 - val_loss: 0.4476 - val_acc: 0.8128\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4077 - acc: 0.8208 - val_loss: 0.4474 - val_acc: 0.8095\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4087 - acc: 0.8227 - val_loss: 0.4475 - val_acc: 0.8112\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4056 - acc: 0.8232 - val_loss: 0.4475 - val_acc: 0.8095\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4077 - acc: 0.8214 - val_loss: 0.4475 - val_acc: 0.8095\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4075 - acc: 0.8256 - val_loss: 0.4470 - val_acc: 0.8062\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4053 - acc: 0.8245 - val_loss: 0.4465 - val_acc: 0.8112\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4070 - acc: 0.8250 - val_loss: 0.4463 - val_acc: 0.8128\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4051 - acc: 0.8252 - val_loss: 0.4457 - val_acc: 0.8144\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4057 - acc: 0.8248 - val_loss: 0.4457 - val_acc: 0.8128\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4036 - acc: 0.8263 - val_loss: 0.4459 - val_acc: 0.8128\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4051 - acc: 0.8248 - val_loss: 0.4458 - val_acc: 0.8144\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4016 - acc: 0.8280 - val_loss: 0.4455 - val_acc: 0.8079\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4029 - acc: 0.8241 - val_loss: 0.4452 - val_acc: 0.8095\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4031 - acc: 0.8258 - val_loss: 0.4450 - val_acc: 0.8128\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4049 - acc: 0.8265 - val_loss: 0.4451 - val_acc: 0.8112\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4016 - acc: 0.8245 - val_loss: 0.4446 - val_acc: 0.8112\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4032 - acc: 0.8248 - val_loss: 0.4442 - val_acc: 0.8128\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3971 - acc: 0.8296 - val_loss: 0.4443 - val_acc: 0.8112\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3993 - acc: 0.8272 - val_loss: 0.4444 - val_acc: 0.8112\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4008 - acc: 0.8256 - val_loss: 0.4442 - val_acc: 0.8112\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3988 - acc: 0.8294 - val_loss: 0.4445 - val_acc: 0.8112\n",
      "Epoch 114/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3999 - acc: 0.8263 - val_loss: 0.4443 - val_acc: 0.8095\n",
      "Epoch 115/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3989 - acc: 0.8276 - val_loss: 0.4443 - val_acc: 0.8095\n",
      "Epoch 116/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3976 - acc: 0.8254 - val_loss: 0.4437 - val_acc: 0.8062\n",
      "Epoch 117/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3978 - acc: 0.8256 - val_loss: 0.4440 - val_acc: 0.8095\n",
      "Epoch 118/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3981 - acc: 0.8301 - val_loss: 0.4435 - val_acc: 0.8079\n",
      "Epoch 119/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3956 - acc: 0.8290 - val_loss: 0.4436 - val_acc: 0.8046\n",
      "Epoch 120/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3942 - acc: 0.8285 - val_loss: 0.4442 - val_acc: 0.8062\n",
      "Epoch 121/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3938 - acc: 0.8318 - val_loss: 0.4435 - val_acc: 0.8079\n",
      "Epoch 122/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3918 - acc: 0.8318 - val_loss: 0.4435 - val_acc: 0.8079\n",
      "Epoch 123/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3934 - acc: 0.8289 - val_loss: 0.4436 - val_acc: 0.8062\n",
      "Epoch 124/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3928 - acc: 0.8283 - val_loss: 0.4432 - val_acc: 0.8079\n",
      "Epoch 125/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3935 - acc: 0.8307 - val_loss: 0.4434 - val_acc: 0.8062\n",
      "Epoch 126/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3927 - acc: 0.8301 - val_loss: 0.4432 - val_acc: 0.8095\n",
      "Epoch 127/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3941 - acc: 0.8320 - val_loss: 0.4429 - val_acc: 0.8046\n",
      "Epoch 128/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3921 - acc: 0.8281 - val_loss: 0.4429 - val_acc: 0.8046\n",
      "Epoch 129/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3902 - acc: 0.8321 - val_loss: 0.4429 - val_acc: 0.8046\n",
      "Epoch 130/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3915 - acc: 0.8358 - val_loss: 0.4429 - val_acc: 0.8062\n",
      "Epoch 131/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3916 - acc: 0.8296 - val_loss: 0.4425 - val_acc: 0.8079\n",
      "Epoch 132/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3897 - acc: 0.8323 - val_loss: 0.4424 - val_acc: 0.8079\n",
      "Epoch 133/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3892 - acc: 0.8296 - val_loss: 0.4424 - val_acc: 0.8079\n",
      "Epoch 134/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3890 - acc: 0.8312 - val_loss: 0.4424 - val_acc: 0.8095\n",
      "Epoch 135/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3890 - acc: 0.8358 - val_loss: 0.4425 - val_acc: 0.8062\n",
      "Epoch 136/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3885 - acc: 0.8311 - val_loss: 0.4421 - val_acc: 0.8095\n",
      "Epoch 137/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3886 - acc: 0.8329 - val_loss: 0.4420 - val_acc: 0.8079\n",
      "Epoch 138/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3877 - acc: 0.8340 - val_loss: 0.4426 - val_acc: 0.8079\n",
      "Epoch 139/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3862 - acc: 0.8320 - val_loss: 0.4423 - val_acc: 0.8095\n",
      "Epoch 140/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3872 - acc: 0.8300 - val_loss: 0.4425 - val_acc: 0.8079\n",
      "Epoch 141/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3860 - acc: 0.8331 - val_loss: 0.4423 - val_acc: 0.8095\n",
      "Epoch 142/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3844 - acc: 0.8327 - val_loss: 0.4419 - val_acc: 0.8095\n",
      "Epoch 143/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3877 - acc: 0.8298 - val_loss: 0.4417 - val_acc: 0.8062\n",
      "Epoch 144/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3845 - acc: 0.8345 - val_loss: 0.4422 - val_acc: 0.8062\n",
      "Epoch 145/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3846 - acc: 0.8320 - val_loss: 0.4417 - val_acc: 0.8112\n",
      "Epoch 146/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3831 - acc: 0.8325 - val_loss: 0.4417 - val_acc: 0.8112\n",
      "Epoch 147/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3838 - acc: 0.8369 - val_loss: 0.4420 - val_acc: 0.8079\n",
      "Epoch 148/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3854 - acc: 0.8351 - val_loss: 0.4420 - val_acc: 0.8062\n",
      "Epoch 149/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3829 - acc: 0.8334 - val_loss: 0.4413 - val_acc: 0.8013\n",
      "Epoch 150/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3828 - acc: 0.8321 - val_loss: 0.4416 - val_acc: 0.8112\n",
      "Epoch 151/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3817 - acc: 0.8347 - val_loss: 0.4416 - val_acc: 0.8128\n",
      "Epoch 152/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3827 - acc: 0.8345 - val_loss: 0.4412 - val_acc: 0.7997\n",
      "Epoch 153/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3822 - acc: 0.8363 - val_loss: 0.4409 - val_acc: 0.8046\n",
      "Epoch 154/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3812 - acc: 0.8329 - val_loss: 0.4411 - val_acc: 0.8112\n",
      "Epoch 155/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3797 - acc: 0.8351 - val_loss: 0.4416 - val_acc: 0.8062\n",
      "Epoch 156/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3818 - acc: 0.8332 - val_loss: 0.4408 - val_acc: 0.8095\n",
      "Epoch 157/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3811 - acc: 0.8356 - val_loss: 0.4404 - val_acc: 0.8062\n",
      "Epoch 158/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3808 - acc: 0.8376 - val_loss: 0.4410 - val_acc: 0.8095\n",
      "Epoch 159/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3779 - acc: 0.8362 - val_loss: 0.4404 - val_acc: 0.8095\n",
      "Epoch 160/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3761 - acc: 0.8393 - val_loss: 0.4406 - val_acc: 0.8095\n",
      "Epoch 161/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3800 - acc: 0.8365 - val_loss: 0.4412 - val_acc: 0.8062\n",
      "Epoch 162/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3802 - acc: 0.8354 - val_loss: 0.4413 - val_acc: 0.8062\n",
      "Epoch 163/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3791 - acc: 0.8343 - val_loss: 0.4410 - val_acc: 0.8046\n",
      "Epoch 164/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3764 - acc: 0.8327 - val_loss: 0.4408 - val_acc: 0.8079\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.3, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_108\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_109 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_308 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_200 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_309 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_201 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_310 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.8231 - acc: 0.4331 - val_loss: 0.7064 - val_acc: 0.4548\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.7062 - acc: 0.5134 - val_loss: 0.6643 - val_acc: 0.5862\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6798 - acc: 0.5712 - val_loss: 0.6508 - val_acc: 0.5846\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6619 - acc: 0.5835 - val_loss: 0.6306 - val_acc: 0.5895\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6375 - acc: 0.6280 - val_loss: 0.6130 - val_acc: 0.7126\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.6160 - acc: 0.6647 - val_loss: 0.6041 - val_acc: 0.7028\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.5982 - acc: 0.6957 - val_loss: 0.5944 - val_acc: 0.6864\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.5936 - acc: 0.6931 - val_loss: 0.5802 - val_acc: 0.7274\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.5779 - acc: 0.7156 - val_loss: 0.5684 - val_acc: 0.7504\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.5675 - acc: 0.7216 - val_loss: 0.5571 - val_acc: 0.7504\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.5548 - acc: 0.7344 - val_loss: 0.5459 - val_acc: 0.7504\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.5434 - acc: 0.7371 - val_loss: 0.5348 - val_acc: 0.7504\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.5345 - acc: 0.7515 - val_loss: 0.5246 - val_acc: 0.7553\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.5193 - acc: 0.7592 - val_loss: 0.5154 - val_acc: 0.7603\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.5069 - acc: 0.7672 - val_loss: 0.5075 - val_acc: 0.7586\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.5022 - acc: 0.7674 - val_loss: 0.5006 - val_acc: 0.7619\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4947 - acc: 0.7741 - val_loss: 0.4940 - val_acc: 0.7619\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4862 - acc: 0.7803 - val_loss: 0.4880 - val_acc: 0.7652\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4784 - acc: 0.7834 - val_loss: 0.4825 - val_acc: 0.7652\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4720 - acc: 0.7909 - val_loss: 0.4778 - val_acc: 0.7668\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4710 - acc: 0.7860 - val_loss: 0.4739 - val_acc: 0.7783\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4678 - acc: 0.7905 - val_loss: 0.4698 - val_acc: 0.7816\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4644 - acc: 0.7986 - val_loss: 0.4651 - val_acc: 0.7816\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4577 - acc: 0.7955 - val_loss: 0.4644 - val_acc: 0.7865\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4559 - acc: 0.7977 - val_loss: 0.4595 - val_acc: 0.7849\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4497 - acc: 0.7978 - val_loss: 0.4573 - val_acc: 0.7816\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4471 - acc: 0.8028 - val_loss: 0.4573 - val_acc: 0.7882\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4412 - acc: 0.8037 - val_loss: 0.4536 - val_acc: 0.7865\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4380 - acc: 0.8066 - val_loss: 0.4526 - val_acc: 0.7882\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4364 - acc: 0.8101 - val_loss: 0.4496 - val_acc: 0.7931\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4354 - acc: 0.8050 - val_loss: 0.4487 - val_acc: 0.7931\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4328 - acc: 0.8123 - val_loss: 0.4484 - val_acc: 0.7931\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4330 - acc: 0.8134 - val_loss: 0.4462 - val_acc: 0.7997\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4280 - acc: 0.8121 - val_loss: 0.4458 - val_acc: 0.7931\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4297 - acc: 0.8135 - val_loss: 0.4471 - val_acc: 0.7947\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4228 - acc: 0.8194 - val_loss: 0.4442 - val_acc: 0.8013\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4226 - acc: 0.8199 - val_loss: 0.4445 - val_acc: 0.7997\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4220 - acc: 0.8155 - val_loss: 0.4425 - val_acc: 0.7997\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4188 - acc: 0.8181 - val_loss: 0.4412 - val_acc: 0.7997\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4135 - acc: 0.8190 - val_loss: 0.4422 - val_acc: 0.8046\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4165 - acc: 0.8181 - val_loss: 0.4417 - val_acc: 0.8062\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4159 - acc: 0.8159 - val_loss: 0.4412 - val_acc: 0.8062\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4109 - acc: 0.8212 - val_loss: 0.4395 - val_acc: 0.8030\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4110 - acc: 0.8243 - val_loss: 0.4397 - val_acc: 0.8112\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.4078 - acc: 0.8265 - val_loss: 0.4379 - val_acc: 0.8030\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4085 - acc: 0.8247 - val_loss: 0.4380 - val_acc: 0.8079\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4050 - acc: 0.8236 - val_loss: 0.4406 - val_acc: 0.8112\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4050 - acc: 0.8223 - val_loss: 0.4372 - val_acc: 0.8030\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4017 - acc: 0.8227 - val_loss: 0.4401 - val_acc: 0.8112\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3999 - acc: 0.8296 - val_loss: 0.4365 - val_acc: 0.8079\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.3992 - acc: 0.8272 - val_loss: 0.4356 - val_acc: 0.8095\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3973 - acc: 0.8296 - val_loss: 0.4369 - val_acc: 0.8095\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3958 - acc: 0.8307 - val_loss: 0.4351 - val_acc: 0.8095\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3958 - acc: 0.8309 - val_loss: 0.4354 - val_acc: 0.8095\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3932 - acc: 0.8323 - val_loss: 0.4354 - val_acc: 0.8062\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3891 - acc: 0.8314 - val_loss: 0.4346 - val_acc: 0.8095\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3898 - acc: 0.8294 - val_loss: 0.4384 - val_acc: 0.8079\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3899 - acc: 0.8340 - val_loss: 0.4339 - val_acc: 0.8062\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.3891 - acc: 0.8342 - val_loss: 0.4358 - val_acc: 0.8079\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3861 - acc: 0.8354 - val_loss: 0.4341 - val_acc: 0.8079\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3862 - acc: 0.8356 - val_loss: 0.4339 - val_acc: 0.8062\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.3823 - acc: 0.8378 - val_loss: 0.4362 - val_acc: 0.8062\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3826 - acc: 0.8338 - val_loss: 0.4344 - val_acc: 0.8079\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.3, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_109\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_110 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_311 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_202 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_312 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_203 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_313 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.8712 - acc: 0.4344 - val_loss: 0.7632 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.7620 - acc: 0.4537 - val_loss: 0.6990 - val_acc: 0.4713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.7084 - acc: 0.5028 - val_loss: 0.6700 - val_acc: 0.6273\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6844 - acc: 0.5481 - val_loss: 0.6567 - val_acc: 0.6076\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6707 - acc: 0.5835 - val_loss: 0.6464 - val_acc: 0.6043\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6572 - acc: 0.6138 - val_loss: 0.6366 - val_acc: 0.6207\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6421 - acc: 0.6413 - val_loss: 0.6265 - val_acc: 0.6913\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6342 - acc: 0.6568 - val_loss: 0.6169 - val_acc: 0.7471\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6215 - acc: 0.6780 - val_loss: 0.6084 - val_acc: 0.7422\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6189 - acc: 0.6767 - val_loss: 0.6005 - val_acc: 0.7389\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6093 - acc: 0.6813 - val_loss: 0.5926 - val_acc: 0.7323\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5935 - acc: 0.7086 - val_loss: 0.5846 - val_acc: 0.7373\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5913 - acc: 0.7057 - val_loss: 0.5768 - val_acc: 0.7455\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5832 - acc: 0.7092 - val_loss: 0.5689 - val_acc: 0.7504\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5700 - acc: 0.7265 - val_loss: 0.5604 - val_acc: 0.7537\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5675 - acc: 0.7322 - val_loss: 0.5525 - val_acc: 0.7521\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5635 - acc: 0.7243 - val_loss: 0.5454 - val_acc: 0.7504\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5542 - acc: 0.7393 - val_loss: 0.5382 - val_acc: 0.7635\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5477 - acc: 0.7407 - val_loss: 0.5316 - val_acc: 0.7635\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5443 - acc: 0.7411 - val_loss: 0.5254 - val_acc: 0.7652\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5341 - acc: 0.7490 - val_loss: 0.5196 - val_acc: 0.7668\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5297 - acc: 0.7530 - val_loss: 0.5146 - val_acc: 0.7635\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5260 - acc: 0.7526 - val_loss: 0.5095 - val_acc: 0.7685\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5188 - acc: 0.7595 - val_loss: 0.5054 - val_acc: 0.7668\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5098 - acc: 0.7710 - val_loss: 0.5011 - val_acc: 0.7718\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5105 - acc: 0.7674 - val_loss: 0.4969 - val_acc: 0.7701\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4986 - acc: 0.7745 - val_loss: 0.4931 - val_acc: 0.7701\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4985 - acc: 0.7736 - val_loss: 0.4899 - val_acc: 0.7718\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4954 - acc: 0.7789 - val_loss: 0.4878 - val_acc: 0.7783\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4853 - acc: 0.7798 - val_loss: 0.4831 - val_acc: 0.7767\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4862 - acc: 0.7783 - val_loss: 0.4800 - val_acc: 0.7783\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4855 - acc: 0.7805 - val_loss: 0.4776 - val_acc: 0.7767\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4825 - acc: 0.7820 - val_loss: 0.4756 - val_acc: 0.7800\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4738 - acc: 0.7915 - val_loss: 0.4729 - val_acc: 0.7783\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4710 - acc: 0.7865 - val_loss: 0.4715 - val_acc: 0.7800\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4726 - acc: 0.7891 - val_loss: 0.4695 - val_acc: 0.7833\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4697 - acc: 0.7933 - val_loss: 0.4672 - val_acc: 0.7833\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4669 - acc: 0.7911 - val_loss: 0.4654 - val_acc: 0.7816\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4646 - acc: 0.7887 - val_loss: 0.4634 - val_acc: 0.7800\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4616 - acc: 0.7920 - val_loss: 0.4625 - val_acc: 0.7833\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4631 - acc: 0.8033 - val_loss: 0.4610 - val_acc: 0.7833\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4588 - acc: 0.7993 - val_loss: 0.4595 - val_acc: 0.7865\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4549 - acc: 0.7938 - val_loss: 0.4586 - val_acc: 0.7865\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4577 - acc: 0.7942 - val_loss: 0.4573 - val_acc: 0.7816\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4518 - acc: 0.8037 - val_loss: 0.4566 - val_acc: 0.7915\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4478 - acc: 0.8048 - val_loss: 0.4550 - val_acc: 0.7882\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4429 - acc: 0.8059 - val_loss: 0.4555 - val_acc: 0.7915\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4464 - acc: 0.8050 - val_loss: 0.4535 - val_acc: 0.7898\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4442 - acc: 0.8090 - val_loss: 0.4516 - val_acc: 0.7964\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4419 - acc: 0.8033 - val_loss: 0.4505 - val_acc: 0.7947\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4427 - acc: 0.8081 - val_loss: 0.4516 - val_acc: 0.7997\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4442 - acc: 0.8072 - val_loss: 0.4503 - val_acc: 0.8013\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4344 - acc: 0.8093 - val_loss: 0.4481 - val_acc: 0.7931\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4362 - acc: 0.8086 - val_loss: 0.4491 - val_acc: 0.8030\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4420 - acc: 0.8073 - val_loss: 0.4487 - val_acc: 0.8030\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4335 - acc: 0.8132 - val_loss: 0.4462 - val_acc: 0.7980\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4358 - acc: 0.8090 - val_loss: 0.4461 - val_acc: 0.8013\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4316 - acc: 0.8093 - val_loss: 0.4457 - val_acc: 0.8013\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4318 - acc: 0.8104 - val_loss: 0.4443 - val_acc: 0.8062\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4261 - acc: 0.8081 - val_loss: 0.4437 - val_acc: 0.8046\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4296 - acc: 0.8163 - val_loss: 0.4441 - val_acc: 0.8046\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4244 - acc: 0.8141 - val_loss: 0.4431 - val_acc: 0.8030\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4224 - acc: 0.8188 - val_loss: 0.4420 - val_acc: 0.8079\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4277 - acc: 0.8183 - val_loss: 0.4414 - val_acc: 0.8030\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4256 - acc: 0.8135 - val_loss: 0.4420 - val_acc: 0.8112\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4206 - acc: 0.8188 - val_loss: 0.4408 - val_acc: 0.8128\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4209 - acc: 0.8181 - val_loss: 0.4403 - val_acc: 0.8128\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4207 - acc: 0.8192 - val_loss: 0.4394 - val_acc: 0.8046\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4202 - acc: 0.8128 - val_loss: 0.4397 - val_acc: 0.8144\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4160 - acc: 0.8196 - val_loss: 0.4406 - val_acc: 0.8079\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4213 - acc: 0.8185 - val_loss: 0.4393 - val_acc: 0.8128\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4155 - acc: 0.8179 - val_loss: 0.4389 - val_acc: 0.8128\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4172 - acc: 0.8174 - val_loss: 0.4385 - val_acc: 0.8095\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4119 - acc: 0.8269 - val_loss: 0.4383 - val_acc: 0.8128\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4148 - acc: 0.8177 - val_loss: 0.4387 - val_acc: 0.8128\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4091 - acc: 0.8212 - val_loss: 0.4376 - val_acc: 0.8095\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4092 - acc: 0.8232 - val_loss: 0.4372 - val_acc: 0.8095\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4099 - acc: 0.8228 - val_loss: 0.4373 - val_acc: 0.8112\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4078 - acc: 0.8223 - val_loss: 0.4383 - val_acc: 0.8144\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4106 - acc: 0.8228 - val_loss: 0.4362 - val_acc: 0.8112\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4078 - acc: 0.8247 - val_loss: 0.4362 - val_acc: 0.8112\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4089 - acc: 0.8265 - val_loss: 0.4379 - val_acc: 0.8144\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4077 - acc: 0.8276 - val_loss: 0.4360 - val_acc: 0.8144\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4054 - acc: 0.8267 - val_loss: 0.4351 - val_acc: 0.8112\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4029 - acc: 0.8258 - val_loss: 0.4350 - val_acc: 0.8128\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4028 - acc: 0.8281 - val_loss: 0.4351 - val_acc: 0.8177\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4066 - acc: 0.8269 - val_loss: 0.4347 - val_acc: 0.8112\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4033 - acc: 0.8258 - val_loss: 0.4352 - val_acc: 0.8095\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4024 - acc: 0.8298 - val_loss: 0.4351 - val_acc: 0.8128\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3969 - acc: 0.8248 - val_loss: 0.4354 - val_acc: 0.8112\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3995 - acc: 0.8287 - val_loss: 0.4355 - val_acc: 0.8112\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3983 - acc: 0.8303 - val_loss: 0.4337 - val_acc: 0.8128\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3953 - acc: 0.8329 - val_loss: 0.4345 - val_acc: 0.8128\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3951 - acc: 0.8321 - val_loss: 0.4333 - val_acc: 0.8095\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3926 - acc: 0.8287 - val_loss: 0.4330 - val_acc: 0.8079\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3944 - acc: 0.8270 - val_loss: 0.4339 - val_acc: 0.8079\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3969 - acc: 0.8307 - val_loss: 0.4347 - val_acc: 0.8112\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3964 - acc: 0.8281 - val_loss: 0.4339 - val_acc: 0.8079\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3932 - acc: 0.8307 - val_loss: 0.4339 - val_acc: 0.8112\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3931 - acc: 0.8316 - val_loss: 0.4352 - val_acc: 0.8112\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.3, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_110\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_111 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_314 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_204 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_315 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_205 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_316 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.7616 - acc: 0.4483 - val_loss: 0.7145 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.7129 - acc: 0.4868 - val_loss: 0.6772 - val_acc: 0.6125\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6863 - acc: 0.5515 - val_loss: 0.6580 - val_acc: 0.6453\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6659 - acc: 0.6034 - val_loss: 0.6477 - val_acc: 0.6010\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6575 - acc: 0.6170 - val_loss: 0.6395 - val_acc: 0.5928\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6471 - acc: 0.6207 - val_loss: 0.6303 - val_acc: 0.6158\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6399 - acc: 0.6395 - val_loss: 0.6206 - val_acc: 0.6847\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6294 - acc: 0.6617 - val_loss: 0.6114 - val_acc: 0.7340\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6166 - acc: 0.6785 - val_loss: 0.6029 - val_acc: 0.7406\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6101 - acc: 0.6866 - val_loss: 0.5943 - val_acc: 0.7356\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6008 - acc: 0.7011 - val_loss: 0.5856 - val_acc: 0.7406\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5887 - acc: 0.7061 - val_loss: 0.5771 - val_acc: 0.7471\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5806 - acc: 0.7207 - val_loss: 0.5694 - val_acc: 0.7471\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5687 - acc: 0.7194 - val_loss: 0.5618 - val_acc: 0.7521\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5615 - acc: 0.7291 - val_loss: 0.5543 - val_acc: 0.7537\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5598 - acc: 0.7291 - val_loss: 0.5473 - val_acc: 0.7537\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5477 - acc: 0.7382 - val_loss: 0.5404 - val_acc: 0.7570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5376 - acc: 0.7504 - val_loss: 0.5337 - val_acc: 0.7635\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5348 - acc: 0.7480 - val_loss: 0.5273 - val_acc: 0.7685\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5275 - acc: 0.7557 - val_loss: 0.5214 - val_acc: 0.7701\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5223 - acc: 0.7570 - val_loss: 0.5161 - val_acc: 0.7668\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5178 - acc: 0.7619 - val_loss: 0.5114 - val_acc: 0.7635\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5137 - acc: 0.7577 - val_loss: 0.5070 - val_acc: 0.7668\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5078 - acc: 0.7716 - val_loss: 0.5028 - val_acc: 0.7718\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5005 - acc: 0.7721 - val_loss: 0.4990 - val_acc: 0.7701\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4998 - acc: 0.7738 - val_loss: 0.4955 - val_acc: 0.7718\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4926 - acc: 0.7785 - val_loss: 0.4923 - val_acc: 0.7750\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4848 - acc: 0.7758 - val_loss: 0.4894 - val_acc: 0.7718\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4844 - acc: 0.7811 - val_loss: 0.4861 - val_acc: 0.7767\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4838 - acc: 0.7794 - val_loss: 0.4831 - val_acc: 0.7750\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4833 - acc: 0.7800 - val_loss: 0.4805 - val_acc: 0.7734\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4757 - acc: 0.7871 - val_loss: 0.4781 - val_acc: 0.7734\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4744 - acc: 0.7893 - val_loss: 0.4758 - val_acc: 0.7734\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4699 - acc: 0.7882 - val_loss: 0.4739 - val_acc: 0.7800\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4650 - acc: 0.7916 - val_loss: 0.4721 - val_acc: 0.7800\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4670 - acc: 0.7882 - val_loss: 0.4704 - val_acc: 0.7833\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4645 - acc: 0.7913 - val_loss: 0.4686 - val_acc: 0.7833\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4594 - acc: 0.8008 - val_loss: 0.4672 - val_acc: 0.7865\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4561 - acc: 0.7988 - val_loss: 0.4650 - val_acc: 0.7931\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4582 - acc: 0.7957 - val_loss: 0.4633 - val_acc: 0.7915\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4563 - acc: 0.7997 - val_loss: 0.4630 - val_acc: 0.7915\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4557 - acc: 0.7958 - val_loss: 0.4602 - val_acc: 0.7898\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4517 - acc: 0.8004 - val_loss: 0.4592 - val_acc: 0.7947\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4476 - acc: 0.8077 - val_loss: 0.4587 - val_acc: 0.7964\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4431 - acc: 0.8070 - val_loss: 0.4568 - val_acc: 0.7931\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4442 - acc: 0.8072 - val_loss: 0.4562 - val_acc: 0.7980\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4401 - acc: 0.8101 - val_loss: 0.4563 - val_acc: 0.7997\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4415 - acc: 0.8079 - val_loss: 0.4538 - val_acc: 0.7964\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4378 - acc: 0.8057 - val_loss: 0.4531 - val_acc: 0.8030\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4346 - acc: 0.8082 - val_loss: 0.4540 - val_acc: 0.8013\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4370 - acc: 0.8097 - val_loss: 0.4511 - val_acc: 0.8030\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4400 - acc: 0.8121 - val_loss: 0.4504 - val_acc: 0.8062\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4346 - acc: 0.8126 - val_loss: 0.4497 - val_acc: 0.8079\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4360 - acc: 0.8119 - val_loss: 0.4491 - val_acc: 0.8079\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4319 - acc: 0.8148 - val_loss: 0.4497 - val_acc: 0.7997\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4335 - acc: 0.8101 - val_loss: 0.4493 - val_acc: 0.8030\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4317 - acc: 0.8172 - val_loss: 0.4482 - val_acc: 0.8079\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4287 - acc: 0.8124 - val_loss: 0.4487 - val_acc: 0.8062\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4232 - acc: 0.8192 - val_loss: 0.4471 - val_acc: 0.8079\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4264 - acc: 0.8113 - val_loss: 0.4463 - val_acc: 0.8079\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4245 - acc: 0.8141 - val_loss: 0.4465 - val_acc: 0.8079\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4272 - acc: 0.8123 - val_loss: 0.4456 - val_acc: 0.8112\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4234 - acc: 0.8146 - val_loss: 0.4450 - val_acc: 0.8144\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4194 - acc: 0.8194 - val_loss: 0.4443 - val_acc: 0.8095\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4175 - acc: 0.8203 - val_loss: 0.4439 - val_acc: 0.8128\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4181 - acc: 0.8225 - val_loss: 0.4450 - val_acc: 0.8112\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4156 - acc: 0.8199 - val_loss: 0.4428 - val_acc: 0.8144\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4148 - acc: 0.8196 - val_loss: 0.4425 - val_acc: 0.8144\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4170 - acc: 0.8159 - val_loss: 0.4434 - val_acc: 0.8112\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4132 - acc: 0.8194 - val_loss: 0.4413 - val_acc: 0.8144\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4133 - acc: 0.8227 - val_loss: 0.4408 - val_acc: 0.8144\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4112 - acc: 0.8183 - val_loss: 0.4422 - val_acc: 0.8112\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4111 - acc: 0.8219 - val_loss: 0.4412 - val_acc: 0.8161\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4116 - acc: 0.8201 - val_loss: 0.4410 - val_acc: 0.8144\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4092 - acc: 0.8190 - val_loss: 0.4419 - val_acc: 0.8128\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4071 - acc: 0.8221 - val_loss: 0.4423 - val_acc: 0.8128\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.3, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_111\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_112 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_317 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_206 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_318 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_207 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_319 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.8309 - acc: 0.4468 - val_loss: 0.7420 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.7414 - acc: 0.4764 - val_loss: 0.6828 - val_acc: 0.5928\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6937 - acc: 0.5455 - val_loss: 0.6600 - val_acc: 0.5878\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6728 - acc: 0.5944 - val_loss: 0.6493 - val_acc: 0.5862\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6595 - acc: 0.6079 - val_loss: 0.6374 - val_acc: 0.5878\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6450 - acc: 0.6325 - val_loss: 0.6238 - val_acc: 0.6355\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6308 - acc: 0.6544 - val_loss: 0.6125 - val_acc: 0.7291\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6216 - acc: 0.6586 - val_loss: 0.6036 - val_acc: 0.7406\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6073 - acc: 0.6763 - val_loss: 0.5955 - val_acc: 0.7159\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5958 - acc: 0.6886 - val_loss: 0.5868 - val_acc: 0.7307\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.5877 - acc: 0.6909 - val_loss: 0.5784 - val_acc: 0.7406\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.5779 - acc: 0.7046 - val_loss: 0.5700 - val_acc: 0.7471\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5662 - acc: 0.7240 - val_loss: 0.5607 - val_acc: 0.7537\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.5632 - acc: 0.7285 - val_loss: 0.5515 - val_acc: 0.7471\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.5464 - acc: 0.7342 - val_loss: 0.5426 - val_acc: 0.7570\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5379 - acc: 0.7490 - val_loss: 0.5339 - val_acc: 0.7586\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5311 - acc: 0.7449 - val_loss: 0.5259 - val_acc: 0.7750\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5202 - acc: 0.7522 - val_loss: 0.5178 - val_acc: 0.7750\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5158 - acc: 0.7626 - val_loss: 0.5111 - val_acc: 0.7734\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5043 - acc: 0.7699 - val_loss: 0.5042 - val_acc: 0.7767\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4997 - acc: 0.7654 - val_loss: 0.4996 - val_acc: 0.7718\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4937 - acc: 0.7721 - val_loss: 0.4938 - val_acc: 0.7668\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4889 - acc: 0.7783 - val_loss: 0.4885 - val_acc: 0.7767\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4841 - acc: 0.7800 - val_loss: 0.4848 - val_acc: 0.7783\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4805 - acc: 0.7836 - val_loss: 0.4814 - val_acc: 0.7816\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4723 - acc: 0.7896 - val_loss: 0.4772 - val_acc: 0.7833\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4682 - acc: 0.7893 - val_loss: 0.4774 - val_acc: 0.7833\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4660 - acc: 0.7935 - val_loss: 0.4729 - val_acc: 0.7849\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4622 - acc: 0.7927 - val_loss: 0.4696 - val_acc: 0.7865\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4602 - acc: 0.7893 - val_loss: 0.4679 - val_acc: 0.7849\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4561 - acc: 0.7929 - val_loss: 0.4672 - val_acc: 0.7849\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4586 - acc: 0.8019 - val_loss: 0.4629 - val_acc: 0.7865\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4481 - acc: 0.7942 - val_loss: 0.4619 - val_acc: 0.7898\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4468 - acc: 0.8008 - val_loss: 0.4606 - val_acc: 0.7898\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4388 - acc: 0.8086 - val_loss: 0.4583 - val_acc: 0.7882\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4454 - acc: 0.8028 - val_loss: 0.4571 - val_acc: 0.7898\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4404 - acc: 0.8033 - val_loss: 0.4560 - val_acc: 0.7915\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4370 - acc: 0.8044 - val_loss: 0.4567 - val_acc: 0.7964\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4395 - acc: 0.8062 - val_loss: 0.4541 - val_acc: 0.7915\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4337 - acc: 0.8103 - val_loss: 0.4545 - val_acc: 0.7980\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4351 - acc: 0.8072 - val_loss: 0.4536 - val_acc: 0.7997\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4301 - acc: 0.8064 - val_loss: 0.4515 - val_acc: 0.7931\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4270 - acc: 0.8150 - val_loss: 0.4518 - val_acc: 0.8062\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4311 - acc: 0.8112 - val_loss: 0.4511 - val_acc: 0.8062\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4270 - acc: 0.8115 - val_loss: 0.4503 - val_acc: 0.8046\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4255 - acc: 0.8134 - val_loss: 0.4486 - val_acc: 0.8013\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4227 - acc: 0.8146 - val_loss: 0.4499 - val_acc: 0.8062\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4230 - acc: 0.8117 - val_loss: 0.4476 - val_acc: 0.8030\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4182 - acc: 0.8165 - val_loss: 0.4465 - val_acc: 0.8013\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4185 - acc: 0.8130 - val_loss: 0.4452 - val_acc: 0.8030\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4168 - acc: 0.8166 - val_loss: 0.4441 - val_acc: 0.8046\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4118 - acc: 0.8181 - val_loss: 0.4449 - val_acc: 0.8062\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4102 - acc: 0.8238 - val_loss: 0.4437 - val_acc: 0.8062\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4126 - acc: 0.8217 - val_loss: 0.4443 - val_acc: 0.8046\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4102 - acc: 0.8199 - val_loss: 0.4431 - val_acc: 0.8079\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4109 - acc: 0.8221 - val_loss: 0.4433 - val_acc: 0.8079\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4084 - acc: 0.8219 - val_loss: 0.4413 - val_acc: 0.8095\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4105 - acc: 0.8196 - val_loss: 0.4425 - val_acc: 0.8079\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4081 - acc: 0.8243 - val_loss: 0.4395 - val_acc: 0.8112\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4040 - acc: 0.8259 - val_loss: 0.4404 - val_acc: 0.8095\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4048 - acc: 0.8214 - val_loss: 0.4418 - val_acc: 0.8079\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4010 - acc: 0.8230 - val_loss: 0.4389 - val_acc: 0.8095\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4024 - acc: 0.8274 - val_loss: 0.4399 - val_acc: 0.8128\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4007 - acc: 0.8261 - val_loss: 0.4386 - val_acc: 0.8128\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3972 - acc: 0.8265 - val_loss: 0.4397 - val_acc: 0.8112\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3939 - acc: 0.8298 - val_loss: 0.4381 - val_acc: 0.8079\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.3947 - acc: 0.8283 - val_loss: 0.4382 - val_acc: 0.8062\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3908 - acc: 0.8265 - val_loss: 0.4382 - val_acc: 0.8095\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3915 - acc: 0.8325 - val_loss: 0.4366 - val_acc: 0.8030\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3897 - acc: 0.8303 - val_loss: 0.4385 - val_acc: 0.8046\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3919 - acc: 0.8345 - val_loss: 0.4375 - val_acc: 0.8030\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3908 - acc: 0.8312 - val_loss: 0.4353 - val_acc: 0.8062\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3889 - acc: 0.8274 - val_loss: 0.4374 - val_acc: 0.8030\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3869 - acc: 0.8352 - val_loss: 0.4354 - val_acc: 0.8030\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3838 - acc: 0.8345 - val_loss: 0.4367 - val_acc: 0.8030\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3822 - acc: 0.8320 - val_loss: 0.4360 - val_acc: 0.8030\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3831 - acc: 0.8327 - val_loss: 0.4361 - val_acc: 0.8046\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.35, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_112\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_113 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_320 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_208 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_321 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_209 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_322 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.7530 - acc: 0.4652 - val_loss: 0.6808 - val_acc: 0.5616\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6823 - acc: 0.5587 - val_loss: 0.6654 - val_acc: 0.5846\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6668 - acc: 0.5849 - val_loss: 0.6460 - val_acc: 0.5846\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6464 - acc: 0.6147 - val_loss: 0.6217 - val_acc: 0.6897\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.6197 - acc: 0.6652 - val_loss: 0.6078 - val_acc: 0.6897\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.6063 - acc: 0.6880 - val_loss: 0.5949 - val_acc: 0.6995\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.5861 - acc: 0.7046 - val_loss: 0.5803 - val_acc: 0.7323\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.5758 - acc: 0.7172 - val_loss: 0.5689 - val_acc: 0.7488\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.5594 - acc: 0.7338 - val_loss: 0.5559 - val_acc: 0.7471\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.5460 - acc: 0.7375 - val_loss: 0.5444 - val_acc: 0.7438\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.5330 - acc: 0.7479 - val_loss: 0.5329 - val_acc: 0.7537\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.5241 - acc: 0.7482 - val_loss: 0.5234 - val_acc: 0.7553\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.5139 - acc: 0.7677 - val_loss: 0.5138 - val_acc: 0.7553\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.5068 - acc: 0.7656 - val_loss: 0.5063 - val_acc: 0.7570\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4979 - acc: 0.7738 - val_loss: 0.4996 - val_acc: 0.7603\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4876 - acc: 0.7781 - val_loss: 0.4948 - val_acc: 0.7718\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.4805 - acc: 0.7818 - val_loss: 0.4870 - val_acc: 0.7668\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4774 - acc: 0.7829 - val_loss: 0.4824 - val_acc: 0.7767\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4681 - acc: 0.7891 - val_loss: 0.4768 - val_acc: 0.7718\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4620 - acc: 0.7924 - val_loss: 0.4739 - val_acc: 0.7783\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.4571 - acc: 0.7977 - val_loss: 0.4714 - val_acc: 0.7833\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4518 - acc: 0.8031 - val_loss: 0.4660 - val_acc: 0.7800\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4515 - acc: 0.8042 - val_loss: 0.4649 - val_acc: 0.7898\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.4485 - acc: 0.8008 - val_loss: 0.4606 - val_acc: 0.7882\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4449 - acc: 0.8041 - val_loss: 0.4592 - val_acc: 0.7882\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4390 - acc: 0.8048 - val_loss: 0.4582 - val_acc: 0.7898\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4363 - acc: 0.8066 - val_loss: 0.4565 - val_acc: 0.7931\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4320 - acc: 0.8113 - val_loss: 0.4548 - val_acc: 0.7931\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4322 - acc: 0.8097 - val_loss: 0.4525 - val_acc: 0.7947\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4303 - acc: 0.8137 - val_loss: 0.4502 - val_acc: 0.7947\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4254 - acc: 0.8104 - val_loss: 0.4490 - val_acc: 0.7947\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4270 - acc: 0.8132 - val_loss: 0.4490 - val_acc: 0.7997\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4213 - acc: 0.8188 - val_loss: 0.4478 - val_acc: 0.7997\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4216 - acc: 0.8170 - val_loss: 0.4483 - val_acc: 0.8013\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4188 - acc: 0.8188 - val_loss: 0.4452 - val_acc: 0.8062\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4163 - acc: 0.8201 - val_loss: 0.4458 - val_acc: 0.8062\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4128 - acc: 0.8192 - val_loss: 0.4440 - val_acc: 0.8013\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4147 - acc: 0.8181 - val_loss: 0.4448 - val_acc: 0.8046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4065 - acc: 0.8232 - val_loss: 0.4456 - val_acc: 0.8030\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4065 - acc: 0.8225 - val_loss: 0.4423 - val_acc: 0.8095\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4040 - acc: 0.8256 - val_loss: 0.4417 - val_acc: 0.8062\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3993 - acc: 0.8261 - val_loss: 0.4409 - val_acc: 0.8095\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.3983 - acc: 0.8274 - val_loss: 0.4426 - val_acc: 0.8062\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4024 - acc: 0.8250 - val_loss: 0.4410 - val_acc: 0.8046\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4008 - acc: 0.8254 - val_loss: 0.4414 - val_acc: 0.8079\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4003 - acc: 0.8276 - val_loss: 0.4405 - val_acc: 0.8062\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3950 - acc: 0.8325 - val_loss: 0.4409 - val_acc: 0.8095\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3934 - acc: 0.8312 - val_loss: 0.4384 - val_acc: 0.8095\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3918 - acc: 0.8325 - val_loss: 0.4416 - val_acc: 0.8095\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3918 - acc: 0.8323 - val_loss: 0.4374 - val_acc: 0.8030\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3905 - acc: 0.8294 - val_loss: 0.4376 - val_acc: 0.8079\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3844 - acc: 0.8332 - val_loss: 0.4368 - val_acc: 0.8079\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.3870 - acc: 0.8327 - val_loss: 0.4382 - val_acc: 0.8079\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3848 - acc: 0.8376 - val_loss: 0.4380 - val_acc: 0.8062\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3797 - acc: 0.8371 - val_loss: 0.4363 - val_acc: 0.8062\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 0.3835 - acc: 0.8354 - val_loss: 0.4362 - val_acc: 0.8079\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3815 - acc: 0.8367 - val_loss: 0.4366 - val_acc: 0.8046\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3749 - acc: 0.8351 - val_loss: 0.4364 - val_acc: 0.8046\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.3730 - acc: 0.8413 - val_loss: 0.4366 - val_acc: 0.8030\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3773 - acc: 0.8384 - val_loss: 0.4355 - val_acc: 0.8046\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3744 - acc: 0.8394 - val_loss: 0.4362 - val_acc: 0.8046\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3719 - acc: 0.8384 - val_loss: 0.4372 - val_acc: 0.8030\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3694 - acc: 0.8389 - val_loss: 0.4370 - val_acc: 0.8030\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3685 - acc: 0.8440 - val_loss: 0.4359 - val_acc: 0.8030\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3689 - acc: 0.8435 - val_loss: 0.4345 - val_acc: 0.8062\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.3641 - acc: 0.8418 - val_loss: 0.4356 - val_acc: 0.8030\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.3640 - acc: 0.8462 - val_loss: 0.4363 - val_acc: 0.8046\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3611 - acc: 0.8449 - val_loss: 0.4373 - val_acc: 0.8013\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.3599 - acc: 0.8453 - val_loss: 0.4368 - val_acc: 0.8062\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3580 - acc: 0.8462 - val_loss: 0.4346 - val_acc: 0.8030\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.35, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_113\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_114 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_323 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_210 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_324 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_211 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_325 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.8480 - acc: 0.4417 - val_loss: 0.7426 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.7471 - acc: 0.4789 - val_loss: 0.6835 - val_acc: 0.5878\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6991 - acc: 0.5422 - val_loss: 0.6614 - val_acc: 0.5862\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6796 - acc: 0.5807 - val_loss: 0.6512 - val_acc: 0.5846\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6684 - acc: 0.6006 - val_loss: 0.6397 - val_acc: 0.5862\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6536 - acc: 0.6192 - val_loss: 0.6269 - val_acc: 0.6223\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6352 - acc: 0.6413 - val_loss: 0.6162 - val_acc: 0.7077\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.6252 - acc: 0.6568 - val_loss: 0.6074 - val_acc: 0.7422\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.6114 - acc: 0.6637 - val_loss: 0.5994 - val_acc: 0.7291\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.6027 - acc: 0.6778 - val_loss: 0.5913 - val_acc: 0.7356\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5910 - acc: 0.6875 - val_loss: 0.5835 - val_acc: 0.7406\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.5845 - acc: 0.6953 - val_loss: 0.5758 - val_acc: 0.7488\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5762 - acc: 0.7083 - val_loss: 0.5676 - val_acc: 0.7537\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5652 - acc: 0.7176 - val_loss: 0.5585 - val_acc: 0.7488\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5555 - acc: 0.7249 - val_loss: 0.5493 - val_acc: 0.7488\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.5484 - acc: 0.7336 - val_loss: 0.5406 - val_acc: 0.7603\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5382 - acc: 0.7449 - val_loss: 0.5328 - val_acc: 0.7734\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5295 - acc: 0.7604 - val_loss: 0.5240 - val_acc: 0.7718\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5196 - acc: 0.7555 - val_loss: 0.5162 - val_acc: 0.7652\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5184 - acc: 0.7568 - val_loss: 0.5103 - val_acc: 0.7767\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5062 - acc: 0.7674 - val_loss: 0.5037 - val_acc: 0.7800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5034 - acc: 0.7699 - val_loss: 0.4979 - val_acc: 0.7734\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4981 - acc: 0.7752 - val_loss: 0.4933 - val_acc: 0.7750\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4882 - acc: 0.7769 - val_loss: 0.4902 - val_acc: 0.7750\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4870 - acc: 0.7789 - val_loss: 0.4855 - val_acc: 0.7833\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4818 - acc: 0.7829 - val_loss: 0.4841 - val_acc: 0.7833\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4772 - acc: 0.7862 - val_loss: 0.4789 - val_acc: 0.7849\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4712 - acc: 0.7816 - val_loss: 0.4758 - val_acc: 0.7833\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4683 - acc: 0.7920 - val_loss: 0.4735 - val_acc: 0.7833\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4658 - acc: 0.7889 - val_loss: 0.4699 - val_acc: 0.7849\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4680 - acc: 0.7878 - val_loss: 0.4675 - val_acc: 0.7865\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4635 - acc: 0.7927 - val_loss: 0.4681 - val_acc: 0.7898\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4627 - acc: 0.7978 - val_loss: 0.4651 - val_acc: 0.7898\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4505 - acc: 0.8019 - val_loss: 0.4613 - val_acc: 0.7849\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4503 - acc: 0.8000 - val_loss: 0.4619 - val_acc: 0.7915\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4475 - acc: 0.8051 - val_loss: 0.4615 - val_acc: 0.7947\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4490 - acc: 0.7980 - val_loss: 0.4579 - val_acc: 0.7915\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4440 - acc: 0.8059 - val_loss: 0.4572 - val_acc: 0.7915\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4436 - acc: 0.8055 - val_loss: 0.4599 - val_acc: 0.7980\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4416 - acc: 0.8088 - val_loss: 0.4547 - val_acc: 0.7947\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4399 - acc: 0.8015 - val_loss: 0.4534 - val_acc: 0.7964\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4382 - acc: 0.8075 - val_loss: 0.4547 - val_acc: 0.7997\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4336 - acc: 0.8090 - val_loss: 0.4521 - val_acc: 0.8013\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4322 - acc: 0.8101 - val_loss: 0.4514 - val_acc: 0.7997\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4294 - acc: 0.8159 - val_loss: 0.4509 - val_acc: 0.7964\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4298 - acc: 0.8128 - val_loss: 0.4514 - val_acc: 0.8030\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4291 - acc: 0.8124 - val_loss: 0.4496 - val_acc: 0.7964\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4275 - acc: 0.8097 - val_loss: 0.4479 - val_acc: 0.7997\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4252 - acc: 0.8143 - val_loss: 0.4493 - val_acc: 0.8030\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4228 - acc: 0.8163 - val_loss: 0.4458 - val_acc: 0.8079\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4189 - acc: 0.8134 - val_loss: 0.4448 - val_acc: 0.8046\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4174 - acc: 0.8207 - val_loss: 0.4472 - val_acc: 0.8062\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4188 - acc: 0.8181 - val_loss: 0.4467 - val_acc: 0.8062\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4214 - acc: 0.8143 - val_loss: 0.4445 - val_acc: 0.8079\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4167 - acc: 0.8205 - val_loss: 0.4441 - val_acc: 0.8062\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4161 - acc: 0.8186 - val_loss: 0.4423 - val_acc: 0.8079\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4154 - acc: 0.8188 - val_loss: 0.4439 - val_acc: 0.8095\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4132 - acc: 0.8214 - val_loss: 0.4415 - val_acc: 0.8079\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4120 - acc: 0.8179 - val_loss: 0.4426 - val_acc: 0.8095\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4106 - acc: 0.8230 - val_loss: 0.4419 - val_acc: 0.8112\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4111 - acc: 0.8207 - val_loss: 0.4404 - val_acc: 0.8112\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4052 - acc: 0.8252 - val_loss: 0.4425 - val_acc: 0.8046\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4063 - acc: 0.8261 - val_loss: 0.4403 - val_acc: 0.8079\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4023 - acc: 0.8239 - val_loss: 0.4393 - val_acc: 0.8095\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4002 - acc: 0.8232 - val_loss: 0.4386 - val_acc: 0.8112\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4034 - acc: 0.8225 - val_loss: 0.4388 - val_acc: 0.8079\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4019 - acc: 0.8261 - val_loss: 0.4381 - val_acc: 0.8095\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3968 - acc: 0.8259 - val_loss: 0.4383 - val_acc: 0.8095\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4004 - acc: 0.8267 - val_loss: 0.4391 - val_acc: 0.8062\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3988 - acc: 0.8256 - val_loss: 0.4392 - val_acc: 0.8079\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3957 - acc: 0.8236 - val_loss: 0.4366 - val_acc: 0.8079\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3967 - acc: 0.8241 - val_loss: 0.4376 - val_acc: 0.8062\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3952 - acc: 0.8274 - val_loss: 0.4384 - val_acc: 0.8062\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3907 - acc: 0.8289 - val_loss: 0.4343 - val_acc: 0.8013\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3909 - acc: 0.8270 - val_loss: 0.4373 - val_acc: 0.8030\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3869 - acc: 0.8349 - val_loss: 0.4349 - val_acc: 0.8030\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3895 - acc: 0.8303 - val_loss: 0.4351 - val_acc: 0.7997\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3899 - acc: 0.8265 - val_loss: 0.4375 - val_acc: 0.8062\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3867 - acc: 0.8270 - val_loss: 0.4348 - val_acc: 0.8030\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.35, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_114\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_115 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_326 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_212 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_327 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.6723 - acc: 0.5917 - val_loss: 0.6468 - val_acc: 0.6716\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6540 - acc: 0.6130 - val_loss: 0.6313 - val_acc: 0.6700\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6387 - acc: 0.6351 - val_loss: 0.6196 - val_acc: 0.7110\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6234 - acc: 0.6537 - val_loss: 0.6098 - val_acc: 0.6913\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6081 - acc: 0.6822 - val_loss: 0.6011 - val_acc: 0.7044\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5988 - acc: 0.6982 - val_loss: 0.5927 - val_acc: 0.7110\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5916 - acc: 0.7008 - val_loss: 0.5845 - val_acc: 0.7225\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5821 - acc: 0.7128 - val_loss: 0.5766 - val_acc: 0.7307\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5712 - acc: 0.7254 - val_loss: 0.5695 - val_acc: 0.7438\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5694 - acc: 0.7267 - val_loss: 0.5630 - val_acc: 0.7504\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5571 - acc: 0.7371 - val_loss: 0.5569 - val_acc: 0.7471\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5501 - acc: 0.7444 - val_loss: 0.5512 - val_acc: 0.7521\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5455 - acc: 0.7438 - val_loss: 0.5455 - val_acc: 0.7586\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5362 - acc: 0.7530 - val_loss: 0.5401 - val_acc: 0.7537\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5344 - acc: 0.7572 - val_loss: 0.5355 - val_acc: 0.7537\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5249 - acc: 0.7604 - val_loss: 0.5306 - val_acc: 0.7603\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5234 - acc: 0.7634 - val_loss: 0.5261 - val_acc: 0.7586\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5184 - acc: 0.7668 - val_loss: 0.5222 - val_acc: 0.7586\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5171 - acc: 0.7646 - val_loss: 0.5184 - val_acc: 0.7619\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5096 - acc: 0.7718 - val_loss: 0.5150 - val_acc: 0.7603\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5061 - acc: 0.7701 - val_loss: 0.5117 - val_acc: 0.7619\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5005 - acc: 0.7761 - val_loss: 0.5086 - val_acc: 0.7586\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4965 - acc: 0.7749 - val_loss: 0.5056 - val_acc: 0.7635\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4975 - acc: 0.7772 - val_loss: 0.5027 - val_acc: 0.7685\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4908 - acc: 0.7805 - val_loss: 0.5001 - val_acc: 0.7652\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4875 - acc: 0.7843 - val_loss: 0.4980 - val_acc: 0.7668\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4856 - acc: 0.7838 - val_loss: 0.4960 - val_acc: 0.7685\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4829 - acc: 0.7838 - val_loss: 0.4935 - val_acc: 0.7685\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4823 - acc: 0.7834 - val_loss: 0.4914 - val_acc: 0.7718\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4794 - acc: 0.7885 - val_loss: 0.4895 - val_acc: 0.7701\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4757 - acc: 0.7889 - val_loss: 0.4875 - val_acc: 0.7734\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4745 - acc: 0.7869 - val_loss: 0.4857 - val_acc: 0.7750\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4721 - acc: 0.7909 - val_loss: 0.4841 - val_acc: 0.7750\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4692 - acc: 0.7944 - val_loss: 0.4825 - val_acc: 0.7734\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4695 - acc: 0.7916 - val_loss: 0.4809 - val_acc: 0.7750\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4637 - acc: 0.7973 - val_loss: 0.4794 - val_acc: 0.7783\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4632 - acc: 0.7986 - val_loss: 0.4779 - val_acc: 0.7767\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4640 - acc: 0.7964 - val_loss: 0.4767 - val_acc: 0.7783\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4589 - acc: 0.8002 - val_loss: 0.4756 - val_acc: 0.7800\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4579 - acc: 0.7966 - val_loss: 0.4744 - val_acc: 0.7783\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4587 - acc: 0.7999 - val_loss: 0.4733 - val_acc: 0.7767\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4525 - acc: 0.8006 - val_loss: 0.4725 - val_acc: 0.7833\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4539 - acc: 0.7975 - val_loss: 0.4711 - val_acc: 0.7800\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4553 - acc: 0.7980 - val_loss: 0.4700 - val_acc: 0.7816\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4525 - acc: 0.8033 - val_loss: 0.4691 - val_acc: 0.7816\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4516 - acc: 0.7999 - val_loss: 0.4682 - val_acc: 0.7816\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4496 - acc: 0.8053 - val_loss: 0.4675 - val_acc: 0.7882\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4498 - acc: 0.8055 - val_loss: 0.4669 - val_acc: 0.7931\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4466 - acc: 0.8039 - val_loss: 0.4658 - val_acc: 0.7915\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4454 - acc: 0.8062 - val_loss: 0.4649 - val_acc: 0.7915\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4440 - acc: 0.8068 - val_loss: 0.4643 - val_acc: 0.7915\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4441 - acc: 0.8095 - val_loss: 0.4636 - val_acc: 0.7898\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4423 - acc: 0.8077 - val_loss: 0.4632 - val_acc: 0.7915\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4402 - acc: 0.8048 - val_loss: 0.4628 - val_acc: 0.7931\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4384 - acc: 0.8126 - val_loss: 0.4626 - val_acc: 0.7964\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4424 - acc: 0.8088 - val_loss: 0.4617 - val_acc: 0.7931\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4361 - acc: 0.8134 - val_loss: 0.4608 - val_acc: 0.7915\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4352 - acc: 0.8112 - val_loss: 0.4600 - val_acc: 0.7931\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4339 - acc: 0.8101 - val_loss: 0.4595 - val_acc: 0.7997\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4345 - acc: 0.8132 - val_loss: 0.4592 - val_acc: 0.7997\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4364 - acc: 0.8134 - val_loss: 0.4585 - val_acc: 0.7980\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4351 - acc: 0.8146 - val_loss: 0.4577 - val_acc: 0.7947\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4315 - acc: 0.8137 - val_loss: 0.4570 - val_acc: 0.7947\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4279 - acc: 0.8141 - val_loss: 0.4566 - val_acc: 0.7964\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4324 - acc: 0.8159 - val_loss: 0.4565 - val_acc: 0.7980\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4293 - acc: 0.8170 - val_loss: 0.4560 - val_acc: 0.7931\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4282 - acc: 0.8150 - val_loss: 0.4557 - val_acc: 0.7947\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4240 - acc: 0.8165 - val_loss: 0.4552 - val_acc: 0.7964\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4247 - acc: 0.8146 - val_loss: 0.4560 - val_acc: 0.8030\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4312 - acc: 0.8141 - val_loss: 0.4547 - val_acc: 0.7997\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4289 - acc: 0.8144 - val_loss: 0.4540 - val_acc: 0.7964\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4256 - acc: 0.8130 - val_loss: 0.4535 - val_acc: 0.7997\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4242 - acc: 0.8181 - val_loss: 0.4535 - val_acc: 0.8013\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4217 - acc: 0.8150 - val_loss: 0.4528 - val_acc: 0.7997\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4253 - acc: 0.8141 - val_loss: 0.4524 - val_acc: 0.8013\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4242 - acc: 0.8150 - val_loss: 0.4525 - val_acc: 0.8046\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4222 - acc: 0.8161 - val_loss: 0.4519 - val_acc: 0.8046\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4228 - acc: 0.8135 - val_loss: 0.4514 - val_acc: 0.7997\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4222 - acc: 0.8141 - val_loss: 0.4513 - val_acc: 0.8046\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4195 - acc: 0.8181 - val_loss: 0.4511 - val_acc: 0.8079\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4179 - acc: 0.8210 - val_loss: 0.4505 - val_acc: 0.8013\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4187 - acc: 0.8181 - val_loss: 0.4502 - val_acc: 0.8046\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4164 - acc: 0.8146 - val_loss: 0.4504 - val_acc: 0.8062\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4159 - acc: 0.8185 - val_loss: 0.4502 - val_acc: 0.8062\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4197 - acc: 0.8183 - val_loss: 0.4500 - val_acc: 0.8046\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4164 - acc: 0.8210 - val_loss: 0.4500 - val_acc: 0.8128\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4160 - acc: 0.8208 - val_loss: 0.4497 - val_acc: 0.8128\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4147 - acc: 0.8197 - val_loss: 0.4490 - val_acc: 0.8046\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4140 - acc: 0.8197 - val_loss: 0.4488 - val_acc: 0.8046\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4121 - acc: 0.8221 - val_loss: 0.4488 - val_acc: 0.8095\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4137 - acc: 0.8232 - val_loss: 0.4485 - val_acc: 0.8095\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4111 - acc: 0.8236 - val_loss: 0.4480 - val_acc: 0.8095\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4108 - acc: 0.8243 - val_loss: 0.4481 - val_acc: 0.8128\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4115 - acc: 0.8214 - val_loss: 0.4477 - val_acc: 0.8161\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4118 - acc: 0.8219 - val_loss: 0.4473 - val_acc: 0.8079\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4096 - acc: 0.8203 - val_loss: 0.4473 - val_acc: 0.8112\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4119 - acc: 0.8212 - val_loss: 0.4477 - val_acc: 0.8128\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4102 - acc: 0.8217 - val_loss: 0.4474 - val_acc: 0.8144\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4078 - acc: 0.8234 - val_loss: 0.4471 - val_acc: 0.8128\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4096 - acc: 0.8225 - val_loss: 0.4469 - val_acc: 0.8128\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4084 - acc: 0.8263 - val_loss: 0.4469 - val_acc: 0.8128\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4098 - acc: 0.8230 - val_loss: 0.4473 - val_acc: 0.8112\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4057 - acc: 0.8225 - val_loss: 0.4461 - val_acc: 0.8161\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4065 - acc: 0.8207 - val_loss: 0.4459 - val_acc: 0.8144\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4047 - acc: 0.8230 - val_loss: 0.4460 - val_acc: 0.8144\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4049 - acc: 0.8239 - val_loss: 0.4459 - val_acc: 0.8144\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4045 - acc: 0.8236 - val_loss: 0.4456 - val_acc: 0.8144\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4050 - acc: 0.8225 - val_loss: 0.4460 - val_acc: 0.8128\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4039 - acc: 0.8258 - val_loss: 0.4459 - val_acc: 0.8112\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4020 - acc: 0.8265 - val_loss: 0.4452 - val_acc: 0.8144\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4006 - acc: 0.8301 - val_loss: 0.4449 - val_acc: 0.8144\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3975 - acc: 0.8285 - val_loss: 0.4452 - val_acc: 0.8128\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4024 - acc: 0.8265 - val_loss: 0.4446 - val_acc: 0.8144\n",
      "Epoch 114/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4025 - acc: 0.8236 - val_loss: 0.4445 - val_acc: 0.8144\n",
      "Epoch 115/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4004 - acc: 0.8258 - val_loss: 0.4447 - val_acc: 0.8128\n",
      "Epoch 116/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4011 - acc: 0.8301 - val_loss: 0.4446 - val_acc: 0.8095\n",
      "Epoch 117/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4003 - acc: 0.8258 - val_loss: 0.4440 - val_acc: 0.8161\n",
      "Epoch 118/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3990 - acc: 0.8289 - val_loss: 0.4441 - val_acc: 0.8112\n",
      "Epoch 119/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4008 - acc: 0.8269 - val_loss: 0.4443 - val_acc: 0.8079\n",
      "Epoch 120/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4009 - acc: 0.8248 - val_loss: 0.4439 - val_acc: 0.8112\n",
      "Epoch 121/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3986 - acc: 0.8269 - val_loss: 0.4438 - val_acc: 0.8062\n",
      "Epoch 122/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3987 - acc: 0.8265 - val_loss: 0.4435 - val_acc: 0.8062\n",
      "Epoch 123/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3974 - acc: 0.8265 - val_loss: 0.4430 - val_acc: 0.8062\n",
      "Epoch 124/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3978 - acc: 0.8312 - val_loss: 0.4428 - val_acc: 0.8046\n",
      "Epoch 125/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3964 - acc: 0.8303 - val_loss: 0.4428 - val_acc: 0.8062\n",
      "Epoch 126/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3965 - acc: 0.8296 - val_loss: 0.4429 - val_acc: 0.8079\n",
      "Epoch 127/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3953 - acc: 0.8280 - val_loss: 0.4429 - val_acc: 0.8046\n",
      "Epoch 128/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3957 - acc: 0.8278 - val_loss: 0.4437 - val_acc: 0.8079\n",
      "Epoch 129/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3956 - acc: 0.8285 - val_loss: 0.4433 - val_acc: 0.8079\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.35, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_115\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_116 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_328 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_213 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_329 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_214 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_330 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.8201 - acc: 0.4377 - val_loss: 0.7079 - val_acc: 0.4499\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.7123 - acc: 0.5025 - val_loss: 0.6652 - val_acc: 0.5862\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6857 - acc: 0.5641 - val_loss: 0.6518 - val_acc: 0.5846\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6657 - acc: 0.5857 - val_loss: 0.6317 - val_acc: 0.5895\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6415 - acc: 0.6216 - val_loss: 0.6152 - val_acc: 0.7209\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.6260 - acc: 0.6493 - val_loss: 0.6063 - val_acc: 0.6979\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.6109 - acc: 0.6818 - val_loss: 0.5969 - val_acc: 0.6979\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.5968 - acc: 0.6887 - val_loss: 0.5840 - val_acc: 0.7209\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.5816 - acc: 0.7141 - val_loss: 0.5720 - val_acc: 0.7406\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.5692 - acc: 0.7198 - val_loss: 0.5617 - val_acc: 0.7553\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.5604 - acc: 0.7331 - val_loss: 0.5509 - val_acc: 0.7553\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.5492 - acc: 0.7422 - val_loss: 0.5406 - val_acc: 0.7488\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.5434 - acc: 0.7404 - val_loss: 0.5307 - val_acc: 0.7537\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.5344 - acc: 0.7480 - val_loss: 0.5223 - val_acc: 0.7570\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.5205 - acc: 0.7604 - val_loss: 0.5136 - val_acc: 0.7619\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.5116 - acc: 0.7608 - val_loss: 0.5063 - val_acc: 0.7635\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.5060 - acc: 0.7698 - val_loss: 0.4995 - val_acc: 0.7619\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4952 - acc: 0.7703 - val_loss: 0.4934 - val_acc: 0.7619\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4895 - acc: 0.7805 - val_loss: 0.4882 - val_acc: 0.7685\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4839 - acc: 0.7776 - val_loss: 0.4822 - val_acc: 0.7701\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4798 - acc: 0.7814 - val_loss: 0.4767 - val_acc: 0.7718\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4739 - acc: 0.7913 - val_loss: 0.4733 - val_acc: 0.7849\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4662 - acc: 0.7889 - val_loss: 0.4680 - val_acc: 0.7750\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4628 - acc: 0.7947 - val_loss: 0.4647 - val_acc: 0.7816\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4593 - acc: 0.7937 - val_loss: 0.4624 - val_acc: 0.7865\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4575 - acc: 0.7975 - val_loss: 0.4594 - val_acc: 0.7849\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4566 - acc: 0.7995 - val_loss: 0.4575 - val_acc: 0.7849\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4458 - acc: 0.8046 - val_loss: 0.4543 - val_acc: 0.7882\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4396 - acc: 0.8082 - val_loss: 0.4525 - val_acc: 0.7898\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4442 - acc: 0.8055 - val_loss: 0.4514 - val_acc: 0.7931\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4404 - acc: 0.8064 - val_loss: 0.4482 - val_acc: 0.7964\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4393 - acc: 0.8057 - val_loss: 0.4478 - val_acc: 0.7931\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4334 - acc: 0.8112 - val_loss: 0.4477 - val_acc: 0.7898\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4324 - acc: 0.8101 - val_loss: 0.4477 - val_acc: 0.7931\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4306 - acc: 0.8135 - val_loss: 0.4458 - val_acc: 0.7915\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.4308 - acc: 0.8112 - val_loss: 0.4438 - val_acc: 0.7947\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4275 - acc: 0.8159 - val_loss: 0.4439 - val_acc: 0.7964\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4245 - acc: 0.8163 - val_loss: 0.4428 - val_acc: 0.7997\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4257 - acc: 0.8181 - val_loss: 0.4432 - val_acc: 0.8013\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4232 - acc: 0.8181 - val_loss: 0.4411 - val_acc: 0.7964\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4197 - acc: 0.8183 - val_loss: 0.4424 - val_acc: 0.8079\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4200 - acc: 0.8207 - val_loss: 0.4391 - val_acc: 0.7947\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4189 - acc: 0.8168 - val_loss: 0.4391 - val_acc: 0.7997\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4152 - acc: 0.8170 - val_loss: 0.4411 - val_acc: 0.8128\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4139 - acc: 0.8225 - val_loss: 0.4386 - val_acc: 0.8079\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4073 - acc: 0.8265 - val_loss: 0.4375 - val_acc: 0.8062\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4119 - acc: 0.8205 - val_loss: 0.4371 - val_acc: 0.8062\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4073 - acc: 0.8219 - val_loss: 0.4376 - val_acc: 0.8112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4087 - acc: 0.8247 - val_loss: 0.4370 - val_acc: 0.8079\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4046 - acc: 0.8276 - val_loss: 0.4378 - val_acc: 0.8095\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3982 - acc: 0.8281 - val_loss: 0.4364 - val_acc: 0.8062\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.4021 - acc: 0.8245 - val_loss: 0.4371 - val_acc: 0.8112\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4014 - acc: 0.8228 - val_loss: 0.4358 - val_acc: 0.8062\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3966 - acc: 0.8261 - val_loss: 0.4361 - val_acc: 0.8079\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3979 - acc: 0.8318 - val_loss: 0.4346 - val_acc: 0.8062\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3938 - acc: 0.8312 - val_loss: 0.4350 - val_acc: 0.8079\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3914 - acc: 0.8340 - val_loss: 0.4338 - val_acc: 0.8079\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.3941 - acc: 0.8269 - val_loss: 0.4356 - val_acc: 0.8062\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3894 - acc: 0.8367 - val_loss: 0.4344 - val_acc: 0.8095\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3906 - acc: 0.8294 - val_loss: 0.4345 - val_acc: 0.8062\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3896 - acc: 0.8342 - val_loss: 0.4340 - val_acc: 0.8062\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3876 - acc: 0.8354 - val_loss: 0.4336 - val_acc: 0.8062\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3863 - acc: 0.8314 - val_loss: 0.4342 - val_acc: 0.8095\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3847 - acc: 0.8343 - val_loss: 0.4357 - val_acc: 0.8062\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3880 - acc: 0.8329 - val_loss: 0.4334 - val_acc: 0.8095\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.3799 - acc: 0.8376 - val_loss: 0.4327 - val_acc: 0.8095\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3783 - acc: 0.8378 - val_loss: 0.4326 - val_acc: 0.8062\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3804 - acc: 0.8393 - val_loss: 0.4335 - val_acc: 0.8062\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3747 - acc: 0.8411 - val_loss: 0.4348 - val_acc: 0.8079\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.3763 - acc: 0.8369 - val_loss: 0.4334 - val_acc: 0.8079\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.3729 - acc: 0.8424 - val_loss: 0.4354 - val_acc: 0.8079\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3717 - acc: 0.8404 - val_loss: 0.4333 - val_acc: 0.8062\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.35, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_116\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_117 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_331 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_215 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_332 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_216 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_333 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 1s 34ms/step - loss: 0.8834 - acc: 0.4373 - val_loss: 0.7650 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.7754 - acc: 0.4537 - val_loss: 0.7011 - val_acc: 0.4581\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.7168 - acc: 0.4970 - val_loss: 0.6715 - val_acc: 0.6305\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6897 - acc: 0.5499 - val_loss: 0.6578 - val_acc: 0.6043\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6743 - acc: 0.5778 - val_loss: 0.6481 - val_acc: 0.6043\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6588 - acc: 0.6085 - val_loss: 0.6393 - val_acc: 0.6207\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6496 - acc: 0.6269 - val_loss: 0.6305 - val_acc: 0.6700\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6367 - acc: 0.6366 - val_loss: 0.6217 - val_acc: 0.7373\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6307 - acc: 0.6459 - val_loss: 0.6130 - val_acc: 0.7455\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6211 - acc: 0.6676 - val_loss: 0.6043 - val_acc: 0.7455\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6097 - acc: 0.6793 - val_loss: 0.5955 - val_acc: 0.7406\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6012 - acc: 0.6898 - val_loss: 0.5872 - val_acc: 0.7488\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5963 - acc: 0.6973 - val_loss: 0.5794 - val_acc: 0.7586\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.5896 - acc: 0.7086 - val_loss: 0.5720 - val_acc: 0.7586\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5849 - acc: 0.7086 - val_loss: 0.5648 - val_acc: 0.7553\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5767 - acc: 0.7070 - val_loss: 0.5579 - val_acc: 0.7553\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5640 - acc: 0.7305 - val_loss: 0.5507 - val_acc: 0.7553\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5616 - acc: 0.7302 - val_loss: 0.5435 - val_acc: 0.7603\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5554 - acc: 0.7280 - val_loss: 0.5363 - val_acc: 0.7635\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5451 - acc: 0.7429 - val_loss: 0.5297 - val_acc: 0.7701\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5449 - acc: 0.7446 - val_loss: 0.5240 - val_acc: 0.7701\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5336 - acc: 0.7493 - val_loss: 0.5189 - val_acc: 0.7701\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.5354 - acc: 0.7444 - val_loss: 0.5142 - val_acc: 0.7685\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5236 - acc: 0.7555 - val_loss: 0.5097 - val_acc: 0.7668\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5247 - acc: 0.7542 - val_loss: 0.5054 - val_acc: 0.7734\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5154 - acc: 0.7617 - val_loss: 0.5011 - val_acc: 0.7750\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5069 - acc: 0.7643 - val_loss: 0.4972 - val_acc: 0.7767\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5085 - acc: 0.7710 - val_loss: 0.4944 - val_acc: 0.7783\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5057 - acc: 0.7670 - val_loss: 0.4909 - val_acc: 0.7800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4982 - acc: 0.7738 - val_loss: 0.4880 - val_acc: 0.7800\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4949 - acc: 0.7772 - val_loss: 0.4854 - val_acc: 0.7833\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4928 - acc: 0.7750 - val_loss: 0.4832 - val_acc: 0.7849\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4906 - acc: 0.7794 - val_loss: 0.4808 - val_acc: 0.7833\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4914 - acc: 0.7761 - val_loss: 0.4787 - val_acc: 0.7833\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4816 - acc: 0.7887 - val_loss: 0.4776 - val_acc: 0.7865\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4773 - acc: 0.7865 - val_loss: 0.4748 - val_acc: 0.7800\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4803 - acc: 0.7869 - val_loss: 0.4729 - val_acc: 0.7800\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4759 - acc: 0.7854 - val_loss: 0.4709 - val_acc: 0.7816\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4790 - acc: 0.7860 - val_loss: 0.4693 - val_acc: 0.7849\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4701 - acc: 0.7911 - val_loss: 0.4678 - val_acc: 0.7816\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4717 - acc: 0.7882 - val_loss: 0.4667 - val_acc: 0.7849\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4738 - acc: 0.7893 - val_loss: 0.4641 - val_acc: 0.7849\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4692 - acc: 0.7937 - val_loss: 0.4616 - val_acc: 0.7833\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4628 - acc: 0.7973 - val_loss: 0.4605 - val_acc: 0.7849\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4645 - acc: 0.7922 - val_loss: 0.4593 - val_acc: 0.7849\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4581 - acc: 0.7980 - val_loss: 0.4585 - val_acc: 0.7849\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4547 - acc: 0.8008 - val_loss: 0.4573 - val_acc: 0.7849\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4546 - acc: 0.8037 - val_loss: 0.4558 - val_acc: 0.7865\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4544 - acc: 0.8035 - val_loss: 0.4553 - val_acc: 0.7915\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4512 - acc: 0.8024 - val_loss: 0.4538 - val_acc: 0.7915\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4543 - acc: 0.8042 - val_loss: 0.4521 - val_acc: 0.7882\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4498 - acc: 0.8050 - val_loss: 0.4515 - val_acc: 0.7915\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4551 - acc: 0.8033 - val_loss: 0.4517 - val_acc: 0.7964\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4518 - acc: 0.8017 - val_loss: 0.4496 - val_acc: 0.7898\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4462 - acc: 0.8046 - val_loss: 0.4498 - val_acc: 0.7964\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4446 - acc: 0.8095 - val_loss: 0.4504 - val_acc: 0.7947\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4439 - acc: 0.8081 - val_loss: 0.4481 - val_acc: 0.7964\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4413 - acc: 0.8037 - val_loss: 0.4472 - val_acc: 0.7980\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4405 - acc: 0.8084 - val_loss: 0.4476 - val_acc: 0.8013\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4398 - acc: 0.8077 - val_loss: 0.4461 - val_acc: 0.7980\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4376 - acc: 0.8064 - val_loss: 0.4452 - val_acc: 0.8030\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4383 - acc: 0.8101 - val_loss: 0.4450 - val_acc: 0.8030\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4333 - acc: 0.8139 - val_loss: 0.4446 - val_acc: 0.8013\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4375 - acc: 0.8104 - val_loss: 0.4443 - val_acc: 0.8013\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4368 - acc: 0.8117 - val_loss: 0.4454 - val_acc: 0.8046\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4299 - acc: 0.8130 - val_loss: 0.4428 - val_acc: 0.8013\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4347 - acc: 0.8090 - val_loss: 0.4424 - val_acc: 0.8030\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4289 - acc: 0.8150 - val_loss: 0.4453 - val_acc: 0.8079\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4269 - acc: 0.8181 - val_loss: 0.4420 - val_acc: 0.8030\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4303 - acc: 0.8148 - val_loss: 0.4410 - val_acc: 0.8062\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4229 - acc: 0.8146 - val_loss: 0.4421 - val_acc: 0.8095\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4215 - acc: 0.8177 - val_loss: 0.4422 - val_acc: 0.8095\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4278 - acc: 0.8161 - val_loss: 0.4410 - val_acc: 0.8095\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4251 - acc: 0.8172 - val_loss: 0.4405 - val_acc: 0.8095\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4235 - acc: 0.8170 - val_loss: 0.4403 - val_acc: 0.8095\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4162 - acc: 0.8190 - val_loss: 0.4402 - val_acc: 0.8095\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4198 - acc: 0.8168 - val_loss: 0.4398 - val_acc: 0.8128\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4191 - acc: 0.8196 - val_loss: 0.4383 - val_acc: 0.8095\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4207 - acc: 0.8172 - val_loss: 0.4387 - val_acc: 0.8095\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4156 - acc: 0.8207 - val_loss: 0.4398 - val_acc: 0.8144\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4197 - acc: 0.8161 - val_loss: 0.4380 - val_acc: 0.8128\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4155 - acc: 0.8225 - val_loss: 0.4384 - val_acc: 0.8144\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4149 - acc: 0.8190 - val_loss: 0.4380 - val_acc: 0.8161\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4165 - acc: 0.8166 - val_loss: 0.4371 - val_acc: 0.8128\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4167 - acc: 0.8199 - val_loss: 0.4374 - val_acc: 0.8128\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4105 - acc: 0.8236 - val_loss: 0.4361 - val_acc: 0.8144\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4103 - acc: 0.8259 - val_loss: 0.4367 - val_acc: 0.8128\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4109 - acc: 0.8254 - val_loss: 0.4368 - val_acc: 0.8128\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4109 - acc: 0.8245 - val_loss: 0.4362 - val_acc: 0.8128\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4120 - acc: 0.8214 - val_loss: 0.4363 - val_acc: 0.8144\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4064 - acc: 0.8263 - val_loss: 0.4355 - val_acc: 0.8112\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4075 - acc: 0.8238 - val_loss: 0.4348 - val_acc: 0.8095\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4063 - acc: 0.8285 - val_loss: 0.4366 - val_acc: 0.8112\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4083 - acc: 0.8248 - val_loss: 0.4360 - val_acc: 0.8095\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4061 - acc: 0.8283 - val_loss: 0.4356 - val_acc: 0.8095\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4092 - acc: 0.8219 - val_loss: 0.4349 - val_acc: 0.8112\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4031 - acc: 0.8285 - val_loss: 0.4349 - val_acc: 0.8112\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.35, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_117\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_118 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_334 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_217 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_335 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_218 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_336 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.7635 - acc: 0.4521 - val_loss: 0.7149 - val_acc: 0.4154\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.7223 - acc: 0.4844 - val_loss: 0.6777 - val_acc: 0.6043\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6876 - acc: 0.5441 - val_loss: 0.6583 - val_acc: 0.6470\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6774 - acc: 0.5727 - val_loss: 0.6482 - val_acc: 0.5977\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6593 - acc: 0.6032 - val_loss: 0.6406 - val_acc: 0.5928\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6508 - acc: 0.6172 - val_loss: 0.6322 - val_acc: 0.6043\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6405 - acc: 0.6327 - val_loss: 0.6232 - val_acc: 0.6601\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6330 - acc: 0.6548 - val_loss: 0.6145 - val_acc: 0.7209\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6227 - acc: 0.6696 - val_loss: 0.6065 - val_acc: 0.7373\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6142 - acc: 0.6847 - val_loss: 0.5985 - val_acc: 0.7455\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6067 - acc: 0.6851 - val_loss: 0.5907 - val_acc: 0.7471\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6006 - acc: 0.6937 - val_loss: 0.5826 - val_acc: 0.7504\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5888 - acc: 0.7048 - val_loss: 0.5751 - val_acc: 0.7521\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5813 - acc: 0.7192 - val_loss: 0.5683 - val_acc: 0.7521\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5721 - acc: 0.7212 - val_loss: 0.5614 - val_acc: 0.7553\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5676 - acc: 0.7283 - val_loss: 0.5545 - val_acc: 0.7521\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5586 - acc: 0.7323 - val_loss: 0.5476 - val_acc: 0.7586\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5486 - acc: 0.7384 - val_loss: 0.5410 - val_acc: 0.7635\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5448 - acc: 0.7431 - val_loss: 0.5347 - val_acc: 0.7635\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5431 - acc: 0.7422 - val_loss: 0.5288 - val_acc: 0.7685\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5305 - acc: 0.7555 - val_loss: 0.5232 - val_acc: 0.7718\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5284 - acc: 0.7553 - val_loss: 0.5181 - val_acc: 0.7701\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5250 - acc: 0.7603 - val_loss: 0.5132 - val_acc: 0.7652\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5155 - acc: 0.7626 - val_loss: 0.5087 - val_acc: 0.7734\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5155 - acc: 0.7654 - val_loss: 0.5045 - val_acc: 0.7718\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5090 - acc: 0.7687 - val_loss: 0.5007 - val_acc: 0.7718\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5032 - acc: 0.7718 - val_loss: 0.4975 - val_acc: 0.7685\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4940 - acc: 0.7791 - val_loss: 0.4942 - val_acc: 0.7718\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4975 - acc: 0.7783 - val_loss: 0.4911 - val_acc: 0.7701\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4943 - acc: 0.7769 - val_loss: 0.4883 - val_acc: 0.7718\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4846 - acc: 0.7814 - val_loss: 0.4859 - val_acc: 0.7767\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4860 - acc: 0.7818 - val_loss: 0.4823 - val_acc: 0.7750\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4890 - acc: 0.7780 - val_loss: 0.4799 - val_acc: 0.7734\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4802 - acc: 0.7849 - val_loss: 0.4781 - val_acc: 0.7816\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4742 - acc: 0.7873 - val_loss: 0.4761 - val_acc: 0.7816\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4676 - acc: 0.7909 - val_loss: 0.4738 - val_acc: 0.7816\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4726 - acc: 0.7957 - val_loss: 0.4721 - val_acc: 0.7800\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4695 - acc: 0.7938 - val_loss: 0.4707 - val_acc: 0.7865\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4713 - acc: 0.7882 - val_loss: 0.4690 - val_acc: 0.7849\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4663 - acc: 0.7973 - val_loss: 0.4672 - val_acc: 0.7882\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4617 - acc: 0.7957 - val_loss: 0.4656 - val_acc: 0.7882\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4596 - acc: 0.7964 - val_loss: 0.4641 - val_acc: 0.7898\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4577 - acc: 0.8004 - val_loss: 0.4622 - val_acc: 0.7865\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4550 - acc: 0.7988 - val_loss: 0.4610 - val_acc: 0.7915\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4568 - acc: 0.8011 - val_loss: 0.4601 - val_acc: 0.7931\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4524 - acc: 0.7984 - val_loss: 0.4588 - val_acc: 0.7964\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4492 - acc: 0.8035 - val_loss: 0.4578 - val_acc: 0.7931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4482 - acc: 0.8050 - val_loss: 0.4572 - val_acc: 0.7964\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4475 - acc: 0.8039 - val_loss: 0.4558 - val_acc: 0.7980\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4451 - acc: 0.8070 - val_loss: 0.4557 - val_acc: 0.7947\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4454 - acc: 0.8041 - val_loss: 0.4547 - val_acc: 0.7980\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4391 - acc: 0.8103 - val_loss: 0.4537 - val_acc: 0.7980\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4437 - acc: 0.8064 - val_loss: 0.4533 - val_acc: 0.7997\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4382 - acc: 0.8057 - val_loss: 0.4521 - val_acc: 0.8030\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4392 - acc: 0.8081 - val_loss: 0.4511 - val_acc: 0.8046\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4400 - acc: 0.8055 - val_loss: 0.4503 - val_acc: 0.8062\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4332 - acc: 0.8099 - val_loss: 0.4503 - val_acc: 0.8046\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4365 - acc: 0.8121 - val_loss: 0.4495 - val_acc: 0.8062\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4312 - acc: 0.8132 - val_loss: 0.4491 - val_acc: 0.8079\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4278 - acc: 0.8143 - val_loss: 0.4486 - val_acc: 0.8079\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4315 - acc: 0.8135 - val_loss: 0.4482 - val_acc: 0.8062\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4316 - acc: 0.8079 - val_loss: 0.4496 - val_acc: 0.8079\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4280 - acc: 0.8123 - val_loss: 0.4468 - val_acc: 0.8046\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4257 - acc: 0.8112 - val_loss: 0.4464 - val_acc: 0.8062\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4285 - acc: 0.8150 - val_loss: 0.4457 - val_acc: 0.8062\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4221 - acc: 0.8179 - val_loss: 0.4454 - val_acc: 0.8095\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4258 - acc: 0.8132 - val_loss: 0.4444 - val_acc: 0.8095\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4214 - acc: 0.8216 - val_loss: 0.4445 - val_acc: 0.8095\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4255 - acc: 0.8179 - val_loss: 0.4435 - val_acc: 0.8095\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4183 - acc: 0.8208 - val_loss: 0.4434 - val_acc: 0.8112\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4224 - acc: 0.8181 - val_loss: 0.4435 - val_acc: 0.8128\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4220 - acc: 0.8185 - val_loss: 0.4420 - val_acc: 0.8128\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4162 - acc: 0.8238 - val_loss: 0.4413 - val_acc: 0.8128\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4166 - acc: 0.8194 - val_loss: 0.4415 - val_acc: 0.8144\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4187 - acc: 0.8214 - val_loss: 0.4423 - val_acc: 0.8128\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4134 - acc: 0.8190 - val_loss: 0.4412 - val_acc: 0.8128\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4180 - acc: 0.8196 - val_loss: 0.4408 - val_acc: 0.8144\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4111 - acc: 0.8205 - val_loss: 0.4416 - val_acc: 0.8095\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4093 - acc: 0.8181 - val_loss: 0.4400 - val_acc: 0.8210\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4108 - acc: 0.8205 - val_loss: 0.4389 - val_acc: 0.8161\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4088 - acc: 0.8248 - val_loss: 0.4390 - val_acc: 0.8194\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4073 - acc: 0.8210 - val_loss: 0.4392 - val_acc: 0.8194\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4086 - acc: 0.8199 - val_loss: 0.4385 - val_acc: 0.8177\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4055 - acc: 0.8232 - val_loss: 0.4387 - val_acc: 0.8177\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4055 - acc: 0.8256 - val_loss: 0.4387 - val_acc: 0.8161\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4081 - acc: 0.8228 - val_loss: 0.4373 - val_acc: 0.8177\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4037 - acc: 0.8263 - val_loss: 0.4374 - val_acc: 0.8177\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4030 - acc: 0.8285 - val_loss: 0.4384 - val_acc: 0.8177\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4005 - acc: 0.8296 - val_loss: 0.4374 - val_acc: 0.8177\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3999 - acc: 0.8245 - val_loss: 0.4369 - val_acc: 0.8177\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3993 - acc: 0.8289 - val_loss: 0.4364 - val_acc: 0.8161\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4017 - acc: 0.8307 - val_loss: 0.4364 - val_acc: 0.8194\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3942 - acc: 0.8280 - val_loss: 0.4357 - val_acc: 0.8194\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3981 - acc: 0.8296 - val_loss: 0.4370 - val_acc: 0.8161\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4005 - acc: 0.8258 - val_loss: 0.4376 - val_acc: 0.8161\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3951 - acc: 0.8276 - val_loss: 0.4361 - val_acc: 0.8161\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3971 - acc: 0.8296 - val_loss: 0.4354 - val_acc: 0.8144\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3920 - acc: 0.8316 - val_loss: 0.4358 - val_acc: 0.8144\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3932 - acc: 0.8307 - val_loss: 0.4351 - val_acc: 0.8144\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3961 - acc: 0.8265 - val_loss: 0.4355 - val_acc: 0.8144\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3924 - acc: 0.8287 - val_loss: 0.4364 - val_acc: 0.8161\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3927 - acc: 0.8356 - val_loss: 0.4360 - val_acc: 0.8161\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3924 - acc: 0.8307 - val_loss: 0.4353 - val_acc: 0.8128\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3868 - acc: 0.8312 - val_loss: 0.4355 - val_acc: 0.8144\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.35, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_118\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_119 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_337 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_219 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_338 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_220 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_339 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.8413 - acc: 0.4459 - val_loss: 0.7430 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7431 - acc: 0.4822 - val_loss: 0.6837 - val_acc: 0.5961\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7024 - acc: 0.5255 - val_loss: 0.6608 - val_acc: 0.5862\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6719 - acc: 0.5953 - val_loss: 0.6514 - val_acc: 0.5846\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6667 - acc: 0.6004 - val_loss: 0.6404 - val_acc: 0.5862\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6514 - acc: 0.6260 - val_loss: 0.6271 - val_acc: 0.6190\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.6358 - acc: 0.6408 - val_loss: 0.6159 - val_acc: 0.7077\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.6228 - acc: 0.6519 - val_loss: 0.6073 - val_acc: 0.7323\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.6108 - acc: 0.6694 - val_loss: 0.5994 - val_acc: 0.7258\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.6043 - acc: 0.6804 - val_loss: 0.5911 - val_acc: 0.7389\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5926 - acc: 0.6882 - val_loss: 0.5829 - val_acc: 0.7471\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.5882 - acc: 0.6951 - val_loss: 0.5749 - val_acc: 0.7488\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5782 - acc: 0.7072 - val_loss: 0.5663 - val_acc: 0.7504\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5698 - acc: 0.7147 - val_loss: 0.5577 - val_acc: 0.7521\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5601 - acc: 0.7267 - val_loss: 0.5492 - val_acc: 0.7488\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.5477 - acc: 0.7393 - val_loss: 0.5406 - val_acc: 0.7570\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5405 - acc: 0.7460 - val_loss: 0.5326 - val_acc: 0.7685\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5327 - acc: 0.7464 - val_loss: 0.5244 - val_acc: 0.7800\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.5203 - acc: 0.7539 - val_loss: 0.5166 - val_acc: 0.7718\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5170 - acc: 0.7604 - val_loss: 0.5103 - val_acc: 0.7767\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5040 - acc: 0.7670 - val_loss: 0.5036 - val_acc: 0.7783\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.5021 - acc: 0.7714 - val_loss: 0.4993 - val_acc: 0.7701\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4989 - acc: 0.7714 - val_loss: 0.4949 - val_acc: 0.7734\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4933 - acc: 0.7723 - val_loss: 0.4905 - val_acc: 0.7767\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4868 - acc: 0.7772 - val_loss: 0.4872 - val_acc: 0.7816\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4761 - acc: 0.7865 - val_loss: 0.4838 - val_acc: 0.7816\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4752 - acc: 0.7843 - val_loss: 0.4792 - val_acc: 0.7833\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4769 - acc: 0.7851 - val_loss: 0.4769 - val_acc: 0.7865\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4681 - acc: 0.7913 - val_loss: 0.4743 - val_acc: 0.7865\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4660 - acc: 0.7949 - val_loss: 0.4712 - val_acc: 0.7865\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4590 - acc: 0.7957 - val_loss: 0.4690 - val_acc: 0.7882\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4571 - acc: 0.7980 - val_loss: 0.4671 - val_acc: 0.7898\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4559 - acc: 0.7984 - val_loss: 0.4662 - val_acc: 0.7915\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4540 - acc: 0.7955 - val_loss: 0.4625 - val_acc: 0.7865\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4472 - acc: 0.8030 - val_loss: 0.4627 - val_acc: 0.7947\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4499 - acc: 0.8048 - val_loss: 0.4616 - val_acc: 0.7931\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4477 - acc: 0.8008 - val_loss: 0.4602 - val_acc: 0.7849\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4471 - acc: 0.8013 - val_loss: 0.4597 - val_acc: 0.7882\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4435 - acc: 0.8030 - val_loss: 0.4587 - val_acc: 0.7947\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4412 - acc: 0.8097 - val_loss: 0.4565 - val_acc: 0.7964\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4423 - acc: 0.8020 - val_loss: 0.4563 - val_acc: 0.7997\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4401 - acc: 0.8061 - val_loss: 0.4537 - val_acc: 0.7931\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4359 - acc: 0.8081 - val_loss: 0.4533 - val_acc: 0.7997\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4322 - acc: 0.8072 - val_loss: 0.4529 - val_acc: 0.7997\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4271 - acc: 0.8113 - val_loss: 0.4528 - val_acc: 0.8030\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4313 - acc: 0.8119 - val_loss: 0.4516 - val_acc: 0.8062\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4293 - acc: 0.8135 - val_loss: 0.4494 - val_acc: 0.7997\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4286 - acc: 0.8112 - val_loss: 0.4506 - val_acc: 0.8062\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4273 - acc: 0.8119 - val_loss: 0.4484 - val_acc: 0.8013\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4220 - acc: 0.8152 - val_loss: 0.4477 - val_acc: 0.8062\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4205 - acc: 0.8132 - val_loss: 0.4466 - val_acc: 0.8095\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4181 - acc: 0.8197 - val_loss: 0.4470 - val_acc: 0.8079\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4215 - acc: 0.8124 - val_loss: 0.4464 - val_acc: 0.8079\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4187 - acc: 0.8152 - val_loss: 0.4449 - val_acc: 0.8062\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4163 - acc: 0.8186 - val_loss: 0.4477 - val_acc: 0.8079\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4134 - acc: 0.8163 - val_loss: 0.4445 - val_acc: 0.8095\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4153 - acc: 0.8170 - val_loss: 0.4440 - val_acc: 0.8079\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4079 - acc: 0.8205 - val_loss: 0.4428 - val_acc: 0.8062\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4056 - acc: 0.8199 - val_loss: 0.4432 - val_acc: 0.8062\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4086 - acc: 0.8227 - val_loss: 0.4446 - val_acc: 0.8046\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4103 - acc: 0.8188 - val_loss: 0.4441 - val_acc: 0.8046\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4055 - acc: 0.8259 - val_loss: 0.4424 - val_acc: 0.8062\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4054 - acc: 0.8228 - val_loss: 0.4430 - val_acc: 0.8095\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4030 - acc: 0.8234 - val_loss: 0.4430 - val_acc: 0.8079\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4041 - acc: 0.8217 - val_loss: 0.4418 - val_acc: 0.8079\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4034 - acc: 0.8265 - val_loss: 0.4419 - val_acc: 0.8079\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4009 - acc: 0.8270 - val_loss: 0.4402 - val_acc: 0.8079\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4000 - acc: 0.8294 - val_loss: 0.4398 - val_acc: 0.8046\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4022 - acc: 0.8267 - val_loss: 0.4393 - val_acc: 0.8095\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3960 - acc: 0.8285 - val_loss: 0.4395 - val_acc: 0.8095\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3929 - acc: 0.8305 - val_loss: 0.4380 - val_acc: 0.8112\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3955 - acc: 0.8285 - val_loss: 0.4386 - val_acc: 0.8095\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.3931 - acc: 0.8312 - val_loss: 0.4367 - val_acc: 0.8062\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3887 - acc: 0.8325 - val_loss: 0.4374 - val_acc: 0.8095\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3914 - acc: 0.8300 - val_loss: 0.4392 - val_acc: 0.8095\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3903 - acc: 0.8311 - val_loss: 0.4387 - val_acc: 0.8095\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3892 - acc: 0.8338 - val_loss: 0.4369 - val_acc: 0.8062\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3870 - acc: 0.8301 - val_loss: 0.4386 - val_acc: 0.8062\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.4, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_119\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_120 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_340 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_221 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_341 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_222 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_342 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.7618 - acc: 0.4594 - val_loss: 0.6819 - val_acc: 0.5534\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6913 - acc: 0.5369 - val_loss: 0.6665 - val_acc: 0.5846\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6694 - acc: 0.5837 - val_loss: 0.6499 - val_acc: 0.5846\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6472 - acc: 0.6119 - val_loss: 0.6255 - val_acc: 0.6470\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.6257 - acc: 0.6568 - val_loss: 0.6099 - val_acc: 0.7094\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.6097 - acc: 0.6818 - val_loss: 0.5981 - val_acc: 0.7011\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.5993 - acc: 0.6911 - val_loss: 0.5842 - val_acc: 0.7209\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.5793 - acc: 0.7148 - val_loss: 0.5719 - val_acc: 0.7471\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.5646 - acc: 0.7349 - val_loss: 0.5604 - val_acc: 0.7488\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.5550 - acc: 0.7345 - val_loss: 0.5476 - val_acc: 0.7488\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.5431 - acc: 0.7391 - val_loss: 0.5360 - val_acc: 0.7504\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.5325 - acc: 0.7502 - val_loss: 0.5254 - val_acc: 0.7537\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.5209 - acc: 0.7577 - val_loss: 0.5169 - val_acc: 0.7586\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.5100 - acc: 0.7617 - val_loss: 0.5086 - val_acc: 0.7586\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.5025 - acc: 0.7701 - val_loss: 0.5008 - val_acc: 0.7570\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4922 - acc: 0.7725 - val_loss: 0.4950 - val_acc: 0.7635\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.4861 - acc: 0.7798 - val_loss: 0.4920 - val_acc: 0.7767\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4794 - acc: 0.7796 - val_loss: 0.4856 - val_acc: 0.7800\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4809 - acc: 0.7820 - val_loss: 0.4799 - val_acc: 0.7668\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4723 - acc: 0.7853 - val_loss: 0.4774 - val_acc: 0.7783\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4640 - acc: 0.7947 - val_loss: 0.4731 - val_acc: 0.7750\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.4671 - acc: 0.7909 - val_loss: 0.4690 - val_acc: 0.7767\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4612 - acc: 0.7922 - val_loss: 0.4685 - val_acc: 0.7816\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.4493 - acc: 0.8000 - val_loss: 0.4637 - val_acc: 0.7800\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4509 - acc: 0.7986 - val_loss: 0.4612 - val_acc: 0.7849\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4441 - acc: 0.8009 - val_loss: 0.4620 - val_acc: 0.7865\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4429 - acc: 0.8035 - val_loss: 0.4572 - val_acc: 0.7882\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4405 - acc: 0.8121 - val_loss: 0.4567 - val_acc: 0.7915\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4389 - acc: 0.8035 - val_loss: 0.4566 - val_acc: 0.7947\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4364 - acc: 0.8090 - val_loss: 0.4523 - val_acc: 0.7980\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4299 - acc: 0.8146 - val_loss: 0.4525 - val_acc: 0.7964\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4337 - acc: 0.8064 - val_loss: 0.4529 - val_acc: 0.7931\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4298 - acc: 0.8106 - val_loss: 0.4515 - val_acc: 0.7947\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4264 - acc: 0.8128 - val_loss: 0.4484 - val_acc: 0.8030\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4255 - acc: 0.8130 - val_loss: 0.4487 - val_acc: 0.7980\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4247 - acc: 0.8159 - val_loss: 0.4480 - val_acc: 0.7997\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4230 - acc: 0.8177 - val_loss: 0.4459 - val_acc: 0.8046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4218 - acc: 0.8165 - val_loss: 0.4459 - val_acc: 0.8013\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4151 - acc: 0.8190 - val_loss: 0.4459 - val_acc: 0.8013\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 0.4150 - acc: 0.8196 - val_loss: 0.4451 - val_acc: 0.8046\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4165 - acc: 0.8163 - val_loss: 0.4453 - val_acc: 0.8062\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4114 - acc: 0.8197 - val_loss: 0.4429 - val_acc: 0.8062\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.4086 - acc: 0.8263 - val_loss: 0.4429 - val_acc: 0.8079\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4121 - acc: 0.8219 - val_loss: 0.4426 - val_acc: 0.8079\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4062 - acc: 0.8258 - val_loss: 0.4404 - val_acc: 0.8062\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.4049 - acc: 0.8265 - val_loss: 0.4414 - val_acc: 0.8046\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4040 - acc: 0.8250 - val_loss: 0.4405 - val_acc: 0.8062\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4040 - acc: 0.8232 - val_loss: 0.4411 - val_acc: 0.8062\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3996 - acc: 0.8276 - val_loss: 0.4419 - val_acc: 0.8079\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4004 - acc: 0.8274 - val_loss: 0.4403 - val_acc: 0.8079\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3995 - acc: 0.8276 - val_loss: 0.4391 - val_acc: 0.8046\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3977 - acc: 0.8276 - val_loss: 0.4397 - val_acc: 0.8079\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.3948 - acc: 0.8269 - val_loss: 0.4376 - val_acc: 0.8046\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3926 - acc: 0.8267 - val_loss: 0.4398 - val_acc: 0.8095\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3907 - acc: 0.8332 - val_loss: 0.4381 - val_acc: 0.8046\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.3878 - acc: 0.8301 - val_loss: 0.4368 - val_acc: 0.8079\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3882 - acc: 0.8329 - val_loss: 0.4401 - val_acc: 0.8046\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3845 - acc: 0.8332 - val_loss: 0.4360 - val_acc: 0.8079\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.3831 - acc: 0.8351 - val_loss: 0.4373 - val_acc: 0.8030\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3807 - acc: 0.8373 - val_loss: 0.4363 - val_acc: 0.8046\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3823 - acc: 0.8347 - val_loss: 0.4374 - val_acc: 0.8079\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 0.3807 - acc: 0.8351 - val_loss: 0.4393 - val_acc: 0.8046\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3784 - acc: 0.8378 - val_loss: 0.4357 - val_acc: 0.8030\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3755 - acc: 0.8362 - val_loss: 0.4369 - val_acc: 0.8030\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3700 - acc: 0.8411 - val_loss: 0.4350 - val_acc: 0.8030\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3716 - acc: 0.8400 - val_loss: 0.4389 - val_acc: 0.8030\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3711 - acc: 0.8391 - val_loss: 0.4371 - val_acc: 0.8062\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3731 - acc: 0.8387 - val_loss: 0.4359 - val_acc: 0.8046\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.3701 - acc: 0.8393 - val_loss: 0.4363 - val_acc: 0.8062\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3661 - acc: 0.8425 - val_loss: 0.4351 - val_acc: 0.8030\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.4, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_120\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_121 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_343 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_223 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_344 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_224 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_345 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.8488 - acc: 0.4516 - val_loss: 0.7432 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7584 - acc: 0.4808 - val_loss: 0.6841 - val_acc: 0.5846\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.7068 - acc: 0.5225 - val_loss: 0.6616 - val_acc: 0.5846\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6829 - acc: 0.5795 - val_loss: 0.6523 - val_acc: 0.5846\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6736 - acc: 0.5930 - val_loss: 0.6421 - val_acc: 0.5846\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6576 - acc: 0.6225 - val_loss: 0.6300 - val_acc: 0.6043\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6403 - acc: 0.6300 - val_loss: 0.6190 - val_acc: 0.6782\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.6335 - acc: 0.6429 - val_loss: 0.6101 - val_acc: 0.7438\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.6192 - acc: 0.6533 - val_loss: 0.6025 - val_acc: 0.7356\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.6112 - acc: 0.6659 - val_loss: 0.5950 - val_acc: 0.7373\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.6035 - acc: 0.6745 - val_loss: 0.5876 - val_acc: 0.7455\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5910 - acc: 0.6988 - val_loss: 0.5803 - val_acc: 0.7455\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5834 - acc: 0.6990 - val_loss: 0.5724 - val_acc: 0.7537\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5753 - acc: 0.7115 - val_loss: 0.5641 - val_acc: 0.7537\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5664 - acc: 0.7147 - val_loss: 0.5555 - val_acc: 0.7521\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5578 - acc: 0.7314 - val_loss: 0.5475 - val_acc: 0.7570\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.5481 - acc: 0.7418 - val_loss: 0.5391 - val_acc: 0.7570\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.5454 - acc: 0.7449 - val_loss: 0.5314 - val_acc: 0.7685\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5311 - acc: 0.7531 - val_loss: 0.5250 - val_acc: 0.7800\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5242 - acc: 0.7557 - val_loss: 0.5175 - val_acc: 0.7750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.5179 - acc: 0.7562 - val_loss: 0.5112 - val_acc: 0.7685\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5138 - acc: 0.7643 - val_loss: 0.5059 - val_acc: 0.7734\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5059 - acc: 0.7718 - val_loss: 0.5019 - val_acc: 0.7750\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.5017 - acc: 0.7770 - val_loss: 0.4959 - val_acc: 0.7734\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4934 - acc: 0.7769 - val_loss: 0.4914 - val_acc: 0.7783\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4923 - acc: 0.7745 - val_loss: 0.4893 - val_acc: 0.7767\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4852 - acc: 0.7801 - val_loss: 0.4864 - val_acc: 0.7816\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4852 - acc: 0.7780 - val_loss: 0.4814 - val_acc: 0.7800\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4800 - acc: 0.7776 - val_loss: 0.4798 - val_acc: 0.7849\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4740 - acc: 0.7867 - val_loss: 0.4799 - val_acc: 0.7833\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4707 - acc: 0.7885 - val_loss: 0.4754 - val_acc: 0.7849\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4614 - acc: 0.7947 - val_loss: 0.4716 - val_acc: 0.7816\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4687 - acc: 0.7858 - val_loss: 0.4709 - val_acc: 0.7849\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4602 - acc: 0.7966 - val_loss: 0.4702 - val_acc: 0.7865\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4575 - acc: 0.7957 - val_loss: 0.4672 - val_acc: 0.7849\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4582 - acc: 0.7946 - val_loss: 0.4664 - val_acc: 0.7882\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4538 - acc: 0.7986 - val_loss: 0.4638 - val_acc: 0.7882\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4498 - acc: 0.8008 - val_loss: 0.4624 - val_acc: 0.7882\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4507 - acc: 0.8024 - val_loss: 0.4616 - val_acc: 0.7898\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4453 - acc: 0.8026 - val_loss: 0.4603 - val_acc: 0.7898\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4416 - acc: 0.8072 - val_loss: 0.4593 - val_acc: 0.7898\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4402 - acc: 0.8070 - val_loss: 0.4583 - val_acc: 0.7915\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4409 - acc: 0.8028 - val_loss: 0.4563 - val_acc: 0.7915\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4431 - acc: 0.8099 - val_loss: 0.4557 - val_acc: 0.7980\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4415 - acc: 0.8020 - val_loss: 0.4553 - val_acc: 0.7980\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4358 - acc: 0.8117 - val_loss: 0.4541 - val_acc: 0.8030\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4352 - acc: 0.8072 - val_loss: 0.4529 - val_acc: 0.8013\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4346 - acc: 0.8115 - val_loss: 0.4515 - val_acc: 0.7997\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4291 - acc: 0.8126 - val_loss: 0.4525 - val_acc: 0.7964\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4279 - acc: 0.8141 - val_loss: 0.4496 - val_acc: 0.7980\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4258 - acc: 0.8148 - val_loss: 0.4512 - val_acc: 0.7997\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4263 - acc: 0.8144 - val_loss: 0.4499 - val_acc: 0.8030\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4244 - acc: 0.8117 - val_loss: 0.4490 - val_acc: 0.8013\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4259 - acc: 0.8137 - val_loss: 0.4497 - val_acc: 0.8046\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4229 - acc: 0.8188 - val_loss: 0.4477 - val_acc: 0.8046\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4209 - acc: 0.8148 - val_loss: 0.4469 - val_acc: 0.8095\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4195 - acc: 0.8161 - val_loss: 0.4478 - val_acc: 0.8013\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4138 - acc: 0.8194 - val_loss: 0.4457 - val_acc: 0.8079\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4184 - acc: 0.8154 - val_loss: 0.4453 - val_acc: 0.8095\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4145 - acc: 0.8190 - val_loss: 0.4479 - val_acc: 0.8013\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4145 - acc: 0.8217 - val_loss: 0.4449 - val_acc: 0.8095\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4149 - acc: 0.8141 - val_loss: 0.4440 - val_acc: 0.8095\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4141 - acc: 0.8161 - val_loss: 0.4456 - val_acc: 0.8079\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4115 - acc: 0.8212 - val_loss: 0.4425 - val_acc: 0.8095\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4095 - acc: 0.8217 - val_loss: 0.4410 - val_acc: 0.8079\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4068 - acc: 0.8214 - val_loss: 0.4424 - val_acc: 0.8095\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4079 - acc: 0.8214 - val_loss: 0.4410 - val_acc: 0.8062\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4084 - acc: 0.8239 - val_loss: 0.4419 - val_acc: 0.8095\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4031 - acc: 0.8276 - val_loss: 0.4426 - val_acc: 0.8079\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4041 - acc: 0.8259 - val_loss: 0.4397 - val_acc: 0.8095\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4030 - acc: 0.8227 - val_loss: 0.4401 - val_acc: 0.8128\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4014 - acc: 0.8261 - val_loss: 0.4409 - val_acc: 0.8062\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3973 - acc: 0.8247 - val_loss: 0.4419 - val_acc: 0.8079\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4006 - acc: 0.8281 - val_loss: 0.4406 - val_acc: 0.8062\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3960 - acc: 0.8292 - val_loss: 0.4388 - val_acc: 0.8079\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3969 - acc: 0.8298 - val_loss: 0.4382 - val_acc: 0.8079\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.3972 - acc: 0.8274 - val_loss: 0.4390 - val_acc: 0.8046\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.3949 - acc: 0.8300 - val_loss: 0.4367 - val_acc: 0.8062\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3927 - acc: 0.8265 - val_loss: 0.4364 - val_acc: 0.8062\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3903 - acc: 0.8318 - val_loss: 0.4377 - val_acc: 0.8062\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3913 - acc: 0.8329 - val_loss: 0.4362 - val_acc: 0.8030\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.3901 - acc: 0.8320 - val_loss: 0.4370 - val_acc: 0.8030\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3896 - acc: 0.8301 - val_loss: 0.4368 - val_acc: 0.8030\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3894 - acc: 0.8340 - val_loss: 0.4376 - val_acc: 0.8062\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3812 - acc: 0.8329 - val_loss: 0.4374 - val_acc: 0.8046\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3873 - acc: 0.8307 - val_loss: 0.4380 - val_acc: 0.8079\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.4, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_121\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_122 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_346 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_225 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_347 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.6750 - acc: 0.5705 - val_loss: 0.6460 - val_acc: 0.6683\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6496 - acc: 0.6192 - val_loss: 0.6303 - val_acc: 0.6667\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6384 - acc: 0.6338 - val_loss: 0.6187 - val_acc: 0.7225\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6232 - acc: 0.6561 - val_loss: 0.6089 - val_acc: 0.7011\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6109 - acc: 0.6813 - val_loss: 0.6007 - val_acc: 0.7028\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6032 - acc: 0.6762 - val_loss: 0.5927 - val_acc: 0.7110\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5884 - acc: 0.7033 - val_loss: 0.5847 - val_acc: 0.7192\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5844 - acc: 0.7077 - val_loss: 0.5764 - val_acc: 0.7291\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5764 - acc: 0.7185 - val_loss: 0.5692 - val_acc: 0.7438\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5682 - acc: 0.7229 - val_loss: 0.5624 - val_acc: 0.7471\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5571 - acc: 0.7376 - val_loss: 0.5561 - val_acc: 0.7471\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5512 - acc: 0.7354 - val_loss: 0.5503 - val_acc: 0.7504\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5471 - acc: 0.7413 - val_loss: 0.5446 - val_acc: 0.7537\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5379 - acc: 0.7513 - val_loss: 0.5394 - val_acc: 0.7586\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5303 - acc: 0.7586 - val_loss: 0.5347 - val_acc: 0.7553\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5271 - acc: 0.7575 - val_loss: 0.5302 - val_acc: 0.7603\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5230 - acc: 0.7610 - val_loss: 0.5264 - val_acc: 0.7570\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5181 - acc: 0.7610 - val_loss: 0.5224 - val_acc: 0.7570\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5128 - acc: 0.7672 - val_loss: 0.5184 - val_acc: 0.7619\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5098 - acc: 0.7736 - val_loss: 0.5149 - val_acc: 0.7619\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5058 - acc: 0.7679 - val_loss: 0.5116 - val_acc: 0.7619\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5027 - acc: 0.7770 - val_loss: 0.5087 - val_acc: 0.7586\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4987 - acc: 0.7732 - val_loss: 0.5059 - val_acc: 0.7586\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4948 - acc: 0.7750 - val_loss: 0.5032 - val_acc: 0.7635\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4937 - acc: 0.7814 - val_loss: 0.5009 - val_acc: 0.7635\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4930 - acc: 0.7760 - val_loss: 0.4983 - val_acc: 0.7685\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4845 - acc: 0.7869 - val_loss: 0.4960 - val_acc: 0.7701\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4846 - acc: 0.7823 - val_loss: 0.4939 - val_acc: 0.7701\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4840 - acc: 0.7812 - val_loss: 0.4916 - val_acc: 0.7685\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4789 - acc: 0.7858 - val_loss: 0.4896 - val_acc: 0.7718\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4792 - acc: 0.7838 - val_loss: 0.4878 - val_acc: 0.7668\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4782 - acc: 0.7862 - val_loss: 0.4862 - val_acc: 0.7718\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4720 - acc: 0.7918 - val_loss: 0.4846 - val_acc: 0.7718\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4689 - acc: 0.7964 - val_loss: 0.4830 - val_acc: 0.7718\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4694 - acc: 0.7920 - val_loss: 0.4816 - val_acc: 0.7734\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4670 - acc: 0.7918 - val_loss: 0.4802 - val_acc: 0.7701\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4664 - acc: 0.7938 - val_loss: 0.4789 - val_acc: 0.7701\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4670 - acc: 0.7949 - val_loss: 0.4776 - val_acc: 0.7783\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4629 - acc: 0.8006 - val_loss: 0.4762 - val_acc: 0.7734\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4604 - acc: 0.8000 - val_loss: 0.4750 - val_acc: 0.7734\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4589 - acc: 0.8000 - val_loss: 0.4739 - val_acc: 0.7816\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4577 - acc: 0.8026 - val_loss: 0.4727 - val_acc: 0.7767\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4587 - acc: 0.8004 - val_loss: 0.4717 - val_acc: 0.7816\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4580 - acc: 0.7955 - val_loss: 0.4708 - val_acc: 0.7816\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4523 - acc: 0.8015 - val_loss: 0.4698 - val_acc: 0.7816\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4513 - acc: 0.8046 - val_loss: 0.4691 - val_acc: 0.7816\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4520 - acc: 0.8059 - val_loss: 0.4684 - val_acc: 0.7865\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4497 - acc: 0.8046 - val_loss: 0.4675 - val_acc: 0.7833\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4506 - acc: 0.8011 - val_loss: 0.4667 - val_acc: 0.7849\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4453 - acc: 0.8095 - val_loss: 0.4660 - val_acc: 0.7898\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4461 - acc: 0.8057 - val_loss: 0.4650 - val_acc: 0.7898\n",
      "Epoch 52/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4434 - acc: 0.8066 - val_loss: 0.4642 - val_acc: 0.7882\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4416 - acc: 0.8092 - val_loss: 0.4635 - val_acc: 0.7947\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4435 - acc: 0.8059 - val_loss: 0.4628 - val_acc: 0.7980\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4440 - acc: 0.8061 - val_loss: 0.4621 - val_acc: 0.7849\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4407 - acc: 0.8106 - val_loss: 0.4618 - val_acc: 0.7865\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4421 - acc: 0.8061 - val_loss: 0.4612 - val_acc: 0.7964\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4359 - acc: 0.8103 - val_loss: 0.4606 - val_acc: 0.7964\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4416 - acc: 0.8044 - val_loss: 0.4598 - val_acc: 0.7931\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4374 - acc: 0.8141 - val_loss: 0.4592 - val_acc: 0.7931\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4372 - acc: 0.8110 - val_loss: 0.4588 - val_acc: 0.7931\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4354 - acc: 0.8135 - val_loss: 0.4585 - val_acc: 0.7980\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4366 - acc: 0.8057 - val_loss: 0.4577 - val_acc: 0.7947\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4316 - acc: 0.8130 - val_loss: 0.4570 - val_acc: 0.7915\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4331 - acc: 0.8115 - val_loss: 0.4566 - val_acc: 0.7947\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4319 - acc: 0.8103 - val_loss: 0.4561 - val_acc: 0.7964\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4311 - acc: 0.8124 - val_loss: 0.4556 - val_acc: 0.7980\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4310 - acc: 0.8128 - val_loss: 0.4554 - val_acc: 0.7947\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4320 - acc: 0.8161 - val_loss: 0.4552 - val_acc: 0.8013\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4265 - acc: 0.8155 - val_loss: 0.4543 - val_acc: 0.7964\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4269 - acc: 0.8143 - val_loss: 0.4538 - val_acc: 0.7964\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4242 - acc: 0.8112 - val_loss: 0.4538 - val_acc: 0.8013\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4232 - acc: 0.8177 - val_loss: 0.4537 - val_acc: 0.7997\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4233 - acc: 0.8152 - val_loss: 0.4532 - val_acc: 0.7964\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4258 - acc: 0.8150 - val_loss: 0.4529 - val_acc: 0.7964\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4199 - acc: 0.8168 - val_loss: 0.4527 - val_acc: 0.7997\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4217 - acc: 0.8144 - val_loss: 0.4524 - val_acc: 0.7997\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4227 - acc: 0.8150 - val_loss: 0.4517 - val_acc: 0.7997\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4239 - acc: 0.8166 - val_loss: 0.4514 - val_acc: 0.8046\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4222 - acc: 0.8135 - val_loss: 0.4509 - val_acc: 0.8046\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4219 - acc: 0.8144 - val_loss: 0.4509 - val_acc: 0.8046\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4223 - acc: 0.8155 - val_loss: 0.4505 - val_acc: 0.8046\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4193 - acc: 0.8163 - val_loss: 0.4501 - val_acc: 0.8013\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4182 - acc: 0.8161 - val_loss: 0.4497 - val_acc: 0.8013\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4178 - acc: 0.8203 - val_loss: 0.4502 - val_acc: 0.8079\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4182 - acc: 0.8186 - val_loss: 0.4496 - val_acc: 0.8079\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4178 - acc: 0.8150 - val_loss: 0.4490 - val_acc: 0.8030\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4182 - acc: 0.8190 - val_loss: 0.4488 - val_acc: 0.8079\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4189 - acc: 0.8194 - val_loss: 0.4489 - val_acc: 0.8079\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4152 - acc: 0.8168 - val_loss: 0.4485 - val_acc: 0.8030\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4145 - acc: 0.8243 - val_loss: 0.4483 - val_acc: 0.8030\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4139 - acc: 0.8201 - val_loss: 0.4481 - val_acc: 0.8112\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4125 - acc: 0.8223 - val_loss: 0.4479 - val_acc: 0.8095\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4134 - acc: 0.8194 - val_loss: 0.4474 - val_acc: 0.8112\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4114 - acc: 0.8239 - val_loss: 0.4471 - val_acc: 0.8095\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4151 - acc: 0.8236 - val_loss: 0.4475 - val_acc: 0.8144\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4132 - acc: 0.8201 - val_loss: 0.4472 - val_acc: 0.8112\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4098 - acc: 0.8239 - val_loss: 0.4468 - val_acc: 0.8079\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4103 - acc: 0.8214 - val_loss: 0.4466 - val_acc: 0.8079\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4092 - acc: 0.8221 - val_loss: 0.4464 - val_acc: 0.8112\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4071 - acc: 0.8230 - val_loss: 0.4462 - val_acc: 0.8128\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4101 - acc: 0.8248 - val_loss: 0.4460 - val_acc: 0.8144\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4082 - acc: 0.8238 - val_loss: 0.4456 - val_acc: 0.8095\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4106 - acc: 0.8221 - val_loss: 0.4457 - val_acc: 0.8144\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4058 - acc: 0.8238 - val_loss: 0.4456 - val_acc: 0.8144\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4074 - acc: 0.8247 - val_loss: 0.4453 - val_acc: 0.8144\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4059 - acc: 0.8214 - val_loss: 0.4452 - val_acc: 0.8128\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4082 - acc: 0.8225 - val_loss: 0.4450 - val_acc: 0.8128\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4062 - acc: 0.8247 - val_loss: 0.4446 - val_acc: 0.8112\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4055 - acc: 0.8225 - val_loss: 0.4450 - val_acc: 0.8144\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4064 - acc: 0.8269 - val_loss: 0.4451 - val_acc: 0.8144\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4030 - acc: 0.8250 - val_loss: 0.4449 - val_acc: 0.8095\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4038 - acc: 0.8285 - val_loss: 0.4447 - val_acc: 0.8079\n",
      "Epoch 114/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4050 - acc: 0.8254 - val_loss: 0.4443 - val_acc: 0.8144\n",
      "Epoch 115/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4030 - acc: 0.8290 - val_loss: 0.4438 - val_acc: 0.8144\n",
      "Epoch 116/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4030 - acc: 0.8256 - val_loss: 0.4436 - val_acc: 0.8161\n",
      "Epoch 117/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4025 - acc: 0.8245 - val_loss: 0.4435 - val_acc: 0.8177\n",
      "Epoch 118/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4004 - acc: 0.8259 - val_loss: 0.4441 - val_acc: 0.8095\n",
      "Epoch 119/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3970 - acc: 0.8250 - val_loss: 0.4437 - val_acc: 0.8128\n",
      "Epoch 120/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3984 - acc: 0.8314 - val_loss: 0.4434 - val_acc: 0.8128\n",
      "Epoch 121/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4018 - acc: 0.8272 - val_loss: 0.4434 - val_acc: 0.8095\n",
      "Epoch 122/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4000 - acc: 0.8278 - val_loss: 0.4435 - val_acc: 0.8112\n",
      "Epoch 123/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3974 - acc: 0.8270 - val_loss: 0.4434 - val_acc: 0.8112\n",
      "Epoch 124/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3966 - acc: 0.8301 - val_loss: 0.4430 - val_acc: 0.8112\n",
      "Epoch 125/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3992 - acc: 0.8278 - val_loss: 0.4428 - val_acc: 0.8095\n",
      "Epoch 126/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3979 - acc: 0.8270 - val_loss: 0.4428 - val_acc: 0.8112\n",
      "Epoch 127/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3952 - acc: 0.8331 - val_loss: 0.4435 - val_acc: 0.8079\n",
      "Epoch 128/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3984 - acc: 0.8256 - val_loss: 0.4431 - val_acc: 0.8079\n",
      "Epoch 129/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3973 - acc: 0.8300 - val_loss: 0.4428 - val_acc: 0.8062\n",
      "Epoch 130/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3961 - acc: 0.8312 - val_loss: 0.4426 - val_acc: 0.8062\n",
      "Epoch 131/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3956 - acc: 0.8303 - val_loss: 0.4424 - val_acc: 0.8046\n",
      "Epoch 132/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3966 - acc: 0.8283 - val_loss: 0.4425 - val_acc: 0.8079\n",
      "Epoch 133/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3956 - acc: 0.8309 - val_loss: 0.4423 - val_acc: 0.8062\n",
      "Epoch 134/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3941 - acc: 0.8303 - val_loss: 0.4420 - val_acc: 0.8046\n",
      "Epoch 135/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3955 - acc: 0.8300 - val_loss: 0.4420 - val_acc: 0.8030\n",
      "Epoch 136/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3943 - acc: 0.8327 - val_loss: 0.4423 - val_acc: 0.8062\n",
      "Epoch 137/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3920 - acc: 0.8327 - val_loss: 0.4421 - val_acc: 0.8079\n",
      "Epoch 138/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3961 - acc: 0.8314 - val_loss: 0.4422 - val_acc: 0.8079\n",
      "Epoch 139/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3948 - acc: 0.8351 - val_loss: 0.4419 - val_acc: 0.8062\n",
      "Epoch 140/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3930 - acc: 0.8334 - val_loss: 0.4417 - val_acc: 0.8062\n",
      "Epoch 141/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3902 - acc: 0.8312 - val_loss: 0.4417 - val_acc: 0.8095\n",
      "Epoch 142/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3930 - acc: 0.8280 - val_loss: 0.4414 - val_acc: 0.8095\n",
      "Epoch 143/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3902 - acc: 0.8345 - val_loss: 0.4410 - val_acc: 0.8095\n",
      "Epoch 144/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3895 - acc: 0.8323 - val_loss: 0.4405 - val_acc: 0.8062\n",
      "Epoch 145/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3899 - acc: 0.8307 - val_loss: 0.4405 - val_acc: 0.8079\n",
      "Epoch 146/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3885 - acc: 0.8303 - val_loss: 0.4412 - val_acc: 0.8079\n",
      "Epoch 147/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3891 - acc: 0.8298 - val_loss: 0.4410 - val_acc: 0.8079\n",
      "Epoch 148/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3882 - acc: 0.8314 - val_loss: 0.4405 - val_acc: 0.8079\n",
      "Epoch 149/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3873 - acc: 0.8285 - val_loss: 0.4403 - val_acc: 0.8079\n",
      "Epoch 150/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3884 - acc: 0.8363 - val_loss: 0.4407 - val_acc: 0.8079\n",
      "Epoch 151/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3868 - acc: 0.8323 - val_loss: 0.4401 - val_acc: 0.8079\n",
      "Epoch 152/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3820 - acc: 0.8376 - val_loss: 0.4401 - val_acc: 0.8079\n",
      "Epoch 153/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3859 - acc: 0.8311 - val_loss: 0.4403 - val_acc: 0.8079\n",
      "Epoch 154/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3868 - acc: 0.8358 - val_loss: 0.4404 - val_acc: 0.8079\n",
      "Epoch 155/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3848 - acc: 0.8320 - val_loss: 0.4405 - val_acc: 0.8079\n",
      "Epoch 156/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3850 - acc: 0.8327 - val_loss: 0.4401 - val_acc: 0.8095\n",
      "Epoch 157/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3849 - acc: 0.8307 - val_loss: 0.4398 - val_acc: 0.8062\n",
      "Epoch 158/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3829 - acc: 0.8356 - val_loss: 0.4399 - val_acc: 0.7997\n",
      "Epoch 159/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3812 - acc: 0.8320 - val_loss: 0.4415 - val_acc: 0.8030\n",
      "Epoch 160/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3835 - acc: 0.8323 - val_loss: 0.4408 - val_acc: 0.8079\n",
      "Epoch 161/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3829 - acc: 0.8340 - val_loss: 0.4402 - val_acc: 0.8112\n",
      "Epoch 162/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3852 - acc: 0.8323 - val_loss: 0.4398 - val_acc: 0.8030\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.4, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_122\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_123 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_348 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_226 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_349 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_227 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_350 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.8324 - acc: 0.4404 - val_loss: 0.7090 - val_acc: 0.4516\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.7209 - acc: 0.4855 - val_loss: 0.6656 - val_acc: 0.5878\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6849 - acc: 0.5689 - val_loss: 0.6521 - val_acc: 0.5846\n",
      "Epoch 4/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6677 - acc: 0.5815 - val_loss: 0.6336 - val_acc: 0.5895\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6450 - acc: 0.6150 - val_loss: 0.6167 - val_acc: 0.6995\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6253 - acc: 0.6548 - val_loss: 0.6069 - val_acc: 0.7225\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.6174 - acc: 0.6654 - val_loss: 0.5981 - val_acc: 0.7044\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.6041 - acc: 0.6873 - val_loss: 0.5864 - val_acc: 0.7258\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.5895 - acc: 0.6986 - val_loss: 0.5755 - val_acc: 0.7373\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.5834 - acc: 0.7055 - val_loss: 0.5654 - val_acc: 0.7488\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.5687 - acc: 0.7278 - val_loss: 0.5558 - val_acc: 0.7422\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.5550 - acc: 0.7280 - val_loss: 0.5461 - val_acc: 0.7438\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.5523 - acc: 0.7362 - val_loss: 0.5357 - val_acc: 0.7537\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.5400 - acc: 0.7437 - val_loss: 0.5265 - val_acc: 0.7553\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.5303 - acc: 0.7493 - val_loss: 0.5183 - val_acc: 0.7553\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.5194 - acc: 0.7603 - val_loss: 0.5105 - val_acc: 0.7586\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.5102 - acc: 0.7637 - val_loss: 0.5036 - val_acc: 0.7603\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.5065 - acc: 0.7634 - val_loss: 0.4978 - val_acc: 0.7652\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.5050 - acc: 0.7674 - val_loss: 0.4917 - val_acc: 0.7635\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.4916 - acc: 0.7734 - val_loss: 0.4868 - val_acc: 0.7652\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4901 - acc: 0.7738 - val_loss: 0.4825 - val_acc: 0.7718\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4797 - acc: 0.7794 - val_loss: 0.4774 - val_acc: 0.7734\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4701 - acc: 0.7920 - val_loss: 0.4730 - val_acc: 0.7734\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4760 - acc: 0.7869 - val_loss: 0.4704 - val_acc: 0.7800\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4661 - acc: 0.7935 - val_loss: 0.4666 - val_acc: 0.7833\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4670 - acc: 0.7887 - val_loss: 0.4627 - val_acc: 0.7816\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4613 - acc: 0.7913 - val_loss: 0.4612 - val_acc: 0.7882\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4584 - acc: 0.8011 - val_loss: 0.4594 - val_acc: 0.7882\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4553 - acc: 0.7975 - val_loss: 0.4560 - val_acc: 0.7833\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4520 - acc: 0.8008 - val_loss: 0.4551 - val_acc: 0.7898\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4429 - acc: 0.8046 - val_loss: 0.4533 - val_acc: 0.7898\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4481 - acc: 0.8002 - val_loss: 0.4512 - val_acc: 0.7931\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4468 - acc: 0.8033 - val_loss: 0.4515 - val_acc: 0.7898\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4413 - acc: 0.8073 - val_loss: 0.4488 - val_acc: 0.7931\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4396 - acc: 0.8075 - val_loss: 0.4462 - val_acc: 0.7947\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4368 - acc: 0.8090 - val_loss: 0.4465 - val_acc: 0.7898\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4298 - acc: 0.8134 - val_loss: 0.4446 - val_acc: 0.7980\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4315 - acc: 0.8139 - val_loss: 0.4461 - val_acc: 0.7915\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4325 - acc: 0.8123 - val_loss: 0.4459 - val_acc: 0.7964\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4275 - acc: 0.8152 - val_loss: 0.4437 - val_acc: 0.7980\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.4217 - acc: 0.8170 - val_loss: 0.4427 - val_acc: 0.7997\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4262 - acc: 0.8139 - val_loss: 0.4432 - val_acc: 0.8013\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4223 - acc: 0.8170 - val_loss: 0.4418 - val_acc: 0.8030\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4204 - acc: 0.8150 - val_loss: 0.4412 - val_acc: 0.8030\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4192 - acc: 0.8170 - val_loss: 0.4399 - val_acc: 0.8046\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4182 - acc: 0.8152 - val_loss: 0.4387 - val_acc: 0.8062\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4179 - acc: 0.8201 - val_loss: 0.4396 - val_acc: 0.8062\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4131 - acc: 0.8250 - val_loss: 0.4404 - val_acc: 0.8112\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.4097 - acc: 0.8214 - val_loss: 0.4380 - val_acc: 0.8046\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4119 - acc: 0.8207 - val_loss: 0.4386 - val_acc: 0.8112\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4088 - acc: 0.8247 - val_loss: 0.4373 - val_acc: 0.8128\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4076 - acc: 0.8248 - val_loss: 0.4371 - val_acc: 0.8112\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4055 - acc: 0.8247 - val_loss: 0.4385 - val_acc: 0.8095\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4068 - acc: 0.8261 - val_loss: 0.4363 - val_acc: 0.8079\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4064 - acc: 0.8243 - val_loss: 0.4366 - val_acc: 0.8095\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4031 - acc: 0.8269 - val_loss: 0.4341 - val_acc: 0.8079\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4023 - acc: 0.8261 - val_loss: 0.4359 - val_acc: 0.8079\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.3998 - acc: 0.8272 - val_loss: 0.4335 - val_acc: 0.8046\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3973 - acc: 0.8269 - val_loss: 0.4356 - val_acc: 0.8079\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3946 - acc: 0.8294 - val_loss: 0.4335 - val_acc: 0.8062\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3943 - acc: 0.8280 - val_loss: 0.4354 - val_acc: 0.8079\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3971 - acc: 0.8283 - val_loss: 0.4335 - val_acc: 0.8062\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3917 - acc: 0.8290 - val_loss: 0.4336 - val_acc: 0.8062\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3907 - acc: 0.8321 - val_loss: 0.4344 - val_acc: 0.8062\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3916 - acc: 0.8352 - val_loss: 0.4343 - val_acc: 0.8062\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3922 - acc: 0.8289 - val_loss: 0.4338 - val_acc: 0.8062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3853 - acc: 0.8358 - val_loss: 0.4320 - val_acc: 0.8030\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3886 - acc: 0.8327 - val_loss: 0.4337 - val_acc: 0.8062\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.3857 - acc: 0.8345 - val_loss: 0.4327 - val_acc: 0.8062\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3825 - acc: 0.8343 - val_loss: 0.4326 - val_acc: 0.8062\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3878 - acc: 0.8331 - val_loss: 0.4338 - val_acc: 0.8062\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3803 - acc: 0.8365 - val_loss: 0.4326 - val_acc: 0.8079\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.4, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_123\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_124 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_351 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_228 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_352 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_229 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_353 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.8847 - acc: 0.4373 - val_loss: 0.7652 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.7848 - acc: 0.4627 - val_loss: 0.7013 - val_acc: 0.4598\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.7253 - acc: 0.4897 - val_loss: 0.6724 - val_acc: 0.6108\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6957 - acc: 0.5280 - val_loss: 0.6598 - val_acc: 0.5944\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6819 - acc: 0.5603 - val_loss: 0.6514 - val_acc: 0.5878\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6711 - acc: 0.5855 - val_loss: 0.6431 - val_acc: 0.5961\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6577 - acc: 0.6034 - val_loss: 0.6348 - val_acc: 0.6207\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6507 - acc: 0.6169 - val_loss: 0.6271 - val_acc: 0.6979\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6370 - acc: 0.6466 - val_loss: 0.6198 - val_acc: 0.7438\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6277 - acc: 0.6477 - val_loss: 0.6120 - val_acc: 0.7455\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6211 - acc: 0.6668 - val_loss: 0.6038 - val_acc: 0.7504\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6157 - acc: 0.6743 - val_loss: 0.5956 - val_acc: 0.7537\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6064 - acc: 0.6904 - val_loss: 0.5877 - val_acc: 0.7537\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6024 - acc: 0.6869 - val_loss: 0.5802 - val_acc: 0.7603\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5972 - acc: 0.6953 - val_loss: 0.5732 - val_acc: 0.7635\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5879 - acc: 0.7026 - val_loss: 0.5665 - val_acc: 0.7570\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5812 - acc: 0.7114 - val_loss: 0.5596 - val_acc: 0.7570\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5750 - acc: 0.7145 - val_loss: 0.5526 - val_acc: 0.7570\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5699 - acc: 0.7174 - val_loss: 0.5461 - val_acc: 0.7586\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5588 - acc: 0.7260 - val_loss: 0.5399 - val_acc: 0.7603\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5583 - acc: 0.7254 - val_loss: 0.5341 - val_acc: 0.7635\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5517 - acc: 0.7316 - val_loss: 0.5292 - val_acc: 0.7652\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5450 - acc: 0.7413 - val_loss: 0.5247 - val_acc: 0.7701\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5450 - acc: 0.7435 - val_loss: 0.5199 - val_acc: 0.7718\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5344 - acc: 0.7462 - val_loss: 0.5152 - val_acc: 0.7701\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5319 - acc: 0.7502 - val_loss: 0.5103 - val_acc: 0.7718\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5194 - acc: 0.7621 - val_loss: 0.5057 - val_acc: 0.7718\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5166 - acc: 0.7632 - val_loss: 0.5019 - val_acc: 0.7750\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5127 - acc: 0.7672 - val_loss: 0.4984 - val_acc: 0.7750\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5158 - acc: 0.7608 - val_loss: 0.4946 - val_acc: 0.7767\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5069 - acc: 0.7699 - val_loss: 0.4918 - val_acc: 0.7750\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5023 - acc: 0.7685 - val_loss: 0.4903 - val_acc: 0.7750\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4982 - acc: 0.7791 - val_loss: 0.4869 - val_acc: 0.7816\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4878 - acc: 0.7783 - val_loss: 0.4832 - val_acc: 0.7767\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4926 - acc: 0.7798 - val_loss: 0.4808 - val_acc: 0.7767\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4867 - acc: 0.7827 - val_loss: 0.4804 - val_acc: 0.7816\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4876 - acc: 0.7814 - val_loss: 0.4784 - val_acc: 0.7849\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4916 - acc: 0.7853 - val_loss: 0.4753 - val_acc: 0.7816\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4819 - acc: 0.7865 - val_loss: 0.4733 - val_acc: 0.7800\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4814 - acc: 0.7871 - val_loss: 0.4715 - val_acc: 0.7816\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4791 - acc: 0.7916 - val_loss: 0.4715 - val_acc: 0.7849\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4805 - acc: 0.7922 - val_loss: 0.4686 - val_acc: 0.7833\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4733 - acc: 0.7915 - val_loss: 0.4659 - val_acc: 0.7783\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4711 - acc: 0.7896 - val_loss: 0.4655 - val_acc: 0.7849\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4656 - acc: 0.8006 - val_loss: 0.4643 - val_acc: 0.7849\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4695 - acc: 0.7935 - val_loss: 0.4627 - val_acc: 0.7833\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4674 - acc: 0.7953 - val_loss: 0.4613 - val_acc: 0.7816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4633 - acc: 0.7946 - val_loss: 0.4611 - val_acc: 0.7865\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4654 - acc: 0.8008 - val_loss: 0.4601 - val_acc: 0.7898\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4589 - acc: 0.7971 - val_loss: 0.4581 - val_acc: 0.7865\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4607 - acc: 0.8042 - val_loss: 0.4566 - val_acc: 0.7849\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4566 - acc: 0.8011 - val_loss: 0.4564 - val_acc: 0.7898\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4564 - acc: 0.8002 - val_loss: 0.4562 - val_acc: 0.7931\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4585 - acc: 0.7980 - val_loss: 0.4534 - val_acc: 0.7865\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4539 - acc: 0.8041 - val_loss: 0.4531 - val_acc: 0.7915\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4469 - acc: 0.8059 - val_loss: 0.4521 - val_acc: 0.7931\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4587 - acc: 0.7982 - val_loss: 0.4510 - val_acc: 0.7915\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4534 - acc: 0.7988 - val_loss: 0.4502 - val_acc: 0.7898\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4460 - acc: 0.8061 - val_loss: 0.4499 - val_acc: 0.7931\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4451 - acc: 0.8044 - val_loss: 0.4494 - val_acc: 0.7980\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4429 - acc: 0.8092 - val_loss: 0.4483 - val_acc: 0.7947\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4460 - acc: 0.8048 - val_loss: 0.4482 - val_acc: 0.7964\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4448 - acc: 0.8128 - val_loss: 0.4472 - val_acc: 0.7997\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4420 - acc: 0.8062 - val_loss: 0.4466 - val_acc: 0.8030\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4426 - acc: 0.8048 - val_loss: 0.4456 - val_acc: 0.8030\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4369 - acc: 0.8113 - val_loss: 0.4457 - val_acc: 0.8030\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4406 - acc: 0.8104 - val_loss: 0.4459 - val_acc: 0.8030\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4399 - acc: 0.8103 - val_loss: 0.4439 - val_acc: 0.8046\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4367 - acc: 0.8134 - val_loss: 0.4431 - val_acc: 0.8062\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4376 - acc: 0.8086 - val_loss: 0.4429 - val_acc: 0.8030\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4349 - acc: 0.8128 - val_loss: 0.4428 - val_acc: 0.7997\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4303 - acc: 0.8186 - val_loss: 0.4417 - val_acc: 0.8046\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4333 - acc: 0.8135 - val_loss: 0.4413 - val_acc: 0.8030\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4334 - acc: 0.8123 - val_loss: 0.4415 - val_acc: 0.8013\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4322 - acc: 0.8152 - val_loss: 0.4414 - val_acc: 0.8030\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4315 - acc: 0.8090 - val_loss: 0.4400 - val_acc: 0.8079\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4274 - acc: 0.8166 - val_loss: 0.4389 - val_acc: 0.8046\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4229 - acc: 0.8150 - val_loss: 0.4401 - val_acc: 0.8062\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4277 - acc: 0.8146 - val_loss: 0.4394 - val_acc: 0.8112\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4258 - acc: 0.8186 - val_loss: 0.4386 - val_acc: 0.8095\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4237 - acc: 0.8179 - val_loss: 0.4404 - val_acc: 0.8062\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4259 - acc: 0.8165 - val_loss: 0.4388 - val_acc: 0.8128\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4197 - acc: 0.8190 - val_loss: 0.4378 - val_acc: 0.8030\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4219 - acc: 0.8161 - val_loss: 0.4379 - val_acc: 0.8112\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4269 - acc: 0.8141 - val_loss: 0.4390 - val_acc: 0.8128\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4200 - acc: 0.8214 - val_loss: 0.4382 - val_acc: 0.8128\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4181 - acc: 0.8205 - val_loss: 0.4371 - val_acc: 0.8062\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4171 - acc: 0.8207 - val_loss: 0.4382 - val_acc: 0.8144\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4179 - acc: 0.8176 - val_loss: 0.4370 - val_acc: 0.8095\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4129 - acc: 0.8216 - val_loss: 0.4373 - val_acc: 0.8095\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4153 - acc: 0.8228 - val_loss: 0.4384 - val_acc: 0.8144\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4165 - acc: 0.8223 - val_loss: 0.4370 - val_acc: 0.8079\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4140 - acc: 0.8194 - val_loss: 0.4359 - val_acc: 0.8062\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4178 - acc: 0.8183 - val_loss: 0.4357 - val_acc: 0.8112\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4129 - acc: 0.8239 - val_loss: 0.4364 - val_acc: 0.8161\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4149 - acc: 0.8179 - val_loss: 0.4352 - val_acc: 0.8112\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4127 - acc: 0.8223 - val_loss: 0.4356 - val_acc: 0.8112\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4119 - acc: 0.8276 - val_loss: 0.4352 - val_acc: 0.8112\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4061 - acc: 0.8258 - val_loss: 0.4363 - val_acc: 0.8144\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4087 - acc: 0.8250 - val_loss: 0.4356 - val_acc: 0.8128\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4102 - acc: 0.8274 - val_loss: 0.4351 - val_acc: 0.8144\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4024 - acc: 0.8309 - val_loss: 0.4345 - val_acc: 0.8112\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4071 - acc: 0.8243 - val_loss: 0.4350 - val_acc: 0.8161\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4043 - acc: 0.8261 - val_loss: 0.4353 - val_acc: 0.8144\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4050 - acc: 0.8294 - val_loss: 0.4343 - val_acc: 0.8112\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4041 - acc: 0.8287 - val_loss: 0.4345 - val_acc: 0.8128\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4045 - acc: 0.8238 - val_loss: 0.4349 - val_acc: 0.8144\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3994 - acc: 0.8283 - val_loss: 0.4341 - val_acc: 0.8095\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4055 - acc: 0.8239 - val_loss: 0.4348 - val_acc: 0.8144\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4043 - acc: 0.8290 - val_loss: 0.4349 - val_acc: 0.8128\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4020 - acc: 0.8312 - val_loss: 0.4333 - val_acc: 0.8112\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4019 - acc: 0.8285 - val_loss: 0.4335 - val_acc: 0.8128\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4020 - acc: 0.8228 - val_loss: 0.4330 - val_acc: 0.8112\n",
      "Epoch 114/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3958 - acc: 0.8367 - val_loss: 0.4335 - val_acc: 0.8112\n",
      "Epoch 115/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3993 - acc: 0.8318 - val_loss: 0.4326 - val_acc: 0.8128\n",
      "Epoch 116/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3987 - acc: 0.8312 - val_loss: 0.4345 - val_acc: 0.8144\n",
      "Epoch 117/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3982 - acc: 0.8329 - val_loss: 0.4341 - val_acc: 0.8128\n",
      "Epoch 118/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3958 - acc: 0.8327 - val_loss: 0.4326 - val_acc: 0.8095\n",
      "Epoch 119/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3933 - acc: 0.8340 - val_loss: 0.4329 - val_acc: 0.8112\n",
      "Epoch 120/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3933 - acc: 0.8342 - val_loss: 0.4328 - val_acc: 0.8112\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.4, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_124\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_125 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_354 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_230 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_355 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_231 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_356 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.7756 - acc: 0.4505 - val_loss: 0.7152 - val_acc: 0.4154\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.7231 - acc: 0.4915 - val_loss: 0.6782 - val_acc: 0.5911\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6904 - acc: 0.5455 - val_loss: 0.6588 - val_acc: 0.6502\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6805 - acc: 0.5656 - val_loss: 0.6487 - val_acc: 0.5977\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6671 - acc: 0.6043 - val_loss: 0.6414 - val_acc: 0.5961\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6532 - acc: 0.6156 - val_loss: 0.6337 - val_acc: 0.5993\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6510 - acc: 0.6194 - val_loss: 0.6251 - val_acc: 0.6388\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6385 - acc: 0.6344 - val_loss: 0.6167 - val_acc: 0.6962\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6289 - acc: 0.6594 - val_loss: 0.6089 - val_acc: 0.7323\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6178 - acc: 0.6712 - val_loss: 0.6012 - val_acc: 0.7406\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6119 - acc: 0.6776 - val_loss: 0.5937 - val_acc: 0.7455\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6014 - acc: 0.6949 - val_loss: 0.5865 - val_acc: 0.7455\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5964 - acc: 0.6993 - val_loss: 0.5792 - val_acc: 0.7471\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5918 - acc: 0.7022 - val_loss: 0.5720 - val_acc: 0.7537\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5818 - acc: 0.7132 - val_loss: 0.5653 - val_acc: 0.7521\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5756 - acc: 0.7163 - val_loss: 0.5589 - val_acc: 0.7471\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5661 - acc: 0.7230 - val_loss: 0.5520 - val_acc: 0.7537\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5570 - acc: 0.7331 - val_loss: 0.5453 - val_acc: 0.7570\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5563 - acc: 0.7282 - val_loss: 0.5389 - val_acc: 0.7701\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5494 - acc: 0.7407 - val_loss: 0.5329 - val_acc: 0.7701\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5425 - acc: 0.7521 - val_loss: 0.5273 - val_acc: 0.7701\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5328 - acc: 0.7552 - val_loss: 0.5219 - val_acc: 0.7734\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5330 - acc: 0.7517 - val_loss: 0.5171 - val_acc: 0.7701\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5210 - acc: 0.7568 - val_loss: 0.5127 - val_acc: 0.7685\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5197 - acc: 0.7619 - val_loss: 0.5086 - val_acc: 0.7685\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5154 - acc: 0.7670 - val_loss: 0.5047 - val_acc: 0.7685\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5097 - acc: 0.7683 - val_loss: 0.5008 - val_acc: 0.7701\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5088 - acc: 0.7659 - val_loss: 0.4974 - val_acc: 0.7701\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5101 - acc: 0.7643 - val_loss: 0.4944 - val_acc: 0.7734\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4933 - acc: 0.7712 - val_loss: 0.4920 - val_acc: 0.7750\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4976 - acc: 0.7765 - val_loss: 0.4891 - val_acc: 0.7750\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4857 - acc: 0.7807 - val_loss: 0.4860 - val_acc: 0.7750\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4878 - acc: 0.7858 - val_loss: 0.4834 - val_acc: 0.7767\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4867 - acc: 0.7840 - val_loss: 0.4814 - val_acc: 0.7767\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4843 - acc: 0.7854 - val_loss: 0.4793 - val_acc: 0.7800\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4812 - acc: 0.7864 - val_loss: 0.4772 - val_acc: 0.7816\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4745 - acc: 0.7900 - val_loss: 0.4748 - val_acc: 0.7783\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4753 - acc: 0.7847 - val_loss: 0.4728 - val_acc: 0.7767\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4737 - acc: 0.7909 - val_loss: 0.4715 - val_acc: 0.7833\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4703 - acc: 0.7867 - val_loss: 0.4695 - val_acc: 0.7849\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4664 - acc: 0.7926 - val_loss: 0.4677 - val_acc: 0.7816\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4669 - acc: 0.7911 - val_loss: 0.4666 - val_acc: 0.7849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4593 - acc: 0.7960 - val_loss: 0.4651 - val_acc: 0.7882\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4606 - acc: 0.7958 - val_loss: 0.4644 - val_acc: 0.7882\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4617 - acc: 0.7999 - val_loss: 0.4636 - val_acc: 0.7882\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4562 - acc: 0.7980 - val_loss: 0.4617 - val_acc: 0.7882\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4526 - acc: 0.8020 - val_loss: 0.4615 - val_acc: 0.7898\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4555 - acc: 0.8020 - val_loss: 0.4593 - val_acc: 0.7882\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4578 - acc: 0.7940 - val_loss: 0.4579 - val_acc: 0.7931\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4526 - acc: 0.8030 - val_loss: 0.4563 - val_acc: 0.7964\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4487 - acc: 0.8028 - val_loss: 0.4556 - val_acc: 0.7915\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4457 - acc: 0.8082 - val_loss: 0.4556 - val_acc: 0.7947\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4489 - acc: 0.8082 - val_loss: 0.4539 - val_acc: 0.7980\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4414 - acc: 0.8053 - val_loss: 0.4538 - val_acc: 0.7964\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4446 - acc: 0.8019 - val_loss: 0.4533 - val_acc: 0.7980\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4471 - acc: 0.8061 - val_loss: 0.4522 - val_acc: 0.8013\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4416 - acc: 0.8119 - val_loss: 0.4518 - val_acc: 0.7997\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4438 - acc: 0.8059 - val_loss: 0.4516 - val_acc: 0.7980\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4366 - acc: 0.8123 - val_loss: 0.4509 - val_acc: 0.7997\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4399 - acc: 0.8086 - val_loss: 0.4496 - val_acc: 0.8013\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4368 - acc: 0.8084 - val_loss: 0.4497 - val_acc: 0.8013\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4356 - acc: 0.8082 - val_loss: 0.4497 - val_acc: 0.8046\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4366 - acc: 0.8093 - val_loss: 0.4478 - val_acc: 0.8030\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4312 - acc: 0.8123 - val_loss: 0.4470 - val_acc: 0.8030\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4321 - acc: 0.8104 - val_loss: 0.4473 - val_acc: 0.8062\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4293 - acc: 0.8166 - val_loss: 0.4461 - val_acc: 0.8046\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4310 - acc: 0.8119 - val_loss: 0.4460 - val_acc: 0.8062\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4247 - acc: 0.8143 - val_loss: 0.4451 - val_acc: 0.8046\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4284 - acc: 0.8139 - val_loss: 0.4447 - val_acc: 0.8062\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4287 - acc: 0.8112 - val_loss: 0.4445 - val_acc: 0.8095\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4259 - acc: 0.8157 - val_loss: 0.4434 - val_acc: 0.8079\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4241 - acc: 0.8141 - val_loss: 0.4424 - val_acc: 0.8046\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4227 - acc: 0.8124 - val_loss: 0.4426 - val_acc: 0.8112\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4213 - acc: 0.8172 - val_loss: 0.4422 - val_acc: 0.8079\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4183 - acc: 0.8223 - val_loss: 0.4428 - val_acc: 0.8112\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4238 - acc: 0.8159 - val_loss: 0.4423 - val_acc: 0.8112\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4222 - acc: 0.8185 - val_loss: 0.4410 - val_acc: 0.8079\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4239 - acc: 0.8207 - val_loss: 0.4403 - val_acc: 0.8095\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4250 - acc: 0.8163 - val_loss: 0.4430 - val_acc: 0.8062\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4183 - acc: 0.8188 - val_loss: 0.4408 - val_acc: 0.8079\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4152 - acc: 0.8241 - val_loss: 0.4397 - val_acc: 0.8046\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4124 - acc: 0.8223 - val_loss: 0.4401 - val_acc: 0.8095\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4153 - acc: 0.8223 - val_loss: 0.4402 - val_acc: 0.8095\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4127 - acc: 0.8228 - val_loss: 0.4394 - val_acc: 0.8128\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4108 - acc: 0.8276 - val_loss: 0.4397 - val_acc: 0.8095\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4129 - acc: 0.8216 - val_loss: 0.4392 - val_acc: 0.8128\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4127 - acc: 0.8217 - val_loss: 0.4397 - val_acc: 0.8095\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4081 - acc: 0.8241 - val_loss: 0.4395 - val_acc: 0.8112\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4112 - acc: 0.8241 - val_loss: 0.4386 - val_acc: 0.8079\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4064 - acc: 0.8250 - val_loss: 0.4387 - val_acc: 0.8095\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4077 - acc: 0.8238 - val_loss: 0.4408 - val_acc: 0.8046\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4055 - acc: 0.8294 - val_loss: 0.4379 - val_acc: 0.8144\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4066 - acc: 0.8216 - val_loss: 0.4372 - val_acc: 0.8128\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4074 - acc: 0.8199 - val_loss: 0.4380 - val_acc: 0.8128\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4052 - acc: 0.8208 - val_loss: 0.4373 - val_acc: 0.8161\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4059 - acc: 0.8232 - val_loss: 0.4372 - val_acc: 0.8144\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4031 - acc: 0.8267 - val_loss: 0.4369 - val_acc: 0.8095\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3989 - acc: 0.8265 - val_loss: 0.4378 - val_acc: 0.8128\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4022 - acc: 0.8278 - val_loss: 0.4394 - val_acc: 0.8079\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3971 - acc: 0.8274 - val_loss: 0.4368 - val_acc: 0.8128\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3967 - acc: 0.8274 - val_loss: 0.4368 - val_acc: 0.8128\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4016 - acc: 0.8314 - val_loss: 0.4386 - val_acc: 0.8095\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3997 - acc: 0.8285 - val_loss: 0.4363 - val_acc: 0.8128\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3959 - acc: 0.8329 - val_loss: 0.4356 - val_acc: 0.8095\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3956 - acc: 0.8280 - val_loss: 0.4365 - val_acc: 0.8112\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4005 - acc: 0.8285 - val_loss: 0.4353 - val_acc: 0.8112\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3945 - acc: 0.8285 - val_loss: 0.4360 - val_acc: 0.8144\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3975 - acc: 0.8314 - val_loss: 0.4369 - val_acc: 0.8112\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3944 - acc: 0.8318 - val_loss: 0.4352 - val_acc: 0.8095\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3921 - acc: 0.8316 - val_loss: 0.4353 - val_acc: 0.8112\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3892 - acc: 0.8347 - val_loss: 0.4360 - val_acc: 0.8144\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3918 - acc: 0.8327 - val_loss: 0.4371 - val_acc: 0.8112\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3936 - acc: 0.8331 - val_loss: 0.4355 - val_acc: 0.8128\n",
      "Epoch 114/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3864 - acc: 0.8336 - val_loss: 0.4356 - val_acc: 0.8144\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.4, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_125\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_126 (InputLayer)       [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_357 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_232 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_358 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_233 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_359 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 1s 33ms/step - loss: 0.8546 - acc: 0.4463 - val_loss: 0.7436 - val_acc: 0.4171\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7642 - acc: 0.4716 - val_loss: 0.6847 - val_acc: 0.5846\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7022 - acc: 0.5287 - val_loss: 0.6614 - val_acc: 0.5895\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6796 - acc: 0.5824 - val_loss: 0.6515 - val_acc: 0.5862\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6762 - acc: 0.5888 - val_loss: 0.6417 - val_acc: 0.5862\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6584 - acc: 0.6107 - val_loss: 0.6298 - val_acc: 0.6026\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6402 - acc: 0.6367 - val_loss: 0.6190 - val_acc: 0.6831\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6324 - acc: 0.6395 - val_loss: 0.6104 - val_acc: 0.7373\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.6168 - acc: 0.6548 - val_loss: 0.6027 - val_acc: 0.7323\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.6089 - acc: 0.6694 - val_loss: 0.5949 - val_acc: 0.7356\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5978 - acc: 0.6886 - val_loss: 0.5874 - val_acc: 0.7422\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5947 - acc: 0.6909 - val_loss: 0.5798 - val_acc: 0.7438\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5796 - acc: 0.7033 - val_loss: 0.5719 - val_acc: 0.7504\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.5718 - acc: 0.7209 - val_loss: 0.5638 - val_acc: 0.7488\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.5641 - acc: 0.7254 - val_loss: 0.5556 - val_acc: 0.7521\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.5590 - acc: 0.7188 - val_loss: 0.5476 - val_acc: 0.7586\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5476 - acc: 0.7375 - val_loss: 0.5396 - val_acc: 0.7570\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5409 - acc: 0.7420 - val_loss: 0.5325 - val_acc: 0.7701\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.5344 - acc: 0.7462 - val_loss: 0.5251 - val_acc: 0.7718\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.5200 - acc: 0.7555 - val_loss: 0.5185 - val_acc: 0.7734\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5175 - acc: 0.7608 - val_loss: 0.5114 - val_acc: 0.7750\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5122 - acc: 0.7646 - val_loss: 0.5056 - val_acc: 0.7701\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.5050 - acc: 0.7710 - val_loss: 0.5008 - val_acc: 0.7734\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4990 - acc: 0.7721 - val_loss: 0.4971 - val_acc: 0.7783\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4935 - acc: 0.7747 - val_loss: 0.4933 - val_acc: 0.7783\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4925 - acc: 0.7739 - val_loss: 0.4885 - val_acc: 0.7800\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4852 - acc: 0.7776 - val_loss: 0.4841 - val_acc: 0.7833\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4821 - acc: 0.7865 - val_loss: 0.4829 - val_acc: 0.7833\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4781 - acc: 0.7865 - val_loss: 0.4806 - val_acc: 0.7865\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4746 - acc: 0.7845 - val_loss: 0.4758 - val_acc: 0.7816\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4666 - acc: 0.7935 - val_loss: 0.4735 - val_acc: 0.7833\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4689 - acc: 0.7905 - val_loss: 0.4735 - val_acc: 0.7882\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4645 - acc: 0.7926 - val_loss: 0.4704 - val_acc: 0.7833\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4595 - acc: 0.7962 - val_loss: 0.4684 - val_acc: 0.7833\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4553 - acc: 0.8008 - val_loss: 0.4676 - val_acc: 0.7833\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4560 - acc: 0.8004 - val_loss: 0.4664 - val_acc: 0.7882\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4495 - acc: 0.7982 - val_loss: 0.4633 - val_acc: 0.7865\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4494 - acc: 0.8028 - val_loss: 0.4619 - val_acc: 0.7849\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4525 - acc: 0.8039 - val_loss: 0.4622 - val_acc: 0.7931\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4428 - acc: 0.8037 - val_loss: 0.4588 - val_acc: 0.7898\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4441 - acc: 0.8075 - val_loss: 0.4584 - val_acc: 0.7931\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4460 - acc: 0.8026 - val_loss: 0.4568 - val_acc: 0.7947\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4406 - acc: 0.8030 - val_loss: 0.4559 - val_acc: 0.7997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4392 - acc: 0.8035 - val_loss: 0.4576 - val_acc: 0.7964\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4358 - acc: 0.8093 - val_loss: 0.4540 - val_acc: 0.8013\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4372 - acc: 0.8108 - val_loss: 0.4526 - val_acc: 0.7997\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4364 - acc: 0.8103 - val_loss: 0.4519 - val_acc: 0.8030\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4314 - acc: 0.8124 - val_loss: 0.4533 - val_acc: 0.7997\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.4262 - acc: 0.8124 - val_loss: 0.4500 - val_acc: 0.7997\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4312 - acc: 0.8126 - val_loss: 0.4504 - val_acc: 0.8030\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4281 - acc: 0.8095 - val_loss: 0.4489 - val_acc: 0.8030\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4243 - acc: 0.8134 - val_loss: 0.4493 - val_acc: 0.8030\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4274 - acc: 0.8095 - val_loss: 0.4487 - val_acc: 0.8046\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4280 - acc: 0.8126 - val_loss: 0.4471 - val_acc: 0.8046\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4226 - acc: 0.8103 - val_loss: 0.4472 - val_acc: 0.8046\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4220 - acc: 0.8144 - val_loss: 0.4485 - val_acc: 0.8079\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4167 - acc: 0.8179 - val_loss: 0.4476 - val_acc: 0.8046\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4173 - acc: 0.8201 - val_loss: 0.4453 - val_acc: 0.8046\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4184 - acc: 0.8130 - val_loss: 0.4473 - val_acc: 0.8030\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4124 - acc: 0.8194 - val_loss: 0.4442 - val_acc: 0.8046\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4166 - acc: 0.8150 - val_loss: 0.4433 - val_acc: 0.8079\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4075 - acc: 0.8217 - val_loss: 0.4458 - val_acc: 0.8079\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4138 - acc: 0.8172 - val_loss: 0.4432 - val_acc: 0.8062\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4113 - acc: 0.8186 - val_loss: 0.4434 - val_acc: 0.8030\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.4092 - acc: 0.8234 - val_loss: 0.4448 - val_acc: 0.8062\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4074 - acc: 0.8238 - val_loss: 0.4403 - val_acc: 0.8079\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4056 - acc: 0.8201 - val_loss: 0.4407 - val_acc: 0.8062\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.4078 - acc: 0.8217 - val_loss: 0.4410 - val_acc: 0.8112\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4059 - acc: 0.8210 - val_loss: 0.4412 - val_acc: 0.8095\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4058 - acc: 0.8221 - val_loss: 0.4417 - val_acc: 0.8112\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4036 - acc: 0.8285 - val_loss: 0.4412 - val_acc: 0.8079\n"
     ]
    }
   ],
   "source": [
    "mlp_perf_metrics4 = make_MLP_exp(X_train_vect, y_train.values, params4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.856413</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.337578</td>\n",
       "      <td>0.437817</td>\n",
       "      <td>56</td>\n",
       "      <td>0.820092</td>\n",
       "      <td>0.803890</td>\n",
       "      <td>0.764253</td>\n",
       "      <td>0.861556</td>\n",
       "      <td>0.783570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.856778</td>\n",
       "      <td>0.819376</td>\n",
       "      <td>0.331602</td>\n",
       "      <td>0.434488</td>\n",
       "      <td>83</td>\n",
       "      <td>0.812869</td>\n",
       "      <td>0.783489</td>\n",
       "      <td>0.775039</td>\n",
       "      <td>0.840961</td>\n",
       "      <td>0.779241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.834884</td>\n",
       "      <td>0.806240</td>\n",
       "      <td>0.384279</td>\n",
       "      <td>0.442817</td>\n",
       "      <td>124</td>\n",
       "      <td>0.814183</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.751926</td>\n",
       "      <td>0.860412</td>\n",
       "      <td>0.775218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.849298</td>\n",
       "      <td>0.807882</td>\n",
       "      <td>0.351228</td>\n",
       "      <td>0.438279</td>\n",
       "      <td>57</td>\n",
       "      <td>0.819435</td>\n",
       "      <td>0.806557</td>\n",
       "      <td>0.758089</td>\n",
       "      <td>0.864989</td>\n",
       "      <td>0.781573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.845831</td>\n",
       "      <td>0.814450</td>\n",
       "      <td>0.355905</td>\n",
       "      <td>0.438476</td>\n",
       "      <td>100</td>\n",
       "      <td>0.821405</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.759630</td>\n",
       "      <td>0.867277</td>\n",
       "      <td>0.783784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.848568</td>\n",
       "      <td>0.811166</td>\n",
       "      <td>0.354372</td>\n",
       "      <td>0.440564</td>\n",
       "      <td>97</td>\n",
       "      <td>0.810243</td>\n",
       "      <td>0.795082</td>\n",
       "      <td>0.747304</td>\n",
       "      <td>0.856979</td>\n",
       "      <td>0.770453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.321684</td>\n",
       "      <td>0.433280</td>\n",
       "      <td>87</td>\n",
       "      <td>0.826001</td>\n",
       "      <td>0.809677</td>\n",
       "      <td>0.773498</td>\n",
       "      <td>0.864989</td>\n",
       "      <td>0.791174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.851305</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.348299</td>\n",
       "      <td>0.438181</td>\n",
       "      <td>57</td>\n",
       "      <td>0.821405</td>\n",
       "      <td>0.820034</td>\n",
       "      <td>0.744222</td>\n",
       "      <td>0.878719</td>\n",
       "      <td>0.780291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.849480</td>\n",
       "      <td>0.811166</td>\n",
       "      <td>0.345813</td>\n",
       "      <td>0.435760</td>\n",
       "      <td>82</td>\n",
       "      <td>0.820092</td>\n",
       "      <td>0.805873</td>\n",
       "      <td>0.761171</td>\n",
       "      <td>0.863844</td>\n",
       "      <td>0.782884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.833972</td>\n",
       "      <td>0.807882</td>\n",
       "      <td>0.381732</td>\n",
       "      <td>0.441768</td>\n",
       "      <td>136</td>\n",
       "      <td>0.814183</td>\n",
       "      <td>0.797078</td>\n",
       "      <td>0.756549</td>\n",
       "      <td>0.856979</td>\n",
       "      <td>0.776285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.846378</td>\n",
       "      <td>0.807882</td>\n",
       "      <td>0.365389</td>\n",
       "      <td>0.435039</td>\n",
       "      <td>57</td>\n",
       "      <td>0.816152</td>\n",
       "      <td>0.804959</td>\n",
       "      <td>0.750385</td>\n",
       "      <td>0.864989</td>\n",
       "      <td>0.776715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.844189</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.367379</td>\n",
       "      <td>0.435492</td>\n",
       "      <td>95</td>\n",
       "      <td>0.820749</td>\n",
       "      <td>0.811258</td>\n",
       "      <td>0.755008</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.782123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.836526</td>\n",
       "      <td>0.806240</td>\n",
       "      <td>0.372325</td>\n",
       "      <td>0.438602</td>\n",
       "      <td>93</td>\n",
       "      <td>0.811556</td>\n",
       "      <td>0.795752</td>\n",
       "      <td>0.750385</td>\n",
       "      <td>0.856979</td>\n",
       "      <td>0.772403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.838716</td>\n",
       "      <td>0.811166</td>\n",
       "      <td>0.368226</td>\n",
       "      <td>0.435644</td>\n",
       "      <td>69</td>\n",
       "      <td>0.818122</td>\n",
       "      <td>0.802932</td>\n",
       "      <td>0.759630</td>\n",
       "      <td>0.861556</td>\n",
       "      <td>0.780681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.843824</td>\n",
       "      <td>0.806240</td>\n",
       "      <td>0.358377</td>\n",
       "      <td>0.437451</td>\n",
       "      <td>54</td>\n",
       "      <td>0.819435</td>\n",
       "      <td>0.808581</td>\n",
       "      <td>0.755008</td>\n",
       "      <td>0.867277</td>\n",
       "      <td>0.780876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.856778</td>\n",
       "      <td>0.817734</td>\n",
       "      <td>0.342719</td>\n",
       "      <td>0.430634</td>\n",
       "      <td>88</td>\n",
       "      <td>0.819435</td>\n",
       "      <td>0.806557</td>\n",
       "      <td>0.758089</td>\n",
       "      <td>0.864989</td>\n",
       "      <td>0.781573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.833425</td>\n",
       "      <td>0.811166</td>\n",
       "      <td>0.387963</td>\n",
       "      <td>0.442536</td>\n",
       "      <td>125</td>\n",
       "      <td>0.813526</td>\n",
       "      <td>0.800659</td>\n",
       "      <td>0.748844</td>\n",
       "      <td>0.861556</td>\n",
       "      <td>0.773885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.836891</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.374389</td>\n",
       "      <td>0.438093</td>\n",
       "      <td>57</td>\n",
       "      <td>0.816152</td>\n",
       "      <td>0.802956</td>\n",
       "      <td>0.753467</td>\n",
       "      <td>0.862700</td>\n",
       "      <td>0.777424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.832695</td>\n",
       "      <td>0.807882</td>\n",
       "      <td>0.385165</td>\n",
       "      <td>0.435819</td>\n",
       "      <td>86</td>\n",
       "      <td>0.820749</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.751926</td>\n",
       "      <td>0.871854</td>\n",
       "      <td>0.781425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.829411</td>\n",
       "      <td>0.807882</td>\n",
       "      <td>0.391997</td>\n",
       "      <td>0.440019</td>\n",
       "      <td>77</td>\n",
       "      <td>0.811556</td>\n",
       "      <td>0.797697</td>\n",
       "      <td>0.747304</td>\n",
       "      <td>0.859268</td>\n",
       "      <td>0.771679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.831600</td>\n",
       "      <td>0.799672</td>\n",
       "      <td>0.376102</td>\n",
       "      <td>0.436206</td>\n",
       "      <td>67</td>\n",
       "      <td>0.816809</td>\n",
       "      <td>0.809365</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.776263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.848020</td>\n",
       "      <td>0.801314</td>\n",
       "      <td>0.343788</td>\n",
       "      <td>0.436324</td>\n",
       "      <td>65</td>\n",
       "      <td>0.824032</td>\n",
       "      <td>0.818030</td>\n",
       "      <td>0.755008</td>\n",
       "      <td>0.875286</td>\n",
       "      <td>0.785256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.832695</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.383978</td>\n",
       "      <td>0.437969</td>\n",
       "      <td>66</td>\n",
       "      <td>0.816152</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.758089</td>\n",
       "      <td>0.859268</td>\n",
       "      <td>0.778481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.831053</td>\n",
       "      <td>0.807882</td>\n",
       "      <td>0.391297</td>\n",
       "      <td>0.443556</td>\n",
       "      <td>119</td>\n",
       "      <td>0.814183</td>\n",
       "      <td>0.791401</td>\n",
       "      <td>0.765794</td>\n",
       "      <td>0.850114</td>\n",
       "      <td>0.778387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.843277</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.369556</td>\n",
       "      <td>0.436009</td>\n",
       "      <td>64</td>\n",
       "      <td>0.814839</td>\n",
       "      <td>0.808403</td>\n",
       "      <td>0.741140</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.773312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.835796</td>\n",
       "      <td>0.806240</td>\n",
       "      <td>0.389426</td>\n",
       "      <td>0.437225</td>\n",
       "      <td>93</td>\n",
       "      <td>0.818122</td>\n",
       "      <td>0.805921</td>\n",
       "      <td>0.755008</td>\n",
       "      <td>0.864989</td>\n",
       "      <td>0.779634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.822295</td>\n",
       "      <td>0.812808</td>\n",
       "      <td>0.396993</td>\n",
       "      <td>0.442096</td>\n",
       "      <td>77</td>\n",
       "      <td>0.810900</td>\n",
       "      <td>0.795417</td>\n",
       "      <td>0.748844</td>\n",
       "      <td>0.856979</td>\n",
       "      <td>0.771429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.839080</td>\n",
       "      <td>0.807882</td>\n",
       "      <td>0.371896</td>\n",
       "      <td>0.437691</td>\n",
       "      <td>75</td>\n",
       "      <td>0.813526</td>\n",
       "      <td>0.794830</td>\n",
       "      <td>0.758089</td>\n",
       "      <td>0.854691</td>\n",
       "      <td>0.776025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.846926</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.356408</td>\n",
       "      <td>0.434826</td>\n",
       "      <td>62</td>\n",
       "      <td>0.816809</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.739599</td>\n",
       "      <td>0.874142</td>\n",
       "      <td>0.774818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.829046</td>\n",
       "      <td>0.814450</td>\n",
       "      <td>0.394303</td>\n",
       "      <td>0.441008</td>\n",
       "      <td>61</td>\n",
       "      <td>0.811556</td>\n",
       "      <td>0.798680</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.860412</td>\n",
       "      <td>0.771315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.835067</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.385423</td>\n",
       "      <td>0.442287</td>\n",
       "      <td>137</td>\n",
       "      <td>0.817466</td>\n",
       "      <td>0.802610</td>\n",
       "      <td>0.758089</td>\n",
       "      <td>0.861556</td>\n",
       "      <td>0.779715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.845466</td>\n",
       "      <td>0.806240</td>\n",
       "      <td>0.365416</td>\n",
       "      <td>0.435540</td>\n",
       "      <td>66</td>\n",
       "      <td>0.819435</td>\n",
       "      <td>0.813758</td>\n",
       "      <td>0.747304</td>\n",
       "      <td>0.872998</td>\n",
       "      <td>0.779116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.833789</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.382990</td>\n",
       "      <td>0.434938</td>\n",
       "      <td>103</td>\n",
       "      <td>0.824032</td>\n",
       "      <td>0.826758</td>\n",
       "      <td>0.742681</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>0.782468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.832330</td>\n",
       "      <td>0.814450</td>\n",
       "      <td>0.383732</td>\n",
       "      <td>0.437607</td>\n",
       "      <td>97</td>\n",
       "      <td>0.812213</td>\n",
       "      <td>0.803005</td>\n",
       "      <td>0.741140</td>\n",
       "      <td>0.864989</td>\n",
       "      <td>0.770833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.839993</td>\n",
       "      <td>0.806240</td>\n",
       "      <td>0.368684</td>\n",
       "      <td>0.434411</td>\n",
       "      <td>79</td>\n",
       "      <td>0.814183</td>\n",
       "      <td>0.799020</td>\n",
       "      <td>0.753467</td>\n",
       "      <td>0.859268</td>\n",
       "      <td>0.775575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.838716</td>\n",
       "      <td>0.801314</td>\n",
       "      <td>0.377197</td>\n",
       "      <td>0.437822</td>\n",
       "      <td>55</td>\n",
       "      <td>0.818779</td>\n",
       "      <td>0.813445</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.872998</td>\n",
       "      <td>0.778135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.833060</td>\n",
       "      <td>0.802956</td>\n",
       "      <td>0.382617</td>\n",
       "      <td>0.434028</td>\n",
       "      <td>75</td>\n",
       "      <td>0.815496</td>\n",
       "      <td>0.804636</td>\n",
       "      <td>0.748844</td>\n",
       "      <td>0.864989</td>\n",
       "      <td>0.775738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.832695</td>\n",
       "      <td>0.807882</td>\n",
       "      <td>0.376357</td>\n",
       "      <td>0.440769</td>\n",
       "      <td>164</td>\n",
       "      <td>0.814183</td>\n",
       "      <td>0.799020</td>\n",
       "      <td>0.753467</td>\n",
       "      <td>0.859268</td>\n",
       "      <td>0.775575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.833789</td>\n",
       "      <td>0.807882</td>\n",
       "      <td>0.382588</td>\n",
       "      <td>0.434394</td>\n",
       "      <td>63</td>\n",
       "      <td>0.814839</td>\n",
       "      <td>0.798374</td>\n",
       "      <td>0.756549</td>\n",
       "      <td>0.858124</td>\n",
       "      <td>0.776899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.831600</td>\n",
       "      <td>0.811166</td>\n",
       "      <td>0.393068</td>\n",
       "      <td>0.435229</td>\n",
       "      <td>100</td>\n",
       "      <td>0.819435</td>\n",
       "      <td>0.819113</td>\n",
       "      <td>0.739599</td>\n",
       "      <td>0.878719</td>\n",
       "      <td>0.777328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.822113</td>\n",
       "      <td>0.812808</td>\n",
       "      <td>0.407122</td>\n",
       "      <td>0.442290</td>\n",
       "      <td>76</td>\n",
       "      <td>0.808273</td>\n",
       "      <td>0.792144</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.854691</td>\n",
       "      <td>0.768254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.832695</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.383052</td>\n",
       "      <td>0.436100</td>\n",
       "      <td>77</td>\n",
       "      <td>0.818779</td>\n",
       "      <td>0.810316</td>\n",
       "      <td>0.750385</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.779200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.846196</td>\n",
       "      <td>0.802956</td>\n",
       "      <td>0.358019</td>\n",
       "      <td>0.434565</td>\n",
       "      <td>70</td>\n",
       "      <td>0.817466</td>\n",
       "      <td>0.807629</td>\n",
       "      <td>0.750385</td>\n",
       "      <td>0.867277</td>\n",
       "      <td>0.777955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.827039</td>\n",
       "      <td>0.802956</td>\n",
       "      <td>0.386743</td>\n",
       "      <td>0.434846</td>\n",
       "      <td>79</td>\n",
       "      <td>0.815496</td>\n",
       "      <td>0.803630</td>\n",
       "      <td>0.750385</td>\n",
       "      <td>0.863844</td>\n",
       "      <td>0.776096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.828498</td>\n",
       "      <td>0.807882</td>\n",
       "      <td>0.395621</td>\n",
       "      <td>0.443306</td>\n",
       "      <td>129</td>\n",
       "      <td>0.814183</td>\n",
       "      <td>0.797078</td>\n",
       "      <td>0.756549</td>\n",
       "      <td>0.856979</td>\n",
       "      <td>0.776285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.840358</td>\n",
       "      <td>0.806240</td>\n",
       "      <td>0.371656</td>\n",
       "      <td>0.433346</td>\n",
       "      <td>72</td>\n",
       "      <td>0.822718</td>\n",
       "      <td>0.827288</td>\n",
       "      <td>0.738059</td>\n",
       "      <td>0.885584</td>\n",
       "      <td>0.780130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.828498</td>\n",
       "      <td>0.811166</td>\n",
       "      <td>0.403144</td>\n",
       "      <td>0.434935</td>\n",
       "      <td>97</td>\n",
       "      <td>0.818779</td>\n",
       "      <td>0.809287</td>\n",
       "      <td>0.751926</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.779553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.831235</td>\n",
       "      <td>0.814450</td>\n",
       "      <td>0.386781</td>\n",
       "      <td>0.435454</td>\n",
       "      <td>104</td>\n",
       "      <td>0.813526</td>\n",
       "      <td>0.805695</td>\n",
       "      <td>0.741140</td>\n",
       "      <td>0.867277</td>\n",
       "      <td>0.772071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.830140</td>\n",
       "      <td>0.806240</td>\n",
       "      <td>0.386977</td>\n",
       "      <td>0.438603</td>\n",
       "      <td>78</td>\n",
       "      <td>0.818122</td>\n",
       "      <td>0.814189</td>\n",
       "      <td>0.742681</td>\n",
       "      <td>0.874142</td>\n",
       "      <td>0.776793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.842547</td>\n",
       "      <td>0.802956</td>\n",
       "      <td>0.366147</td>\n",
       "      <td>0.435054</td>\n",
       "      <td>70</td>\n",
       "      <td>0.817466</td>\n",
       "      <td>0.806612</td>\n",
       "      <td>0.751926</td>\n",
       "      <td>0.866133</td>\n",
       "      <td>0.778309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.830688</td>\n",
       "      <td>0.807882</td>\n",
       "      <td>0.387309</td>\n",
       "      <td>0.437968</td>\n",
       "      <td>86</td>\n",
       "      <td>0.819435</td>\n",
       "      <td>0.816949</td>\n",
       "      <td>0.742681</td>\n",
       "      <td>0.876430</td>\n",
       "      <td>0.778047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.832330</td>\n",
       "      <td>0.802956</td>\n",
       "      <td>0.385236</td>\n",
       "      <td>0.439803</td>\n",
       "      <td>162</td>\n",
       "      <td>0.815496</td>\n",
       "      <td>0.797735</td>\n",
       "      <td>0.759630</td>\n",
       "      <td>0.856979</td>\n",
       "      <td>0.778216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.836526</td>\n",
       "      <td>0.807882</td>\n",
       "      <td>0.380288</td>\n",
       "      <td>0.432582</td>\n",
       "      <td>72</td>\n",
       "      <td>0.820092</td>\n",
       "      <td>0.813022</td>\n",
       "      <td>0.750385</td>\n",
       "      <td>0.871854</td>\n",
       "      <td>0.780449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.834154</td>\n",
       "      <td>0.811166</td>\n",
       "      <td>0.393332</td>\n",
       "      <td>0.432795</td>\n",
       "      <td>120</td>\n",
       "      <td>0.820092</td>\n",
       "      <td>0.819421</td>\n",
       "      <td>0.741140</td>\n",
       "      <td>0.878719</td>\n",
       "      <td>0.778317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.833607</td>\n",
       "      <td>0.814450</td>\n",
       "      <td>0.386423</td>\n",
       "      <td>0.435615</td>\n",
       "      <td>114</td>\n",
       "      <td>0.814183</td>\n",
       "      <td>0.809122</td>\n",
       "      <td>0.738059</td>\n",
       "      <td>0.870709</td>\n",
       "      <td>0.771958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.828498</td>\n",
       "      <td>0.807882</td>\n",
       "      <td>0.403576</td>\n",
       "      <td>0.441213</td>\n",
       "      <td>71</td>\n",
       "      <td>0.816152</td>\n",
       "      <td>0.809045</td>\n",
       "      <td>0.744222</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.775281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_layers  layer_conf      lr  dropout  batch_size  accuracy_train  \\\n",
       "0          2  [512, 256]  0.0001     0.01        1250        0.856413   \n",
       "1          2  [256, 256]  0.0001     0.01        1250        0.856778   \n",
       "2          1       [128]  0.0001     0.01        1250        0.834884   \n",
       "3          2  [512, 128]  0.0001     0.01        1250        0.849298   \n",
       "4          2   [256, 32]  0.0001     0.01        1250        0.845831   \n",
       "5          2  [128, 128]  0.0001     0.01        1250        0.848568   \n",
       "6          2  [256, 256]  0.0001     0.01        1250        0.862069   \n",
       "7          2  [512, 256]  0.0001     0.10        1250        0.851305   \n",
       "8          2  [256, 256]  0.0001     0.10        1250        0.849480   \n",
       "9          1       [128]  0.0001     0.10        1250        0.833972   \n",
       "10         2  [512, 128]  0.0001     0.10        1250        0.846378   \n",
       "11         2   [256, 32]  0.0001     0.10        1250        0.844189   \n",
       "12         2  [128, 128]  0.0001     0.10        1250        0.836526   \n",
       "13         2  [256, 256]  0.0001     0.10        1250        0.838716   \n",
       "14         2  [512, 256]  0.0001     0.15        1250        0.843824   \n",
       "15         2  [256, 256]  0.0001     0.15        1250        0.856778   \n",
       "16         1       [128]  0.0001     0.15        1250        0.833425   \n",
       "17         2  [512, 128]  0.0001     0.15        1250        0.836891   \n",
       "18         2   [256, 32]  0.0001     0.15        1250        0.832695   \n",
       "19         2  [128, 128]  0.0001     0.15        1250        0.829411   \n",
       "20         2  [256, 256]  0.0001     0.15        1250        0.831600   \n",
       "21         2  [512, 256]  0.0001     0.20        1250        0.848020   \n",
       "22         2  [256, 256]  0.0001     0.20        1250        0.832695   \n",
       "23         1       [128]  0.0001     0.20        1250        0.831053   \n",
       "24         2  [512, 128]  0.0001     0.20        1250        0.843277   \n",
       "25         2   [256, 32]  0.0001     0.20        1250        0.835796   \n",
       "26         2  [128, 128]  0.0001     0.20        1250        0.822295   \n",
       "27         2  [256, 256]  0.0001     0.20        1250        0.839080   \n",
       "28         2  [512, 256]  0.0001     0.25        1250        0.846926   \n",
       "29         2  [256, 256]  0.0001     0.25        1250        0.829046   \n",
       "30         1       [128]  0.0001     0.25        1250        0.835067   \n",
       "31         2  [512, 128]  0.0001     0.25        1250        0.845466   \n",
       "32         2   [256, 32]  0.0001     0.25        1250        0.833789   \n",
       "33         2  [128, 128]  0.0001     0.25        1250        0.832330   \n",
       "34         2  [256, 256]  0.0001     0.25        1250        0.839993   \n",
       "35         2  [512, 256]  0.0001     0.30        1250        0.838716   \n",
       "36         2  [256, 256]  0.0001     0.30        1250        0.833060   \n",
       "37         1       [128]  0.0001     0.30        1250        0.832695   \n",
       "38         2  [512, 128]  0.0001     0.30        1250        0.833789   \n",
       "39         2   [256, 32]  0.0001     0.30        1250        0.831600   \n",
       "40         2  [128, 128]  0.0001     0.30        1250        0.822113   \n",
       "41         2  [256, 256]  0.0001     0.30        1250        0.832695   \n",
       "42         2  [512, 256]  0.0001     0.35        1250        0.846196   \n",
       "43         2  [256, 256]  0.0001     0.35        1250        0.827039   \n",
       "44         1       [128]  0.0001     0.35        1250        0.828498   \n",
       "45         2  [512, 128]  0.0001     0.35        1250        0.840358   \n",
       "46         2   [256, 32]  0.0001     0.35        1250        0.828498   \n",
       "47         2  [128, 128]  0.0001     0.35        1250        0.831235   \n",
       "48         2  [256, 256]  0.0001     0.35        1250        0.830140   \n",
       "49         2  [512, 256]  0.0001     0.40        1250        0.842547   \n",
       "50         2  [256, 256]  0.0001     0.40        1250        0.830688   \n",
       "51         1       [128]  0.0001     0.40        1250        0.832330   \n",
       "52         2  [512, 128]  0.0001     0.40        1250        0.836526   \n",
       "53         2   [256, 32]  0.0001     0.40        1250        0.834154   \n",
       "54         2  [128, 128]  0.0001     0.40        1250        0.833607   \n",
       "55         2  [256, 256]  0.0001     0.40        1250        0.828498   \n",
       "\n",
       "    accuracy_val  loss_train  loss_val  epochs  accuracy  precision    recall  \\\n",
       "0       0.804598    0.337578  0.437817      56  0.820092   0.803890  0.764253   \n",
       "1       0.819376    0.331602  0.434488      83  0.812869   0.783489  0.775039   \n",
       "2       0.806240    0.384279  0.442817     124  0.814183   0.800000  0.751926   \n",
       "3       0.807882    0.351228  0.438279      57  0.819435   0.806557  0.758089   \n",
       "4       0.814450    0.355905  0.438476     100  0.821405   0.809524  0.759630   \n",
       "5       0.811166    0.354372  0.440564      97  0.810243   0.795082  0.747304   \n",
       "6       0.816092    0.321684  0.433280      87  0.826001   0.809677  0.773498   \n",
       "7       0.804598    0.348299  0.438181      57  0.821405   0.820034  0.744222   \n",
       "8       0.811166    0.345813  0.435760      82  0.820092   0.805873  0.761171   \n",
       "9       0.807882    0.381732  0.441768     136  0.814183   0.797078  0.756549   \n",
       "10      0.807882    0.365389  0.435039      57  0.816152   0.804959  0.750385   \n",
       "11      0.809524    0.367379  0.435492      95  0.820749   0.811258  0.755008   \n",
       "12      0.806240    0.372325  0.438602      93  0.811556   0.795752  0.750385   \n",
       "13      0.811166    0.368226  0.435644      69  0.818122   0.802932  0.759630   \n",
       "14      0.806240    0.358377  0.437451      54  0.819435   0.808581  0.755008   \n",
       "15      0.817734    0.342719  0.430634      88  0.819435   0.806557  0.758089   \n",
       "16      0.811166    0.387963  0.442536     125  0.813526   0.800659  0.748844   \n",
       "17      0.809524    0.374389  0.438093      57  0.816152   0.802956  0.753467   \n",
       "18      0.807882    0.385165  0.435819      86  0.820749   0.813333  0.751926   \n",
       "19      0.807882    0.391997  0.440019      77  0.811556   0.797697  0.747304   \n",
       "20      0.799672    0.376102  0.436206      67  0.816809   0.809365  0.745763   \n",
       "21      0.801314    0.343788  0.436324      65  0.824032   0.818030  0.755008   \n",
       "22      0.804598    0.383978  0.437969      66  0.816152   0.800000  0.758089   \n",
       "23      0.807882    0.391297  0.443556     119  0.814183   0.791401  0.765794   \n",
       "24      0.809524    0.369556  0.436009      64  0.814839   0.808403  0.741140   \n",
       "25      0.806240    0.389426  0.437225      93  0.818122   0.805921  0.755008   \n",
       "26      0.812808    0.396993  0.442096      77  0.810900   0.795417  0.748844   \n",
       "27      0.807882    0.371896  0.437691      75  0.813526   0.794830  0.758089   \n",
       "28      0.804598    0.356408  0.434826      62  0.816809   0.813559  0.739599   \n",
       "29      0.814450    0.394303  0.441008      61  0.811556   0.798680  0.745763   \n",
       "30      0.804598    0.385423  0.442287     137  0.817466   0.802610  0.758089   \n",
       "31      0.806240    0.365416  0.435540      66  0.819435   0.813758  0.747304   \n",
       "32      0.809524    0.382990  0.434938     103  0.824032   0.826758  0.742681   \n",
       "33      0.814450    0.383732  0.437607      97  0.812213   0.803005  0.741140   \n",
       "34      0.806240    0.368684  0.434411      79  0.814183   0.799020  0.753467   \n",
       "35      0.801314    0.377197  0.437822      55  0.818779   0.813445  0.745763   \n",
       "36      0.802956    0.382617  0.434028      75  0.815496   0.804636  0.748844   \n",
       "37      0.807882    0.376357  0.440769     164  0.814183   0.799020  0.753467   \n",
       "38      0.807882    0.382588  0.434394      63  0.814839   0.798374  0.756549   \n",
       "39      0.811166    0.393068  0.435229     100  0.819435   0.819113  0.739599   \n",
       "40      0.812808    0.407122  0.442290      76  0.808273   0.792144  0.745763   \n",
       "41      0.804598    0.383052  0.436100      77  0.818779   0.810316  0.750385   \n",
       "42      0.802956    0.358019  0.434565      70  0.817466   0.807629  0.750385   \n",
       "43      0.802956    0.386743  0.434846      79  0.815496   0.803630  0.750385   \n",
       "44      0.807882    0.395621  0.443306     129  0.814183   0.797078  0.756549   \n",
       "45      0.806240    0.371656  0.433346      72  0.822718   0.827288  0.738059   \n",
       "46      0.811166    0.403144  0.434935      97  0.818779   0.809287  0.751926   \n",
       "47      0.814450    0.386781  0.435454     104  0.813526   0.805695  0.741140   \n",
       "48      0.806240    0.386977  0.438603      78  0.818122   0.814189  0.742681   \n",
       "49      0.802956    0.366147  0.435054      70  0.817466   0.806612  0.751926   \n",
       "50      0.807882    0.387309  0.437968      86  0.819435   0.816949  0.742681   \n",
       "51      0.802956    0.385236  0.439803     162  0.815496   0.797735  0.759630   \n",
       "52      0.807882    0.380288  0.432582      72  0.820092   0.813022  0.750385   \n",
       "53      0.811166    0.393332  0.432795     120  0.820092   0.819421  0.741140   \n",
       "54      0.814450    0.386423  0.435615     114  0.814183   0.809122  0.738059   \n",
       "55      0.807882    0.403576  0.441213      71  0.816152   0.809045  0.744222   \n",
       "\n",
       "    specificity  f1_score  \n",
       "0      0.861556  0.783570  \n",
       "1      0.840961  0.779241  \n",
       "2      0.860412  0.775218  \n",
       "3      0.864989  0.781573  \n",
       "4      0.867277  0.783784  \n",
       "5      0.856979  0.770453  \n",
       "6      0.864989  0.791174  \n",
       "7      0.878719  0.780291  \n",
       "8      0.863844  0.782884  \n",
       "9      0.856979  0.776285  \n",
       "10     0.864989  0.776715  \n",
       "11     0.869565  0.782123  \n",
       "12     0.856979  0.772403  \n",
       "13     0.861556  0.780681  \n",
       "14     0.867277  0.780876  \n",
       "15     0.864989  0.781573  \n",
       "16     0.861556  0.773885  \n",
       "17     0.862700  0.777424  \n",
       "18     0.871854  0.781425  \n",
       "19     0.859268  0.771679  \n",
       "20     0.869565  0.776263  \n",
       "21     0.875286  0.785256  \n",
       "22     0.859268  0.778481  \n",
       "23     0.850114  0.778387  \n",
       "24     0.869565  0.773312  \n",
       "25     0.864989  0.779634  \n",
       "26     0.856979  0.771429  \n",
       "27     0.854691  0.776025  \n",
       "28     0.874142  0.774818  \n",
       "29     0.860412  0.771315  \n",
       "30     0.861556  0.779715  \n",
       "31     0.872998  0.779116  \n",
       "32     0.884439  0.782468  \n",
       "33     0.864989  0.770833  \n",
       "34     0.859268  0.775575  \n",
       "35     0.872998  0.778135  \n",
       "36     0.864989  0.775738  \n",
       "37     0.859268  0.775575  \n",
       "38     0.858124  0.776899  \n",
       "39     0.878719  0.777328  \n",
       "40     0.854691  0.768254  \n",
       "41     0.869565  0.779200  \n",
       "42     0.867277  0.777955  \n",
       "43     0.863844  0.776096  \n",
       "44     0.856979  0.776285  \n",
       "45     0.885584  0.780130  \n",
       "46     0.868421  0.779553  \n",
       "47     0.867277  0.772071  \n",
       "48     0.874142  0.776793  \n",
       "49     0.866133  0.778309  \n",
       "50     0.876430  0.778047  \n",
       "51     0.856979  0.778216  \n",
       "52     0.871854  0.780449  \n",
       "53     0.878719  0.778317  \n",
       "54     0.870709  0.771958  \n",
       "55     0.869565  0.775281  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_perf_metrics4.to_csv('mlp_perf_metrics4.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1440 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<Figure size 1152x1440 with 6 Axes>,\n",
       " array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f8c7f8fdf60>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7f8c74128da0>],\n",
       "        [<matplotlib.axes._subplots.AxesSubplot object at 0x7f8c5029ceb8>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7f8c307c7fd0>],\n",
       "        [<matplotlib.axes._subplots.AxesSubplot object at 0x7f8c3067c588>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7f8c743a6240>]],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics4['layer_conf'] = mlp_perf_metrics4['layer_conf'].astype(str)\n",
    "plot_mlp_summ_performance(mlp_perf_metrics4, 'dropout', 'layer_conf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_perf_metrics4['layer_conf'] = mlp_perf_metrics4['layer_conf'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.321684</td>\n",
       "      <td>0.433280</td>\n",
       "      <td>87</td>\n",
       "      <td>0.826001</td>\n",
       "      <td>0.809677</td>\n",
       "      <td>0.773498</td>\n",
       "      <td>0.864989</td>\n",
       "      <td>0.791174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.848020</td>\n",
       "      <td>0.801314</td>\n",
       "      <td>0.343788</td>\n",
       "      <td>0.436324</td>\n",
       "      <td>65</td>\n",
       "      <td>0.824032</td>\n",
       "      <td>0.818030</td>\n",
       "      <td>0.755008</td>\n",
       "      <td>0.875286</td>\n",
       "      <td>0.785256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.833789</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.382990</td>\n",
       "      <td>0.434938</td>\n",
       "      <td>103</td>\n",
       "      <td>0.824032</td>\n",
       "      <td>0.826758</td>\n",
       "      <td>0.742681</td>\n",
       "      <td>0.884439</td>\n",
       "      <td>0.782468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.840358</td>\n",
       "      <td>0.806240</td>\n",
       "      <td>0.371656</td>\n",
       "      <td>0.433346</td>\n",
       "      <td>72</td>\n",
       "      <td>0.822718</td>\n",
       "      <td>0.827288</td>\n",
       "      <td>0.738059</td>\n",
       "      <td>0.885584</td>\n",
       "      <td>0.780130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.845831</td>\n",
       "      <td>0.814450</td>\n",
       "      <td>0.355905</td>\n",
       "      <td>0.438476</td>\n",
       "      <td>100</td>\n",
       "      <td>0.821405</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.759630</td>\n",
       "      <td>0.867277</td>\n",
       "      <td>0.783784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_layers  layer_conf      lr  dropout  batch_size  accuracy_train  \\\n",
       "6          2  [256, 256]  0.0001     0.01        1250        0.862069   \n",
       "21         2  [512, 256]  0.0001     0.20        1250        0.848020   \n",
       "32         2   [256, 32]  0.0001     0.25        1250        0.833789   \n",
       "45         2  [512, 128]  0.0001     0.35        1250        0.840358   \n",
       "4          2   [256, 32]  0.0001     0.01        1250        0.845831   \n",
       "\n",
       "    accuracy_val  loss_train  loss_val  epochs  accuracy  precision    recall  \\\n",
       "6       0.816092    0.321684  0.433280      87  0.826001   0.809677  0.773498   \n",
       "21      0.801314    0.343788  0.436324      65  0.824032   0.818030  0.755008   \n",
       "32      0.809524    0.382990  0.434938     103  0.824032   0.826758  0.742681   \n",
       "45      0.806240    0.371656  0.433346      72  0.822718   0.827288  0.738059   \n",
       "4       0.814450    0.355905  0.438476     100  0.821405   0.809524  0.759630   \n",
       "\n",
       "    specificity  f1_score  \n",
       "6      0.864989  0.791174  \n",
       "21     0.875286  0.785256  \n",
       "32     0.884439  0.782468  \n",
       "45     0.885584  0.780130  \n",
       "4      0.867277  0.783784  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics4.nlargest(5, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.321684</td>\n",
       "      <td>0.433280</td>\n",
       "      <td>87</td>\n",
       "      <td>0.826001</td>\n",
       "      <td>0.809677</td>\n",
       "      <td>0.773498</td>\n",
       "      <td>0.864989</td>\n",
       "      <td>0.791174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.848020</td>\n",
       "      <td>0.801314</td>\n",
       "      <td>0.343788</td>\n",
       "      <td>0.436324</td>\n",
       "      <td>65</td>\n",
       "      <td>0.824032</td>\n",
       "      <td>0.818030</td>\n",
       "      <td>0.755008</td>\n",
       "      <td>0.875286</td>\n",
       "      <td>0.785256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.845831</td>\n",
       "      <td>0.814450</td>\n",
       "      <td>0.355905</td>\n",
       "      <td>0.438476</td>\n",
       "      <td>100</td>\n",
       "      <td>0.821405</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.759630</td>\n",
       "      <td>0.867277</td>\n",
       "      <td>0.783784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.856413</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.337578</td>\n",
       "      <td>0.437817</td>\n",
       "      <td>56</td>\n",
       "      <td>0.820092</td>\n",
       "      <td>0.803890</td>\n",
       "      <td>0.764253</td>\n",
       "      <td>0.861556</td>\n",
       "      <td>0.783570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.849480</td>\n",
       "      <td>0.811166</td>\n",
       "      <td>0.345813</td>\n",
       "      <td>0.435760</td>\n",
       "      <td>82</td>\n",
       "      <td>0.820092</td>\n",
       "      <td>0.805873</td>\n",
       "      <td>0.761171</td>\n",
       "      <td>0.863844</td>\n",
       "      <td>0.782884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_layers  layer_conf      lr  dropout  batch_size  accuracy_train  \\\n",
       "6          2  [256, 256]  0.0001     0.01        1250        0.862069   \n",
       "21         2  [512, 256]  0.0001     0.20        1250        0.848020   \n",
       "4          2   [256, 32]  0.0001     0.01        1250        0.845831   \n",
       "0          2  [512, 256]  0.0001     0.01        1250        0.856413   \n",
       "8          2  [256, 256]  0.0001     0.10        1250        0.849480   \n",
       "\n",
       "    accuracy_val  loss_train  loss_val  epochs  accuracy  precision    recall  \\\n",
       "6       0.816092    0.321684  0.433280      87  0.826001   0.809677  0.773498   \n",
       "21      0.801314    0.343788  0.436324      65  0.824032   0.818030  0.755008   \n",
       "4       0.814450    0.355905  0.438476     100  0.821405   0.809524  0.759630   \n",
       "0       0.804598    0.337578  0.437817      56  0.820092   0.803890  0.764253   \n",
       "8       0.811166    0.345813  0.435760      82  0.820092   0.805873  0.761171   \n",
       "\n",
       "    specificity  f1_score  \n",
       "6      0.864989  0.791174  \n",
       "21     0.875286  0.785256  \n",
       "4      0.867277  0.783784  \n",
       "0      0.861556  0.783570  \n",
       "8      0.863844  0.782884  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics4.nlargest(5, 'f1_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model based on [256, 256] architecture, `batch_size=1250` and having `dropout=0.01` outperforms the metrics on all the performed experiments. Dropout values 0.2 seem to achieve a global balanced performance on all the grided configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try different dropout values on each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid parameters\n",
    "best_arch2 = [[512, 256],\n",
    "             [256, 256],\n",
    "             [512, 128],\n",
    "             [256, 32],\n",
    "             [128, 128],\n",
    "             [256, 256]]\n",
    "\n",
    "params5 = {\n",
    "        'lay_conf': best_arch2,\n",
    "        'lr': [1e-4],\n",
    "        'dropout': [[0.2, 0.1], [0.2, 0.01], [0.2, 0.3], [0.2, 0.4]],\n",
    "        'max_epochs': [300],\n",
    "        'batch_size': [1250],\n",
    "        'seed': [123456]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.1], 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 2s 61ms/step - loss: 0.7443 - acc: 0.4454 - val_loss: 0.6857 - val_acc: 0.5468\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6720 - acc: 0.5762 - val_loss: 0.6732 - val_acc: 0.5583\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6522 - acc: 0.5829 - val_loss: 0.6466 - val_acc: 0.5632\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6229 - acc: 0.6419 - val_loss: 0.6151 - val_acc: 0.7044\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.6018 - acc: 0.7075 - val_loss: 0.5967 - val_acc: 0.7044\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5871 - acc: 0.7075 - val_loss: 0.5789 - val_acc: 0.7143\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5681 - acc: 0.7274 - val_loss: 0.5635 - val_acc: 0.7291\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.5544 - acc: 0.7493 - val_loss: 0.5471 - val_acc: 0.7406\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5369 - acc: 0.7573 - val_loss: 0.5292 - val_acc: 0.7389\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.5219 - acc: 0.7610 - val_loss: 0.5141 - val_acc: 0.7521\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5089 - acc: 0.7712 - val_loss: 0.5009 - val_acc: 0.7603\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4959 - acc: 0.7783 - val_loss: 0.4892 - val_acc: 0.7652\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4848 - acc: 0.7834 - val_loss: 0.4805 - val_acc: 0.7685\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4783 - acc: 0.7854 - val_loss: 0.4738 - val_acc: 0.7718\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4664 - acc: 0.7913 - val_loss: 0.4683 - val_acc: 0.7750\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4610 - acc: 0.7944 - val_loss: 0.4642 - val_acc: 0.7816\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4560 - acc: 0.7999 - val_loss: 0.4603 - val_acc: 0.7882\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4470 - acc: 0.8020 - val_loss: 0.4555 - val_acc: 0.7882\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4448 - acc: 0.8057 - val_loss: 0.4553 - val_acc: 0.7865\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4397 - acc: 0.8088 - val_loss: 0.4499 - val_acc: 0.7980\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4358 - acc: 0.8108 - val_loss: 0.4498 - val_acc: 0.7882\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4292 - acc: 0.8126 - val_loss: 0.4466 - val_acc: 0.7898\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4257 - acc: 0.8108 - val_loss: 0.4441 - val_acc: 0.7964\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4223 - acc: 0.8146 - val_loss: 0.4455 - val_acc: 0.7849\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4230 - acc: 0.8183 - val_loss: 0.4417 - val_acc: 0.7898\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4188 - acc: 0.8146 - val_loss: 0.4408 - val_acc: 0.7931\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4132 - acc: 0.8208 - val_loss: 0.4401 - val_acc: 0.7931\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4102 - acc: 0.8234 - val_loss: 0.4393 - val_acc: 0.7915\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4080 - acc: 0.8252 - val_loss: 0.4378 - val_acc: 0.7931\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4050 - acc: 0.8256 - val_loss: 0.4374 - val_acc: 0.7980\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4053 - acc: 0.8216 - val_loss: 0.4370 - val_acc: 0.7980\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4042 - acc: 0.8225 - val_loss: 0.4364 - val_acc: 0.7964\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3992 - acc: 0.8272 - val_loss: 0.4370 - val_acc: 0.7980\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3998 - acc: 0.8272 - val_loss: 0.4347 - val_acc: 0.7964\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3964 - acc: 0.8278 - val_loss: 0.4370 - val_acc: 0.7964\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3926 - acc: 0.8309 - val_loss: 0.4340 - val_acc: 0.7997\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3899 - acc: 0.8321 - val_loss: 0.4354 - val_acc: 0.7980\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3894 - acc: 0.8312 - val_loss: 0.4362 - val_acc: 0.7997\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3872 - acc: 0.8316 - val_loss: 0.4361 - val_acc: 0.7997\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3862 - acc: 0.8281 - val_loss: 0.4339 - val_acc: 0.8013\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3834 - acc: 0.8340 - val_loss: 0.4325 - val_acc: 0.7997\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3783 - acc: 0.8360 - val_loss: 0.4332 - val_acc: 0.7997\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3772 - acc: 0.8371 - val_loss: 0.4334 - val_acc: 0.7997\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3786 - acc: 0.8338 - val_loss: 0.4358 - val_acc: 0.8046\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3759 - acc: 0.8407 - val_loss: 0.4308 - val_acc: 0.8013\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3753 - acc: 0.8356 - val_loss: 0.4359 - val_acc: 0.8079\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3726 - acc: 0.8404 - val_loss: 0.4360 - val_acc: 0.8095\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3702 - acc: 0.8360 - val_loss: 0.4342 - val_acc: 0.8013\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3648 - acc: 0.8418 - val_loss: 0.4350 - val_acc: 0.8030\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3630 - acc: 0.8444 - val_loss: 0.4341 - val_acc: 0.7997\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.1], 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 38ms/step - loss: 0.8252 - acc: 0.4286 - val_loss: 0.7269 - val_acc: 0.4417\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7275 - acc: 0.4702 - val_loss: 0.6790 - val_acc: 0.6059\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6814 - acc: 0.5722 - val_loss: 0.6659 - val_acc: 0.5649\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6587 - acc: 0.6076 - val_loss: 0.6588 - val_acc: 0.5550\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6529 - acc: 0.6032 - val_loss: 0.6440 - val_acc: 0.5649\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6302 - acc: 0.6402 - val_loss: 0.6259 - val_acc: 0.6486\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6149 - acc: 0.6816 - val_loss: 0.6110 - val_acc: 0.7143\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6059 - acc: 0.6893 - val_loss: 0.5996 - val_acc: 0.7077\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5939 - acc: 0.7028 - val_loss: 0.5891 - val_acc: 0.7110\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5816 - acc: 0.7108 - val_loss: 0.5788 - val_acc: 0.7192\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5753 - acc: 0.7240 - val_loss: 0.5687 - val_acc: 0.7340\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5623 - acc: 0.7336 - val_loss: 0.5571 - val_acc: 0.7340\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5543 - acc: 0.7354 - val_loss: 0.5449 - val_acc: 0.7340\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5412 - acc: 0.7510 - val_loss: 0.5329 - val_acc: 0.7373\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5291 - acc: 0.7561 - val_loss: 0.5218 - val_acc: 0.7521\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5209 - acc: 0.7597 - val_loss: 0.5122 - val_acc: 0.7619\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5140 - acc: 0.7674 - val_loss: 0.5035 - val_acc: 0.7734\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5075 - acc: 0.7734 - val_loss: 0.4934 - val_acc: 0.7750\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4955 - acc: 0.7798 - val_loss: 0.4876 - val_acc: 0.7783\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4866 - acc: 0.7834 - val_loss: 0.4806 - val_acc: 0.7865\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4805 - acc: 0.7853 - val_loss: 0.4774 - val_acc: 0.7849\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4741 - acc: 0.7887 - val_loss: 0.4730 - val_acc: 0.7849\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4682 - acc: 0.7889 - val_loss: 0.4684 - val_acc: 0.7849\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4653 - acc: 0.7924 - val_loss: 0.4647 - val_acc: 0.7833\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4593 - acc: 0.7977 - val_loss: 0.4645 - val_acc: 0.7833\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4538 - acc: 0.7999 - val_loss: 0.4594 - val_acc: 0.7882\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4565 - acc: 0.7962 - val_loss: 0.4596 - val_acc: 0.7849\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4483 - acc: 0.8046 - val_loss: 0.4590 - val_acc: 0.7833\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4439 - acc: 0.8011 - val_loss: 0.4543 - val_acc: 0.7915\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4389 - acc: 0.8072 - val_loss: 0.4541 - val_acc: 0.7882\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4444 - acc: 0.8055 - val_loss: 0.4534 - val_acc: 0.7882\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4352 - acc: 0.8113 - val_loss: 0.4509 - val_acc: 0.7898\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4327 - acc: 0.8115 - val_loss: 0.4502 - val_acc: 0.7931\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4314 - acc: 0.8113 - val_loss: 0.4490 - val_acc: 0.7915\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4303 - acc: 0.8137 - val_loss: 0.4492 - val_acc: 0.7947\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4244 - acc: 0.8146 - val_loss: 0.4455 - val_acc: 0.7997\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4262 - acc: 0.8144 - val_loss: 0.4454 - val_acc: 0.7947\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4242 - acc: 0.8123 - val_loss: 0.4483 - val_acc: 0.7915\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4196 - acc: 0.8196 - val_loss: 0.4445 - val_acc: 0.7947\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4170 - acc: 0.8207 - val_loss: 0.4445 - val_acc: 0.7964\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4175 - acc: 0.8168 - val_loss: 0.4438 - val_acc: 0.7947\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4133 - acc: 0.8223 - val_loss: 0.4436 - val_acc: 0.7980\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4121 - acc: 0.8181 - val_loss: 0.4414 - val_acc: 0.8013\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4120 - acc: 0.8208 - val_loss: 0.4419 - val_acc: 0.7964\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4101 - acc: 0.8210 - val_loss: 0.4434 - val_acc: 0.7980\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4077 - acc: 0.8234 - val_loss: 0.4420 - val_acc: 0.7964\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4047 - acc: 0.8250 - val_loss: 0.4406 - val_acc: 0.7997\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4069 - acc: 0.8245 - val_loss: 0.4421 - val_acc: 0.7980\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4046 - acc: 0.8234 - val_loss: 0.4397 - val_acc: 0.8030\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4036 - acc: 0.8221 - val_loss: 0.4411 - val_acc: 0.7997\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4045 - acc: 0.8203 - val_loss: 0.4401 - val_acc: 0.7997\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3981 - acc: 0.8292 - val_loss: 0.4393 - val_acc: 0.8013\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3993 - acc: 0.8263 - val_loss: 0.4393 - val_acc: 0.7997\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3951 - acc: 0.8298 - val_loss: 0.4374 - val_acc: 0.8062\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3951 - acc: 0.8311 - val_loss: 0.4408 - val_acc: 0.7997\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3892 - acc: 0.8305 - val_loss: 0.4377 - val_acc: 0.8062\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3916 - acc: 0.8280 - val_loss: 0.4390 - val_acc: 0.8013\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3910 - acc: 0.8298 - val_loss: 0.4376 - val_acc: 0.8046\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3887 - acc: 0.8309 - val_loss: 0.4363 - val_acc: 0.8030\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3897 - acc: 0.8285 - val_loss: 0.4397 - val_acc: 0.7997\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3845 - acc: 0.8340 - val_loss: 0.4362 - val_acc: 0.7997\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3865 - acc: 0.8292 - val_loss: 0.4374 - val_acc: 0.8013\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3816 - acc: 0.8373 - val_loss: 0.4389 - val_acc: 0.8013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3828 - acc: 0.8303 - val_loss: 0.4359 - val_acc: 0.8013\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3785 - acc: 0.8342 - val_loss: 0.4416 - val_acc: 0.7997\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3775 - acc: 0.8380 - val_loss: 0.4352 - val_acc: 0.8030\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3775 - acc: 0.8356 - val_loss: 0.4425 - val_acc: 0.7997\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3759 - acc: 0.8400 - val_loss: 0.4369 - val_acc: 0.8030\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3758 - acc: 0.8365 - val_loss: 0.4382 - val_acc: 0.7997\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3747 - acc: 0.8382 - val_loss: 0.4391 - val_acc: 0.8013\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3722 - acc: 0.8433 - val_loss: 0.4356 - val_acc: 0.8013\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.1], 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.8132 - acc: 0.4238 - val_loss: 0.7053 - val_acc: 0.4696\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6943 - acc: 0.5247 - val_loss: 0.6738 - val_acc: 0.5534\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6680 - acc: 0.5826 - val_loss: 0.6609 - val_acc: 0.5583\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6474 - acc: 0.5900 - val_loss: 0.6322 - val_acc: 0.5977\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6211 - acc: 0.6448 - val_loss: 0.6081 - val_acc: 0.7176\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6016 - acc: 0.7011 - val_loss: 0.5941 - val_acc: 0.6913\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5913 - acc: 0.7004 - val_loss: 0.5809 - val_acc: 0.6929\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5745 - acc: 0.7196 - val_loss: 0.5674 - val_acc: 0.7192\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5610 - acc: 0.7373 - val_loss: 0.5540 - val_acc: 0.7274\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5466 - acc: 0.7457 - val_loss: 0.5379 - val_acc: 0.7438\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5334 - acc: 0.7584 - val_loss: 0.5237 - val_acc: 0.7455\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5245 - acc: 0.7597 - val_loss: 0.5107 - val_acc: 0.7455\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5111 - acc: 0.7705 - val_loss: 0.4997 - val_acc: 0.7603\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4984 - acc: 0.7767 - val_loss: 0.4909 - val_acc: 0.7652\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4888 - acc: 0.7800 - val_loss: 0.4837 - val_acc: 0.7652\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4795 - acc: 0.7862 - val_loss: 0.4781 - val_acc: 0.7718\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4736 - acc: 0.7873 - val_loss: 0.4726 - val_acc: 0.7767\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4667 - acc: 0.7940 - val_loss: 0.4683 - val_acc: 0.7800\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4614 - acc: 0.7953 - val_loss: 0.4646 - val_acc: 0.7849\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4555 - acc: 0.8042 - val_loss: 0.4607 - val_acc: 0.7865\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4495 - acc: 0.7999 - val_loss: 0.4592 - val_acc: 0.7849\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4453 - acc: 0.8079 - val_loss: 0.4558 - val_acc: 0.7931\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4415 - acc: 0.8061 - val_loss: 0.4539 - val_acc: 0.7865\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4388 - acc: 0.8075 - val_loss: 0.4536 - val_acc: 0.7915\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4340 - acc: 0.8123 - val_loss: 0.4502 - val_acc: 0.7898\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4296 - acc: 0.8121 - val_loss: 0.4504 - val_acc: 0.7947\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4306 - acc: 0.8110 - val_loss: 0.4472 - val_acc: 0.7947\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4256 - acc: 0.8154 - val_loss: 0.4460 - val_acc: 0.7964\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4212 - acc: 0.8186 - val_loss: 0.4453 - val_acc: 0.7964\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4202 - acc: 0.8223 - val_loss: 0.4450 - val_acc: 0.7931\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4176 - acc: 0.8194 - val_loss: 0.4427 - val_acc: 0.7947\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4152 - acc: 0.8250 - val_loss: 0.4431 - val_acc: 0.7997\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4134 - acc: 0.8216 - val_loss: 0.4410 - val_acc: 0.7947\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4119 - acc: 0.8196 - val_loss: 0.4417 - val_acc: 0.7947\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4106 - acc: 0.8234 - val_loss: 0.4411 - val_acc: 0.7980\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4087 - acc: 0.8207 - val_loss: 0.4389 - val_acc: 0.7980\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4048 - acc: 0.8263 - val_loss: 0.4391 - val_acc: 0.8013\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4028 - acc: 0.8228 - val_loss: 0.4379 - val_acc: 0.7980\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3996 - acc: 0.8290 - val_loss: 0.4399 - val_acc: 0.7980\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3995 - acc: 0.8252 - val_loss: 0.4383 - val_acc: 0.8013\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3936 - acc: 0.8314 - val_loss: 0.4397 - val_acc: 0.7997\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3955 - acc: 0.8316 - val_loss: 0.4360 - val_acc: 0.8095\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3942 - acc: 0.8238 - val_loss: 0.4397 - val_acc: 0.7947\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3901 - acc: 0.8300 - val_loss: 0.4364 - val_acc: 0.8030\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3882 - acc: 0.8300 - val_loss: 0.4363 - val_acc: 0.8030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3886 - acc: 0.8323 - val_loss: 0.4351 - val_acc: 0.8079\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3848 - acc: 0.8336 - val_loss: 0.4370 - val_acc: 0.8030\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3874 - acc: 0.8336 - val_loss: 0.4368 - val_acc: 0.7997\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3821 - acc: 0.8327 - val_loss: 0.4355 - val_acc: 0.8079\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3840 - acc: 0.8305 - val_loss: 0.4367 - val_acc: 0.7980\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3789 - acc: 0.8413 - val_loss: 0.4350 - val_acc: 0.8112\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3795 - acc: 0.8378 - val_loss: 0.4338 - val_acc: 0.8112\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3753 - acc: 0.8387 - val_loss: 0.4369 - val_acc: 0.8046\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3745 - acc: 0.8393 - val_loss: 0.4340 - val_acc: 0.8095\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3720 - acc: 0.8371 - val_loss: 0.4371 - val_acc: 0.8095\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3737 - acc: 0.8400 - val_loss: 0.4335 - val_acc: 0.8095\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3711 - acc: 0.8363 - val_loss: 0.4351 - val_acc: 0.8062\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3685 - acc: 0.8409 - val_loss: 0.4346 - val_acc: 0.8112\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3666 - acc: 0.8369 - val_loss: 0.4343 - val_acc: 0.8128\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3612 - acc: 0.8497 - val_loss: 0.4361 - val_acc: 0.8046\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3647 - acc: 0.8438 - val_loss: 0.4317 - val_acc: 0.8128\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3609 - acc: 0.8415 - val_loss: 0.4382 - val_acc: 0.8046\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3624 - acc: 0.8453 - val_loss: 0.4319 - val_acc: 0.8112\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3609 - acc: 0.8409 - val_loss: 0.4349 - val_acc: 0.8079\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3596 - acc: 0.8449 - val_loss: 0.4344 - val_acc: 0.8112\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3527 - acc: 0.8460 - val_loss: 0.4351 - val_acc: 0.8112\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.1], 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 1s 31ms/step - loss: 0.8623 - acc: 0.4233 - val_loss: 0.7523 - val_acc: 0.4433\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.7537 - acc: 0.4402 - val_loss: 0.6978 - val_acc: 0.4680\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6962 - acc: 0.5211 - val_loss: 0.6741 - val_acc: 0.6207\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6719 - acc: 0.5984 - val_loss: 0.6635 - val_acc: 0.5796\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6565 - acc: 0.6196 - val_loss: 0.6529 - val_acc: 0.5928\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6443 - acc: 0.6344 - val_loss: 0.6393 - val_acc: 0.6338\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6262 - acc: 0.6767 - val_loss: 0.6248 - val_acc: 0.7028\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6144 - acc: 0.6937 - val_loss: 0.6126 - val_acc: 0.7126\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6065 - acc: 0.7088 - val_loss: 0.6024 - val_acc: 0.7028\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5932 - acc: 0.7170 - val_loss: 0.5934 - val_acc: 0.7028\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5873 - acc: 0.7185 - val_loss: 0.5851 - val_acc: 0.7176\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5770 - acc: 0.7252 - val_loss: 0.5767 - val_acc: 0.7209\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5676 - acc: 0.7382 - val_loss: 0.5678 - val_acc: 0.7241\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5631 - acc: 0.7329 - val_loss: 0.5589 - val_acc: 0.7258\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5522 - acc: 0.7426 - val_loss: 0.5498 - val_acc: 0.7274\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5440 - acc: 0.7457 - val_loss: 0.5403 - val_acc: 0.7389\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5337 - acc: 0.7546 - val_loss: 0.5311 - val_acc: 0.7455\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5286 - acc: 0.7595 - val_loss: 0.5229 - val_acc: 0.7553\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5193 - acc: 0.7652 - val_loss: 0.5150 - val_acc: 0.7553\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5133 - acc: 0.7661 - val_loss: 0.5078 - val_acc: 0.7553\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5096 - acc: 0.7732 - val_loss: 0.5017 - val_acc: 0.7668\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5016 - acc: 0.7723 - val_loss: 0.4952 - val_acc: 0.7701\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4982 - acc: 0.7754 - val_loss: 0.4899 - val_acc: 0.7734\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4934 - acc: 0.7781 - val_loss: 0.4853 - val_acc: 0.7750\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4843 - acc: 0.7827 - val_loss: 0.4811 - val_acc: 0.7734\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4848 - acc: 0.7860 - val_loss: 0.4774 - val_acc: 0.7767\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4775 - acc: 0.7847 - val_loss: 0.4742 - val_acc: 0.7783\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4714 - acc: 0.7938 - val_loss: 0.4725 - val_acc: 0.7718\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4704 - acc: 0.7955 - val_loss: 0.4680 - val_acc: 0.7865\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4639 - acc: 0.7927 - val_loss: 0.4656 - val_acc: 0.7882\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4653 - acc: 0.7929 - val_loss: 0.4639 - val_acc: 0.7849\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4593 - acc: 0.7991 - val_loss: 0.4609 - val_acc: 0.7898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4532 - acc: 0.8026 - val_loss: 0.4585 - val_acc: 0.7915\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4489 - acc: 0.8048 - val_loss: 0.4566 - val_acc: 0.7915\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4521 - acc: 0.8026 - val_loss: 0.4553 - val_acc: 0.7915\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4464 - acc: 0.7991 - val_loss: 0.4545 - val_acc: 0.7882\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4449 - acc: 0.8026 - val_loss: 0.4541 - val_acc: 0.7898\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4415 - acc: 0.8072 - val_loss: 0.4519 - val_acc: 0.7931\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4431 - acc: 0.8077 - val_loss: 0.4505 - val_acc: 0.7931\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4343 - acc: 0.8126 - val_loss: 0.4499 - val_acc: 0.7931\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4361 - acc: 0.8135 - val_loss: 0.4488 - val_acc: 0.7947\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4325 - acc: 0.8115 - val_loss: 0.4470 - val_acc: 0.7947\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4350 - acc: 0.8110 - val_loss: 0.4469 - val_acc: 0.7964\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4275 - acc: 0.8143 - val_loss: 0.4472 - val_acc: 0.7980\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4291 - acc: 0.8139 - val_loss: 0.4455 - val_acc: 0.7980\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4266 - acc: 0.8150 - val_loss: 0.4443 - val_acc: 0.7980\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4264 - acc: 0.8106 - val_loss: 0.4423 - val_acc: 0.8030\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4241 - acc: 0.8183 - val_loss: 0.4423 - val_acc: 0.8013\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4188 - acc: 0.8230 - val_loss: 0.4462 - val_acc: 0.7964\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4182 - acc: 0.8185 - val_loss: 0.4406 - val_acc: 0.8062\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4237 - acc: 0.8154 - val_loss: 0.4418 - val_acc: 0.8030\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4175 - acc: 0.8177 - val_loss: 0.4433 - val_acc: 0.7997\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4146 - acc: 0.8185 - val_loss: 0.4409 - val_acc: 0.8013\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4149 - acc: 0.8192 - val_loss: 0.4416 - val_acc: 0.8013\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4152 - acc: 0.8223 - val_loss: 0.4397 - val_acc: 0.8030\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4147 - acc: 0.8216 - val_loss: 0.4398 - val_acc: 0.8046\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4109 - acc: 0.8183 - val_loss: 0.4392 - val_acc: 0.8046\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4102 - acc: 0.8238 - val_loss: 0.4392 - val_acc: 0.8046\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4086 - acc: 0.8232 - val_loss: 0.4370 - val_acc: 0.8030\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4092 - acc: 0.8219 - val_loss: 0.4376 - val_acc: 0.8046\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4080 - acc: 0.8230 - val_loss: 0.4393 - val_acc: 0.8013\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4097 - acc: 0.8256 - val_loss: 0.4365 - val_acc: 0.8062\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4045 - acc: 0.8254 - val_loss: 0.4358 - val_acc: 0.8030\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4019 - acc: 0.8287 - val_loss: 0.4374 - val_acc: 0.8013\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3996 - acc: 0.8278 - val_loss: 0.4356 - val_acc: 0.8046\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4008 - acc: 0.8241 - val_loss: 0.4360 - val_acc: 0.8046\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3975 - acc: 0.8278 - val_loss: 0.4393 - val_acc: 0.8013\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3981 - acc: 0.8296 - val_loss: 0.4348 - val_acc: 0.8030\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4010 - acc: 0.8265 - val_loss: 0.4345 - val_acc: 0.8030\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3962 - acc: 0.8278 - val_loss: 0.4372 - val_acc: 0.7997\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3960 - acc: 0.8272 - val_loss: 0.4340 - val_acc: 0.8046\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3988 - acc: 0.8263 - val_loss: 0.4347 - val_acc: 0.8030\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3953 - acc: 0.8263 - val_loss: 0.4367 - val_acc: 0.8030\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3959 - acc: 0.8300 - val_loss: 0.4352 - val_acc: 0.8030\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3902 - acc: 0.8347 - val_loss: 0.4343 - val_acc: 0.8046\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3950 - acc: 0.8281 - val_loss: 0.4361 - val_acc: 0.8030\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.1], 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.7579 - acc: 0.4293 - val_loss: 0.7042 - val_acc: 0.4450\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.7066 - acc: 0.4853 - val_loss: 0.6754 - val_acc: 0.6059\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6772 - acc: 0.5864 - val_loss: 0.6620 - val_acc: 0.6240\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6579 - acc: 0.6260 - val_loss: 0.6548 - val_acc: 0.5764\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6491 - acc: 0.6249 - val_loss: 0.6474 - val_acc: 0.5714\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6382 - acc: 0.6360 - val_loss: 0.6368 - val_acc: 0.6108\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6260 - acc: 0.6605 - val_loss: 0.6250 - val_acc: 0.6782\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6136 - acc: 0.6971 - val_loss: 0.6132 - val_acc: 0.7094\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6048 - acc: 0.7088 - val_loss: 0.6025 - val_acc: 0.7258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5944 - acc: 0.7165 - val_loss: 0.5926 - val_acc: 0.7159\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5809 - acc: 0.7243 - val_loss: 0.5831 - val_acc: 0.7274\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5743 - acc: 0.7230 - val_loss: 0.5738 - val_acc: 0.7356\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5665 - acc: 0.7347 - val_loss: 0.5641 - val_acc: 0.7356\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5556 - acc: 0.7468 - val_loss: 0.5541 - val_acc: 0.7373\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5465 - acc: 0.7490 - val_loss: 0.5438 - val_acc: 0.7471\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5416 - acc: 0.7493 - val_loss: 0.5340 - val_acc: 0.7570\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5320 - acc: 0.7550 - val_loss: 0.5250 - val_acc: 0.7537\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5265 - acc: 0.7583 - val_loss: 0.5173 - val_acc: 0.7570\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5163 - acc: 0.7665 - val_loss: 0.5098 - val_acc: 0.7619\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5119 - acc: 0.7708 - val_loss: 0.5020 - val_acc: 0.7685\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5049 - acc: 0.7758 - val_loss: 0.4962 - val_acc: 0.7718\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4983 - acc: 0.7741 - val_loss: 0.4907 - val_acc: 0.7783\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4933 - acc: 0.7774 - val_loss: 0.4855 - val_acc: 0.7800\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4851 - acc: 0.7834 - val_loss: 0.4812 - val_acc: 0.7816\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4837 - acc: 0.7871 - val_loss: 0.4771 - val_acc: 0.7816\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4795 - acc: 0.7825 - val_loss: 0.4730 - val_acc: 0.7816\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4721 - acc: 0.7898 - val_loss: 0.4710 - val_acc: 0.7783\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4680 - acc: 0.7918 - val_loss: 0.4692 - val_acc: 0.7783\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4665 - acc: 0.7938 - val_loss: 0.4653 - val_acc: 0.7800\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4622 - acc: 0.7933 - val_loss: 0.4634 - val_acc: 0.7849\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4597 - acc: 0.7975 - val_loss: 0.4614 - val_acc: 0.7833\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4581 - acc: 0.7971 - val_loss: 0.4607 - val_acc: 0.7816\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4529 - acc: 0.7991 - val_loss: 0.4580 - val_acc: 0.7947\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4493 - acc: 0.8050 - val_loss: 0.4578 - val_acc: 0.7833\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4480 - acc: 0.8019 - val_loss: 0.4572 - val_acc: 0.7833\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4503 - acc: 0.8044 - val_loss: 0.4552 - val_acc: 0.7800\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4461 - acc: 0.8044 - val_loss: 0.4534 - val_acc: 0.7947\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4451 - acc: 0.8055 - val_loss: 0.4537 - val_acc: 0.7833\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4407 - acc: 0.8077 - val_loss: 0.4516 - val_acc: 0.7898\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4356 - acc: 0.8097 - val_loss: 0.4496 - val_acc: 0.8013\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4352 - acc: 0.8101 - val_loss: 0.4501 - val_acc: 0.7898\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4361 - acc: 0.8079 - val_loss: 0.4485 - val_acc: 0.7915\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4287 - acc: 0.8113 - val_loss: 0.4476 - val_acc: 0.7947\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4325 - acc: 0.8073 - val_loss: 0.4471 - val_acc: 0.7947\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4313 - acc: 0.8103 - val_loss: 0.4471 - val_acc: 0.7898\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4260 - acc: 0.8165 - val_loss: 0.4459 - val_acc: 0.7947\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4266 - acc: 0.8134 - val_loss: 0.4439 - val_acc: 0.8046\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4254 - acc: 0.8150 - val_loss: 0.4454 - val_acc: 0.7915\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4216 - acc: 0.8154 - val_loss: 0.4451 - val_acc: 0.7915\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4176 - acc: 0.8165 - val_loss: 0.4423 - val_acc: 0.8046\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4202 - acc: 0.8183 - val_loss: 0.4435 - val_acc: 0.7931\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4169 - acc: 0.8223 - val_loss: 0.4432 - val_acc: 0.7964\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4152 - acc: 0.8216 - val_loss: 0.4417 - val_acc: 0.7980\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4182 - acc: 0.8177 - val_loss: 0.4418 - val_acc: 0.7964\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4129 - acc: 0.8192 - val_loss: 0.4404 - val_acc: 0.7980\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4108 - acc: 0.8234 - val_loss: 0.4406 - val_acc: 0.7964\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4095 - acc: 0.8208 - val_loss: 0.4397 - val_acc: 0.7980\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4083 - acc: 0.8245 - val_loss: 0.4405 - val_acc: 0.7964\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4075 - acc: 0.8245 - val_loss: 0.4403 - val_acc: 0.7964\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4099 - acc: 0.8232 - val_loss: 0.4387 - val_acc: 0.7997\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4068 - acc: 0.8227 - val_loss: 0.4396 - val_acc: 0.7980\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4064 - acc: 0.8247 - val_loss: 0.4397 - val_acc: 0.7980\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4043 - acc: 0.8245 - val_loss: 0.4381 - val_acc: 0.7980\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4005 - acc: 0.8239 - val_loss: 0.4396 - val_acc: 0.7980\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4021 - acc: 0.8263 - val_loss: 0.4405 - val_acc: 0.7964\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4028 - acc: 0.8287 - val_loss: 0.4380 - val_acc: 0.7980\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3990 - acc: 0.8258 - val_loss: 0.4387 - val_acc: 0.7980\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3984 - acc: 0.8303 - val_loss: 0.4394 - val_acc: 0.7997\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3947 - acc: 0.8250 - val_loss: 0.4380 - val_acc: 0.8013\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3975 - acc: 0.8272 - val_loss: 0.4390 - val_acc: 0.8030\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3936 - acc: 0.8321 - val_loss: 0.4403 - val_acc: 0.8013\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.1], 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.8273 - acc: 0.4269 - val_loss: 0.7269 - val_acc: 0.4417\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7271 - acc: 0.4641 - val_loss: 0.6790 - val_acc: 0.6076\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6806 - acc: 0.5734 - val_loss: 0.6645 - val_acc: 0.5665\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6605 - acc: 0.6130 - val_loss: 0.6564 - val_acc: 0.5567\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6472 - acc: 0.6154 - val_loss: 0.6421 - val_acc: 0.5665\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6325 - acc: 0.6460 - val_loss: 0.6246 - val_acc: 0.6650\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6129 - acc: 0.6898 - val_loss: 0.6102 - val_acc: 0.7176\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6017 - acc: 0.6980 - val_loss: 0.5984 - val_acc: 0.7061\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5918 - acc: 0.6979 - val_loss: 0.5878 - val_acc: 0.7044\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5803 - acc: 0.7084 - val_loss: 0.5779 - val_acc: 0.7291\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5695 - acc: 0.7198 - val_loss: 0.5673 - val_acc: 0.7340\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5599 - acc: 0.7367 - val_loss: 0.5560 - val_acc: 0.7356\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5485 - acc: 0.7444 - val_loss: 0.5441 - val_acc: 0.7373\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5370 - acc: 0.7521 - val_loss: 0.5320 - val_acc: 0.7356\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5295 - acc: 0.7542 - val_loss: 0.5212 - val_acc: 0.7586\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5165 - acc: 0.7659 - val_loss: 0.5115 - val_acc: 0.7619\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5083 - acc: 0.7725 - val_loss: 0.5021 - val_acc: 0.7734\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5001 - acc: 0.7703 - val_loss: 0.4923 - val_acc: 0.7701\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4968 - acc: 0.7749 - val_loss: 0.4860 - val_acc: 0.7800\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4876 - acc: 0.7794 - val_loss: 0.4818 - val_acc: 0.7816\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4790 - acc: 0.7827 - val_loss: 0.4765 - val_acc: 0.7898\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4745 - acc: 0.7876 - val_loss: 0.4717 - val_acc: 0.7865\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4686 - acc: 0.7918 - val_loss: 0.4685 - val_acc: 0.7849\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4636 - acc: 0.7909 - val_loss: 0.4654 - val_acc: 0.7849\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4597 - acc: 0.7947 - val_loss: 0.4611 - val_acc: 0.7882\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4517 - acc: 0.7991 - val_loss: 0.4590 - val_acc: 0.7915\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4463 - acc: 0.8020 - val_loss: 0.4574 - val_acc: 0.7898\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4422 - acc: 0.8079 - val_loss: 0.4561 - val_acc: 0.7898\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4401 - acc: 0.8064 - val_loss: 0.4538 - val_acc: 0.7915\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4399 - acc: 0.8099 - val_loss: 0.4544 - val_acc: 0.7898\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4380 - acc: 0.8093 - val_loss: 0.4510 - val_acc: 0.7931\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4346 - acc: 0.8075 - val_loss: 0.4492 - val_acc: 0.7915\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4338 - acc: 0.8082 - val_loss: 0.4490 - val_acc: 0.7915\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4282 - acc: 0.8104 - val_loss: 0.4489 - val_acc: 0.7915\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4252 - acc: 0.8148 - val_loss: 0.4463 - val_acc: 0.7931\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4253 - acc: 0.8150 - val_loss: 0.4479 - val_acc: 0.7964\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4266 - acc: 0.8110 - val_loss: 0.4459 - val_acc: 0.7931\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4206 - acc: 0.8155 - val_loss: 0.4460 - val_acc: 0.7964\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4185 - acc: 0.8179 - val_loss: 0.4438 - val_acc: 0.7964\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4153 - acc: 0.8168 - val_loss: 0.4428 - val_acc: 0.7980\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4146 - acc: 0.8228 - val_loss: 0.4422 - val_acc: 0.7964\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4131 - acc: 0.8199 - val_loss: 0.4426 - val_acc: 0.7947\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4068 - acc: 0.8216 - val_loss: 0.4468 - val_acc: 0.7980\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4087 - acc: 0.8225 - val_loss: 0.4403 - val_acc: 0.8030\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4058 - acc: 0.8239 - val_loss: 0.4426 - val_acc: 0.7997\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4069 - acc: 0.8223 - val_loss: 0.4405 - val_acc: 0.7980\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4030 - acc: 0.8228 - val_loss: 0.4408 - val_acc: 0.7980\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4032 - acc: 0.8250 - val_loss: 0.4405 - val_acc: 0.7997\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4026 - acc: 0.8259 - val_loss: 0.4403 - val_acc: 0.8013\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.01], 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 39ms/step - loss: 0.7474 - acc: 0.4388 - val_loss: 0.6858 - val_acc: 0.5468\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6731 - acc: 0.5738 - val_loss: 0.6746 - val_acc: 0.5583\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6521 - acc: 0.5787 - val_loss: 0.6468 - val_acc: 0.5632\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6248 - acc: 0.6371 - val_loss: 0.6156 - val_acc: 0.7011\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5996 - acc: 0.7134 - val_loss: 0.5970 - val_acc: 0.7044\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5845 - acc: 0.7095 - val_loss: 0.5792 - val_acc: 0.7159\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5663 - acc: 0.7349 - val_loss: 0.5645 - val_acc: 0.7323\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5521 - acc: 0.7500 - val_loss: 0.5466 - val_acc: 0.7389\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5345 - acc: 0.7573 - val_loss: 0.5284 - val_acc: 0.7389\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5201 - acc: 0.7625 - val_loss: 0.5142 - val_acc: 0.7537\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5101 - acc: 0.7734 - val_loss: 0.5025 - val_acc: 0.7521\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4943 - acc: 0.7840 - val_loss: 0.4905 - val_acc: 0.7635\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4858 - acc: 0.7858 - val_loss: 0.4823 - val_acc: 0.7668\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4775 - acc: 0.7885 - val_loss: 0.4756 - val_acc: 0.7718\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4662 - acc: 0.7951 - val_loss: 0.4700 - val_acc: 0.7800\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4595 - acc: 0.7980 - val_loss: 0.4639 - val_acc: 0.7800\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4514 - acc: 0.8031 - val_loss: 0.4611 - val_acc: 0.7833\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4479 - acc: 0.8037 - val_loss: 0.4563 - val_acc: 0.7915\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4413 - acc: 0.8046 - val_loss: 0.4539 - val_acc: 0.7882\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4355 - acc: 0.8113 - val_loss: 0.4519 - val_acc: 0.7833\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4327 - acc: 0.8130 - val_loss: 0.4489 - val_acc: 0.7849\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4303 - acc: 0.8110 - val_loss: 0.4483 - val_acc: 0.7849\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4252 - acc: 0.8155 - val_loss: 0.4449 - val_acc: 0.7898\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4223 - acc: 0.8166 - val_loss: 0.4447 - val_acc: 0.7865\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4204 - acc: 0.8144 - val_loss: 0.4431 - val_acc: 0.7882\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4143 - acc: 0.8207 - val_loss: 0.4433 - val_acc: 0.7898\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4107 - acc: 0.8188 - val_loss: 0.4406 - val_acc: 0.7947\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4105 - acc: 0.8196 - val_loss: 0.4433 - val_acc: 0.7947\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4090 - acc: 0.8201 - val_loss: 0.4383 - val_acc: 0.7947\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4061 - acc: 0.8248 - val_loss: 0.4396 - val_acc: 0.7931\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4011 - acc: 0.8283 - val_loss: 0.4372 - val_acc: 0.7980\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4015 - acc: 0.8217 - val_loss: 0.4387 - val_acc: 0.7964\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4001 - acc: 0.8243 - val_loss: 0.4362 - val_acc: 0.8013\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3951 - acc: 0.8296 - val_loss: 0.4371 - val_acc: 0.7997\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3943 - acc: 0.8287 - val_loss: 0.4365 - val_acc: 0.8030\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3920 - acc: 0.8281 - val_loss: 0.4364 - val_acc: 0.8013\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3903 - acc: 0.8352 - val_loss: 0.4339 - val_acc: 0.8030\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3871 - acc: 0.8334 - val_loss: 0.4356 - val_acc: 0.7997\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3849 - acc: 0.8329 - val_loss: 0.4343 - val_acc: 0.8030\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3826 - acc: 0.8358 - val_loss: 0.4347 - val_acc: 0.8030\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3782 - acc: 0.8371 - val_loss: 0.4347 - val_acc: 0.8030\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3808 - acc: 0.8365 - val_loss: 0.4325 - val_acc: 0.8013\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3781 - acc: 0.8360 - val_loss: 0.4357 - val_acc: 0.8062\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3750 - acc: 0.8360 - val_loss: 0.4326 - val_acc: 0.8013\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3737 - acc: 0.8391 - val_loss: 0.4354 - val_acc: 0.8062\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3714 - acc: 0.8389 - val_loss: 0.4342 - val_acc: 0.8079\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3690 - acc: 0.8411 - val_loss: 0.4361 - val_acc: 0.8079\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.01], 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.8268 - acc: 0.4297 - val_loss: 0.7265 - val_acc: 0.4417\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7266 - acc: 0.4696 - val_loss: 0.6786 - val_acc: 0.6059\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6840 - acc: 0.5727 - val_loss: 0.6644 - val_acc: 0.5665\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6625 - acc: 0.6039 - val_loss: 0.6565 - val_acc: 0.5567\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6474 - acc: 0.6101 - val_loss: 0.6412 - val_acc: 0.5665\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6251 - acc: 0.6597 - val_loss: 0.6234 - val_acc: 0.6683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6114 - acc: 0.6962 - val_loss: 0.6090 - val_acc: 0.7225\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6004 - acc: 0.6926 - val_loss: 0.5973 - val_acc: 0.7110\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5862 - acc: 0.7077 - val_loss: 0.5867 - val_acc: 0.7110\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5796 - acc: 0.7141 - val_loss: 0.5771 - val_acc: 0.7241\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5689 - acc: 0.7234 - val_loss: 0.5666 - val_acc: 0.7356\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5566 - acc: 0.7371 - val_loss: 0.5538 - val_acc: 0.7323\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5480 - acc: 0.7356 - val_loss: 0.5408 - val_acc: 0.7340\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5374 - acc: 0.7495 - val_loss: 0.5300 - val_acc: 0.7504\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5261 - acc: 0.7608 - val_loss: 0.5193 - val_acc: 0.7603\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5140 - acc: 0.7661 - val_loss: 0.5088 - val_acc: 0.7652\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5067 - acc: 0.7699 - val_loss: 0.4994 - val_acc: 0.7734\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4970 - acc: 0.7729 - val_loss: 0.4911 - val_acc: 0.7783\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4922 - acc: 0.7769 - val_loss: 0.4843 - val_acc: 0.7849\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4805 - acc: 0.7871 - val_loss: 0.4776 - val_acc: 0.7849\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4763 - acc: 0.7843 - val_loss: 0.4749 - val_acc: 0.7865\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4717 - acc: 0.7878 - val_loss: 0.4703 - val_acc: 0.7833\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4687 - acc: 0.7885 - val_loss: 0.4650 - val_acc: 0.7865\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4606 - acc: 0.7957 - val_loss: 0.4645 - val_acc: 0.7833\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4572 - acc: 0.7984 - val_loss: 0.4610 - val_acc: 0.7882\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4531 - acc: 0.8004 - val_loss: 0.4579 - val_acc: 0.7898\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4469 - acc: 0.7997 - val_loss: 0.4595 - val_acc: 0.7833\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4446 - acc: 0.8033 - val_loss: 0.4552 - val_acc: 0.7915\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4412 - acc: 0.8050 - val_loss: 0.4532 - val_acc: 0.7915\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4378 - acc: 0.8097 - val_loss: 0.4527 - val_acc: 0.7898\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4334 - acc: 0.8082 - val_loss: 0.4509 - val_acc: 0.7898\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4310 - acc: 0.8130 - val_loss: 0.4517 - val_acc: 0.7898\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4319 - acc: 0.8101 - val_loss: 0.4491 - val_acc: 0.7882\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4276 - acc: 0.8132 - val_loss: 0.4479 - val_acc: 0.7882\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4261 - acc: 0.8112 - val_loss: 0.4464 - val_acc: 0.7964\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4211 - acc: 0.8154 - val_loss: 0.4479 - val_acc: 0.7915\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4222 - acc: 0.8137 - val_loss: 0.4456 - val_acc: 0.7915\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4194 - acc: 0.8170 - val_loss: 0.4449 - val_acc: 0.7947\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4193 - acc: 0.8137 - val_loss: 0.4435 - val_acc: 0.7931\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4175 - acc: 0.8185 - val_loss: 0.4477 - val_acc: 0.7931\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4153 - acc: 0.8186 - val_loss: 0.4412 - val_acc: 0.7980\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4118 - acc: 0.8196 - val_loss: 0.4443 - val_acc: 0.7964\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4106 - acc: 0.8238 - val_loss: 0.4418 - val_acc: 0.8030\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4121 - acc: 0.8227 - val_loss: 0.4400 - val_acc: 0.8013\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4103 - acc: 0.8205 - val_loss: 0.4447 - val_acc: 0.7997\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4066 - acc: 0.8203 - val_loss: 0.4390 - val_acc: 0.7997\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4032 - acc: 0.8265 - val_loss: 0.4434 - val_acc: 0.7997\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4046 - acc: 0.8259 - val_loss: 0.4403 - val_acc: 0.8046\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4034 - acc: 0.8252 - val_loss: 0.4389 - val_acc: 0.8030\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3973 - acc: 0.8252 - val_loss: 0.4435 - val_acc: 0.7980\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3977 - acc: 0.8283 - val_loss: 0.4374 - val_acc: 0.8046\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3991 - acc: 0.8267 - val_loss: 0.4378 - val_acc: 0.8013\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3946 - acc: 0.8318 - val_loss: 0.4393 - val_acc: 0.7997\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3959 - acc: 0.8276 - val_loss: 0.4383 - val_acc: 0.7997\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3948 - acc: 0.8325 - val_loss: 0.4403 - val_acc: 0.7997\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3923 - acc: 0.8296 - val_loss: 0.4382 - val_acc: 0.8013\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.01], 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.8085 - acc: 0.4240 - val_loss: 0.7051 - val_acc: 0.4696\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6969 - acc: 0.5085 - val_loss: 0.6738 - val_acc: 0.5534\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6673 - acc: 0.5798 - val_loss: 0.6599 - val_acc: 0.5583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6443 - acc: 0.5902 - val_loss: 0.6317 - val_acc: 0.5977\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6170 - acc: 0.6524 - val_loss: 0.6068 - val_acc: 0.7192\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6003 - acc: 0.7074 - val_loss: 0.5932 - val_acc: 0.6913\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5854 - acc: 0.7081 - val_loss: 0.5788 - val_acc: 0.6929\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5707 - acc: 0.7271 - val_loss: 0.5650 - val_acc: 0.7159\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5590 - acc: 0.7347 - val_loss: 0.5507 - val_acc: 0.7307\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5457 - acc: 0.7506 - val_loss: 0.5352 - val_acc: 0.7356\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5300 - acc: 0.7519 - val_loss: 0.5219 - val_acc: 0.7389\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5175 - acc: 0.7614 - val_loss: 0.5090 - val_acc: 0.7422\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5070 - acc: 0.7736 - val_loss: 0.4989 - val_acc: 0.7570\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4962 - acc: 0.7800 - val_loss: 0.4895 - val_acc: 0.7570\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4866 - acc: 0.7842 - val_loss: 0.4821 - val_acc: 0.7619\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4809 - acc: 0.7840 - val_loss: 0.4769 - val_acc: 0.7767\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4731 - acc: 0.7876 - val_loss: 0.4715 - val_acc: 0.7750\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4632 - acc: 0.7953 - val_loss: 0.4703 - val_acc: 0.7783\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4578 - acc: 0.7989 - val_loss: 0.4635 - val_acc: 0.7849\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4513 - acc: 0.8028 - val_loss: 0.4602 - val_acc: 0.7833\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4468 - acc: 0.8041 - val_loss: 0.4592 - val_acc: 0.7849\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4417 - acc: 0.8084 - val_loss: 0.4553 - val_acc: 0.7865\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4380 - acc: 0.8090 - val_loss: 0.4534 - val_acc: 0.7849\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4339 - acc: 0.8092 - val_loss: 0.4530 - val_acc: 0.7882\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4315 - acc: 0.8159 - val_loss: 0.4496 - val_acc: 0.7898\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4263 - acc: 0.8126 - val_loss: 0.4487 - val_acc: 0.7915\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4248 - acc: 0.8190 - val_loss: 0.4471 - val_acc: 0.7931\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4217 - acc: 0.8174 - val_loss: 0.4452 - val_acc: 0.7980\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4189 - acc: 0.8174 - val_loss: 0.4448 - val_acc: 0.7898\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4158 - acc: 0.8236 - val_loss: 0.4434 - val_acc: 0.7947\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4121 - acc: 0.8214 - val_loss: 0.4431 - val_acc: 0.7915\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4114 - acc: 0.8227 - val_loss: 0.4412 - val_acc: 0.7964\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4098 - acc: 0.8208 - val_loss: 0.4406 - val_acc: 0.7980\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4078 - acc: 0.8234 - val_loss: 0.4385 - val_acc: 0.8013\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4077 - acc: 0.8192 - val_loss: 0.4399 - val_acc: 0.7997\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4024 - acc: 0.8261 - val_loss: 0.4384 - val_acc: 0.7997\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4046 - acc: 0.8238 - val_loss: 0.4387 - val_acc: 0.8013\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4028 - acc: 0.8270 - val_loss: 0.4404 - val_acc: 0.7964\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4028 - acc: 0.8210 - val_loss: 0.4370 - val_acc: 0.8046\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3959 - acc: 0.8270 - val_loss: 0.4436 - val_acc: 0.7931\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3942 - acc: 0.8298 - val_loss: 0.4366 - val_acc: 0.8095\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3926 - acc: 0.8225 - val_loss: 0.4420 - val_acc: 0.7931\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3948 - acc: 0.8321 - val_loss: 0.4361 - val_acc: 0.8046\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3904 - acc: 0.8309 - val_loss: 0.4361 - val_acc: 0.8046\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3870 - acc: 0.8356 - val_loss: 0.4383 - val_acc: 0.7964\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3845 - acc: 0.8325 - val_loss: 0.4354 - val_acc: 0.8079\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3848 - acc: 0.8321 - val_loss: 0.4371 - val_acc: 0.7997\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3843 - acc: 0.8340 - val_loss: 0.4345 - val_acc: 0.8046\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3819 - acc: 0.8336 - val_loss: 0.4349 - val_acc: 0.8046\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3804 - acc: 0.8354 - val_loss: 0.4359 - val_acc: 0.8030\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3790 - acc: 0.8382 - val_loss: 0.4331 - val_acc: 0.8095\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3732 - acc: 0.8396 - val_loss: 0.4371 - val_acc: 0.8030\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3718 - acc: 0.8418 - val_loss: 0.4323 - val_acc: 0.8079\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - acc: 0.8413 - val_loss: 0.4352 - val_acc: 0.8030\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3701 - acc: 0.8425 - val_loss: 0.4330 - val_acc: 0.8079\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3658 - acc: 0.8387 - val_loss: 0.4350 - val_acc: 0.8030\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3646 - acc: 0.8422 - val_loss: 0.4348 - val_acc: 0.8046\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3641 - acc: 0.8424 - val_loss: 0.4355 - val_acc: 0.8062\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.01], 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 29ms/step - loss: 0.8549 - acc: 0.4235 - val_loss: 0.7518 - val_acc: 0.4433\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.7508 - acc: 0.4384 - val_loss: 0.6971 - val_acc: 0.4713\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6969 - acc: 0.5176 - val_loss: 0.6737 - val_acc: 0.6273\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6710 - acc: 0.5953 - val_loss: 0.6630 - val_acc: 0.5813\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6548 - acc: 0.6236 - val_loss: 0.6524 - val_acc: 0.5878\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6398 - acc: 0.6501 - val_loss: 0.6386 - val_acc: 0.6355\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6264 - acc: 0.6787 - val_loss: 0.6242 - val_acc: 0.6995\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6131 - acc: 0.7039 - val_loss: 0.6118 - val_acc: 0.7110\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6017 - acc: 0.7123 - val_loss: 0.6013 - val_acc: 0.7028\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5929 - acc: 0.7148 - val_loss: 0.5917 - val_acc: 0.6979\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5823 - acc: 0.7192 - val_loss: 0.5828 - val_acc: 0.7126\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5709 - acc: 0.7305 - val_loss: 0.5740 - val_acc: 0.7258\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5633 - acc: 0.7437 - val_loss: 0.5650 - val_acc: 0.7323\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5558 - acc: 0.7411 - val_loss: 0.5554 - val_acc: 0.7258\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5494 - acc: 0.7451 - val_loss: 0.5465 - val_acc: 0.7356\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5375 - acc: 0.7453 - val_loss: 0.5381 - val_acc: 0.7389\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5304 - acc: 0.7530 - val_loss: 0.5299 - val_acc: 0.7455\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5238 - acc: 0.7665 - val_loss: 0.5216 - val_acc: 0.7504\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5143 - acc: 0.7677 - val_loss: 0.5138 - val_acc: 0.7537\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5104 - acc: 0.7663 - val_loss: 0.5066 - val_acc: 0.7553\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5038 - acc: 0.7712 - val_loss: 0.5008 - val_acc: 0.7635\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4973 - acc: 0.7818 - val_loss: 0.4954 - val_acc: 0.7668\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4892 - acc: 0.7831 - val_loss: 0.4897 - val_acc: 0.7635\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4846 - acc: 0.7820 - val_loss: 0.4852 - val_acc: 0.7685\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4792 - acc: 0.7867 - val_loss: 0.4822 - val_acc: 0.7767\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4754 - acc: 0.7865 - val_loss: 0.4775 - val_acc: 0.7800\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4693 - acc: 0.7933 - val_loss: 0.4734 - val_acc: 0.7816\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4652 - acc: 0.7966 - val_loss: 0.4706 - val_acc: 0.7865\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4621 - acc: 0.7929 - val_loss: 0.4684 - val_acc: 0.7849\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4592 - acc: 0.7999 - val_loss: 0.4650 - val_acc: 0.7947\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4539 - acc: 0.8000 - val_loss: 0.4624 - val_acc: 0.7931\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4506 - acc: 0.8024 - val_loss: 0.4610 - val_acc: 0.7882\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4481 - acc: 0.8015 - val_loss: 0.4586 - val_acc: 0.7947\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4451 - acc: 0.8086 - val_loss: 0.4563 - val_acc: 0.7947\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4427 - acc: 0.8093 - val_loss: 0.4566 - val_acc: 0.7882\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4401 - acc: 0.8068 - val_loss: 0.4531 - val_acc: 0.7964\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4370 - acc: 0.8082 - val_loss: 0.4522 - val_acc: 0.7898\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4364 - acc: 0.8092 - val_loss: 0.4510 - val_acc: 0.7898\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4332 - acc: 0.8061 - val_loss: 0.4495 - val_acc: 0.7915\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4328 - acc: 0.8066 - val_loss: 0.4477 - val_acc: 0.7964\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4291 - acc: 0.8137 - val_loss: 0.4476 - val_acc: 0.7980\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4288 - acc: 0.8148 - val_loss: 0.4460 - val_acc: 0.7980\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4268 - acc: 0.8137 - val_loss: 0.4456 - val_acc: 0.7980\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4228 - acc: 0.8170 - val_loss: 0.4439 - val_acc: 0.8030\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4219 - acc: 0.8168 - val_loss: 0.4437 - val_acc: 0.8013\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4204 - acc: 0.8163 - val_loss: 0.4437 - val_acc: 0.7997\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4180 - acc: 0.8212 - val_loss: 0.4443 - val_acc: 0.7997\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4164 - acc: 0.8194 - val_loss: 0.4418 - val_acc: 0.8062\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4192 - acc: 0.8163 - val_loss: 0.4420 - val_acc: 0.8030\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4144 - acc: 0.8194 - val_loss: 0.4406 - val_acc: 0.8079\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4127 - acc: 0.8210 - val_loss: 0.4396 - val_acc: 0.8079\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4122 - acc: 0.8203 - val_loss: 0.4422 - val_acc: 0.7980\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4108 - acc: 0.8210 - val_loss: 0.4379 - val_acc: 0.8046\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4109 - acc: 0.8168 - val_loss: 0.4382 - val_acc: 0.8046\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4054 - acc: 0.8236 - val_loss: 0.4411 - val_acc: 0.7964\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4065 - acc: 0.8219 - val_loss: 0.4367 - val_acc: 0.8013\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4059 - acc: 0.8216 - val_loss: 0.4381 - val_acc: 0.8030\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4054 - acc: 0.8261 - val_loss: 0.4385 - val_acc: 0.7997\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4044 - acc: 0.8185 - val_loss: 0.4358 - val_acc: 0.8062\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3991 - acc: 0.8289 - val_loss: 0.4374 - val_acc: 0.7997\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3990 - acc: 0.8289 - val_loss: 0.4364 - val_acc: 0.8046\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3985 - acc: 0.8272 - val_loss: 0.4357 - val_acc: 0.8046\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3992 - acc: 0.8269 - val_loss: 0.4368 - val_acc: 0.8013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3955 - acc: 0.8274 - val_loss: 0.4341 - val_acc: 0.8046\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3990 - acc: 0.8267 - val_loss: 0.4363 - val_acc: 0.7997\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3936 - acc: 0.8314 - val_loss: 0.4344 - val_acc: 0.8046\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3937 - acc: 0.8312 - val_loss: 0.4350 - val_acc: 0.8046\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3906 - acc: 0.8314 - val_loss: 0.4355 - val_acc: 0.8013\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3915 - acc: 0.8307 - val_loss: 0.4340 - val_acc: 0.8046\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3902 - acc: 0.8305 - val_loss: 0.4339 - val_acc: 0.8046\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3913 - acc: 0.8316 - val_loss: 0.4335 - val_acc: 0.8046\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3907 - acc: 0.8356 - val_loss: 0.4340 - val_acc: 0.8030\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3858 - acc: 0.8365 - val_loss: 0.4344 - val_acc: 0.8030\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3864 - acc: 0.8336 - val_loss: 0.4343 - val_acc: 0.8013\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3840 - acc: 0.8318 - val_loss: 0.4352 - val_acc: 0.8013\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3863 - acc: 0.8323 - val_loss: 0.4336 - val_acc: 0.8030\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.01], 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.7589 - acc: 0.4253 - val_loss: 0.7042 - val_acc: 0.4450\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7034 - acc: 0.4849 - val_loss: 0.6751 - val_acc: 0.6125\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6731 - acc: 0.6094 - val_loss: 0.6616 - val_acc: 0.6223\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6579 - acc: 0.6357 - val_loss: 0.6548 - val_acc: 0.5714\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 0.6451 - acc: 0.6304 - val_loss: 0.6474 - val_acc: 0.5747\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6376 - acc: 0.6285 - val_loss: 0.6370 - val_acc: 0.6026\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6266 - acc: 0.6597 - val_loss: 0.6249 - val_acc: 0.6782\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6141 - acc: 0.6971 - val_loss: 0.6129 - val_acc: 0.7094\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6046 - acc: 0.7105 - val_loss: 0.6024 - val_acc: 0.7241\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5945 - acc: 0.7159 - val_loss: 0.5928 - val_acc: 0.7209\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5822 - acc: 0.7256 - val_loss: 0.5833 - val_acc: 0.7225\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5732 - acc: 0.7294 - val_loss: 0.5738 - val_acc: 0.7356\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5659 - acc: 0.7398 - val_loss: 0.5636 - val_acc: 0.7356\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5536 - acc: 0.7473 - val_loss: 0.5530 - val_acc: 0.7373\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5440 - acc: 0.7506 - val_loss: 0.5426 - val_acc: 0.7455\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5375 - acc: 0.7542 - val_loss: 0.5335 - val_acc: 0.7504\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5290 - acc: 0.7603 - val_loss: 0.5246 - val_acc: 0.7537\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5196 - acc: 0.7707 - val_loss: 0.5166 - val_acc: 0.7603\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5140 - acc: 0.7699 - val_loss: 0.5080 - val_acc: 0.7668\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5090 - acc: 0.7707 - val_loss: 0.5005 - val_acc: 0.7652\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5011 - acc: 0.7721 - val_loss: 0.4950 - val_acc: 0.7734\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4931 - acc: 0.7805 - val_loss: 0.4901 - val_acc: 0.7767\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4906 - acc: 0.7770 - val_loss: 0.4858 - val_acc: 0.7783\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4875 - acc: 0.7818 - val_loss: 0.4805 - val_acc: 0.7816\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4797 - acc: 0.7840 - val_loss: 0.4774 - val_acc: 0.7833\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4766 - acc: 0.7882 - val_loss: 0.4748 - val_acc: 0.7767\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4733 - acc: 0.7913 - val_loss: 0.4720 - val_acc: 0.7783\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4660 - acc: 0.7953 - val_loss: 0.4678 - val_acc: 0.7882\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4614 - acc: 0.7893 - val_loss: 0.4672 - val_acc: 0.7800\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4598 - acc: 0.7971 - val_loss: 0.4654 - val_acc: 0.7783\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4592 - acc: 0.8004 - val_loss: 0.4616 - val_acc: 0.7898\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4528 - acc: 0.8019 - val_loss: 0.4600 - val_acc: 0.7898\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4526 - acc: 0.8000 - val_loss: 0.4602 - val_acc: 0.7833\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4469 - acc: 0.8042 - val_loss: 0.4576 - val_acc: 0.7898\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4464 - acc: 0.8033 - val_loss: 0.4553 - val_acc: 0.7964\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4436 - acc: 0.8051 - val_loss: 0.4549 - val_acc: 0.7898\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4436 - acc: 0.8092 - val_loss: 0.4569 - val_acc: 0.7783\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4398 - acc: 0.8070 - val_loss: 0.4518 - val_acc: 0.7947\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4364 - acc: 0.8064 - val_loss: 0.4505 - val_acc: 0.7980\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4348 - acc: 0.8115 - val_loss: 0.4525 - val_acc: 0.7833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4358 - acc: 0.8068 - val_loss: 0.4489 - val_acc: 0.7980\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4322 - acc: 0.8119 - val_loss: 0.4491 - val_acc: 0.7931\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4298 - acc: 0.8159 - val_loss: 0.4514 - val_acc: 0.7849\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4277 - acc: 0.8152 - val_loss: 0.4476 - val_acc: 0.7980\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4265 - acc: 0.8143 - val_loss: 0.4476 - val_acc: 0.7964\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4264 - acc: 0.8141 - val_loss: 0.4482 - val_acc: 0.7915\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4240 - acc: 0.8179 - val_loss: 0.4459 - val_acc: 0.7964\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4210 - acc: 0.8176 - val_loss: 0.4455 - val_acc: 0.7964\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4193 - acc: 0.8194 - val_loss: 0.4462 - val_acc: 0.7947\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4205 - acc: 0.8179 - val_loss: 0.4444 - val_acc: 0.7980\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4207 - acc: 0.8159 - val_loss: 0.4424 - val_acc: 0.7997\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4156 - acc: 0.8192 - val_loss: 0.4442 - val_acc: 0.7980\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4113 - acc: 0.8223 - val_loss: 0.4421 - val_acc: 0.8013\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4155 - acc: 0.8205 - val_loss: 0.4425 - val_acc: 0.8013\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4128 - acc: 0.8197 - val_loss: 0.4418 - val_acc: 0.7997\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4125 - acc: 0.8228 - val_loss: 0.4426 - val_acc: 0.7964\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4135 - acc: 0.8194 - val_loss: 0.4404 - val_acc: 0.8030\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4086 - acc: 0.8238 - val_loss: 0.4422 - val_acc: 0.7931\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4060 - acc: 0.8230 - val_loss: 0.4408 - val_acc: 0.7964\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4116 - acc: 0.8208 - val_loss: 0.4396 - val_acc: 0.8030\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4072 - acc: 0.8219 - val_loss: 0.4440 - val_acc: 0.7964\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4053 - acc: 0.8283 - val_loss: 0.4394 - val_acc: 0.7997\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4038 - acc: 0.8239 - val_loss: 0.4410 - val_acc: 0.7947\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4036 - acc: 0.8278 - val_loss: 0.4413 - val_acc: 0.7947\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4015 - acc: 0.8236 - val_loss: 0.4392 - val_acc: 0.7964\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4004 - acc: 0.8243 - val_loss: 0.4406 - val_acc: 0.7980\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4003 - acc: 0.8283 - val_loss: 0.4397 - val_acc: 0.7980\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3984 - acc: 0.8272 - val_loss: 0.4373 - val_acc: 0.8046\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3968 - acc: 0.8265 - val_loss: 0.4396 - val_acc: 0.7997\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3923 - acc: 0.8331 - val_loss: 0.4398 - val_acc: 0.7980\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3957 - acc: 0.8254 - val_loss: 0.4378 - val_acc: 0.7997\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3954 - acc: 0.8294 - val_loss: 0.4386 - val_acc: 0.7997\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3940 - acc: 0.8312 - val_loss: 0.4396 - val_acc: 0.7997\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.01], 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.8225 - acc: 0.4293 - val_loss: 0.7264 - val_acc: 0.4417\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.7262 - acc: 0.4649 - val_loss: 0.6788 - val_acc: 0.6108\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6811 - acc: 0.5780 - val_loss: 0.6659 - val_acc: 0.5632\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6582 - acc: 0.6055 - val_loss: 0.6584 - val_acc: 0.5550\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6451 - acc: 0.6083 - val_loss: 0.6428 - val_acc: 0.5649\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6287 - acc: 0.6488 - val_loss: 0.6251 - val_acc: 0.6585\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6119 - acc: 0.6990 - val_loss: 0.6104 - val_acc: 0.7176\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6026 - acc: 0.6966 - val_loss: 0.5983 - val_acc: 0.7110\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5920 - acc: 0.6951 - val_loss: 0.5874 - val_acc: 0.7077\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5796 - acc: 0.7083 - val_loss: 0.5768 - val_acc: 0.7241\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5702 - acc: 0.7229 - val_loss: 0.5664 - val_acc: 0.7340\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5563 - acc: 0.7360 - val_loss: 0.5549 - val_acc: 0.7340\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5489 - acc: 0.7402 - val_loss: 0.5424 - val_acc: 0.7340\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5362 - acc: 0.7499 - val_loss: 0.5308 - val_acc: 0.7422\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5274 - acc: 0.7599 - val_loss: 0.5195 - val_acc: 0.7603\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5182 - acc: 0.7666 - val_loss: 0.5093 - val_acc: 0.7652\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5071 - acc: 0.7708 - val_loss: 0.4998 - val_acc: 0.7767\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4978 - acc: 0.7760 - val_loss: 0.4924 - val_acc: 0.7800\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4954 - acc: 0.7765 - val_loss: 0.4852 - val_acc: 0.7849\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4858 - acc: 0.7834 - val_loss: 0.4820 - val_acc: 0.7816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4782 - acc: 0.7885 - val_loss: 0.4763 - val_acc: 0.7882\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4711 - acc: 0.7898 - val_loss: 0.4702 - val_acc: 0.7849\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4677 - acc: 0.7876 - val_loss: 0.4695 - val_acc: 0.7816\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4603 - acc: 0.7909 - val_loss: 0.4650 - val_acc: 0.7833\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4545 - acc: 0.7973 - val_loss: 0.4621 - val_acc: 0.7882\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4492 - acc: 0.8026 - val_loss: 0.4603 - val_acc: 0.7915\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4518 - acc: 0.7975 - val_loss: 0.4583 - val_acc: 0.7898\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4465 - acc: 0.8057 - val_loss: 0.4578 - val_acc: 0.7882\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4433 - acc: 0.8024 - val_loss: 0.4547 - val_acc: 0.7915\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4400 - acc: 0.8075 - val_loss: 0.4555 - val_acc: 0.7865\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4388 - acc: 0.8077 - val_loss: 0.4538 - val_acc: 0.7915\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4341 - acc: 0.8112 - val_loss: 0.4495 - val_acc: 0.7980\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4336 - acc: 0.8090 - val_loss: 0.4534 - val_acc: 0.7915\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4307 - acc: 0.8119 - val_loss: 0.4487 - val_acc: 0.7964\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4270 - acc: 0.8134 - val_loss: 0.4470 - val_acc: 0.7964\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4287 - acc: 0.8134 - val_loss: 0.4486 - val_acc: 0.7931\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4220 - acc: 0.8128 - val_loss: 0.4444 - val_acc: 0.7980\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4224 - acc: 0.8150 - val_loss: 0.4459 - val_acc: 0.7898\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4191 - acc: 0.8188 - val_loss: 0.4450 - val_acc: 0.7915\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4170 - acc: 0.8208 - val_loss: 0.4434 - val_acc: 0.7947\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4137 - acc: 0.8155 - val_loss: 0.4437 - val_acc: 0.7915\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4126 - acc: 0.8214 - val_loss: 0.4458 - val_acc: 0.7915\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4124 - acc: 0.8205 - val_loss: 0.4413 - val_acc: 0.7964\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4129 - acc: 0.8186 - val_loss: 0.4440 - val_acc: 0.7947\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4082 - acc: 0.8252 - val_loss: 0.4429 - val_acc: 0.7947\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4077 - acc: 0.8203 - val_loss: 0.4401 - val_acc: 0.7980\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4031 - acc: 0.8252 - val_loss: 0.4418 - val_acc: 0.7980\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4037 - acc: 0.8232 - val_loss: 0.4403 - val_acc: 0.7964\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4010 - acc: 0.8245 - val_loss: 0.4399 - val_acc: 0.7980\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3985 - acc: 0.8269 - val_loss: 0.4392 - val_acc: 0.7997\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3984 - acc: 0.8278 - val_loss: 0.4408 - val_acc: 0.7980\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4009 - acc: 0.8228 - val_loss: 0.4393 - val_acc: 0.7997\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3964 - acc: 0.8280 - val_loss: 0.4390 - val_acc: 0.7997\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3930 - acc: 0.8290 - val_loss: 0.4383 - val_acc: 0.7980\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3941 - acc: 0.8290 - val_loss: 0.4402 - val_acc: 0.7997\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3922 - acc: 0.8327 - val_loss: 0.4363 - val_acc: 0.8013\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3890 - acc: 0.8292 - val_loss: 0.4410 - val_acc: 0.7964\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3909 - acc: 0.8329 - val_loss: 0.4375 - val_acc: 0.8030\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3855 - acc: 0.8340 - val_loss: 0.4367 - val_acc: 0.8013\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3887 - acc: 0.8311 - val_loss: 0.4396 - val_acc: 0.8030\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3829 - acc: 0.8338 - val_loss: 0.4367 - val_acc: 0.7980\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.3], 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 1s 43ms/step - loss: 0.7553 - acc: 0.4408 - val_loss: 0.6860 - val_acc: 0.5468\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6778 - acc: 0.5652 - val_loss: 0.6734 - val_acc: 0.5599\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6580 - acc: 0.5858 - val_loss: 0.6503 - val_acc: 0.5599\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6303 - acc: 0.6296 - val_loss: 0.6181 - val_acc: 0.7011\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.6080 - acc: 0.6907 - val_loss: 0.5993 - val_acc: 0.7028\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5923 - acc: 0.7028 - val_loss: 0.5826 - val_acc: 0.7094\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5779 - acc: 0.7218 - val_loss: 0.5670 - val_acc: 0.7274\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5605 - acc: 0.7409 - val_loss: 0.5531 - val_acc: 0.7373\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5471 - acc: 0.7477 - val_loss: 0.5356 - val_acc: 0.7488\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5306 - acc: 0.7599 - val_loss: 0.5202 - val_acc: 0.7537\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5219 - acc: 0.7635 - val_loss: 0.5068 - val_acc: 0.7537\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5057 - acc: 0.7750 - val_loss: 0.4956 - val_acc: 0.7652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4965 - acc: 0.7752 - val_loss: 0.4862 - val_acc: 0.7685\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4855 - acc: 0.7801 - val_loss: 0.4786 - val_acc: 0.7701\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4761 - acc: 0.7827 - val_loss: 0.4722 - val_acc: 0.7701\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4682 - acc: 0.7927 - val_loss: 0.4672 - val_acc: 0.7783\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4634 - acc: 0.7911 - val_loss: 0.4619 - val_acc: 0.7800\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4564 - acc: 0.7999 - val_loss: 0.4607 - val_acc: 0.7849\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4501 - acc: 0.8066 - val_loss: 0.4552 - val_acc: 0.7865\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4476 - acc: 0.8008 - val_loss: 0.4547 - val_acc: 0.7882\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4398 - acc: 0.8077 - val_loss: 0.4505 - val_acc: 0.7898\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4390 - acc: 0.8055 - val_loss: 0.4480 - val_acc: 0.7915\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4310 - acc: 0.8117 - val_loss: 0.4493 - val_acc: 0.7882\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4298 - acc: 0.8143 - val_loss: 0.4456 - val_acc: 0.7865\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4320 - acc: 0.8097 - val_loss: 0.4465 - val_acc: 0.7865\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.4226 - acc: 0.8163 - val_loss: 0.4440 - val_acc: 0.7898\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4226 - acc: 0.8157 - val_loss: 0.4426 - val_acc: 0.7882\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4196 - acc: 0.8181 - val_loss: 0.4408 - val_acc: 0.7931\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4159 - acc: 0.8157 - val_loss: 0.4415 - val_acc: 0.7931\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4134 - acc: 0.8219 - val_loss: 0.4397 - val_acc: 0.7931\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4092 - acc: 0.8196 - val_loss: 0.4398 - val_acc: 0.7964\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4093 - acc: 0.8272 - val_loss: 0.4396 - val_acc: 0.7964\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4074 - acc: 0.8234 - val_loss: 0.4371 - val_acc: 0.7964\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4044 - acc: 0.8256 - val_loss: 0.4390 - val_acc: 0.7980\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4009 - acc: 0.8250 - val_loss: 0.4371 - val_acc: 0.7964\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3992 - acc: 0.8290 - val_loss: 0.4377 - val_acc: 0.7980\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3977 - acc: 0.8307 - val_loss: 0.4365 - val_acc: 0.7997\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3960 - acc: 0.8269 - val_loss: 0.4352 - val_acc: 0.8046\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3947 - acc: 0.8259 - val_loss: 0.4364 - val_acc: 0.8030\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3909 - acc: 0.8347 - val_loss: 0.4355 - val_acc: 0.8046\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3894 - acc: 0.8323 - val_loss: 0.4347 - val_acc: 0.8046\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3889 - acc: 0.8307 - val_loss: 0.4343 - val_acc: 0.8046\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3846 - acc: 0.8363 - val_loss: 0.4329 - val_acc: 0.8030\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3853 - acc: 0.8316 - val_loss: 0.4339 - val_acc: 0.8030\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3793 - acc: 0.8367 - val_loss: 0.4339 - val_acc: 0.8030\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3801 - acc: 0.8327 - val_loss: 0.4328 - val_acc: 0.8013\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3812 - acc: 0.8352 - val_loss: 0.4354 - val_acc: 0.8046\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3811 - acc: 0.8358 - val_loss: 0.4333 - val_acc: 0.7997\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3744 - acc: 0.8349 - val_loss: 0.4330 - val_acc: 0.8030\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3721 - acc: 0.8411 - val_loss: 0.4331 - val_acc: 0.8046\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3714 - acc: 0.8389 - val_loss: 0.4350 - val_acc: 0.8046\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.3], 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.8271 - acc: 0.4280 - val_loss: 0.7268 - val_acc: 0.4417\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.7330 - acc: 0.4609 - val_loss: 0.6788 - val_acc: 0.6059\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6815 - acc: 0.5676 - val_loss: 0.6656 - val_acc: 0.5665\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6650 - acc: 0.5977 - val_loss: 0.6592 - val_acc: 0.5550\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6503 - acc: 0.6167 - val_loss: 0.6456 - val_acc: 0.5649\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6375 - acc: 0.6378 - val_loss: 0.6277 - val_acc: 0.6371\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6193 - acc: 0.6712 - val_loss: 0.6129 - val_acc: 0.7143\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6085 - acc: 0.6811 - val_loss: 0.6011 - val_acc: 0.7061\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5971 - acc: 0.6918 - val_loss: 0.5907 - val_acc: 0.7077\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5869 - acc: 0.7011 - val_loss: 0.5806 - val_acc: 0.7143\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5796 - acc: 0.7077 - val_loss: 0.5707 - val_acc: 0.7340\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5691 - acc: 0.7185 - val_loss: 0.5603 - val_acc: 0.7307\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5548 - acc: 0.7338 - val_loss: 0.5488 - val_acc: 0.7373\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5500 - acc: 0.7378 - val_loss: 0.5379 - val_acc: 0.7340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5384 - acc: 0.7468 - val_loss: 0.5282 - val_acc: 0.7504\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5303 - acc: 0.7539 - val_loss: 0.5176 - val_acc: 0.7586\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5209 - acc: 0.7623 - val_loss: 0.5080 - val_acc: 0.7685\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5120 - acc: 0.7687 - val_loss: 0.4997 - val_acc: 0.7685\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5014 - acc: 0.7743 - val_loss: 0.4922 - val_acc: 0.7767\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4942 - acc: 0.7776 - val_loss: 0.4855 - val_acc: 0.7849\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4846 - acc: 0.7803 - val_loss: 0.4808 - val_acc: 0.7833\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4797 - acc: 0.7825 - val_loss: 0.4750 - val_acc: 0.7865\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4761 - acc: 0.7853 - val_loss: 0.4718 - val_acc: 0.7849\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4708 - acc: 0.7916 - val_loss: 0.4692 - val_acc: 0.7816\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4644 - acc: 0.7924 - val_loss: 0.4638 - val_acc: 0.7898\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4619 - acc: 0.7927 - val_loss: 0.4614 - val_acc: 0.7898\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4581 - acc: 0.7995 - val_loss: 0.4613 - val_acc: 0.7882\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4543 - acc: 0.8019 - val_loss: 0.4570 - val_acc: 0.7915\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4475 - acc: 0.7975 - val_loss: 0.4568 - val_acc: 0.7882\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4491 - acc: 0.8031 - val_loss: 0.4551 - val_acc: 0.7915\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4456 - acc: 0.8057 - val_loss: 0.4527 - val_acc: 0.7915\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4447 - acc: 0.8066 - val_loss: 0.4513 - val_acc: 0.7931\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4397 - acc: 0.8072 - val_loss: 0.4496 - val_acc: 0.7964\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4386 - acc: 0.8095 - val_loss: 0.4498 - val_acc: 0.7931\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4307 - acc: 0.8113 - val_loss: 0.4486 - val_acc: 0.7898\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4339 - acc: 0.8079 - val_loss: 0.4487 - val_acc: 0.7898\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4314 - acc: 0.8119 - val_loss: 0.4478 - val_acc: 0.7898\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4254 - acc: 0.8124 - val_loss: 0.4460 - val_acc: 0.7898\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4220 - acc: 0.8137 - val_loss: 0.4448 - val_acc: 0.7882\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4207 - acc: 0.8139 - val_loss: 0.4444 - val_acc: 0.7898\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4198 - acc: 0.8197 - val_loss: 0.4442 - val_acc: 0.7898\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4193 - acc: 0.8170 - val_loss: 0.4433 - val_acc: 0.7915\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4199 - acc: 0.8174 - val_loss: 0.4433 - val_acc: 0.7931\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4162 - acc: 0.8188 - val_loss: 0.4437 - val_acc: 0.7964\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4142 - acc: 0.8203 - val_loss: 0.4400 - val_acc: 0.7980\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4133 - acc: 0.8217 - val_loss: 0.4436 - val_acc: 0.7947\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4139 - acc: 0.8192 - val_loss: 0.4424 - val_acc: 0.7980\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4110 - acc: 0.8194 - val_loss: 0.4391 - val_acc: 0.7997\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4085 - acc: 0.8239 - val_loss: 0.4425 - val_acc: 0.7980\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4089 - acc: 0.8245 - val_loss: 0.4399 - val_acc: 0.7964\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4051 - acc: 0.8205 - val_loss: 0.4383 - val_acc: 0.8046\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4021 - acc: 0.8243 - val_loss: 0.4404 - val_acc: 0.7980\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4005 - acc: 0.8254 - val_loss: 0.4386 - val_acc: 0.8013\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3999 - acc: 0.8252 - val_loss: 0.4393 - val_acc: 0.8013\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3987 - acc: 0.8287 - val_loss: 0.4400 - val_acc: 0.7997\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3981 - acc: 0.8261 - val_loss: 0.4378 - val_acc: 0.8030\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3945 - acc: 0.8298 - val_loss: 0.4394 - val_acc: 0.7997\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3934 - acc: 0.8252 - val_loss: 0.4404 - val_acc: 0.7997\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3937 - acc: 0.8270 - val_loss: 0.4382 - val_acc: 0.8013\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3917 - acc: 0.8292 - val_loss: 0.4388 - val_acc: 0.8013\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3926 - acc: 0.8276 - val_loss: 0.4379 - val_acc: 0.8013\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.3], 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 1s 45ms/step - loss: 0.8117 - acc: 0.4298 - val_loss: 0.7052 - val_acc: 0.4713\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.7034 - acc: 0.5047 - val_loss: 0.6746 - val_acc: 0.5534\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6754 - acc: 0.5784 - val_loss: 0.6621 - val_acc: 0.5583\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6552 - acc: 0.5941 - val_loss: 0.6357 - val_acc: 0.5813\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.6309 - acc: 0.6285 - val_loss: 0.6120 - val_acc: 0.7159\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6125 - acc: 0.6889 - val_loss: 0.5977 - val_acc: 0.6979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5956 - acc: 0.6975 - val_loss: 0.5846 - val_acc: 0.6946\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5812 - acc: 0.7052 - val_loss: 0.5713 - val_acc: 0.7192\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5728 - acc: 0.7256 - val_loss: 0.5585 - val_acc: 0.7291\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5542 - acc: 0.7453 - val_loss: 0.5436 - val_acc: 0.7373\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5441 - acc: 0.7469 - val_loss: 0.5298 - val_acc: 0.7455\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5293 - acc: 0.7544 - val_loss: 0.5171 - val_acc: 0.7406\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5176 - acc: 0.7677 - val_loss: 0.5068 - val_acc: 0.7553\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5120 - acc: 0.7681 - val_loss: 0.4965 - val_acc: 0.7570\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5047 - acc: 0.7734 - val_loss: 0.4885 - val_acc: 0.7635\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4927 - acc: 0.7794 - val_loss: 0.4826 - val_acc: 0.7701\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4900 - acc: 0.7758 - val_loss: 0.4767 - val_acc: 0.7701\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4765 - acc: 0.7867 - val_loss: 0.4720 - val_acc: 0.7783\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4715 - acc: 0.7898 - val_loss: 0.4691 - val_acc: 0.7783\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4641 - acc: 0.7938 - val_loss: 0.4648 - val_acc: 0.7833\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4589 - acc: 0.7955 - val_loss: 0.4614 - val_acc: 0.7865\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4544 - acc: 0.8002 - val_loss: 0.4594 - val_acc: 0.7931\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4527 - acc: 0.8015 - val_loss: 0.4569 - val_acc: 0.7947\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4454 - acc: 0.8057 - val_loss: 0.4551 - val_acc: 0.7931\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4428 - acc: 0.8051 - val_loss: 0.4537 - val_acc: 0.7915\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4406 - acc: 0.8141 - val_loss: 0.4537 - val_acc: 0.7964\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4371 - acc: 0.8104 - val_loss: 0.4498 - val_acc: 0.7898\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4351 - acc: 0.8055 - val_loss: 0.4503 - val_acc: 0.7964\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4273 - acc: 0.8207 - val_loss: 0.4483 - val_acc: 0.7980\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4287 - acc: 0.8139 - val_loss: 0.4493 - val_acc: 0.7915\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4245 - acc: 0.8203 - val_loss: 0.4473 - val_acc: 0.7947\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4217 - acc: 0.8192 - val_loss: 0.4463 - val_acc: 0.7964\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4202 - acc: 0.8196 - val_loss: 0.4451 - val_acc: 0.7980\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4163 - acc: 0.8185 - val_loss: 0.4447 - val_acc: 0.7980\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4181 - acc: 0.8190 - val_loss: 0.4441 - val_acc: 0.7980\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4152 - acc: 0.8227 - val_loss: 0.4447 - val_acc: 0.7882\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4116 - acc: 0.8265 - val_loss: 0.4415 - val_acc: 0.8013\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4094 - acc: 0.8223 - val_loss: 0.4421 - val_acc: 0.7997\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4073 - acc: 0.8278 - val_loss: 0.4415 - val_acc: 0.7997\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4087 - acc: 0.8221 - val_loss: 0.4413 - val_acc: 0.7980\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4053 - acc: 0.8225 - val_loss: 0.4405 - val_acc: 0.7997\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4025 - acc: 0.8272 - val_loss: 0.4414 - val_acc: 0.7964\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4009 - acc: 0.8270 - val_loss: 0.4389 - val_acc: 0.8013\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3996 - acc: 0.8303 - val_loss: 0.4402 - val_acc: 0.8013\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3962 - acc: 0.8290 - val_loss: 0.4403 - val_acc: 0.7997\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3946 - acc: 0.8298 - val_loss: 0.4382 - val_acc: 0.7997\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3944 - acc: 0.8312 - val_loss: 0.4386 - val_acc: 0.8030\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3903 - acc: 0.8281 - val_loss: 0.4374 - val_acc: 0.8030\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3922 - acc: 0.8351 - val_loss: 0.4381 - val_acc: 0.8013\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3901 - acc: 0.8309 - val_loss: 0.4361 - val_acc: 0.8013\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3892 - acc: 0.8325 - val_loss: 0.4379 - val_acc: 0.7997\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3821 - acc: 0.8365 - val_loss: 0.4363 - val_acc: 0.8046\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3836 - acc: 0.8374 - val_loss: 0.4374 - val_acc: 0.8046\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3799 - acc: 0.8363 - val_loss: 0.4357 - val_acc: 0.8046\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3808 - acc: 0.8332 - val_loss: 0.4378 - val_acc: 0.8046\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3794 - acc: 0.8362 - val_loss: 0.4367 - val_acc: 0.8046\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3780 - acc: 0.8327 - val_loss: 0.4376 - val_acc: 0.8046\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3772 - acc: 0.8376 - val_loss: 0.4366 - val_acc: 0.8062\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3747 - acc: 0.8369 - val_loss: 0.4366 - val_acc: 0.8095\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.3], 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 38ms/step - loss: 0.8657 - acc: 0.4249 - val_loss: 0.7519 - val_acc: 0.4433\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.7606 - acc: 0.4444 - val_loss: 0.6979 - val_acc: 0.4696\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.7028 - acc: 0.5061 - val_loss: 0.6745 - val_acc: 0.6223\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6728 - acc: 0.5767 - val_loss: 0.6641 - val_acc: 0.5780\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6609 - acc: 0.6125 - val_loss: 0.6548 - val_acc: 0.5878\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6512 - acc: 0.6240 - val_loss: 0.6434 - val_acc: 0.6240\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6377 - acc: 0.6398 - val_loss: 0.6298 - val_acc: 0.6962\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6192 - acc: 0.6860 - val_loss: 0.6174 - val_acc: 0.7077\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6152 - acc: 0.6955 - val_loss: 0.6071 - val_acc: 0.7044\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6029 - acc: 0.6968 - val_loss: 0.5979 - val_acc: 0.7028\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5978 - acc: 0.7010 - val_loss: 0.5890 - val_acc: 0.7126\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5913 - acc: 0.7059 - val_loss: 0.5803 - val_acc: 0.7209\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5787 - acc: 0.7159 - val_loss: 0.5715 - val_acc: 0.7225\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5732 - acc: 0.7238 - val_loss: 0.5627 - val_acc: 0.7307\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5641 - acc: 0.7336 - val_loss: 0.5539 - val_acc: 0.7323\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5525 - acc: 0.7407 - val_loss: 0.5453 - val_acc: 0.7406\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5527 - acc: 0.7360 - val_loss: 0.5371 - val_acc: 0.7422\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5429 - acc: 0.7482 - val_loss: 0.5295 - val_acc: 0.7455\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5389 - acc: 0.7515 - val_loss: 0.5225 - val_acc: 0.7553\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5273 - acc: 0.7643 - val_loss: 0.5161 - val_acc: 0.7570\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.5221 - acc: 0.7539 - val_loss: 0.5104 - val_acc: 0.7586\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5144 - acc: 0.7626 - val_loss: 0.5052 - val_acc: 0.7635\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5106 - acc: 0.7663 - val_loss: 0.5008 - val_acc: 0.7635\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5077 - acc: 0.7690 - val_loss: 0.4959 - val_acc: 0.7635\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5037 - acc: 0.7739 - val_loss: 0.4918 - val_acc: 0.7734\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5014 - acc: 0.7758 - val_loss: 0.4883 - val_acc: 0.7734\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4974 - acc: 0.7809 - val_loss: 0.4851 - val_acc: 0.7750\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4902 - acc: 0.7831 - val_loss: 0.4816 - val_acc: 0.7816\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4916 - acc: 0.7767 - val_loss: 0.4793 - val_acc: 0.7750\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4845 - acc: 0.7836 - val_loss: 0.4767 - val_acc: 0.7783\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4772 - acc: 0.7931 - val_loss: 0.4742 - val_acc: 0.7783\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4775 - acc: 0.7889 - val_loss: 0.4711 - val_acc: 0.7849\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4749 - acc: 0.7873 - val_loss: 0.4687 - val_acc: 0.7865\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4716 - acc: 0.7878 - val_loss: 0.4673 - val_acc: 0.7800\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4686 - acc: 0.7968 - val_loss: 0.4646 - val_acc: 0.7865\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4609 - acc: 0.8006 - val_loss: 0.4622 - val_acc: 0.7898\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4611 - acc: 0.7991 - val_loss: 0.4612 - val_acc: 0.7865\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4571 - acc: 0.7989 - val_loss: 0.4594 - val_acc: 0.7865\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4564 - acc: 0.8011 - val_loss: 0.4574 - val_acc: 0.7898\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4536 - acc: 0.8075 - val_loss: 0.4564 - val_acc: 0.7882\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4477 - acc: 0.8042 - val_loss: 0.4548 - val_acc: 0.7898\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4502 - acc: 0.7980 - val_loss: 0.4538 - val_acc: 0.7898\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4522 - acc: 0.8015 - val_loss: 0.4531 - val_acc: 0.7898\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4415 - acc: 0.8073 - val_loss: 0.4520 - val_acc: 0.7898\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4429 - acc: 0.8088 - val_loss: 0.4501 - val_acc: 0.7915\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4418 - acc: 0.8081 - val_loss: 0.4490 - val_acc: 0.7931\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4443 - acc: 0.8062 - val_loss: 0.4504 - val_acc: 0.7898\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4376 - acc: 0.8077 - val_loss: 0.4495 - val_acc: 0.7898\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4318 - acc: 0.8163 - val_loss: 0.4466 - val_acc: 0.7980\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4373 - acc: 0.8079 - val_loss: 0.4463 - val_acc: 0.7947\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4295 - acc: 0.8143 - val_loss: 0.4461 - val_acc: 0.7980\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4306 - acc: 0.8161 - val_loss: 0.4451 - val_acc: 0.7980\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4293 - acc: 0.8150 - val_loss: 0.4434 - val_acc: 0.7997\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4259 - acc: 0.8176 - val_loss: 0.4433 - val_acc: 0.7997\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4248 - acc: 0.8157 - val_loss: 0.4451 - val_acc: 0.7947\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4199 - acc: 0.8227 - val_loss: 0.4408 - val_acc: 0.8013\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4234 - acc: 0.8181 - val_loss: 0.4404 - val_acc: 0.8030\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4225 - acc: 0.8201 - val_loss: 0.4421 - val_acc: 0.7980\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4228 - acc: 0.8201 - val_loss: 0.4402 - val_acc: 0.8013\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4186 - acc: 0.8214 - val_loss: 0.4414 - val_acc: 0.7997\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4175 - acc: 0.8221 - val_loss: 0.4394 - val_acc: 0.8062\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4169 - acc: 0.8216 - val_loss: 0.4405 - val_acc: 0.7997\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4151 - acc: 0.8225 - val_loss: 0.4403 - val_acc: 0.8013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4112 - acc: 0.8214 - val_loss: 0.4403 - val_acc: 0.8013\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4170 - acc: 0.8210 - val_loss: 0.4391 - val_acc: 0.8030\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4093 - acc: 0.8243 - val_loss: 0.4398 - val_acc: 0.7964\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4096 - acc: 0.8227 - val_loss: 0.4380 - val_acc: 0.8030\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4128 - acc: 0.8216 - val_loss: 0.4374 - val_acc: 0.8046\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4111 - acc: 0.8227 - val_loss: 0.4380 - val_acc: 0.7997\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4148 - acc: 0.8228 - val_loss: 0.4374 - val_acc: 0.7997\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4076 - acc: 0.8276 - val_loss: 0.4383 - val_acc: 0.7997\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4107 - acc: 0.8196 - val_loss: 0.4379 - val_acc: 0.7997\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4075 - acc: 0.8267 - val_loss: 0.4367 - val_acc: 0.8013\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4057 - acc: 0.8296 - val_loss: 0.4399 - val_acc: 0.7980\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4025 - acc: 0.8252 - val_loss: 0.4367 - val_acc: 0.8046\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4009 - acc: 0.8285 - val_loss: 0.4371 - val_acc: 0.8013\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4012 - acc: 0.8292 - val_loss: 0.4382 - val_acc: 0.8013\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4045 - acc: 0.8294 - val_loss: 0.4368 - val_acc: 0.8030\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.3], 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.7602 - acc: 0.4393 - val_loss: 0.7046 - val_acc: 0.4450\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.7115 - acc: 0.4844 - val_loss: 0.6758 - val_acc: 0.6010\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6798 - acc: 0.5711 - val_loss: 0.6627 - val_acc: 0.6190\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6631 - acc: 0.6121 - val_loss: 0.6562 - val_acc: 0.5698\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6528 - acc: 0.6183 - val_loss: 0.6495 - val_acc: 0.5698\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6441 - acc: 0.6232 - val_loss: 0.6399 - val_acc: 0.5961\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.6366 - acc: 0.6320 - val_loss: 0.6285 - val_acc: 0.6601\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6214 - acc: 0.6681 - val_loss: 0.6176 - val_acc: 0.7061\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6123 - acc: 0.6889 - val_loss: 0.6074 - val_acc: 0.7176\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6016 - acc: 0.6920 - val_loss: 0.5981 - val_acc: 0.7225\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5938 - acc: 0.7081 - val_loss: 0.5893 - val_acc: 0.7258\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5838 - acc: 0.7185 - val_loss: 0.5808 - val_acc: 0.7291\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5752 - acc: 0.7254 - val_loss: 0.5727 - val_acc: 0.7323\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5691 - acc: 0.7334 - val_loss: 0.5640 - val_acc: 0.7373\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5602 - acc: 0.7338 - val_loss: 0.5547 - val_acc: 0.7406\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5540 - acc: 0.7413 - val_loss: 0.5453 - val_acc: 0.7438\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5444 - acc: 0.7458 - val_loss: 0.5363 - val_acc: 0.7553\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5369 - acc: 0.7552 - val_loss: 0.5283 - val_acc: 0.7537\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5285 - acc: 0.7572 - val_loss: 0.5197 - val_acc: 0.7521\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5251 - acc: 0.7599 - val_loss: 0.5119 - val_acc: 0.7619\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5178 - acc: 0.7619 - val_loss: 0.5044 - val_acc: 0.7635\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5085 - acc: 0.7712 - val_loss: 0.4984 - val_acc: 0.7668\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5065 - acc: 0.7708 - val_loss: 0.4930 - val_acc: 0.7734\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4982 - acc: 0.7776 - val_loss: 0.4881 - val_acc: 0.7750\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4958 - acc: 0.7787 - val_loss: 0.4839 - val_acc: 0.7767\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4912 - acc: 0.7800 - val_loss: 0.4805 - val_acc: 0.7800\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4874 - acc: 0.7809 - val_loss: 0.4763 - val_acc: 0.7783\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4813 - acc: 0.7843 - val_loss: 0.4738 - val_acc: 0.7800\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4797 - acc: 0.7801 - val_loss: 0.4713 - val_acc: 0.7783\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4759 - acc: 0.7822 - val_loss: 0.4690 - val_acc: 0.7800\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4700 - acc: 0.7935 - val_loss: 0.4685 - val_acc: 0.7718\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4659 - acc: 0.7962 - val_loss: 0.4644 - val_acc: 0.7816\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4605 - acc: 0.7957 - val_loss: 0.4623 - val_acc: 0.7849\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4635 - acc: 0.7966 - val_loss: 0.4625 - val_acc: 0.7800\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4545 - acc: 0.8009 - val_loss: 0.4603 - val_acc: 0.7816\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4561 - acc: 0.7986 - val_loss: 0.4580 - val_acc: 0.7947\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4509 - acc: 0.8030 - val_loss: 0.4580 - val_acc: 0.7816\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4496 - acc: 0.8033 - val_loss: 0.4570 - val_acc: 0.7800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4481 - acc: 0.8037 - val_loss: 0.4548 - val_acc: 0.7980\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4441 - acc: 0.8046 - val_loss: 0.4538 - val_acc: 0.7947\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4421 - acc: 0.8070 - val_loss: 0.4563 - val_acc: 0.7800\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4438 - acc: 0.8070 - val_loss: 0.4523 - val_acc: 0.7915\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4385 - acc: 0.8106 - val_loss: 0.4513 - val_acc: 0.7947\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4402 - acc: 0.8075 - val_loss: 0.4505 - val_acc: 0.7882\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4346 - acc: 0.8106 - val_loss: 0.4505 - val_acc: 0.7816\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4341 - acc: 0.8166 - val_loss: 0.4506 - val_acc: 0.7800\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4352 - acc: 0.8119 - val_loss: 0.4490 - val_acc: 0.7849\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4307 - acc: 0.8113 - val_loss: 0.4476 - val_acc: 0.7964\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4348 - acc: 0.8099 - val_loss: 0.4469 - val_acc: 0.7980\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4282 - acc: 0.8143 - val_loss: 0.4493 - val_acc: 0.7833\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4291 - acc: 0.8144 - val_loss: 0.4469 - val_acc: 0.7915\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4270 - acc: 0.8170 - val_loss: 0.4452 - val_acc: 0.8030\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4271 - acc: 0.8144 - val_loss: 0.4479 - val_acc: 0.7898\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4247 - acc: 0.8214 - val_loss: 0.4449 - val_acc: 0.7964\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4207 - acc: 0.8210 - val_loss: 0.4453 - val_acc: 0.7980\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4178 - acc: 0.8177 - val_loss: 0.4453 - val_acc: 0.7947\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4173 - acc: 0.8165 - val_loss: 0.4448 - val_acc: 0.7947\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4133 - acc: 0.8174 - val_loss: 0.4431 - val_acc: 0.8030\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4181 - acc: 0.8174 - val_loss: 0.4453 - val_acc: 0.7964\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4147 - acc: 0.8234 - val_loss: 0.4447 - val_acc: 0.7964\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4164 - acc: 0.8181 - val_loss: 0.4428 - val_acc: 0.7964\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4117 - acc: 0.8214 - val_loss: 0.4435 - val_acc: 0.7980\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4091 - acc: 0.8221 - val_loss: 0.4439 - val_acc: 0.7997\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4117 - acc: 0.8214 - val_loss: 0.4410 - val_acc: 0.8046\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4070 - acc: 0.8234 - val_loss: 0.4428 - val_acc: 0.7980\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4094 - acc: 0.8227 - val_loss: 0.4418 - val_acc: 0.7980\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4067 - acc: 0.8219 - val_loss: 0.4406 - val_acc: 0.7980\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4020 - acc: 0.8298 - val_loss: 0.4420 - val_acc: 0.7964\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4069 - acc: 0.8261 - val_loss: 0.4426 - val_acc: 0.7980\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4050 - acc: 0.8239 - val_loss: 0.4402 - val_acc: 0.7997\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4052 - acc: 0.8241 - val_loss: 0.4411 - val_acc: 0.7980\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4027 - acc: 0.8287 - val_loss: 0.4442 - val_acc: 0.7997\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4023 - acc: 0.8290 - val_loss: 0.4396 - val_acc: 0.7997\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4001 - acc: 0.8269 - val_loss: 0.4393 - val_acc: 0.8030\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3988 - acc: 0.8300 - val_loss: 0.4422 - val_acc: 0.8013\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3977 - acc: 0.8303 - val_loss: 0.4413 - val_acc: 0.8013\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3960 - acc: 0.8300 - val_loss: 0.4390 - val_acc: 0.8046\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3968 - acc: 0.8303 - val_loss: 0.4419 - val_acc: 0.8013\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3926 - acc: 0.8320 - val_loss: 0.4401 - val_acc: 0.8013\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3968 - acc: 0.8278 - val_loss: 0.4396 - val_acc: 0.8013\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3913 - acc: 0.8321 - val_loss: 0.4403 - val_acc: 0.8013\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3904 - acc: 0.8309 - val_loss: 0.4392 - val_acc: 0.8030\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.3], 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.8262 - acc: 0.4320 - val_loss: 0.7267 - val_acc: 0.4417\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7285 - acc: 0.4751 - val_loss: 0.6790 - val_acc: 0.6059\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6812 - acc: 0.5722 - val_loss: 0.6652 - val_acc: 0.5665\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6637 - acc: 0.6026 - val_loss: 0.6580 - val_acc: 0.5550\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6546 - acc: 0.6180 - val_loss: 0.6436 - val_acc: 0.5649\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6346 - acc: 0.6400 - val_loss: 0.6261 - val_acc: 0.6502\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6183 - acc: 0.6740 - val_loss: 0.6116 - val_acc: 0.7192\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6070 - acc: 0.6867 - val_loss: 0.6002 - val_acc: 0.7077\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6017 - acc: 0.6851 - val_loss: 0.5897 - val_acc: 0.7110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5877 - acc: 0.7039 - val_loss: 0.5799 - val_acc: 0.7307\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5803 - acc: 0.7132 - val_loss: 0.5704 - val_acc: 0.7340\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5637 - acc: 0.7274 - val_loss: 0.5592 - val_acc: 0.7307\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5555 - acc: 0.7331 - val_loss: 0.5476 - val_acc: 0.7406\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5432 - acc: 0.7387 - val_loss: 0.5367 - val_acc: 0.7373\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5366 - acc: 0.7513 - val_loss: 0.5268 - val_acc: 0.7521\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5271 - acc: 0.7583 - val_loss: 0.5174 - val_acc: 0.7619\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5210 - acc: 0.7630 - val_loss: 0.5068 - val_acc: 0.7652\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5094 - acc: 0.7694 - val_loss: 0.4983 - val_acc: 0.7718\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4982 - acc: 0.7665 - val_loss: 0.4918 - val_acc: 0.7800\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4921 - acc: 0.7772 - val_loss: 0.4846 - val_acc: 0.7833\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4881 - acc: 0.7798 - val_loss: 0.4793 - val_acc: 0.7865\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4837 - acc: 0.7831 - val_loss: 0.4758 - val_acc: 0.7898\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4738 - acc: 0.7860 - val_loss: 0.4710 - val_acc: 0.7849\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4704 - acc: 0.7933 - val_loss: 0.4693 - val_acc: 0.7800\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4688 - acc: 0.7907 - val_loss: 0.4649 - val_acc: 0.7882\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4645 - acc: 0.7947 - val_loss: 0.4620 - val_acc: 0.7898\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4608 - acc: 0.7955 - val_loss: 0.4596 - val_acc: 0.7931\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4533 - acc: 0.8028 - val_loss: 0.4578 - val_acc: 0.7931\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4504 - acc: 0.8017 - val_loss: 0.4561 - val_acc: 0.7898\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4483 - acc: 0.8008 - val_loss: 0.4550 - val_acc: 0.7915\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4434 - acc: 0.8090 - val_loss: 0.4529 - val_acc: 0.7931\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4406 - acc: 0.8046 - val_loss: 0.4514 - val_acc: 0.7931\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4379 - acc: 0.8104 - val_loss: 0.4539 - val_acc: 0.7865\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4373 - acc: 0.8088 - val_loss: 0.4482 - val_acc: 0.7915\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4321 - acc: 0.8075 - val_loss: 0.4483 - val_acc: 0.7931\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4341 - acc: 0.8090 - val_loss: 0.4479 - val_acc: 0.7898\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4307 - acc: 0.8113 - val_loss: 0.4462 - val_acc: 0.7898\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4269 - acc: 0.8135 - val_loss: 0.4478 - val_acc: 0.7964\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4200 - acc: 0.8165 - val_loss: 0.4437 - val_acc: 0.7947\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4216 - acc: 0.8128 - val_loss: 0.4442 - val_acc: 0.7931\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4178 - acc: 0.8172 - val_loss: 0.4442 - val_acc: 0.7964\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4180 - acc: 0.8197 - val_loss: 0.4439 - val_acc: 0.7964\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4187 - acc: 0.8135 - val_loss: 0.4417 - val_acc: 0.7947\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4161 - acc: 0.8190 - val_loss: 0.4425 - val_acc: 0.7947\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4154 - acc: 0.8192 - val_loss: 0.4437 - val_acc: 0.8013\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4172 - acc: 0.8168 - val_loss: 0.4405 - val_acc: 0.7964\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4115 - acc: 0.8217 - val_loss: 0.4450 - val_acc: 0.7997\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4113 - acc: 0.8217 - val_loss: 0.4392 - val_acc: 0.7947\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4107 - acc: 0.8219 - val_loss: 0.4414 - val_acc: 0.7964\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4067 - acc: 0.8281 - val_loss: 0.4400 - val_acc: 0.7997\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4061 - acc: 0.8254 - val_loss: 0.4383 - val_acc: 0.8030\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4027 - acc: 0.8241 - val_loss: 0.4424 - val_acc: 0.7980\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4047 - acc: 0.8219 - val_loss: 0.4390 - val_acc: 0.8013\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4043 - acc: 0.8234 - val_loss: 0.4396 - val_acc: 0.8030\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4009 - acc: 0.8245 - val_loss: 0.4407 - val_acc: 0.7997\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3982 - acc: 0.8281 - val_loss: 0.4386 - val_acc: 0.8030\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.4], 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 1s 43ms/step - loss: 0.7502 - acc: 0.4528 - val_loss: 0.6859 - val_acc: 0.5484\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6789 - acc: 0.5630 - val_loss: 0.6735 - val_acc: 0.5599\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6630 - acc: 0.5942 - val_loss: 0.6502 - val_acc: 0.5599\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6324 - acc: 0.6351 - val_loss: 0.6188 - val_acc: 0.7011\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6126 - acc: 0.6855 - val_loss: 0.6002 - val_acc: 0.7028\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5965 - acc: 0.6920 - val_loss: 0.5844 - val_acc: 0.7094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5814 - acc: 0.7081 - val_loss: 0.5698 - val_acc: 0.7307\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5664 - acc: 0.7362 - val_loss: 0.5569 - val_acc: 0.7340\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5509 - acc: 0.7426 - val_loss: 0.5386 - val_acc: 0.7438\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5346 - acc: 0.7473 - val_loss: 0.5230 - val_acc: 0.7422\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5247 - acc: 0.7566 - val_loss: 0.5103 - val_acc: 0.7488\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5124 - acc: 0.7685 - val_loss: 0.4988 - val_acc: 0.7553\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5018 - acc: 0.7749 - val_loss: 0.4879 - val_acc: 0.7652\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4940 - acc: 0.7750 - val_loss: 0.4803 - val_acc: 0.7685\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4859 - acc: 0.7847 - val_loss: 0.4737 - val_acc: 0.7750\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4738 - acc: 0.7845 - val_loss: 0.4676 - val_acc: 0.7750\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4713 - acc: 0.7885 - val_loss: 0.4642 - val_acc: 0.7849\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4609 - acc: 0.7931 - val_loss: 0.4598 - val_acc: 0.7783\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4585 - acc: 0.7900 - val_loss: 0.4574 - val_acc: 0.7865\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4542 - acc: 0.7997 - val_loss: 0.4577 - val_acc: 0.7882\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4478 - acc: 0.8037 - val_loss: 0.4529 - val_acc: 0.7849\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4445 - acc: 0.7989 - val_loss: 0.4538 - val_acc: 0.7882\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4409 - acc: 0.8081 - val_loss: 0.4509 - val_acc: 0.7898\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4335 - acc: 0.8090 - val_loss: 0.4479 - val_acc: 0.7931\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4322 - acc: 0.8095 - val_loss: 0.4495 - val_acc: 0.7849\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4303 - acc: 0.8123 - val_loss: 0.4446 - val_acc: 0.7964\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4299 - acc: 0.8123 - val_loss: 0.4445 - val_acc: 0.7898\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4255 - acc: 0.8135 - val_loss: 0.4428 - val_acc: 0.7931\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4221 - acc: 0.8150 - val_loss: 0.4428 - val_acc: 0.7882\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4181 - acc: 0.8192 - val_loss: 0.4410 - val_acc: 0.7931\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4179 - acc: 0.8121 - val_loss: 0.4405 - val_acc: 0.7915\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4169 - acc: 0.8172 - val_loss: 0.4413 - val_acc: 0.7947\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4143 - acc: 0.8168 - val_loss: 0.4390 - val_acc: 0.7947\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4116 - acc: 0.8177 - val_loss: 0.4433 - val_acc: 0.7931\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4023 - acc: 0.8252 - val_loss: 0.4372 - val_acc: 0.7997\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4010 - acc: 0.8270 - val_loss: 0.4406 - val_acc: 0.7947\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4034 - acc: 0.8238 - val_loss: 0.4377 - val_acc: 0.7980\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4023 - acc: 0.8267 - val_loss: 0.4378 - val_acc: 0.7980\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3993 - acc: 0.8259 - val_loss: 0.4377 - val_acc: 0.7997\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3981 - acc: 0.8263 - val_loss: 0.4366 - val_acc: 0.8030\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3953 - acc: 0.8285 - val_loss: 0.4378 - val_acc: 0.7997\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4005 - acc: 0.8309 - val_loss: 0.4408 - val_acc: 0.8030\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3916 - acc: 0.8316 - val_loss: 0.4358 - val_acc: 0.8013\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3903 - acc: 0.8309 - val_loss: 0.4377 - val_acc: 0.8046\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3898 - acc: 0.8332 - val_loss: 0.4369 - val_acc: 0.8030\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3891 - acc: 0.8342 - val_loss: 0.4382 - val_acc: 0.8013\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3860 - acc: 0.8298 - val_loss: 0.4364 - val_acc: 0.7997\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3868 - acc: 0.8336 - val_loss: 0.4363 - val_acc: 0.7997\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.4], 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.8330 - acc: 0.4280 - val_loss: 0.7277 - val_acc: 0.4417\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7326 - acc: 0.4766 - val_loss: 0.6797 - val_acc: 0.6108\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6846 - acc: 0.5601 - val_loss: 0.6651 - val_acc: 0.5649\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6703 - acc: 0.5908 - val_loss: 0.6582 - val_acc: 0.5550\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6552 - acc: 0.6112 - val_loss: 0.6454 - val_acc: 0.5649\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6353 - acc: 0.6409 - val_loss: 0.6290 - val_acc: 0.6322\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6232 - acc: 0.6636 - val_loss: 0.6146 - val_acc: 0.7110\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6121 - acc: 0.6789 - val_loss: 0.6032 - val_acc: 0.7094\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6023 - acc: 0.6924 - val_loss: 0.5930 - val_acc: 0.7077\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5926 - acc: 0.6999 - val_loss: 0.5834 - val_acc: 0.7209\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5797 - acc: 0.7070 - val_loss: 0.5736 - val_acc: 0.7291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5722 - acc: 0.7245 - val_loss: 0.5636 - val_acc: 0.7340\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5620 - acc: 0.7285 - val_loss: 0.5529 - val_acc: 0.7356\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5517 - acc: 0.7360 - val_loss: 0.5418 - val_acc: 0.7373\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5464 - acc: 0.7424 - val_loss: 0.5316 - val_acc: 0.7422\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5294 - acc: 0.7531 - val_loss: 0.5221 - val_acc: 0.7521\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5253 - acc: 0.7586 - val_loss: 0.5126 - val_acc: 0.7619\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5189 - acc: 0.7635 - val_loss: 0.5035 - val_acc: 0.7701\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5035 - acc: 0.7703 - val_loss: 0.4950 - val_acc: 0.7734\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4980 - acc: 0.7716 - val_loss: 0.4888 - val_acc: 0.7767\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4961 - acc: 0.7767 - val_loss: 0.4823 - val_acc: 0.7849\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4916 - acc: 0.7781 - val_loss: 0.4769 - val_acc: 0.7849\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4798 - acc: 0.7843 - val_loss: 0.4729 - val_acc: 0.7849\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4714 - acc: 0.7922 - val_loss: 0.4706 - val_acc: 0.7816\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4677 - acc: 0.7896 - val_loss: 0.4660 - val_acc: 0.7849\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4659 - acc: 0.7913 - val_loss: 0.4637 - val_acc: 0.7865\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4614 - acc: 0.7935 - val_loss: 0.4620 - val_acc: 0.7865\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4549 - acc: 0.8002 - val_loss: 0.4591 - val_acc: 0.7898\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4559 - acc: 0.8000 - val_loss: 0.4560 - val_acc: 0.7898\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4529 - acc: 0.7978 - val_loss: 0.4543 - val_acc: 0.7898\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4463 - acc: 0.8033 - val_loss: 0.4544 - val_acc: 0.7865\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4458 - acc: 0.8013 - val_loss: 0.4529 - val_acc: 0.7898\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4375 - acc: 0.8090 - val_loss: 0.4498 - val_acc: 0.7915\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4386 - acc: 0.8070 - val_loss: 0.4507 - val_acc: 0.7915\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4382 - acc: 0.8135 - val_loss: 0.4482 - val_acc: 0.7964\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4302 - acc: 0.8070 - val_loss: 0.4475 - val_acc: 0.7898\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4321 - acc: 0.8072 - val_loss: 0.4483 - val_acc: 0.7947\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4280 - acc: 0.8095 - val_loss: 0.4451 - val_acc: 0.7947\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4287 - acc: 0.8126 - val_loss: 0.4495 - val_acc: 0.7964\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4256 - acc: 0.8141 - val_loss: 0.4443 - val_acc: 0.7980\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4232 - acc: 0.8146 - val_loss: 0.4441 - val_acc: 0.7964\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4230 - acc: 0.8166 - val_loss: 0.4455 - val_acc: 0.7915\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4192 - acc: 0.8165 - val_loss: 0.4417 - val_acc: 0.7980\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4175 - acc: 0.8177 - val_loss: 0.4433 - val_acc: 0.7931\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4156 - acc: 0.8205 - val_loss: 0.4423 - val_acc: 0.7947\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4111 - acc: 0.8245 - val_loss: 0.4412 - val_acc: 0.7931\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4112 - acc: 0.8223 - val_loss: 0.4419 - val_acc: 0.7947\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4118 - acc: 0.8225 - val_loss: 0.4451 - val_acc: 0.7964\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4086 - acc: 0.8228 - val_loss: 0.4396 - val_acc: 0.7997\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4081 - acc: 0.8236 - val_loss: 0.4420 - val_acc: 0.7964\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4078 - acc: 0.8230 - val_loss: 0.4426 - val_acc: 0.7964\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4063 - acc: 0.8232 - val_loss: 0.4388 - val_acc: 0.8030\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4016 - acc: 0.8239 - val_loss: 0.4416 - val_acc: 0.7997\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4046 - acc: 0.8256 - val_loss: 0.4390 - val_acc: 0.7997\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3978 - acc: 0.8267 - val_loss: 0.4446 - val_acc: 0.7964\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4019 - acc: 0.8283 - val_loss: 0.4393 - val_acc: 0.7997\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3954 - acc: 0.8305 - val_loss: 0.4385 - val_acc: 0.8030\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3964 - acc: 0.8283 - val_loss: 0.4411 - val_acc: 0.7980\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3947 - acc: 0.8336 - val_loss: 0.4378 - val_acc: 0.8062\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3936 - acc: 0.8311 - val_loss: 0.4393 - val_acc: 0.7997\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3938 - acc: 0.8325 - val_loss: 0.4397 - val_acc: 0.7997\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3939 - acc: 0.8285 - val_loss: 0.4367 - val_acc: 0.8046\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3898 - acc: 0.8325 - val_loss: 0.4443 - val_acc: 0.7882\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3918 - acc: 0.8320 - val_loss: 0.4362 - val_acc: 0.8046\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3876 - acc: 0.8300 - val_loss: 0.4376 - val_acc: 0.8030\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3847 - acc: 0.8374 - val_loss: 0.4398 - val_acc: 0.7964\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3837 - acc: 0.8345 - val_loss: 0.4390 - val_acc: 0.7980\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3872 - acc: 0.8334 - val_loss: 0.4391 - val_acc: 0.8013\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3838 - acc: 0.8354 - val_loss: 0.4386 - val_acc: 0.8013\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.4], 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 34ms/step - loss: 0.8267 - acc: 0.4286 - val_loss: 0.7069 - val_acc: 0.4663\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.7120 - acc: 0.5059 - val_loss: 0.6752 - val_acc: 0.5517\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6753 - acc: 0.5745 - val_loss: 0.6626 - val_acc: 0.5583\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6572 - acc: 0.5982 - val_loss: 0.6383 - val_acc: 0.5714\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6347 - acc: 0.6309 - val_loss: 0.6139 - val_acc: 0.7011\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6095 - acc: 0.6802 - val_loss: 0.5986 - val_acc: 0.7028\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6033 - acc: 0.6878 - val_loss: 0.5863 - val_acc: 0.6962\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5865 - acc: 0.7044 - val_loss: 0.5736 - val_acc: 0.7077\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5731 - acc: 0.7174 - val_loss: 0.5612 - val_acc: 0.7258\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5654 - acc: 0.7291 - val_loss: 0.5474 - val_acc: 0.7340\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5503 - acc: 0.7464 - val_loss: 0.5343 - val_acc: 0.7471\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5366 - acc: 0.7510 - val_loss: 0.5219 - val_acc: 0.7455\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5264 - acc: 0.7630 - val_loss: 0.5117 - val_acc: 0.7488\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5162 - acc: 0.7652 - val_loss: 0.5014 - val_acc: 0.7537\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5083 - acc: 0.7712 - val_loss: 0.4926 - val_acc: 0.7635\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4987 - acc: 0.7708 - val_loss: 0.4863 - val_acc: 0.7701\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4879 - acc: 0.7833 - val_loss: 0.4790 - val_acc: 0.7685\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4860 - acc: 0.7876 - val_loss: 0.4744 - val_acc: 0.7718\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4816 - acc: 0.7865 - val_loss: 0.4707 - val_acc: 0.7767\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4748 - acc: 0.7840 - val_loss: 0.4664 - val_acc: 0.7783\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4699 - acc: 0.7882 - val_loss: 0.4665 - val_acc: 0.7882\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4602 - acc: 0.8019 - val_loss: 0.4614 - val_acc: 0.7898\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4555 - acc: 0.7971 - val_loss: 0.4593 - val_acc: 0.7898\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4530 - acc: 0.7975 - val_loss: 0.4596 - val_acc: 0.7947\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4470 - acc: 0.8022 - val_loss: 0.4549 - val_acc: 0.7882\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4457 - acc: 0.8019 - val_loss: 0.4542 - val_acc: 0.7915\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4425 - acc: 0.8081 - val_loss: 0.4539 - val_acc: 0.7997\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4405 - acc: 0.8115 - val_loss: 0.4508 - val_acc: 0.7931\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4348 - acc: 0.8135 - val_loss: 0.4520 - val_acc: 0.7997\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4324 - acc: 0.8073 - val_loss: 0.4487 - val_acc: 0.7947\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4298 - acc: 0.8128 - val_loss: 0.4523 - val_acc: 0.7947\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4278 - acc: 0.8115 - val_loss: 0.4478 - val_acc: 0.7997\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4256 - acc: 0.8121 - val_loss: 0.4473 - val_acc: 0.7947\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4260 - acc: 0.8172 - val_loss: 0.4470 - val_acc: 0.7947\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4183 - acc: 0.8157 - val_loss: 0.4456 - val_acc: 0.7980\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4180 - acc: 0.8212 - val_loss: 0.4461 - val_acc: 0.7947\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4174 - acc: 0.8168 - val_loss: 0.4434 - val_acc: 0.7980\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4198 - acc: 0.8177 - val_loss: 0.4458 - val_acc: 0.7980\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4156 - acc: 0.8207 - val_loss: 0.4422 - val_acc: 0.7980\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4111 - acc: 0.8210 - val_loss: 0.4407 - val_acc: 0.8013\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4084 - acc: 0.8247 - val_loss: 0.4428 - val_acc: 0.7980\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4093 - acc: 0.8269 - val_loss: 0.4399 - val_acc: 0.7997\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4096 - acc: 0.8252 - val_loss: 0.4407 - val_acc: 0.7980\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4046 - acc: 0.8265 - val_loss: 0.4414 - val_acc: 0.7964\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3982 - acc: 0.8305 - val_loss: 0.4406 - val_acc: 0.7947\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4028 - acc: 0.8250 - val_loss: 0.4389 - val_acc: 0.7980\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3990 - acc: 0.8298 - val_loss: 0.4411 - val_acc: 0.7997\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3996 - acc: 0.8314 - val_loss: 0.4420 - val_acc: 0.7980\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3961 - acc: 0.8307 - val_loss: 0.4388 - val_acc: 0.7980\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3964 - acc: 0.8327 - val_loss: 0.4418 - val_acc: 0.7997\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3955 - acc: 0.8321 - val_loss: 0.4375 - val_acc: 0.8013\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3926 - acc: 0.8323 - val_loss: 0.4382 - val_acc: 0.8013\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3943 - acc: 0.8332 - val_loss: 0.4408 - val_acc: 0.7980\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3897 - acc: 0.8309 - val_loss: 0.4361 - val_acc: 0.8046\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3889 - acc: 0.8301 - val_loss: 0.4410 - val_acc: 0.7980\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3841 - acc: 0.8369 - val_loss: 0.4365 - val_acc: 0.7997\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3838 - acc: 0.8362 - val_loss: 0.4367 - val_acc: 0.7980\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3862 - acc: 0.8342 - val_loss: 0.4402 - val_acc: 0.7997\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3810 - acc: 0.8352 - val_loss: 0.4347 - val_acc: 0.8079\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3815 - acc: 0.8332 - val_loss: 0.4395 - val_acc: 0.8030\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3784 - acc: 0.8362 - val_loss: 0.4348 - val_acc: 0.8062\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3807 - acc: 0.8363 - val_loss: 0.4374 - val_acc: 0.8046\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3768 - acc: 0.8389 - val_loss: 0.4372 - val_acc: 0.8062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3748 - acc: 0.8400 - val_loss: 0.4340 - val_acc: 0.8079\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3714 - acc: 0.8398 - val_loss: 0.4387 - val_acc: 0.8013\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3733 - acc: 0.8376 - val_loss: 0.4337 - val_acc: 0.8079\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3707 - acc: 0.8433 - val_loss: 0.4363 - val_acc: 0.8062\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3682 - acc: 0.8413 - val_loss: 0.4349 - val_acc: 0.8030\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3686 - acc: 0.8407 - val_loss: 0.4362 - val_acc: 0.8062\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3634 - acc: 0.8449 - val_loss: 0.4357 - val_acc: 0.8030\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3649 - acc: 0.8446 - val_loss: 0.4392 - val_acc: 0.8079\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.4], 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_22 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.8659 - acc: 0.4298 - val_loss: 0.7528 - val_acc: 0.4433\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7681 - acc: 0.4470 - val_loss: 0.6987 - val_acc: 0.4647\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.7083 - acc: 0.4979 - val_loss: 0.6753 - val_acc: 0.6240\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6800 - acc: 0.5627 - val_loss: 0.6648 - val_acc: 0.5829\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6646 - acc: 0.5933 - val_loss: 0.6554 - val_acc: 0.5944\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6537 - acc: 0.6234 - val_loss: 0.6439 - val_acc: 0.6256\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6420 - acc: 0.6417 - val_loss: 0.6309 - val_acc: 0.7028\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6309 - acc: 0.6636 - val_loss: 0.6195 - val_acc: 0.7077\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6223 - acc: 0.6663 - val_loss: 0.6095 - val_acc: 0.7077\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6105 - acc: 0.6882 - val_loss: 0.6001 - val_acc: 0.7061\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6060 - acc: 0.6953 - val_loss: 0.5913 - val_acc: 0.7192\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5941 - acc: 0.7055 - val_loss: 0.5830 - val_acc: 0.7209\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5897 - acc: 0.7063 - val_loss: 0.5748 - val_acc: 0.7274\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5814 - acc: 0.7145 - val_loss: 0.5657 - val_acc: 0.7291\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5767 - acc: 0.7130 - val_loss: 0.5572 - val_acc: 0.7274\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5611 - acc: 0.7373 - val_loss: 0.5492 - val_acc: 0.7356\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5580 - acc: 0.7349 - val_loss: 0.5413 - val_acc: 0.7438\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5557 - acc: 0.7360 - val_loss: 0.5341 - val_acc: 0.7488\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5465 - acc: 0.7466 - val_loss: 0.5271 - val_acc: 0.7537\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5359 - acc: 0.7541 - val_loss: 0.5206 - val_acc: 0.7586\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5309 - acc: 0.7557 - val_loss: 0.5145 - val_acc: 0.7586\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5325 - acc: 0.7491 - val_loss: 0.5097 - val_acc: 0.7635\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5249 - acc: 0.7586 - val_loss: 0.5055 - val_acc: 0.7701\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5180 - acc: 0.7628 - val_loss: 0.5004 - val_acc: 0.7685\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5142 - acc: 0.7672 - val_loss: 0.4963 - val_acc: 0.7718\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5048 - acc: 0.7816 - val_loss: 0.4932 - val_acc: 0.7783\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5044 - acc: 0.7692 - val_loss: 0.4890 - val_acc: 0.7767\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4963 - acc: 0.7812 - val_loss: 0.4854 - val_acc: 0.7800\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4947 - acc: 0.7814 - val_loss: 0.4831 - val_acc: 0.7767\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4930 - acc: 0.7796 - val_loss: 0.4797 - val_acc: 0.7816\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4923 - acc: 0.7822 - val_loss: 0.4767 - val_acc: 0.7816\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4884 - acc: 0.7845 - val_loss: 0.4739 - val_acc: 0.7833\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4824 - acc: 0.7873 - val_loss: 0.4727 - val_acc: 0.7800\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4791 - acc: 0.7922 - val_loss: 0.4702 - val_acc: 0.7816\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4790 - acc: 0.7847 - val_loss: 0.4677 - val_acc: 0.7816\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4748 - acc: 0.7978 - val_loss: 0.4661 - val_acc: 0.7800\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4726 - acc: 0.7893 - val_loss: 0.4641 - val_acc: 0.7882\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4690 - acc: 0.7991 - val_loss: 0.4630 - val_acc: 0.7865\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4649 - acc: 0.7989 - val_loss: 0.4609 - val_acc: 0.7898\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4679 - acc: 0.7960 - val_loss: 0.4598 - val_acc: 0.7865\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4673 - acc: 0.7958 - val_loss: 0.4579 - val_acc: 0.7964\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4615 - acc: 0.8000 - val_loss: 0.4573 - val_acc: 0.7915\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4564 - acc: 0.7995 - val_loss: 0.4571 - val_acc: 0.7882\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4558 - acc: 0.8020 - val_loss: 0.4553 - val_acc: 0.7915\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4529 - acc: 0.8013 - val_loss: 0.4541 - val_acc: 0.7931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4539 - acc: 0.8030 - val_loss: 0.4544 - val_acc: 0.7865\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4491 - acc: 0.8022 - val_loss: 0.4523 - val_acc: 0.7898\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4451 - acc: 0.8103 - val_loss: 0.4509 - val_acc: 0.7947\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4415 - acc: 0.8061 - val_loss: 0.4497 - val_acc: 0.7931\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4424 - acc: 0.8090 - val_loss: 0.4507 - val_acc: 0.7865\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4483 - acc: 0.8088 - val_loss: 0.4490 - val_acc: 0.7947\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4444 - acc: 0.8055 - val_loss: 0.4478 - val_acc: 0.7964\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4409 - acc: 0.8086 - val_loss: 0.4479 - val_acc: 0.7915\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4360 - acc: 0.8119 - val_loss: 0.4465 - val_acc: 0.7964\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4381 - acc: 0.8099 - val_loss: 0.4467 - val_acc: 0.7898\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4358 - acc: 0.8126 - val_loss: 0.4458 - val_acc: 0.7915\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4330 - acc: 0.8104 - val_loss: 0.4445 - val_acc: 0.8030\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4332 - acc: 0.8188 - val_loss: 0.4450 - val_acc: 0.7931\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4290 - acc: 0.8174 - val_loss: 0.4440 - val_acc: 0.7931\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4294 - acc: 0.8174 - val_loss: 0.4433 - val_acc: 0.7964\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4293 - acc: 0.8192 - val_loss: 0.4417 - val_acc: 0.7964\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4276 - acc: 0.8139 - val_loss: 0.4432 - val_acc: 0.7980\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4250 - acc: 0.8216 - val_loss: 0.4413 - val_acc: 0.8013\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4287 - acc: 0.8155 - val_loss: 0.4411 - val_acc: 0.7980\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4165 - acc: 0.8197 - val_loss: 0.4413 - val_acc: 0.8013\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4222 - acc: 0.8177 - val_loss: 0.4414 - val_acc: 0.8013\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4257 - acc: 0.8137 - val_loss: 0.4405 - val_acc: 0.8013\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4174 - acc: 0.8217 - val_loss: 0.4396 - val_acc: 0.8013\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4231 - acc: 0.8197 - val_loss: 0.4392 - val_acc: 0.8013\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4241 - acc: 0.8216 - val_loss: 0.4401 - val_acc: 0.8013\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4142 - acc: 0.8252 - val_loss: 0.4392 - val_acc: 0.8013\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4204 - acc: 0.8185 - val_loss: 0.4395 - val_acc: 0.8013\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4132 - acc: 0.8221 - val_loss: 0.4387 - val_acc: 0.8030\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4187 - acc: 0.8221 - val_loss: 0.4391 - val_acc: 0.8030\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4113 - acc: 0.8201 - val_loss: 0.4380 - val_acc: 0.8062\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4081 - acc: 0.8256 - val_loss: 0.4376 - val_acc: 0.8046\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4124 - acc: 0.8225 - val_loss: 0.4393 - val_acc: 0.8013\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4135 - acc: 0.8238 - val_loss: 0.4372 - val_acc: 0.8062\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4025 - acc: 0.8261 - val_loss: 0.4378 - val_acc: 0.8062\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4136 - acc: 0.8234 - val_loss: 0.4399 - val_acc: 0.7997\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4072 - acc: 0.8250 - val_loss: 0.4361 - val_acc: 0.8046\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4037 - acc: 0.8303 - val_loss: 0.4374 - val_acc: 0.8062\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4079 - acc: 0.8303 - val_loss: 0.4375 - val_acc: 0.8030\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4070 - acc: 0.8243 - val_loss: 0.4363 - val_acc: 0.8046\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4053 - acc: 0.8292 - val_loss: 0.4362 - val_acc: 0.8046\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4044 - acc: 0.8307 - val_loss: 0.4365 - val_acc: 0.8013\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.4], 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_23 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 1s 34ms/step - loss: 0.7642 - acc: 0.4417 - val_loss: 0.7043 - val_acc: 0.4450\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.7103 - acc: 0.4944 - val_loss: 0.6758 - val_acc: 0.6043\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6839 - acc: 0.5588 - val_loss: 0.6630 - val_acc: 0.6273\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6647 - acc: 0.5977 - val_loss: 0.6567 - val_acc: 0.5698\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6542 - acc: 0.6152 - val_loss: 0.6502 - val_acc: 0.5665\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6481 - acc: 0.6298 - val_loss: 0.6406 - val_acc: 0.5895\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6398 - acc: 0.6378 - val_loss: 0.6293 - val_acc: 0.6634\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6252 - acc: 0.6645 - val_loss: 0.6184 - val_acc: 0.7061\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6174 - acc: 0.6752 - val_loss: 0.6085 - val_acc: 0.7258\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6088 - acc: 0.6884 - val_loss: 0.5992 - val_acc: 0.7225\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6039 - acc: 0.6979 - val_loss: 0.5902 - val_acc: 0.7241\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5887 - acc: 0.7090 - val_loss: 0.5813 - val_acc: 0.7323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5817 - acc: 0.7108 - val_loss: 0.5721 - val_acc: 0.7340\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5740 - acc: 0.7185 - val_loss: 0.5628 - val_acc: 0.7373\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5667 - acc: 0.7320 - val_loss: 0.5538 - val_acc: 0.7438\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5562 - acc: 0.7404 - val_loss: 0.5452 - val_acc: 0.7422\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5455 - acc: 0.7469 - val_loss: 0.5368 - val_acc: 0.7504\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5406 - acc: 0.7488 - val_loss: 0.5281 - val_acc: 0.7553\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5343 - acc: 0.7531 - val_loss: 0.5196 - val_acc: 0.7570\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5299 - acc: 0.7568 - val_loss: 0.5122 - val_acc: 0.7635\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5227 - acc: 0.7583 - val_loss: 0.5057 - val_acc: 0.7652\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5151 - acc: 0.7632 - val_loss: 0.5001 - val_acc: 0.7668\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5069 - acc: 0.7745 - val_loss: 0.4940 - val_acc: 0.7734\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5073 - acc: 0.7685 - val_loss: 0.4889 - val_acc: 0.7767\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4991 - acc: 0.7785 - val_loss: 0.4846 - val_acc: 0.7734\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4927 - acc: 0.7785 - val_loss: 0.4809 - val_acc: 0.7750\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4881 - acc: 0.7796 - val_loss: 0.4776 - val_acc: 0.7783\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4846 - acc: 0.7816 - val_loss: 0.4741 - val_acc: 0.7783\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4774 - acc: 0.7854 - val_loss: 0.4717 - val_acc: 0.7800\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4775 - acc: 0.7907 - val_loss: 0.4692 - val_acc: 0.7800\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4721 - acc: 0.7876 - val_loss: 0.4672 - val_acc: 0.7767\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4683 - acc: 0.7911 - val_loss: 0.4642 - val_acc: 0.7800\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4687 - acc: 0.7926 - val_loss: 0.4630 - val_acc: 0.7750\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4666 - acc: 0.7991 - val_loss: 0.4624 - val_acc: 0.7783\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4586 - acc: 0.7940 - val_loss: 0.4598 - val_acc: 0.7816\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4559 - acc: 0.7933 - val_loss: 0.4584 - val_acc: 0.7833\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4564 - acc: 0.8002 - val_loss: 0.4582 - val_acc: 0.7816\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4548 - acc: 0.7946 - val_loss: 0.4565 - val_acc: 0.7833\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4546 - acc: 0.7999 - val_loss: 0.4549 - val_acc: 0.7882\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4515 - acc: 0.8041 - val_loss: 0.4535 - val_acc: 0.7964\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4489 - acc: 0.8046 - val_loss: 0.4542 - val_acc: 0.7783\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4472 - acc: 0.8050 - val_loss: 0.4529 - val_acc: 0.7816\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4431 - acc: 0.8055 - val_loss: 0.4513 - val_acc: 0.7931\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4398 - acc: 0.8112 - val_loss: 0.4520 - val_acc: 0.7800\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4320 - acc: 0.8154 - val_loss: 0.4493 - val_acc: 0.8013\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4405 - acc: 0.8050 - val_loss: 0.4497 - val_acc: 0.7849\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4325 - acc: 0.8108 - val_loss: 0.4521 - val_acc: 0.7800\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4350 - acc: 0.8070 - val_loss: 0.4474 - val_acc: 0.8013\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4333 - acc: 0.8104 - val_loss: 0.4482 - val_acc: 0.7833\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4301 - acc: 0.8134 - val_loss: 0.4489 - val_acc: 0.7849\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4316 - acc: 0.8134 - val_loss: 0.4474 - val_acc: 0.7833\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4274 - acc: 0.8183 - val_loss: 0.4458 - val_acc: 0.7980\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4265 - acc: 0.8112 - val_loss: 0.4473 - val_acc: 0.7898\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4258 - acc: 0.8176 - val_loss: 0.4487 - val_acc: 0.7882\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4275 - acc: 0.8128 - val_loss: 0.4449 - val_acc: 0.7947\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4211 - acc: 0.8176 - val_loss: 0.4447 - val_acc: 0.7997\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4243 - acc: 0.8159 - val_loss: 0.4456 - val_acc: 0.7931\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4207 - acc: 0.8177 - val_loss: 0.4448 - val_acc: 0.7947\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4215 - acc: 0.8174 - val_loss: 0.4423 - val_acc: 0.8030\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4196 - acc: 0.8186 - val_loss: 0.4426 - val_acc: 0.7997\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4164 - acc: 0.8201 - val_loss: 0.4446 - val_acc: 0.7964\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4170 - acc: 0.8146 - val_loss: 0.4424 - val_acc: 0.8013\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4189 - acc: 0.8201 - val_loss: 0.4420 - val_acc: 0.7997\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4108 - acc: 0.8208 - val_loss: 0.4445 - val_acc: 0.7997\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4137 - acc: 0.8223 - val_loss: 0.4430 - val_acc: 0.7964\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4126 - acc: 0.8192 - val_loss: 0.4405 - val_acc: 0.8046\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4115 - acc: 0.8219 - val_loss: 0.4442 - val_acc: 0.7997\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4127 - acc: 0.8227 - val_loss: 0.4431 - val_acc: 0.7997\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4071 - acc: 0.8228 - val_loss: 0.4399 - val_acc: 0.8030\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4126 - acc: 0.8196 - val_loss: 0.4402 - val_acc: 0.7980\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4069 - acc: 0.8227 - val_loss: 0.4406 - val_acc: 0.7997\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4076 - acc: 0.8241 - val_loss: 0.4399 - val_acc: 0.7997\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4030 - acc: 0.8254 - val_loss: 0.4402 - val_acc: 0.8013\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4034 - acc: 0.8205 - val_loss: 0.4400 - val_acc: 0.8013\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3978 - acc: 0.8290 - val_loss: 0.4402 - val_acc: 0.8030\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3996 - acc: 0.8307 - val_loss: 0.4398 - val_acc: 0.8013\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3982 - acc: 0.8227 - val_loss: 0.4392 - val_acc: 0.8013\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3977 - acc: 0.8267 - val_loss: 0.4383 - val_acc: 0.8013\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4011 - acc: 0.8287 - val_loss: 0.4398 - val_acc: 0.7997\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3981 - acc: 0.8287 - val_loss: 0.4391 - val_acc: 0.7997\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3967 - acc: 0.8280 - val_loss: 0.4364 - val_acc: 0.8046\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3941 - acc: 0.8294 - val_loss: 0.4384 - val_acc: 0.8013\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3953 - acc: 0.8318 - val_loss: 0.4400 - val_acc: 0.7997\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3951 - acc: 0.8270 - val_loss: 0.4371 - val_acc: 0.8062\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3939 - acc: 0.8283 - val_loss: 0.4386 - val_acc: 0.8013\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3963 - acc: 0.8294 - val_loss: 0.4389 - val_acc: 0.7997\n",
      "Training with parameters {'batch_size': 1250, 'dropout': [0.2, 0.4], 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_24 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.8288 - acc: 0.4333 - val_loss: 0.7276 - val_acc: 0.4417\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7321 - acc: 0.4747 - val_loss: 0.6796 - val_acc: 0.6076\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6845 - acc: 0.5670 - val_loss: 0.6652 - val_acc: 0.5649\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6667 - acc: 0.5981 - val_loss: 0.6579 - val_acc: 0.5550\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6559 - acc: 0.6132 - val_loss: 0.6445 - val_acc: 0.5649\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6350 - acc: 0.6415 - val_loss: 0.6275 - val_acc: 0.6453\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6209 - acc: 0.6705 - val_loss: 0.6134 - val_acc: 0.7159\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6109 - acc: 0.6813 - val_loss: 0.6018 - val_acc: 0.7061\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6015 - acc: 0.6915 - val_loss: 0.5915 - val_acc: 0.7110\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5926 - acc: 0.6990 - val_loss: 0.5817 - val_acc: 0.7192\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5776 - acc: 0.7105 - val_loss: 0.5722 - val_acc: 0.7340\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5711 - acc: 0.7238 - val_loss: 0.5618 - val_acc: 0.7340\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5600 - acc: 0.7349 - val_loss: 0.5504 - val_acc: 0.7373\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5534 - acc: 0.7380 - val_loss: 0.5391 - val_acc: 0.7373\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.5435 - acc: 0.7453 - val_loss: 0.5294 - val_acc: 0.7488\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5311 - acc: 0.7550 - val_loss: 0.5200 - val_acc: 0.7553\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5239 - acc: 0.7572 - val_loss: 0.5105 - val_acc: 0.7603\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5191 - acc: 0.7595 - val_loss: 0.5020 - val_acc: 0.7701\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5072 - acc: 0.7716 - val_loss: 0.4944 - val_acc: 0.7767\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5027 - acc: 0.7729 - val_loss: 0.4887 - val_acc: 0.7734\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4960 - acc: 0.7752 - val_loss: 0.4820 - val_acc: 0.7833\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4888 - acc: 0.7789 - val_loss: 0.4774 - val_acc: 0.7833\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4823 - acc: 0.7893 - val_loss: 0.4741 - val_acc: 0.7849\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4750 - acc: 0.7900 - val_loss: 0.4701 - val_acc: 0.7816\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4730 - acc: 0.7843 - val_loss: 0.4667 - val_acc: 0.7849\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4647 - acc: 0.7909 - val_loss: 0.4653 - val_acc: 0.7849\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4607 - acc: 0.7975 - val_loss: 0.4624 - val_acc: 0.7865\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4581 - acc: 0.7969 - val_loss: 0.4581 - val_acc: 0.7898\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4551 - acc: 0.7962 - val_loss: 0.4564 - val_acc: 0.7931\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4517 - acc: 0.7993 - val_loss: 0.4583 - val_acc: 0.7800\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4463 - acc: 0.8044 - val_loss: 0.4535 - val_acc: 0.7915\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4449 - acc: 0.8042 - val_loss: 0.4514 - val_acc: 0.7931\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4398 - acc: 0.8095 - val_loss: 0.4537 - val_acc: 0.7931\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4387 - acc: 0.8072 - val_loss: 0.4488 - val_acc: 0.7980\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4373 - acc: 0.8103 - val_loss: 0.4491 - val_acc: 0.7898\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4316 - acc: 0.8113 - val_loss: 0.4487 - val_acc: 0.7915\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4321 - acc: 0.8082 - val_loss: 0.4458 - val_acc: 0.7931\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4299 - acc: 0.8154 - val_loss: 0.4466 - val_acc: 0.7898\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4265 - acc: 0.8126 - val_loss: 0.4447 - val_acc: 0.7947\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4277 - acc: 0.8137 - val_loss: 0.4461 - val_acc: 0.7980\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4206 - acc: 0.8163 - val_loss: 0.4445 - val_acc: 0.7947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4203 - acc: 0.8157 - val_loss: 0.4425 - val_acc: 0.8013\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4190 - acc: 0.8199 - val_loss: 0.4443 - val_acc: 0.7947\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4155 - acc: 0.8212 - val_loss: 0.4412 - val_acc: 0.7964\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4134 - acc: 0.8168 - val_loss: 0.4415 - val_acc: 0.7964\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4157 - acc: 0.8201 - val_loss: 0.4449 - val_acc: 0.7964\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4125 - acc: 0.8185 - val_loss: 0.4413 - val_acc: 0.7964\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4113 - acc: 0.8208 - val_loss: 0.4448 - val_acc: 0.7997\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4116 - acc: 0.8188 - val_loss: 0.4420 - val_acc: 0.7964\n"
     ]
    }
   ],
   "source": [
    "mlp_perf_metrics5 = make_MLP_exp(X_train_vect, y_train.values, params5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.1]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.844371</td>\n",
       "      <td>0.799672</td>\n",
       "      <td>0.362952</td>\n",
       "      <td>0.434138</td>\n",
       "      <td>50</td>\n",
       "      <td>0.820749</td>\n",
       "      <td>0.825119</td>\n",
       "      <td>0.760997</td>\n",
       "      <td>0.869203</td>\n",
       "      <td>0.791762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.1]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.843277</td>\n",
       "      <td>0.801314</td>\n",
       "      <td>0.372175</td>\n",
       "      <td>0.435570</td>\n",
       "      <td>71</td>\n",
       "      <td>0.820092</td>\n",
       "      <td>0.829032</td>\n",
       "      <td>0.753666</td>\n",
       "      <td>0.873960</td>\n",
       "      <td>0.789555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.1]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.846013</td>\n",
       "      <td>0.811166</td>\n",
       "      <td>0.352726</td>\n",
       "      <td>0.435091</td>\n",
       "      <td>66</td>\n",
       "      <td>0.816152</td>\n",
       "      <td>0.819048</td>\n",
       "      <td>0.756598</td>\n",
       "      <td>0.864447</td>\n",
       "      <td>0.786585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.1]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.828134</td>\n",
       "      <td>0.802956</td>\n",
       "      <td>0.395003</td>\n",
       "      <td>0.436097</td>\n",
       "      <td>76</td>\n",
       "      <td>0.818122</td>\n",
       "      <td>0.827141</td>\n",
       "      <td>0.750733</td>\n",
       "      <td>0.872771</td>\n",
       "      <td>0.787087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.1]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.832147</td>\n",
       "      <td>0.801314</td>\n",
       "      <td>0.393620</td>\n",
       "      <td>0.440261</td>\n",
       "      <td>71</td>\n",
       "      <td>0.814839</td>\n",
       "      <td>0.822581</td>\n",
       "      <td>0.747801</td>\n",
       "      <td>0.869203</td>\n",
       "      <td>0.783410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.1]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.825944</td>\n",
       "      <td>0.801314</td>\n",
       "      <td>0.402604</td>\n",
       "      <td>0.440297</td>\n",
       "      <td>49</td>\n",
       "      <td>0.810243</td>\n",
       "      <td>0.816425</td>\n",
       "      <td>0.743402</td>\n",
       "      <td>0.864447</td>\n",
       "      <td>0.778204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.01]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.841087</td>\n",
       "      <td>0.807882</td>\n",
       "      <td>0.368997</td>\n",
       "      <td>0.436075</td>\n",
       "      <td>47</td>\n",
       "      <td>0.822062</td>\n",
       "      <td>0.835237</td>\n",
       "      <td>0.750733</td>\n",
       "      <td>0.879905</td>\n",
       "      <td>0.790734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.01]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.829593</td>\n",
       "      <td>0.801314</td>\n",
       "      <td>0.392296</td>\n",
       "      <td>0.438242</td>\n",
       "      <td>56</td>\n",
       "      <td>0.818122</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.747801</td>\n",
       "      <td>0.875149</td>\n",
       "      <td>0.786430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.01]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.842365</td>\n",
       "      <td>0.806240</td>\n",
       "      <td>0.364131</td>\n",
       "      <td>0.435487</td>\n",
       "      <td>58</td>\n",
       "      <td>0.820092</td>\n",
       "      <td>0.824841</td>\n",
       "      <td>0.759531</td>\n",
       "      <td>0.869203</td>\n",
       "      <td>0.790840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.01]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.832330</td>\n",
       "      <td>0.802956</td>\n",
       "      <td>0.386347</td>\n",
       "      <td>0.433641</td>\n",
       "      <td>76</td>\n",
       "      <td>0.815496</td>\n",
       "      <td>0.827080</td>\n",
       "      <td>0.743402</td>\n",
       "      <td>0.873960</td>\n",
       "      <td>0.783012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.01]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.831235</td>\n",
       "      <td>0.799672</td>\n",
       "      <td>0.394013</td>\n",
       "      <td>0.439607</td>\n",
       "      <td>73</td>\n",
       "      <td>0.816152</td>\n",
       "      <td>0.824194</td>\n",
       "      <td>0.749267</td>\n",
       "      <td>0.870392</td>\n",
       "      <td>0.784946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.01]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.833789</td>\n",
       "      <td>0.798030</td>\n",
       "      <td>0.382924</td>\n",
       "      <td>0.436679</td>\n",
       "      <td>61</td>\n",
       "      <td>0.820092</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.747801</td>\n",
       "      <td>0.878716</td>\n",
       "      <td>0.788253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.3]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.838898</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.371425</td>\n",
       "      <td>0.434963</td>\n",
       "      <td>51</td>\n",
       "      <td>0.818779</td>\n",
       "      <td>0.838333</td>\n",
       "      <td>0.737537</td>\n",
       "      <td>0.884661</td>\n",
       "      <td>0.784711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.3]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.801314</td>\n",
       "      <td>0.392574</td>\n",
       "      <td>0.437902</td>\n",
       "      <td>61</td>\n",
       "      <td>0.820749</td>\n",
       "      <td>0.838017</td>\n",
       "      <td>0.743402</td>\n",
       "      <td>0.883472</td>\n",
       "      <td>0.787879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.3]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.836891</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.374691</td>\n",
       "      <td>0.436608</td>\n",
       "      <td>59</td>\n",
       "      <td>0.814183</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.743402</td>\n",
       "      <td>0.871581</td>\n",
       "      <td>0.781804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.3]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.829411</td>\n",
       "      <td>0.802956</td>\n",
       "      <td>0.404546</td>\n",
       "      <td>0.436752</td>\n",
       "      <td>78</td>\n",
       "      <td>0.811556</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.740469</td>\n",
       "      <td>0.869203</td>\n",
       "      <td>0.778720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.3]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.830870</td>\n",
       "      <td>0.802956</td>\n",
       "      <td>0.390450</td>\n",
       "      <td>0.439243</td>\n",
       "      <td>82</td>\n",
       "      <td>0.814839</td>\n",
       "      <td>0.826797</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.873960</td>\n",
       "      <td>0.782071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.3]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.828134</td>\n",
       "      <td>0.802956</td>\n",
       "      <td>0.398168</td>\n",
       "      <td>0.438641</td>\n",
       "      <td>56</td>\n",
       "      <td>0.818122</td>\n",
       "      <td>0.836938</td>\n",
       "      <td>0.737537</td>\n",
       "      <td>0.883472</td>\n",
       "      <td>0.784100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.4]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.833607</td>\n",
       "      <td>0.799672</td>\n",
       "      <td>0.386806</td>\n",
       "      <td>0.436261</td>\n",
       "      <td>48</td>\n",
       "      <td>0.817466</td>\n",
       "      <td>0.828990</td>\n",
       "      <td>0.746334</td>\n",
       "      <td>0.875149</td>\n",
       "      <td>0.785494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.4]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.835432</td>\n",
       "      <td>0.801314</td>\n",
       "      <td>0.383843</td>\n",
       "      <td>0.438633</td>\n",
       "      <td>69</td>\n",
       "      <td>0.818122</td>\n",
       "      <td>0.831424</td>\n",
       "      <td>0.744868</td>\n",
       "      <td>0.877527</td>\n",
       "      <td>0.785770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.4]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.844554</td>\n",
       "      <td>0.807882</td>\n",
       "      <td>0.364921</td>\n",
       "      <td>0.439176</td>\n",
       "      <td>71</td>\n",
       "      <td>0.818779</td>\n",
       "      <td>0.827419</td>\n",
       "      <td>0.752199</td>\n",
       "      <td>0.872771</td>\n",
       "      <td>0.788018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.4]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.830688</td>\n",
       "      <td>0.801314</td>\n",
       "      <td>0.404421</td>\n",
       "      <td>0.436516</td>\n",
       "      <td>86</td>\n",
       "      <td>0.812213</td>\n",
       "      <td>0.827815</td>\n",
       "      <td>0.733138</td>\n",
       "      <td>0.876338</td>\n",
       "      <td>0.777605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.4]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.829411</td>\n",
       "      <td>0.799672</td>\n",
       "      <td>0.396290</td>\n",
       "      <td>0.438927</td>\n",
       "      <td>86</td>\n",
       "      <td>0.821405</td>\n",
       "      <td>0.831715</td>\n",
       "      <td>0.753666</td>\n",
       "      <td>0.876338</td>\n",
       "      <td>0.790769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>[0.2, 0.4]</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.818829</td>\n",
       "      <td>0.796387</td>\n",
       "      <td>0.411629</td>\n",
       "      <td>0.441970</td>\n",
       "      <td>49</td>\n",
       "      <td>0.813526</td>\n",
       "      <td>0.825163</td>\n",
       "      <td>0.740469</td>\n",
       "      <td>0.872771</td>\n",
       "      <td>0.780526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_layers  layer_conf      lr      dropout  batch_size  accuracy_train  \\\n",
       "0          2  [512, 256]  0.0001   [0.2, 0.1]        1250        0.844371   \n",
       "1          2  [256, 256]  0.0001   [0.2, 0.1]        1250        0.843277   \n",
       "2          2  [512, 128]  0.0001   [0.2, 0.1]        1250        0.846013   \n",
       "3          2   [256, 32]  0.0001   [0.2, 0.1]        1250        0.828134   \n",
       "4          2  [128, 128]  0.0001   [0.2, 0.1]        1250        0.832147   \n",
       "5          2  [256, 256]  0.0001   [0.2, 0.1]        1250        0.825944   \n",
       "6          2  [512, 256]  0.0001  [0.2, 0.01]        1250        0.841087   \n",
       "7          2  [256, 256]  0.0001  [0.2, 0.01]        1250        0.829593   \n",
       "8          2  [512, 128]  0.0001  [0.2, 0.01]        1250        0.842365   \n",
       "9          2   [256, 32]  0.0001  [0.2, 0.01]        1250        0.832330   \n",
       "10         2  [128, 128]  0.0001  [0.2, 0.01]        1250        0.831235   \n",
       "11         2  [256, 256]  0.0001  [0.2, 0.01]        1250        0.833789   \n",
       "12         2  [512, 256]  0.0001   [0.2, 0.3]        1250        0.838898   \n",
       "13         2  [256, 256]  0.0001   [0.2, 0.3]        1250        0.827586   \n",
       "14         2  [512, 128]  0.0001   [0.2, 0.3]        1250        0.836891   \n",
       "15         2   [256, 32]  0.0001   [0.2, 0.3]        1250        0.829411   \n",
       "16         2  [128, 128]  0.0001   [0.2, 0.3]        1250        0.830870   \n",
       "17         2  [256, 256]  0.0001   [0.2, 0.3]        1250        0.828134   \n",
       "18         2  [512, 256]  0.0001   [0.2, 0.4]        1250        0.833607   \n",
       "19         2  [256, 256]  0.0001   [0.2, 0.4]        1250        0.835432   \n",
       "20         2  [512, 128]  0.0001   [0.2, 0.4]        1250        0.844554   \n",
       "21         2   [256, 32]  0.0001   [0.2, 0.4]        1250        0.830688   \n",
       "22         2  [128, 128]  0.0001   [0.2, 0.4]        1250        0.829411   \n",
       "23         2  [256, 256]  0.0001   [0.2, 0.4]        1250        0.818829   \n",
       "\n",
       "    accuracy_val  loss_train  loss_val  epochs  accuracy  precision    recall  \\\n",
       "0       0.799672    0.362952  0.434138      50  0.820749   0.825119  0.760997   \n",
       "1       0.801314    0.372175  0.435570      71  0.820092   0.829032  0.753666   \n",
       "2       0.811166    0.352726  0.435091      66  0.816152   0.819048  0.756598   \n",
       "3       0.802956    0.395003  0.436097      76  0.818122   0.827141  0.750733   \n",
       "4       0.801314    0.393620  0.440261      71  0.814839   0.822581  0.747801   \n",
       "5       0.801314    0.402604  0.440297      49  0.810243   0.816425  0.743402   \n",
       "6       0.807882    0.368997  0.436075      47  0.822062   0.835237  0.750733   \n",
       "7       0.801314    0.392296  0.438242      56  0.818122   0.829268  0.747801   \n",
       "8       0.806240    0.364131  0.435487      58  0.820092   0.824841  0.759531   \n",
       "9       0.802956    0.386347  0.433641      76  0.815496   0.827080  0.743402   \n",
       "10      0.799672    0.394013  0.439607      73  0.816152   0.824194  0.749267   \n",
       "11      0.798030    0.382924  0.436679      61  0.820092   0.833333  0.747801   \n",
       "12      0.804598    0.371425  0.434963      51  0.818779   0.838333  0.737537   \n",
       "13      0.801314    0.392574  0.437902      61  0.820749   0.838017  0.743402   \n",
       "14      0.809524    0.374691  0.436608      59  0.814183   0.824390  0.743402   \n",
       "15      0.802956    0.404546  0.436752      78  0.811556   0.821138  0.740469   \n",
       "16      0.802956    0.390450  0.439243      82  0.814839   0.826797  0.741935   \n",
       "17      0.802956    0.398168  0.438641      56  0.818122   0.836938  0.737537   \n",
       "18      0.799672    0.386806  0.436261      48  0.817466   0.828990  0.746334   \n",
       "19      0.801314    0.383843  0.438633      69  0.818122   0.831424  0.744868   \n",
       "20      0.807882    0.364921  0.439176      71  0.818779   0.827419  0.752199   \n",
       "21      0.801314    0.404421  0.436516      86  0.812213   0.827815  0.733138   \n",
       "22      0.799672    0.396290  0.438927      86  0.821405   0.831715  0.753666   \n",
       "23      0.796387    0.411629  0.441970      49  0.813526   0.825163  0.740469   \n",
       "\n",
       "    specificity  f1_score  \n",
       "0      0.869203  0.791762  \n",
       "1      0.873960  0.789555  \n",
       "2      0.864447  0.786585  \n",
       "3      0.872771  0.787087  \n",
       "4      0.869203  0.783410  \n",
       "5      0.864447  0.778204  \n",
       "6      0.879905  0.790734  \n",
       "7      0.875149  0.786430  \n",
       "8      0.869203  0.790840  \n",
       "9      0.873960  0.783012  \n",
       "10     0.870392  0.784946  \n",
       "11     0.878716  0.788253  \n",
       "12     0.884661  0.784711  \n",
       "13     0.883472  0.787879  \n",
       "14     0.871581  0.781804  \n",
       "15     0.869203  0.778720  \n",
       "16     0.873960  0.782071  \n",
       "17     0.883472  0.784100  \n",
       "18     0.875149  0.785494  \n",
       "19     0.877527  0.785770  \n",
       "20     0.872771  0.788018  \n",
       "21     0.876338  0.777605  \n",
       "22     0.876338  0.790769  \n",
       "23     0.872771  0.780526  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_perf_metrics5.to_csv('mlp_perf_metrics5.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No significative performance gain is reachen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's propose other activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params6 = {\n",
    "        'lay_conf': best_arch1,\n",
    "        'lr': [1e-4],\n",
    "        'dropout': [0.2],\n",
    "        'max_epochs': [300],\n",
    "        'batch_size': [1250],\n",
    "        'inner_act_func': ['sigmoid'],\n",
    "        'seed': [123456]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters {'batch_size': 1250, 'dropout': 0.2, 'inner_act_func': 'sigmoid', 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.7506 - acc: 0.4450 - val_loss: 0.6813 - val_acc: 0.5599\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6781 - acc: 0.5610 - val_loss: 0.6633 - val_acc: 0.5649\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6563 - acc: 0.5878 - val_loss: 0.6356 - val_acc: 0.5681\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6297 - acc: 0.6240 - val_loss: 0.6025 - val_acc: 0.7192\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6073 - acc: 0.6955 - val_loss: 0.5815 - val_acc: 0.7340\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.5907 - acc: 0.7059 - val_loss: 0.5626 - val_acc: 0.7389\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5748 - acc: 0.7176 - val_loss: 0.5462 - val_acc: 0.7767\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5598 - acc: 0.7354 - val_loss: 0.5311 - val_acc: 0.7833\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5415 - acc: 0.7490 - val_loss: 0.5155 - val_acc: 0.7767\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5304 - acc: 0.7502 - val_loss: 0.5027 - val_acc: 0.7849\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5169 - acc: 0.7665 - val_loss: 0.4935 - val_acc: 0.7931\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5047 - acc: 0.7681 - val_loss: 0.4821 - val_acc: 0.7816\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4949 - acc: 0.7725 - val_loss: 0.4734 - val_acc: 0.7833\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4866 - acc: 0.7778 - val_loss: 0.4677 - val_acc: 0.7997\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4797 - acc: 0.7811 - val_loss: 0.4596 - val_acc: 0.7915\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4739 - acc: 0.7818 - val_loss: 0.4535 - val_acc: 0.7947\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4657 - acc: 0.7873 - val_loss: 0.4482 - val_acc: 0.7964\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4574 - acc: 0.7958 - val_loss: 0.4445 - val_acc: 0.8030\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4509 - acc: 0.7975 - val_loss: 0.4404 - val_acc: 0.7997\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4475 - acc: 0.8026 - val_loss: 0.4371 - val_acc: 0.7964\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4446 - acc: 0.8033 - val_loss: 0.4339 - val_acc: 0.8046\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4383 - acc: 0.8050 - val_loss: 0.4307 - val_acc: 0.8030\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4297 - acc: 0.8097 - val_loss: 0.4283 - val_acc: 0.8030\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4349 - acc: 0.8104 - val_loss: 0.4264 - val_acc: 0.8062\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4299 - acc: 0.8108 - val_loss: 0.4252 - val_acc: 0.8079\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4259 - acc: 0.8161 - val_loss: 0.4230 - val_acc: 0.8079\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4234 - acc: 0.8155 - val_loss: 0.4213 - val_acc: 0.8112\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4205 - acc: 0.8192 - val_loss: 0.4201 - val_acc: 0.8112\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4155 - acc: 0.8186 - val_loss: 0.4198 - val_acc: 0.8112\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4163 - acc: 0.8172 - val_loss: 0.4192 - val_acc: 0.8112\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4110 - acc: 0.8177 - val_loss: 0.4176 - val_acc: 0.8112\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4088 - acc: 0.8245 - val_loss: 0.4161 - val_acc: 0.8161\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4138 - acc: 0.8188 - val_loss: 0.4158 - val_acc: 0.8144\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4044 - acc: 0.8272 - val_loss: 0.4158 - val_acc: 0.8177\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4009 - acc: 0.8278 - val_loss: 0.4151 - val_acc: 0.8210\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4034 - acc: 0.8223 - val_loss: 0.4137 - val_acc: 0.8210\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3995 - acc: 0.8234 - val_loss: 0.4135 - val_acc: 0.8194\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3978 - acc: 0.8276 - val_loss: 0.4130 - val_acc: 0.8210\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3939 - acc: 0.8290 - val_loss: 0.4128 - val_acc: 0.8210\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3948 - acc: 0.8267 - val_loss: 0.4130 - val_acc: 0.8243\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3908 - acc: 0.8292 - val_loss: 0.4127 - val_acc: 0.8227\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3896 - acc: 0.8280 - val_loss: 0.4122 - val_acc: 0.8210\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3886 - acc: 0.8280 - val_loss: 0.4120 - val_acc: 0.8194\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3857 - acc: 0.8311 - val_loss: 0.4111 - val_acc: 0.8243\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3837 - acc: 0.8327 - val_loss: 0.4105 - val_acc: 0.8227\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3811 - acc: 0.8349 - val_loss: 0.4110 - val_acc: 0.8243\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3822 - acc: 0.8294 - val_loss: 0.4110 - val_acc: 0.8276\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3780 - acc: 0.8336 - val_loss: 0.4102 - val_acc: 0.8276\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3731 - acc: 0.8374 - val_loss: 0.4099 - val_acc: 0.8292\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3736 - acc: 0.8389 - val_loss: 0.4106 - val_acc: 0.8276\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - acc: 0.8378 - val_loss: 0.4094 - val_acc: 0.8309\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3710 - acc: 0.8385 - val_loss: 0.4088 - val_acc: 0.8276\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3678 - acc: 0.8378 - val_loss: 0.4094 - val_acc: 0.8309\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3670 - acc: 0.8371 - val_loss: 0.4107 - val_acc: 0.8276\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3628 - acc: 0.8418 - val_loss: 0.4124 - val_acc: 0.8325\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3596 - acc: 0.8425 - val_loss: 0.4109 - val_acc: 0.8325\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3585 - acc: 0.8449 - val_loss: 0.4105 - val_acc: 0.8342\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.2, 'inner_act_func': 'sigmoid', 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.8276 - acc: 0.4280 - val_loss: 0.7278 - val_acc: 0.4368\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7296 - acc: 0.4651 - val_loss: 0.6768 - val_acc: 0.6240\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6837 - acc: 0.5647 - val_loss: 0.6590 - val_acc: 0.5698\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6600 - acc: 0.6079 - val_loss: 0.6488 - val_acc: 0.5649\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6493 - acc: 0.6139 - val_loss: 0.6323 - val_acc: 0.5665\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6329 - acc: 0.6400 - val_loss: 0.6119 - val_acc: 0.6305\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6180 - acc: 0.6831 - val_loss: 0.5950 - val_acc: 0.7504\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6088 - acc: 0.6807 - val_loss: 0.5820 - val_acc: 0.7570\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5978 - acc: 0.6964 - val_loss: 0.5696 - val_acc: 0.7537\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5851 - acc: 0.6973 - val_loss: 0.5583 - val_acc: 0.7668\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5740 - acc: 0.7139 - val_loss: 0.5478 - val_acc: 0.7750\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5644 - acc: 0.7225 - val_loss: 0.5368 - val_acc: 0.7816\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5573 - acc: 0.7302 - val_loss: 0.5262 - val_acc: 0.7849\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5456 - acc: 0.7417 - val_loss: 0.5155 - val_acc: 0.7882\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5351 - acc: 0.7477 - val_loss: 0.5055 - val_acc: 0.7882\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5240 - acc: 0.7535 - val_loss: 0.4965 - val_acc: 0.8013\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5135 - acc: 0.7635 - val_loss: 0.4887 - val_acc: 0.8095\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5023 - acc: 0.7758 - val_loss: 0.4803 - val_acc: 0.8079\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4975 - acc: 0.7643 - val_loss: 0.4732 - val_acc: 0.8062\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4937 - acc: 0.7745 - val_loss: 0.4672 - val_acc: 0.8046\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4836 - acc: 0.7818 - val_loss: 0.4612 - val_acc: 0.8095\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4817 - acc: 0.7785 - val_loss: 0.4574 - val_acc: 0.8095\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4759 - acc: 0.7847 - val_loss: 0.4520 - val_acc: 0.8079\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4708 - acc: 0.7849 - val_loss: 0.4487 - val_acc: 0.8046\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4644 - acc: 0.7891 - val_loss: 0.4463 - val_acc: 0.8030\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4592 - acc: 0.7964 - val_loss: 0.4427 - val_acc: 0.8046\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4549 - acc: 0.7955 - val_loss: 0.4398 - val_acc: 0.8013\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4525 - acc: 0.8000 - val_loss: 0.4386 - val_acc: 0.8030\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4500 - acc: 0.7997 - val_loss: 0.4354 - val_acc: 0.8144\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4500 - acc: 0.7986 - val_loss: 0.4333 - val_acc: 0.8095\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4461 - acc: 0.8019 - val_loss: 0.4318 - val_acc: 0.8095\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4404 - acc: 0.8059 - val_loss: 0.4297 - val_acc: 0.8161\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4395 - acc: 0.8031 - val_loss: 0.4282 - val_acc: 0.8144\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4369 - acc: 0.8062 - val_loss: 0.4264 - val_acc: 0.8161\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4350 - acc: 0.8050 - val_loss: 0.4250 - val_acc: 0.8144\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4329 - acc: 0.8075 - val_loss: 0.4247 - val_acc: 0.8128\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4358 - acc: 0.8057 - val_loss: 0.4232 - val_acc: 0.8112\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4293 - acc: 0.8070 - val_loss: 0.4223 - val_acc: 0.8144\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4323 - acc: 0.8104 - val_loss: 0.4212 - val_acc: 0.8112\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4197 - acc: 0.8103 - val_loss: 0.4205 - val_acc: 0.8128\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4214 - acc: 0.8139 - val_loss: 0.4198 - val_acc: 0.8128\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4201 - acc: 0.8154 - val_loss: 0.4188 - val_acc: 0.8144\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4188 - acc: 0.8139 - val_loss: 0.4182 - val_acc: 0.8161\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4198 - acc: 0.8134 - val_loss: 0.4172 - val_acc: 0.8144\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4128 - acc: 0.8194 - val_loss: 0.4173 - val_acc: 0.8128\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4130 - acc: 0.8203 - val_loss: 0.4172 - val_acc: 0.8161\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4172 - acc: 0.8197 - val_loss: 0.4161 - val_acc: 0.8128\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4105 - acc: 0.8199 - val_loss: 0.4154 - val_acc: 0.8177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4108 - acc: 0.8176 - val_loss: 0.4152 - val_acc: 0.8177\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4088 - acc: 0.8216 - val_loss: 0.4145 - val_acc: 0.8177\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4065 - acc: 0.8238 - val_loss: 0.4145 - val_acc: 0.8194\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.4028 - acc: 0.8281 - val_loss: 0.4143 - val_acc: 0.8177\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4040 - acc: 0.8210 - val_loss: 0.4138 - val_acc: 0.8227\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3991 - acc: 0.8267 - val_loss: 0.4136 - val_acc: 0.8243\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3989 - acc: 0.8263 - val_loss: 0.4132 - val_acc: 0.8128\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4011 - acc: 0.8239 - val_loss: 0.4131 - val_acc: 0.8128\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4009 - acc: 0.8254 - val_loss: 0.4146 - val_acc: 0.8161\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3983 - acc: 0.8238 - val_loss: 0.4142 - val_acc: 0.8128\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3940 - acc: 0.8263 - val_loss: 0.4128 - val_acc: 0.8144\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3953 - acc: 0.8269 - val_loss: 0.4131 - val_acc: 0.8177\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3923 - acc: 0.8247 - val_loss: 0.4135 - val_acc: 0.8128\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3924 - acc: 0.8298 - val_loss: 0.4134 - val_acc: 0.8128\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3883 - acc: 0.8301 - val_loss: 0.4133 - val_acc: 0.8095\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3887 - acc: 0.8320 - val_loss: 0.4128 - val_acc: 0.8112\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.2, 'inner_act_func': 'sigmoid', 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_27 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.6669 - acc: 0.5975 - val_loss: 0.6436 - val_acc: 0.6437\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6450 - acc: 0.6265 - val_loss: 0.6301 - val_acc: 0.6026\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6322 - acc: 0.6453 - val_loss: 0.6155 - val_acc: 0.6700\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6181 - acc: 0.6670 - val_loss: 0.6002 - val_acc: 0.7143\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6064 - acc: 0.6964 - val_loss: 0.5873 - val_acc: 0.7438\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5949 - acc: 0.7119 - val_loss: 0.5762 - val_acc: 0.7471\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5850 - acc: 0.7101 - val_loss: 0.5659 - val_acc: 0.7504\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5756 - acc: 0.7219 - val_loss: 0.5568 - val_acc: 0.7619\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5679 - acc: 0.7256 - val_loss: 0.5487 - val_acc: 0.7718\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5596 - acc: 0.7391 - val_loss: 0.5410 - val_acc: 0.7718\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5530 - acc: 0.7404 - val_loss: 0.5339 - val_acc: 0.7718\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5464 - acc: 0.7455 - val_loss: 0.5273 - val_acc: 0.7865\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5378 - acc: 0.7480 - val_loss: 0.5211 - val_acc: 0.7865\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5340 - acc: 0.7546 - val_loss: 0.5156 - val_acc: 0.7931\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5290 - acc: 0.7581 - val_loss: 0.5103 - val_acc: 0.7898\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5200 - acc: 0.7668 - val_loss: 0.5051 - val_acc: 0.7931\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5164 - acc: 0.7688 - val_loss: 0.5008 - val_acc: 0.7980\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5132 - acc: 0.7672 - val_loss: 0.4962 - val_acc: 0.7980\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5088 - acc: 0.7661 - val_loss: 0.4918 - val_acc: 0.7931\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5031 - acc: 0.7739 - val_loss: 0.4881 - val_acc: 0.7931\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4994 - acc: 0.7739 - val_loss: 0.4849 - val_acc: 0.7997\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4950 - acc: 0.7801 - val_loss: 0.4816 - val_acc: 0.7980\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4913 - acc: 0.7791 - val_loss: 0.4786 - val_acc: 0.7964\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4889 - acc: 0.7840 - val_loss: 0.4755 - val_acc: 0.7964\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4868 - acc: 0.7807 - val_loss: 0.4727 - val_acc: 0.7964\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4828 - acc: 0.7829 - val_loss: 0.4700 - val_acc: 0.8062\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4793 - acc: 0.7869 - val_loss: 0.4677 - val_acc: 0.8112\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4771 - acc: 0.7909 - val_loss: 0.4653 - val_acc: 0.8046\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4758 - acc: 0.7909 - val_loss: 0.4635 - val_acc: 0.8046\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4720 - acc: 0.7884 - val_loss: 0.4615 - val_acc: 0.8128\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4695 - acc: 0.7940 - val_loss: 0.4598 - val_acc: 0.8194\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4655 - acc: 0.7949 - val_loss: 0.4576 - val_acc: 0.8144\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4636 - acc: 0.7933 - val_loss: 0.4559 - val_acc: 0.8112\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4628 - acc: 0.7937 - val_loss: 0.4542 - val_acc: 0.8112\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4606 - acc: 0.7977 - val_loss: 0.4528 - val_acc: 0.8112\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4605 - acc: 0.7989 - val_loss: 0.4515 - val_acc: 0.8112\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4575 - acc: 0.7988 - val_loss: 0.4506 - val_acc: 0.8128\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4583 - acc: 0.8013 - val_loss: 0.4493 - val_acc: 0.8112\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4537 - acc: 0.8031 - val_loss: 0.4485 - val_acc: 0.8079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4526 - acc: 0.8024 - val_loss: 0.4474 - val_acc: 0.8128\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4530 - acc: 0.8008 - val_loss: 0.4462 - val_acc: 0.8128\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4476 - acc: 0.8064 - val_loss: 0.4450 - val_acc: 0.8128\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4468 - acc: 0.8011 - val_loss: 0.4442 - val_acc: 0.8062\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4460 - acc: 0.8112 - val_loss: 0.4433 - val_acc: 0.8112\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4441 - acc: 0.8082 - val_loss: 0.4423 - val_acc: 0.8112\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4447 - acc: 0.8079 - val_loss: 0.4416 - val_acc: 0.8062\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4439 - acc: 0.8110 - val_loss: 0.4407 - val_acc: 0.8062\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4425 - acc: 0.8050 - val_loss: 0.4398 - val_acc: 0.8079\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4407 - acc: 0.8106 - val_loss: 0.4391 - val_acc: 0.8128\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4408 - acc: 0.8093 - val_loss: 0.4384 - val_acc: 0.8030\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4408 - acc: 0.8061 - val_loss: 0.4376 - val_acc: 0.8144\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4368 - acc: 0.8112 - val_loss: 0.4368 - val_acc: 0.8046\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4364 - acc: 0.8103 - val_loss: 0.4360 - val_acc: 0.8046\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4362 - acc: 0.8121 - val_loss: 0.4354 - val_acc: 0.8144\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4339 - acc: 0.8103 - val_loss: 0.4350 - val_acc: 0.8112\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4356 - acc: 0.8093 - val_loss: 0.4347 - val_acc: 0.8046\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4330 - acc: 0.8095 - val_loss: 0.4342 - val_acc: 0.8046\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4327 - acc: 0.8143 - val_loss: 0.4338 - val_acc: 0.8062\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4314 - acc: 0.8115 - val_loss: 0.4333 - val_acc: 0.8079\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4299 - acc: 0.8181 - val_loss: 0.4327 - val_acc: 0.8062\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4284 - acc: 0.8150 - val_loss: 0.4321 - val_acc: 0.8062\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4313 - acc: 0.8146 - val_loss: 0.4315 - val_acc: 0.8062\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4268 - acc: 0.8176 - val_loss: 0.4313 - val_acc: 0.8062\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4252 - acc: 0.8159 - val_loss: 0.4309 - val_acc: 0.8030\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4251 - acc: 0.8155 - val_loss: 0.4303 - val_acc: 0.8030\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4226 - acc: 0.8183 - val_loss: 0.4297 - val_acc: 0.8046\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4247 - acc: 0.8159 - val_loss: 0.4291 - val_acc: 0.8046\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4234 - acc: 0.8152 - val_loss: 0.4289 - val_acc: 0.8062\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4211 - acc: 0.8165 - val_loss: 0.4285 - val_acc: 0.8046\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4208 - acc: 0.8185 - val_loss: 0.4283 - val_acc: 0.8128\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4211 - acc: 0.8165 - val_loss: 0.4281 - val_acc: 0.8030\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4210 - acc: 0.8170 - val_loss: 0.4279 - val_acc: 0.8030\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4191 - acc: 0.8196 - val_loss: 0.4274 - val_acc: 0.8013\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4193 - acc: 0.8177 - val_loss: 0.4270 - val_acc: 0.8030\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4183 - acc: 0.8199 - val_loss: 0.4263 - val_acc: 0.8079\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4166 - acc: 0.8179 - val_loss: 0.4259 - val_acc: 0.8046\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4137 - acc: 0.8210 - val_loss: 0.4256 - val_acc: 0.8062\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4150 - acc: 0.8199 - val_loss: 0.4255 - val_acc: 0.8079\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4141 - acc: 0.8210 - val_loss: 0.4254 - val_acc: 0.8030\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4133 - acc: 0.8207 - val_loss: 0.4255 - val_acc: 0.8013\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4146 - acc: 0.8199 - val_loss: 0.4252 - val_acc: 0.8030\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4139 - acc: 0.8185 - val_loss: 0.4248 - val_acc: 0.8046\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4137 - acc: 0.8216 - val_loss: 0.4247 - val_acc: 0.8030\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4131 - acc: 0.8228 - val_loss: 0.4241 - val_acc: 0.8030\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4122 - acc: 0.8212 - val_loss: 0.4237 - val_acc: 0.8062\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4102 - acc: 0.8232 - val_loss: 0.4231 - val_acc: 0.8079\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4113 - acc: 0.8243 - val_loss: 0.4227 - val_acc: 0.8062\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4115 - acc: 0.8221 - val_loss: 0.4227 - val_acc: 0.8062\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4093 - acc: 0.8210 - val_loss: 0.4224 - val_acc: 0.8079\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4085 - acc: 0.8217 - val_loss: 0.4221 - val_acc: 0.8079\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4089 - acc: 0.8185 - val_loss: 0.4222 - val_acc: 0.8079\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4080 - acc: 0.8207 - val_loss: 0.4223 - val_acc: 0.8079\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4088 - acc: 0.8219 - val_loss: 0.4219 - val_acc: 0.8079\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4060 - acc: 0.8247 - val_loss: 0.4213 - val_acc: 0.8144\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4051 - acc: 0.8225 - val_loss: 0.4215 - val_acc: 0.8079\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4063 - acc: 0.8263 - val_loss: 0.4210 - val_acc: 0.8112\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4054 - acc: 0.8252 - val_loss: 0.4208 - val_acc: 0.8112\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4038 - acc: 0.8221 - val_loss: 0.4211 - val_acc: 0.8079\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4044 - acc: 0.8265 - val_loss: 0.4209 - val_acc: 0.8112\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4027 - acc: 0.8254 - val_loss: 0.4202 - val_acc: 0.8128\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4052 - acc: 0.8254 - val_loss: 0.4202 - val_acc: 0.8112\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4015 - acc: 0.8272 - val_loss: 0.4216 - val_acc: 0.8128\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4032 - acc: 0.8243 - val_loss: 0.4204 - val_acc: 0.8128\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4018 - acc: 0.8245 - val_loss: 0.4199 - val_acc: 0.8144\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3992 - acc: 0.8274 - val_loss: 0.4198 - val_acc: 0.8128\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3994 - acc: 0.8270 - val_loss: 0.4200 - val_acc: 0.8177\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4014 - acc: 0.8239 - val_loss: 0.4194 - val_acc: 0.8177\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3987 - acc: 0.8276 - val_loss: 0.4193 - val_acc: 0.8128\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3993 - acc: 0.8274 - val_loss: 0.4198 - val_acc: 0.8177\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3978 - acc: 0.8281 - val_loss: 0.4197 - val_acc: 0.8194\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3976 - acc: 0.8283 - val_loss: 0.4195 - val_acc: 0.8177\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3966 - acc: 0.8292 - val_loss: 0.4194 - val_acc: 0.8177\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3950 - acc: 0.8245 - val_loss: 0.4193 - val_acc: 0.8177\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.2, 'inner_act_func': 'sigmoid', 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_28 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.8125 - acc: 0.4244 - val_loss: 0.7104 - val_acc: 0.4433\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6991 - acc: 0.5151 - val_loss: 0.6734 - val_acc: 0.5583\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6713 - acc: 0.5795 - val_loss: 0.6546 - val_acc: 0.5649\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6499 - acc: 0.5981 - val_loss: 0.6248 - val_acc: 0.6010\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6258 - acc: 0.6450 - val_loss: 0.5990 - val_acc: 0.7241\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6060 - acc: 0.6864 - val_loss: 0.5818 - val_acc: 0.7291\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5928 - acc: 0.6959 - val_loss: 0.5657 - val_acc: 0.7307\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5812 - acc: 0.7126 - val_loss: 0.5494 - val_acc: 0.7537\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5650 - acc: 0.7272 - val_loss: 0.5356 - val_acc: 0.7652\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5518 - acc: 0.7373 - val_loss: 0.5226 - val_acc: 0.7652\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5416 - acc: 0.7400 - val_loss: 0.5113 - val_acc: 0.7685\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5314 - acc: 0.7495 - val_loss: 0.5011 - val_acc: 0.7816\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5160 - acc: 0.7625 - val_loss: 0.4915 - val_acc: 0.7898\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5039 - acc: 0.7677 - val_loss: 0.4827 - val_acc: 0.7947\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4993 - acc: 0.7739 - val_loss: 0.4746 - val_acc: 0.8046\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4904 - acc: 0.7772 - val_loss: 0.4677 - val_acc: 0.8079\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4821 - acc: 0.7814 - val_loss: 0.4614 - val_acc: 0.8013\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4788 - acc: 0.7812 - val_loss: 0.4555 - val_acc: 0.8112\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4687 - acc: 0.7909 - val_loss: 0.4509 - val_acc: 0.8095\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4645 - acc: 0.7937 - val_loss: 0.4473 - val_acc: 0.8079\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4570 - acc: 0.7964 - val_loss: 0.4432 - val_acc: 0.8079\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4549 - acc: 0.7982 - val_loss: 0.4400 - val_acc: 0.8128\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4486 - acc: 0.8002 - val_loss: 0.4381 - val_acc: 0.8062\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4448 - acc: 0.8059 - val_loss: 0.4353 - val_acc: 0.8112\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4419 - acc: 0.8033 - val_loss: 0.4319 - val_acc: 0.8079\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4366 - acc: 0.8092 - val_loss: 0.4298 - val_acc: 0.8095\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4377 - acc: 0.8123 - val_loss: 0.4286 - val_acc: 0.8144\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4347 - acc: 0.8072 - val_loss: 0.4275 - val_acc: 0.8046\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4331 - acc: 0.8103 - val_loss: 0.4267 - val_acc: 0.8079\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4324 - acc: 0.8110 - val_loss: 0.4251 - val_acc: 0.8046\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4229 - acc: 0.8159 - val_loss: 0.4234 - val_acc: 0.8079\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4242 - acc: 0.8148 - val_loss: 0.4222 - val_acc: 0.8112\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4236 - acc: 0.8172 - val_loss: 0.4221 - val_acc: 0.8112\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4189 - acc: 0.8148 - val_loss: 0.4201 - val_acc: 0.8112\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4198 - acc: 0.8210 - val_loss: 0.4199 - val_acc: 0.8112\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4213 - acc: 0.8176 - val_loss: 0.4182 - val_acc: 0.8095\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4142 - acc: 0.8181 - val_loss: 0.4177 - val_acc: 0.8095\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4139 - acc: 0.8179 - val_loss: 0.4163 - val_acc: 0.8079\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4113 - acc: 0.8223 - val_loss: 0.4154 - val_acc: 0.8095\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4055 - acc: 0.8236 - val_loss: 0.4147 - val_acc: 0.8144\n",
      "Epoch 41/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4030 - acc: 0.8234 - val_loss: 0.4148 - val_acc: 0.8144\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4040 - acc: 0.8225 - val_loss: 0.4138 - val_acc: 0.8144\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4046 - acc: 0.8252 - val_loss: 0.4139 - val_acc: 0.8194\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4008 - acc: 0.8283 - val_loss: 0.4131 - val_acc: 0.8177\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3996 - acc: 0.8281 - val_loss: 0.4125 - val_acc: 0.8194\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3961 - acc: 0.8252 - val_loss: 0.4129 - val_acc: 0.8194\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3972 - acc: 0.8311 - val_loss: 0.4126 - val_acc: 0.8276\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3921 - acc: 0.8245 - val_loss: 0.4128 - val_acc: 0.8177\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3923 - acc: 0.8305 - val_loss: 0.4121 - val_acc: 0.8259\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3913 - acc: 0.8280 - val_loss: 0.4112 - val_acc: 0.8210\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3890 - acc: 0.8303 - val_loss: 0.4104 - val_acc: 0.8177\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3864 - acc: 0.8325 - val_loss: 0.4106 - val_acc: 0.8227\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3865 - acc: 0.8342 - val_loss: 0.4103 - val_acc: 0.8210\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3826 - acc: 0.8347 - val_loss: 0.4100 - val_acc: 0.8227\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3830 - acc: 0.8334 - val_loss: 0.4099 - val_acc: 0.8227\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3804 - acc: 0.8376 - val_loss: 0.4099 - val_acc: 0.8243\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3798 - acc: 0.8318 - val_loss: 0.4099 - val_acc: 0.8210\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3772 - acc: 0.8340 - val_loss: 0.4103 - val_acc: 0.8227\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3769 - acc: 0.8402 - val_loss: 0.4100 - val_acc: 0.8227\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3757 - acc: 0.8376 - val_loss: 0.4109 - val_acc: 0.8227\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3742 - acc: 0.8360 - val_loss: 0.4105 - val_acc: 0.8292\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.2, 'inner_act_func': 'sigmoid', 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.8643 - acc: 0.4266 - val_loss: 0.7574 - val_acc: 0.4351\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7562 - acc: 0.4459 - val_loss: 0.6980 - val_acc: 0.4844\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.7018 - acc: 0.5056 - val_loss: 0.6721 - val_acc: 0.6158\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6722 - acc: 0.5882 - val_loss: 0.6590 - val_acc: 0.5993\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6606 - acc: 0.6072 - val_loss: 0.6460 - val_acc: 0.6043\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6469 - acc: 0.6378 - val_loss: 0.6302 - val_acc: 0.6535\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6323 - acc: 0.6643 - val_loss: 0.6143 - val_acc: 0.7241\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6210 - acc: 0.6789 - val_loss: 0.6003 - val_acc: 0.7521\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6138 - acc: 0.6886 - val_loss: 0.5885 - val_acc: 0.7455\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6014 - acc: 0.6973 - val_loss: 0.5774 - val_acc: 0.7521\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5932 - acc: 0.7099 - val_loss: 0.5675 - val_acc: 0.7586\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5857 - acc: 0.7150 - val_loss: 0.5584 - val_acc: 0.7652\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5755 - acc: 0.7201 - val_loss: 0.5498 - val_acc: 0.7701\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5684 - acc: 0.7309 - val_loss: 0.5412 - val_acc: 0.7750\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5640 - acc: 0.7336 - val_loss: 0.5326 - val_acc: 0.7783\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5516 - acc: 0.7418 - val_loss: 0.5244 - val_acc: 0.7816\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5489 - acc: 0.7411 - val_loss: 0.5170 - val_acc: 0.7849\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5410 - acc: 0.7426 - val_loss: 0.5098 - val_acc: 0.7865\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5342 - acc: 0.7511 - val_loss: 0.5024 - val_acc: 0.7882\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5240 - acc: 0.7566 - val_loss: 0.4963 - val_acc: 0.7947\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5259 - acc: 0.7568 - val_loss: 0.4905 - val_acc: 0.8013\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5131 - acc: 0.7634 - val_loss: 0.4855 - val_acc: 0.7980\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5091 - acc: 0.7683 - val_loss: 0.4800 - val_acc: 0.8062\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5017 - acc: 0.7705 - val_loss: 0.4745 - val_acc: 0.8079\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5012 - acc: 0.7767 - val_loss: 0.4697 - val_acc: 0.8030\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4933 - acc: 0.7736 - val_loss: 0.4654 - val_acc: 0.8046\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4859 - acc: 0.7791 - val_loss: 0.4618 - val_acc: 0.8062\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4868 - acc: 0.7780 - val_loss: 0.4571 - val_acc: 0.8046\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4806 - acc: 0.7893 - val_loss: 0.4536 - val_acc: 0.8062\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4818 - acc: 0.7772 - val_loss: 0.4507 - val_acc: 0.8079\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4753 - acc: 0.7878 - val_loss: 0.4486 - val_acc: 0.8095\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4714 - acc: 0.7874 - val_loss: 0.4455 - val_acc: 0.8062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4664 - acc: 0.7871 - val_loss: 0.4430 - val_acc: 0.8079\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4646 - acc: 0.7964 - val_loss: 0.4412 - val_acc: 0.8079\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4632 - acc: 0.7922 - val_loss: 0.4388 - val_acc: 0.8013\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4619 - acc: 0.7911 - val_loss: 0.4376 - val_acc: 0.8030\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4545 - acc: 0.8013 - val_loss: 0.4374 - val_acc: 0.8062\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4531 - acc: 0.8035 - val_loss: 0.4347 - val_acc: 0.8046\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4506 - acc: 0.8046 - val_loss: 0.4332 - val_acc: 0.8046\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4509 - acc: 0.8048 - val_loss: 0.4320 - val_acc: 0.8030\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4530 - acc: 0.7962 - val_loss: 0.4302 - val_acc: 0.8030\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4459 - acc: 0.8017 - val_loss: 0.4287 - val_acc: 0.8062\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4420 - acc: 0.8028 - val_loss: 0.4279 - val_acc: 0.8079\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4379 - acc: 0.8081 - val_loss: 0.4264 - val_acc: 0.8079\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4433 - acc: 0.8041 - val_loss: 0.4253 - val_acc: 0.8062\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4369 - acc: 0.8066 - val_loss: 0.4243 - val_acc: 0.8079\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4329 - acc: 0.8079 - val_loss: 0.4238 - val_acc: 0.8062\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4336 - acc: 0.8126 - val_loss: 0.4243 - val_acc: 0.8079\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4319 - acc: 0.8113 - val_loss: 0.4227 - val_acc: 0.8079\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4284 - acc: 0.8117 - val_loss: 0.4218 - val_acc: 0.8062\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4280 - acc: 0.8148 - val_loss: 0.4218 - val_acc: 0.8062\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4257 - acc: 0.8137 - val_loss: 0.4205 - val_acc: 0.8079\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4281 - acc: 0.8113 - val_loss: 0.4195 - val_acc: 0.8079\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4253 - acc: 0.8110 - val_loss: 0.4189 - val_acc: 0.8095\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4224 - acc: 0.8121 - val_loss: 0.4186 - val_acc: 0.8095\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4204 - acc: 0.8115 - val_loss: 0.4183 - val_acc: 0.8112\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4269 - acc: 0.8144 - val_loss: 0.4181 - val_acc: 0.8112\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4259 - acc: 0.8177 - val_loss: 0.4176 - val_acc: 0.8144\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4199 - acc: 0.8172 - val_loss: 0.4171 - val_acc: 0.8161\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4201 - acc: 0.8163 - val_loss: 0.4169 - val_acc: 0.8128\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4172 - acc: 0.8155 - val_loss: 0.4150 - val_acc: 0.8194\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4157 - acc: 0.8154 - val_loss: 0.4146 - val_acc: 0.8194\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4120 - acc: 0.8227 - val_loss: 0.4141 - val_acc: 0.8194\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4134 - acc: 0.8214 - val_loss: 0.4138 - val_acc: 0.8194\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4096 - acc: 0.8212 - val_loss: 0.4131 - val_acc: 0.8194\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4139 - acc: 0.8188 - val_loss: 0.4128 - val_acc: 0.8194\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4102 - acc: 0.8227 - val_loss: 0.4128 - val_acc: 0.8210\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4114 - acc: 0.8256 - val_loss: 0.4131 - val_acc: 0.8177\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4093 - acc: 0.8186 - val_loss: 0.4127 - val_acc: 0.8194\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4087 - acc: 0.8177 - val_loss: 0.4120 - val_acc: 0.8194\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4077 - acc: 0.8269 - val_loss: 0.4114 - val_acc: 0.8177\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4078 - acc: 0.8250 - val_loss: 0.4112 - val_acc: 0.8177\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4019 - acc: 0.8250 - val_loss: 0.4107 - val_acc: 0.8177\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4012 - acc: 0.8258 - val_loss: 0.4108 - val_acc: 0.8210\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4033 - acc: 0.8227 - val_loss: 0.4103 - val_acc: 0.8210\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3994 - acc: 0.8280 - val_loss: 0.4108 - val_acc: 0.8210\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4013 - acc: 0.8219 - val_loss: 0.4104 - val_acc: 0.8210\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4004 - acc: 0.8241 - val_loss: 0.4095 - val_acc: 0.8227\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3987 - acc: 0.8292 - val_loss: 0.4098 - val_acc: 0.8227\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3971 - acc: 0.8278 - val_loss: 0.4101 - val_acc: 0.8177\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3954 - acc: 0.8241 - val_loss: 0.4092 - val_acc: 0.8227\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3953 - acc: 0.8312 - val_loss: 0.4092 - val_acc: 0.8243\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3980 - acc: 0.8252 - val_loss: 0.4093 - val_acc: 0.8210\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3941 - acc: 0.8312 - val_loss: 0.4091 - val_acc: 0.8194\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3967 - acc: 0.8270 - val_loss: 0.4087 - val_acc: 0.8210\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3922 - acc: 0.8329 - val_loss: 0.4086 - val_acc: 0.8177\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3912 - acc: 0.8323 - val_loss: 0.4085 - val_acc: 0.8194\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3909 - acc: 0.8311 - val_loss: 0.4083 - val_acc: 0.8227\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3909 - acc: 0.8305 - val_loss: 0.4082 - val_acc: 0.8177\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3872 - acc: 0.8362 - val_loss: 0.4084 - val_acc: 0.8194\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3884 - acc: 0.8331 - val_loss: 0.4088 - val_acc: 0.8210\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3837 - acc: 0.8356 - val_loss: 0.4084 - val_acc: 0.8194\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3866 - acc: 0.8327 - val_loss: 0.4081 - val_acc: 0.8227\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3845 - acc: 0.8329 - val_loss: 0.4093 - val_acc: 0.8210\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3852 - acc: 0.8338 - val_loss: 0.4080 - val_acc: 0.8210\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3805 - acc: 0.8367 - val_loss: 0.4077 - val_acc: 0.8194\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3855 - acc: 0.8290 - val_loss: 0.4082 - val_acc: 0.8227\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3823 - acc: 0.8327 - val_loss: 0.4075 - val_acc: 0.8292\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3821 - acc: 0.8323 - val_loss: 0.4077 - val_acc: 0.8194\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3773 - acc: 0.8400 - val_loss: 0.4084 - val_acc: 0.8210\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3804 - acc: 0.8367 - val_loss: 0.4086 - val_acc: 0.8210\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3801 - acc: 0.8382 - val_loss: 0.4079 - val_acc: 0.8210\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3779 - acc: 0.8369 - val_loss: 0.4082 - val_acc: 0.8227\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.2, 'inner_act_func': 'sigmoid', 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_30 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.7584 - acc: 0.4339 - val_loss: 0.7058 - val_acc: 0.4516\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.7073 - acc: 0.4906 - val_loss: 0.6747 - val_acc: 0.6240\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6765 - acc: 0.5860 - val_loss: 0.6594 - val_acc: 0.6273\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6631 - acc: 0.6099 - val_loss: 0.6505 - val_acc: 0.5698\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6494 - acc: 0.6238 - val_loss: 0.6415 - val_acc: 0.5747\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6437 - acc: 0.6253 - val_loss: 0.6295 - val_acc: 0.5993\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6310 - acc: 0.6464 - val_loss: 0.6157 - val_acc: 0.6847\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6224 - acc: 0.6731 - val_loss: 0.6026 - val_acc: 0.7356\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6124 - acc: 0.6946 - val_loss: 0.5912 - val_acc: 0.7455\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6019 - acc: 0.6999 - val_loss: 0.5802 - val_acc: 0.7521\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5931 - acc: 0.7121 - val_loss: 0.5696 - val_acc: 0.7619\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5835 - acc: 0.7219 - val_loss: 0.5595 - val_acc: 0.7619\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5776 - acc: 0.7190 - val_loss: 0.5496 - val_acc: 0.7685\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5652 - acc: 0.7307 - val_loss: 0.5403 - val_acc: 0.7767\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5574 - acc: 0.7367 - val_loss: 0.5309 - val_acc: 0.7816\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5484 - acc: 0.7433 - val_loss: 0.5218 - val_acc: 0.7833\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5415 - acc: 0.7455 - val_loss: 0.5132 - val_acc: 0.7865\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5347 - acc: 0.7460 - val_loss: 0.5056 - val_acc: 0.7931\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5268 - acc: 0.7541 - val_loss: 0.4987 - val_acc: 0.7898\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5188 - acc: 0.7590 - val_loss: 0.4916 - val_acc: 0.7947\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5136 - acc: 0.7650 - val_loss: 0.4854 - val_acc: 0.7997\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5072 - acc: 0.7694 - val_loss: 0.4799 - val_acc: 0.7997\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5011 - acc: 0.7674 - val_loss: 0.4748 - val_acc: 0.8030\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4942 - acc: 0.7758 - val_loss: 0.4701 - val_acc: 0.8079\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4872 - acc: 0.7838 - val_loss: 0.4652 - val_acc: 0.8062\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4822 - acc: 0.7812 - val_loss: 0.4611 - val_acc: 0.8046\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4822 - acc: 0.7807 - val_loss: 0.4573 - val_acc: 0.8046\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4781 - acc: 0.7858 - val_loss: 0.4542 - val_acc: 0.8128\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4754 - acc: 0.7898 - val_loss: 0.4508 - val_acc: 0.8095\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4718 - acc: 0.7842 - val_loss: 0.4480 - val_acc: 0.8112\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4670 - acc: 0.7902 - val_loss: 0.4464 - val_acc: 0.8144\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4619 - acc: 0.7900 - val_loss: 0.4435 - val_acc: 0.8177\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4593 - acc: 0.7935 - val_loss: 0.4418 - val_acc: 0.8194\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4601 - acc: 0.7966 - val_loss: 0.4400 - val_acc: 0.8161\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4557 - acc: 0.7982 - val_loss: 0.4376 - val_acc: 0.8177\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4520 - acc: 0.7997 - val_loss: 0.4361 - val_acc: 0.8161\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4494 - acc: 0.8015 - val_loss: 0.4347 - val_acc: 0.8144\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4480 - acc: 0.7991 - val_loss: 0.4327 - val_acc: 0.8210\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4512 - acc: 0.7975 - val_loss: 0.4318 - val_acc: 0.8194\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4433 - acc: 0.7991 - val_loss: 0.4303 - val_acc: 0.8177\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4423 - acc: 0.8020 - val_loss: 0.4291 - val_acc: 0.8177\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4441 - acc: 0.7999 - val_loss: 0.4284 - val_acc: 0.8161\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4380 - acc: 0.8086 - val_loss: 0.4272 - val_acc: 0.8194\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4391 - acc: 0.8059 - val_loss: 0.4261 - val_acc: 0.8177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4351 - acc: 0.8084 - val_loss: 0.4255 - val_acc: 0.8161\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4336 - acc: 0.8112 - val_loss: 0.4250 - val_acc: 0.8128\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4335 - acc: 0.8108 - val_loss: 0.4239 - val_acc: 0.8177\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4328 - acc: 0.8130 - val_loss: 0.4230 - val_acc: 0.8144\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4287 - acc: 0.8112 - val_loss: 0.4225 - val_acc: 0.8095\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4281 - acc: 0.8117 - val_loss: 0.4214 - val_acc: 0.8161\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4288 - acc: 0.8082 - val_loss: 0.4207 - val_acc: 0.8161\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4273 - acc: 0.8135 - val_loss: 0.4200 - val_acc: 0.8144\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4297 - acc: 0.8121 - val_loss: 0.4195 - val_acc: 0.8144\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4240 - acc: 0.8132 - val_loss: 0.4187 - val_acc: 0.8161\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4235 - acc: 0.8185 - val_loss: 0.4184 - val_acc: 0.8128\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4182 - acc: 0.8183 - val_loss: 0.4181 - val_acc: 0.8095\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4233 - acc: 0.8139 - val_loss: 0.4178 - val_acc: 0.8210\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4174 - acc: 0.8161 - val_loss: 0.4167 - val_acc: 0.8128\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4198 - acc: 0.8179 - val_loss: 0.4161 - val_acc: 0.8144\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4114 - acc: 0.8186 - val_loss: 0.4155 - val_acc: 0.8128\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4143 - acc: 0.8214 - val_loss: 0.4153 - val_acc: 0.8161\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4148 - acc: 0.8210 - val_loss: 0.4151 - val_acc: 0.8079\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4123 - acc: 0.8243 - val_loss: 0.4146 - val_acc: 0.8112\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4162 - acc: 0.8163 - val_loss: 0.4145 - val_acc: 0.8128\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4108 - acc: 0.8176 - val_loss: 0.4142 - val_acc: 0.8177\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4103 - acc: 0.8241 - val_loss: 0.4133 - val_acc: 0.8128\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4092 - acc: 0.8216 - val_loss: 0.4130 - val_acc: 0.8144\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4038 - acc: 0.8219 - val_loss: 0.4127 - val_acc: 0.8144\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4044 - acc: 0.8203 - val_loss: 0.4132 - val_acc: 0.8177\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4027 - acc: 0.8228 - val_loss: 0.4129 - val_acc: 0.8144\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4052 - acc: 0.8227 - val_loss: 0.4127 - val_acc: 0.8161\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4027 - acc: 0.8239 - val_loss: 0.4122 - val_acc: 0.8161\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4029 - acc: 0.8230 - val_loss: 0.4120 - val_acc: 0.8144\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4013 - acc: 0.8274 - val_loss: 0.4114 - val_acc: 0.8177\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4020 - acc: 0.8236 - val_loss: 0.4113 - val_acc: 0.8194\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3979 - acc: 0.8234 - val_loss: 0.4110 - val_acc: 0.8177\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3960 - acc: 0.8287 - val_loss: 0.4116 - val_acc: 0.8161\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3960 - acc: 0.8248 - val_loss: 0.4114 - val_acc: 0.8177\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3961 - acc: 0.8258 - val_loss: 0.4120 - val_acc: 0.8194\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3952 - acc: 0.8248 - val_loss: 0.4113 - val_acc: 0.8194\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3948 - acc: 0.8259 - val_loss: 0.4110 - val_acc: 0.8177\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.2, 'inner_act_func': 'sigmoid', 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'seed': 123456}\n",
      "Model: \"model_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_31 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.8261 - acc: 0.4317 - val_loss: 0.7277 - val_acc: 0.4368\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7320 - acc: 0.4631 - val_loss: 0.6763 - val_acc: 0.6273\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6839 - acc: 0.5638 - val_loss: 0.6590 - val_acc: 0.5714\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6639 - acc: 0.6063 - val_loss: 0.6489 - val_acc: 0.5649\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6500 - acc: 0.6074 - val_loss: 0.6321 - val_acc: 0.5665\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6363 - acc: 0.6371 - val_loss: 0.6120 - val_acc: 0.6404\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6192 - acc: 0.6749 - val_loss: 0.5957 - val_acc: 0.7488\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6112 - acc: 0.6867 - val_loss: 0.5824 - val_acc: 0.7570\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5961 - acc: 0.6906 - val_loss: 0.5701 - val_acc: 0.7553\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5864 - acc: 0.7055 - val_loss: 0.5584 - val_acc: 0.7685\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5746 - acc: 0.7117 - val_loss: 0.5481 - val_acc: 0.7750\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5638 - acc: 0.7327 - val_loss: 0.5366 - val_acc: 0.7849\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5546 - acc: 0.7296 - val_loss: 0.5252 - val_acc: 0.7750\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5445 - acc: 0.7371 - val_loss: 0.5152 - val_acc: 0.7849\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5344 - acc: 0.7473 - val_loss: 0.5067 - val_acc: 0.8013\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5274 - acc: 0.7510 - val_loss: 0.4976 - val_acc: 0.8030\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5156 - acc: 0.7577 - val_loss: 0.4882 - val_acc: 0.8046\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5048 - acc: 0.7635 - val_loss: 0.4805 - val_acc: 0.8062\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5007 - acc: 0.7699 - val_loss: 0.4739 - val_acc: 0.8062\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4959 - acc: 0.7732 - val_loss: 0.4693 - val_acc: 0.8079\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4846 - acc: 0.7787 - val_loss: 0.4629 - val_acc: 0.8095\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4837 - acc: 0.7818 - val_loss: 0.4575 - val_acc: 0.8112\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4734 - acc: 0.7864 - val_loss: 0.4541 - val_acc: 0.8062\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4684 - acc: 0.7935 - val_loss: 0.4497 - val_acc: 0.8079\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4674 - acc: 0.7860 - val_loss: 0.4472 - val_acc: 0.8062\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4641 - acc: 0.7915 - val_loss: 0.4436 - val_acc: 0.8046\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4558 - acc: 0.7944 - val_loss: 0.4412 - val_acc: 0.8062\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4564 - acc: 0.7937 - val_loss: 0.4384 - val_acc: 0.8144\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4549 - acc: 0.7973 - val_loss: 0.4376 - val_acc: 0.8030\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4483 - acc: 0.7993 - val_loss: 0.4347 - val_acc: 0.8128\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4476 - acc: 0.7966 - val_loss: 0.4331 - val_acc: 0.8128\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4473 - acc: 0.8024 - val_loss: 0.4329 - val_acc: 0.8062\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4419 - acc: 0.8035 - val_loss: 0.4300 - val_acc: 0.8177\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4369 - acc: 0.8055 - val_loss: 0.4284 - val_acc: 0.8161\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4364 - acc: 0.8068 - val_loss: 0.4274 - val_acc: 0.8128\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4326 - acc: 0.8086 - val_loss: 0.4254 - val_acc: 0.8144\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4329 - acc: 0.8097 - val_loss: 0.4243 - val_acc: 0.8144\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4283 - acc: 0.8128 - val_loss: 0.4233 - val_acc: 0.8144\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4247 - acc: 0.8106 - val_loss: 0.4225 - val_acc: 0.8128\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4250 - acc: 0.8143 - val_loss: 0.4214 - val_acc: 0.8112\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4255 - acc: 0.8086 - val_loss: 0.4207 - val_acc: 0.8161\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4221 - acc: 0.8150 - val_loss: 0.4196 - val_acc: 0.8161\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4175 - acc: 0.8155 - val_loss: 0.4188 - val_acc: 0.8128\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4197 - acc: 0.8172 - val_loss: 0.4188 - val_acc: 0.8177\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4161 - acc: 0.8170 - val_loss: 0.4183 - val_acc: 0.8144\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4150 - acc: 0.8148 - val_loss: 0.4185 - val_acc: 0.8128\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4137 - acc: 0.8190 - val_loss: 0.4182 - val_acc: 0.8177\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4134 - acc: 0.8221 - val_loss: 0.4162 - val_acc: 0.8144\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4099 - acc: 0.8207 - val_loss: 0.4153 - val_acc: 0.8161\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4093 - acc: 0.8194 - val_loss: 0.4150 - val_acc: 0.8161\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4082 - acc: 0.8217 - val_loss: 0.4144 - val_acc: 0.8095\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4088 - acc: 0.8165 - val_loss: 0.4149 - val_acc: 0.8177\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4068 - acc: 0.8269 - val_loss: 0.4139 - val_acc: 0.8144\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4005 - acc: 0.8256 - val_loss: 0.4131 - val_acc: 0.8079\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3990 - acc: 0.8230 - val_loss: 0.4126 - val_acc: 0.8161\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3956 - acc: 0.8269 - val_loss: 0.4124 - val_acc: 0.8177\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3993 - acc: 0.8238 - val_loss: 0.4123 - val_acc: 0.8079\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3970 - acc: 0.8272 - val_loss: 0.4129 - val_acc: 0.8128\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3976 - acc: 0.8270 - val_loss: 0.4124 - val_acc: 0.8112\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3939 - acc: 0.8265 - val_loss: 0.4122 - val_acc: 0.8128\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3959 - acc: 0.8281 - val_loss: 0.4117 - val_acc: 0.8128\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3910 - acc: 0.8289 - val_loss: 0.4111 - val_acc: 0.8144\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3880 - acc: 0.8269 - val_loss: 0.4122 - val_acc: 0.8144\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3860 - acc: 0.8285 - val_loss: 0.4122 - val_acc: 0.8144\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3905 - acc: 0.8307 - val_loss: 0.4118 - val_acc: 0.8144\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3864 - acc: 0.8321 - val_loss: 0.4134 - val_acc: 0.8144\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3847 - acc: 0.8311 - val_loss: 0.4125 - val_acc: 0.8128\n"
     ]
    }
   ],
   "source": [
    "mlp_perf_metrics6 = make_MLP_exp(X_train_vect, y_train.values, params6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.844919</td>\n",
       "      <td>0.834154</td>\n",
       "      <td>0.358540</td>\n",
       "      <td>0.410538</td>\n",
       "      <td>57</td>\n",
       "      <td>0.820749</td>\n",
       "      <td>0.828431</td>\n",
       "      <td>0.751111</td>\n",
       "      <td>0.876179</td>\n",
       "      <td>0.787879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.831965</td>\n",
       "      <td>0.811166</td>\n",
       "      <td>0.388654</td>\n",
       "      <td>0.412814</td>\n",
       "      <td>64</td>\n",
       "      <td>0.817466</td>\n",
       "      <td>0.833613</td>\n",
       "      <td>0.734815</td>\n",
       "      <td>0.883255</td>\n",
       "      <td>0.781102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.824485</td>\n",
       "      <td>0.817734</td>\n",
       "      <td>0.395030</td>\n",
       "      <td>0.419332</td>\n",
       "      <td>113</td>\n",
       "      <td>0.818779</td>\n",
       "      <td>0.826514</td>\n",
       "      <td>0.748148</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.785381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.835979</td>\n",
       "      <td>0.829228</td>\n",
       "      <td>0.374227</td>\n",
       "      <td>0.410503</td>\n",
       "      <td>61</td>\n",
       "      <td>0.819435</td>\n",
       "      <td>0.824675</td>\n",
       "      <td>0.752593</td>\n",
       "      <td>0.872642</td>\n",
       "      <td>0.786987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.836891</td>\n",
       "      <td>0.822660</td>\n",
       "      <td>0.377859</td>\n",
       "      <td>0.408175</td>\n",
       "      <td>103</td>\n",
       "      <td>0.822718</td>\n",
       "      <td>0.844974</td>\n",
       "      <td>0.734815</td>\n",
       "      <td>0.892689</td>\n",
       "      <td>0.786054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.825944</td>\n",
       "      <td>0.817734</td>\n",
       "      <td>0.394808</td>\n",
       "      <td>0.411047</td>\n",
       "      <td>81</td>\n",
       "      <td>0.817466</td>\n",
       "      <td>0.822764</td>\n",
       "      <td>0.749630</td>\n",
       "      <td>0.871462</td>\n",
       "      <td>0.784496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.831053</td>\n",
       "      <td>0.812808</td>\n",
       "      <td>0.384728</td>\n",
       "      <td>0.412468</td>\n",
       "      <td>67</td>\n",
       "      <td>0.818122</td>\n",
       "      <td>0.823052</td>\n",
       "      <td>0.751111</td>\n",
       "      <td>0.871462</td>\n",
       "      <td>0.785438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_layers  layer_conf      lr  dropout  batch_size  accuracy_train  \\\n",
       "0         2  [512, 256]  0.0001      0.2        1250        0.844919   \n",
       "1         2  [256, 256]  0.0001      0.2        1250        0.831965   \n",
       "2         1       [128]  0.0001      0.2        1250        0.824485   \n",
       "3         2  [512, 128]  0.0001      0.2        1250        0.835979   \n",
       "4         2   [256, 32]  0.0001      0.2        1250        0.836891   \n",
       "5         2  [128, 128]  0.0001      0.2        1250        0.825944   \n",
       "6         2  [256, 256]  0.0001      0.2        1250        0.831053   \n",
       "\n",
       "   accuracy_val  loss_train  loss_val  epochs  accuracy  precision    recall  \\\n",
       "0      0.834154    0.358540  0.410538      57  0.820749   0.828431  0.751111   \n",
       "1      0.811166    0.388654  0.412814      64  0.817466   0.833613  0.734815   \n",
       "2      0.817734    0.395030  0.419332     113  0.818779   0.826514  0.748148   \n",
       "3      0.829228    0.374227  0.410503      61  0.819435   0.824675  0.752593   \n",
       "4      0.822660    0.377859  0.408175     103  0.822718   0.844974  0.734815   \n",
       "5      0.817734    0.394808  0.411047      81  0.817466   0.822764  0.749630   \n",
       "6      0.812808    0.384728  0.412468      67  0.818122   0.823052  0.751111   \n",
       "\n",
       "   specificity  f1_score  \n",
       "0     0.876179  0.787879  \n",
       "1     0.883255  0.781102  \n",
       "2     0.875000  0.785381  \n",
       "3     0.872642  0.786987  \n",
       "4     0.892689  0.786054  \n",
       "5     0.871462  0.784496  \n",
       "6     0.871462  0.785438  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "params7 = {\n",
    "        'lay_conf': best_arch1,\n",
    "        'lr': [1e-4],\n",
    "        'dropout': [0.2],\n",
    "        'max_epochs': [300],\n",
    "        'batch_size': [1250],\n",
    "        'out_act_func': ['relu'],\n",
    "        'seed': [123456]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters {'batch_size': 1250, 'dropout': 0.2, 'lay_conf': [512, 256], 'lr': 0.0001, 'max_epochs': 300, 'out_act_func': 'relu', 'seed': 123456}\n",
      "Model: \"model_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_32 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.7504 - acc: 0.4381 - val_loss: 0.6786 - val_acc: 0.5681\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6758 - acc: 0.5709 - val_loss: 0.6585 - val_acc: 0.5747\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6535 - acc: 0.5930 - val_loss: 0.6303 - val_acc: 0.5796\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6287 - acc: 0.6398 - val_loss: 0.6011 - val_acc: 0.7176\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6045 - acc: 0.7055 - val_loss: 0.5829 - val_acc: 0.6897\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5893 - acc: 0.7095 - val_loss: 0.5652 - val_acc: 0.7061\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5698 - acc: 0.7291 - val_loss: 0.5502 - val_acc: 0.7438\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5570 - acc: 0.7400 - val_loss: 0.5372 - val_acc: 0.7619\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5413 - acc: 0.7448 - val_loss: 0.5235 - val_acc: 0.7603\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5231 - acc: 0.7573 - val_loss: 0.5122 - val_acc: 0.7783\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5091 - acc: 0.7705 - val_loss: 0.5040 - val_acc: 0.7898\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4995 - acc: 0.7780 - val_loss: 0.4950 - val_acc: 0.7915\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4901 - acc: 0.7791 - val_loss: 0.4867 - val_acc: 0.7931\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4782 - acc: 0.7842 - val_loss: 0.4798 - val_acc: 0.7947\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4694 - acc: 0.7931 - val_loss: 0.4745 - val_acc: 0.7980\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4619 - acc: 0.7926 - val_loss: 0.4675 - val_acc: 0.7997\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4569 - acc: 0.7988 - val_loss: 0.4644 - val_acc: 0.8046\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4508 - acc: 0.8013 - val_loss: 0.4595 - val_acc: 0.8095\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4494 - acc: 0.7978 - val_loss: 0.4581 - val_acc: 0.8112\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4400 - acc: 0.8008 - val_loss: 0.4542 - val_acc: 0.8112\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4406 - acc: 0.8073 - val_loss: 0.4522 - val_acc: 0.8095\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4368 - acc: 0.8068 - val_loss: 0.4485 - val_acc: 0.8128\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4309 - acc: 0.8064 - val_loss: 0.4463 - val_acc: 0.8177\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4258 - acc: 0.8092 - val_loss: 0.4471 - val_acc: 0.8112\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4239 - acc: 0.8159 - val_loss: 0.4427 - val_acc: 0.8177\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4226 - acc: 0.8139 - val_loss: 0.4424 - val_acc: 0.8161\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4197 - acc: 0.8181 - val_loss: 0.4441 - val_acc: 0.8177\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4160 - acc: 0.8234 - val_loss: 0.4388 - val_acc: 0.8128\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4149 - acc: 0.8212 - val_loss: 0.4398 - val_acc: 0.8144\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4099 - acc: 0.8254 - val_loss: 0.4368 - val_acc: 0.8095\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4121 - acc: 0.8201 - val_loss: 0.4365 - val_acc: 0.8112\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4065 - acc: 0.8230 - val_loss: 0.4380 - val_acc: 0.8128\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4017 - acc: 0.8270 - val_loss: 0.4353 - val_acc: 0.8128\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4036 - acc: 0.8223 - val_loss: 0.4357 - val_acc: 0.8128\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3987 - acc: 0.8263 - val_loss: 0.4331 - val_acc: 0.8112\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3976 - acc: 0.8265 - val_loss: 0.4360 - val_acc: 0.8112\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3968 - acc: 0.8258 - val_loss: 0.4309 - val_acc: 0.8112\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3966 - acc: 0.8243 - val_loss: 0.4357 - val_acc: 0.8112\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3918 - acc: 0.8298 - val_loss: 0.4295 - val_acc: 0.8144\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3913 - acc: 0.8274 - val_loss: 0.4342 - val_acc: 0.8112\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3888 - acc: 0.8316 - val_loss: 0.4299 - val_acc: 0.8144\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3870 - acc: 0.8329 - val_loss: 0.4300 - val_acc: 0.8128\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3849 - acc: 0.8316 - val_loss: 0.4296 - val_acc: 0.8128\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3864 - acc: 0.8327 - val_loss: 0.4297 - val_acc: 0.8128\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.2, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'out_act_func': 'relu', 'seed': 123456}\n",
      "Model: \"model_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_33 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 34ms/step - loss: 0.8245 - acc: 0.4335 - val_loss: 0.7370 - val_acc: 0.4269\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7265 - acc: 0.4702 - val_loss: 0.6791 - val_acc: 0.6026\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6847 - acc: 0.5634 - val_loss: 0.6569 - val_acc: 0.5796\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6643 - acc: 0.6028 - val_loss: 0.6443 - val_acc: 0.5747\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6502 - acc: 0.6174 - val_loss: 0.6276 - val_acc: 0.5862\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6322 - acc: 0.6429 - val_loss: 0.6105 - val_acc: 0.6552\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6195 - acc: 0.6783 - val_loss: 0.5959 - val_acc: 0.7225\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6038 - acc: 0.6979 - val_loss: 0.5836 - val_acc: 0.7274\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5935 - acc: 0.6922 - val_loss: 0.5725 - val_acc: 0.7340\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5852 - acc: 0.7028 - val_loss: 0.5623 - val_acc: 0.7455\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5706 - acc: 0.7218 - val_loss: 0.5528 - val_acc: 0.7488\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5617 - acc: 0.7272 - val_loss: 0.5436 - val_acc: 0.7652\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5496 - acc: 0.7362 - val_loss: 0.5340 - val_acc: 0.7652\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5415 - acc: 0.7468 - val_loss: 0.5245 - val_acc: 0.7652\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5287 - acc: 0.7550 - val_loss: 0.5158 - val_acc: 0.7701\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5191 - acc: 0.7635 - val_loss: 0.5088 - val_acc: 0.7750\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5126 - acc: 0.7603 - val_loss: 0.5010 - val_acc: 0.7718\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5009 - acc: 0.7765 - val_loss: 0.4950 - val_acc: 0.7783\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4927 - acc: 0.7818 - val_loss: 0.4913 - val_acc: 0.7865\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4901 - acc: 0.7769 - val_loss: 0.4858 - val_acc: 0.7849\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4819 - acc: 0.7796 - val_loss: 0.4808 - val_acc: 0.7865\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4748 - acc: 0.7873 - val_loss: 0.4798 - val_acc: 0.7915\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4672 - acc: 0.7891 - val_loss: 0.4746 - val_acc: 0.7980\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4617 - acc: 0.7951 - val_loss: 0.4716 - val_acc: 0.7997\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4593 - acc: 0.7933 - val_loss: 0.4689 - val_acc: 0.7964\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4558 - acc: 0.7969 - val_loss: 0.4658 - val_acc: 0.7964\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4545 - acc: 0.7949 - val_loss: 0.4641 - val_acc: 0.8030\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4492 - acc: 0.8011 - val_loss: 0.4607 - val_acc: 0.7997\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4455 - acc: 0.8037 - val_loss: 0.4593 - val_acc: 0.8030\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4400 - acc: 0.8048 - val_loss: 0.4564 - val_acc: 0.7980\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4385 - acc: 0.8030 - val_loss: 0.4549 - val_acc: 0.7997\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4344 - acc: 0.8097 - val_loss: 0.4562 - val_acc: 0.8030\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4328 - acc: 0.8123 - val_loss: 0.4516 - val_acc: 0.7997\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4293 - acc: 0.8113 - val_loss: 0.4516 - val_acc: 0.7997\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4283 - acc: 0.8124 - val_loss: 0.4510 - val_acc: 0.7997\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4260 - acc: 0.8108 - val_loss: 0.4492 - val_acc: 0.8013\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4242 - acc: 0.8150 - val_loss: 0.4491 - val_acc: 0.8062\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4248 - acc: 0.8132 - val_loss: 0.4488 - val_acc: 0.8062\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4206 - acc: 0.8134 - val_loss: 0.4454 - val_acc: 0.8161\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4186 - acc: 0.8144 - val_loss: 0.4482 - val_acc: 0.8079\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4192 - acc: 0.8150 - val_loss: 0.4436 - val_acc: 0.8144\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4148 - acc: 0.8165 - val_loss: 0.4456 - val_acc: 0.8128\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4115 - acc: 0.8196 - val_loss: 0.4431 - val_acc: 0.8161\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4138 - acc: 0.8194 - val_loss: 0.4421 - val_acc: 0.8161\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4122 - acc: 0.8186 - val_loss: 0.4399 - val_acc: 0.8177\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4097 - acc: 0.8177 - val_loss: 0.4393 - val_acc: 0.8210\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4067 - acc: 0.8221 - val_loss: 0.4408 - val_acc: 0.8177\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4069 - acc: 0.8243 - val_loss: 0.4380 - val_acc: 0.8161\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4054 - acc: 0.8228 - val_loss: 0.4404 - val_acc: 0.8161\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4073 - acc: 0.8230 - val_loss: 0.4357 - val_acc: 0.8210\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4019 - acc: 0.8263 - val_loss: 0.4418 - val_acc: 0.8128\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3991 - acc: 0.8280 - val_loss: 0.4343 - val_acc: 0.8177\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4003 - acc: 0.8216 - val_loss: 0.4362 - val_acc: 0.8161\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3964 - acc: 0.8243 - val_loss: 0.4341 - val_acc: 0.8161\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3951 - acc: 0.8261 - val_loss: 0.4340 - val_acc: 0.8177\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3963 - acc: 0.8283 - val_loss: 0.4336 - val_acc: 0.8194\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3923 - acc: 0.8289 - val_loss: 0.4326 - val_acc: 0.8194\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3922 - acc: 0.8278 - val_loss: 0.4350 - val_acc: 0.8161\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3882 - acc: 0.8300 - val_loss: 0.4309 - val_acc: 0.8177\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3898 - acc: 0.8285 - val_loss: 0.4335 - val_acc: 0.8161\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3848 - acc: 0.8351 - val_loss: 0.4314 - val_acc: 0.8177\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3879 - acc: 0.8303 - val_loss: 0.4302 - val_acc: 0.8194\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3864 - acc: 0.8327 - val_loss: 0.4330 - val_acc: 0.8161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3828 - acc: 0.8320 - val_loss: 0.4296 - val_acc: 0.8177\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3817 - acc: 0.8300 - val_loss: 0.4310 - val_acc: 0.8144\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3846 - acc: 0.8347 - val_loss: 0.4284 - val_acc: 0.8144\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3810 - acc: 0.8318 - val_loss: 0.4315 - val_acc: 0.8144\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3795 - acc: 0.8343 - val_loss: 0.4276 - val_acc: 0.8161\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3779 - acc: 0.8365 - val_loss: 0.4300 - val_acc: 0.8144\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3803 - acc: 0.8342 - val_loss: 0.4279 - val_acc: 0.8161\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3708 - acc: 0.8387 - val_loss: 0.4285 - val_acc: 0.8144\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3744 - acc: 0.8345 - val_loss: 0.4309 - val_acc: 0.8161\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3718 - acc: 0.8382 - val_loss: 0.4290 - val_acc: 0.8128\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.2, 'lay_conf': [128], 'lr': 0.0001, 'max_epochs': 300, 'out_act_func': 'relu', 'seed': 123456}\n",
      "Model: \"model_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_34 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 1s 64ms/step - loss: 0.6680 - acc: 0.6004 - val_loss: 0.6417 - val_acc: 0.6749\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6466 - acc: 0.6284 - val_loss: 0.6266 - val_acc: 0.6552\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6301 - acc: 0.6455 - val_loss: 0.6127 - val_acc: 0.6913\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6167 - acc: 0.6732 - val_loss: 0.6000 - val_acc: 0.7028\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6079 - acc: 0.6893 - val_loss: 0.5894 - val_acc: 0.7061\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5930 - acc: 0.7041 - val_loss: 0.5793 - val_acc: 0.7126\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5845 - acc: 0.7114 - val_loss: 0.5702 - val_acc: 0.7192\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5746 - acc: 0.7271 - val_loss: 0.5621 - val_acc: 0.7323\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5690 - acc: 0.7329 - val_loss: 0.5549 - val_acc: 0.7356\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5586 - acc: 0.7422 - val_loss: 0.5482 - val_acc: 0.7389\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5494 - acc: 0.7502 - val_loss: 0.5420 - val_acc: 0.7570\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5447 - acc: 0.7486 - val_loss: 0.5363 - val_acc: 0.7586\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5357 - acc: 0.7575 - val_loss: 0.5311 - val_acc: 0.7635\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5292 - acc: 0.7619 - val_loss: 0.5268 - val_acc: 0.7701\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.5236 - acc: 0.7645 - val_loss: 0.5220 - val_acc: 0.7685\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5190 - acc: 0.7677 - val_loss: 0.5180 - val_acc: 0.7701\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5141 - acc: 0.7654 - val_loss: 0.5141 - val_acc: 0.7685\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5089 - acc: 0.7749 - val_loss: 0.5107 - val_acc: 0.7750\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5053 - acc: 0.7776 - val_loss: 0.5075 - val_acc: 0.7833\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4990 - acc: 0.7792 - val_loss: 0.5034 - val_acc: 0.7783\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4939 - acc: 0.7776 - val_loss: 0.5004 - val_acc: 0.7882\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4912 - acc: 0.7853 - val_loss: 0.4976 - val_acc: 0.7882\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4873 - acc: 0.7831 - val_loss: 0.4959 - val_acc: 0.7833\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4854 - acc: 0.7851 - val_loss: 0.4927 - val_acc: 0.7882\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4819 - acc: 0.7812 - val_loss: 0.4898 - val_acc: 0.7882\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4790 - acc: 0.7871 - val_loss: 0.4875 - val_acc: 0.7898\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4762 - acc: 0.7902 - val_loss: 0.4862 - val_acc: 0.7931\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4748 - acc: 0.7907 - val_loss: 0.4841 - val_acc: 0.7931\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4705 - acc: 0.7896 - val_loss: 0.4818 - val_acc: 0.7898\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4695 - acc: 0.7876 - val_loss: 0.4800 - val_acc: 0.7915\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4676 - acc: 0.7924 - val_loss: 0.4783 - val_acc: 0.7947\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4646 - acc: 0.7918 - val_loss: 0.4767 - val_acc: 0.7947\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4647 - acc: 0.7973 - val_loss: 0.4752 - val_acc: 0.7947\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4612 - acc: 0.7969 - val_loss: 0.4734 - val_acc: 0.7964\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4596 - acc: 0.7964 - val_loss: 0.4722 - val_acc: 0.7980\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4573 - acc: 0.8000 - val_loss: 0.4711 - val_acc: 0.7997\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4547 - acc: 0.8030 - val_loss: 0.4700 - val_acc: 0.7964\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4525 - acc: 0.8033 - val_loss: 0.4682 - val_acc: 0.8046\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4512 - acc: 0.8062 - val_loss: 0.4673 - val_acc: 0.8030\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4501 - acc: 0.8024 - val_loss: 0.4667 - val_acc: 0.7980\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4488 - acc: 0.8050 - val_loss: 0.4650 - val_acc: 0.8046\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4465 - acc: 0.8037 - val_loss: 0.4640 - val_acc: 0.7997\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4444 - acc: 0.8081 - val_loss: 0.4633 - val_acc: 0.7980\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4453 - acc: 0.8035 - val_loss: 0.4616 - val_acc: 0.8062\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4420 - acc: 0.8079 - val_loss: 0.4612 - val_acc: 0.7997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4432 - acc: 0.8112 - val_loss: 0.4605 - val_acc: 0.7997\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4419 - acc: 0.8090 - val_loss: 0.4596 - val_acc: 0.8013\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4403 - acc: 0.8037 - val_loss: 0.4585 - val_acc: 0.8046\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4380 - acc: 0.8104 - val_loss: 0.4577 - val_acc: 0.8046\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4361 - acc: 0.8088 - val_loss: 0.4576 - val_acc: 0.8062\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4349 - acc: 0.8150 - val_loss: 0.4567 - val_acc: 0.8062\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4366 - acc: 0.8108 - val_loss: 0.4556 - val_acc: 0.8079\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4355 - acc: 0.8106 - val_loss: 0.4546 - val_acc: 0.8046\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4335 - acc: 0.8101 - val_loss: 0.4541 - val_acc: 0.8079\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4332 - acc: 0.8104 - val_loss: 0.4536 - val_acc: 0.8079\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4322 - acc: 0.8132 - val_loss: 0.4529 - val_acc: 0.8079\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4284 - acc: 0.8115 - val_loss: 0.4518 - val_acc: 0.8079\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4282 - acc: 0.8176 - val_loss: 0.4518 - val_acc: 0.8062\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4283 - acc: 0.8166 - val_loss: 0.4503 - val_acc: 0.8079\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4267 - acc: 0.8176 - val_loss: 0.4498 - val_acc: 0.8112\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4248 - acc: 0.8143 - val_loss: 0.4509 - val_acc: 0.8062\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4245 - acc: 0.8172 - val_loss: 0.4507 - val_acc: 0.8062\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4235 - acc: 0.8176 - val_loss: 0.4490 - val_acc: 0.8128\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4244 - acc: 0.8192 - val_loss: 0.4484 - val_acc: 0.8161\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4240 - acc: 0.8159 - val_loss: 0.4482 - val_acc: 0.8144\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4216 - acc: 0.8168 - val_loss: 0.4483 - val_acc: 0.8112\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4221 - acc: 0.8197 - val_loss: 0.4475 - val_acc: 0.8144\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4193 - acc: 0.8228 - val_loss: 0.4472 - val_acc: 0.8144\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4196 - acc: 0.8194 - val_loss: 0.4463 - val_acc: 0.8177\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4193 - acc: 0.8163 - val_loss: 0.4463 - val_acc: 0.8144\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4169 - acc: 0.8223 - val_loss: 0.4455 - val_acc: 0.8144\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4172 - acc: 0.8176 - val_loss: 0.4453 - val_acc: 0.8095\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4148 - acc: 0.8197 - val_loss: 0.4440 - val_acc: 0.8112\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4150 - acc: 0.8185 - val_loss: 0.4440 - val_acc: 0.8128\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4134 - acc: 0.8186 - val_loss: 0.4443 - val_acc: 0.8112\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4159 - acc: 0.8248 - val_loss: 0.4444 - val_acc: 0.8128\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4144 - acc: 0.8196 - val_loss: 0.4427 - val_acc: 0.8128\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4143 - acc: 0.8214 - val_loss: 0.4426 - val_acc: 0.8144\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4130 - acc: 0.8207 - val_loss: 0.4434 - val_acc: 0.8144\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4121 - acc: 0.8230 - val_loss: 0.4421 - val_acc: 0.8177\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4090 - acc: 0.8232 - val_loss: 0.4422 - val_acc: 0.8161\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4112 - acc: 0.8254 - val_loss: 0.4422 - val_acc: 0.8128\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4119 - acc: 0.8241 - val_loss: 0.4414 - val_acc: 0.8177\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4118 - acc: 0.8265 - val_loss: 0.4407 - val_acc: 0.8161\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4100 - acc: 0.8190 - val_loss: 0.4423 - val_acc: 0.8112\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4086 - acc: 0.8252 - val_loss: 0.4415 - val_acc: 0.8112\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4065 - acc: 0.8234 - val_loss: 0.4398 - val_acc: 0.8161\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4064 - acc: 0.8238 - val_loss: 0.4393 - val_acc: 0.8144\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4076 - acc: 0.8239 - val_loss: 0.4404 - val_acc: 0.8161\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4077 - acc: 0.8232 - val_loss: 0.4414 - val_acc: 0.8128\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4073 - acc: 0.8232 - val_loss: 0.4391 - val_acc: 0.8128\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4062 - acc: 0.8250 - val_loss: 0.4389 - val_acc: 0.8128\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4063 - acc: 0.8232 - val_loss: 0.4398 - val_acc: 0.8194\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4034 - acc: 0.8258 - val_loss: 0.4399 - val_acc: 0.8177\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4010 - acc: 0.8241 - val_loss: 0.4382 - val_acc: 0.8128\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4040 - acc: 0.8250 - val_loss: 0.4382 - val_acc: 0.8128\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4030 - acc: 0.8259 - val_loss: 0.4388 - val_acc: 0.8144\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4013 - acc: 0.8238 - val_loss: 0.4384 - val_acc: 0.8144\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4014 - acc: 0.8270 - val_loss: 0.4383 - val_acc: 0.8177\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4009 - acc: 0.8274 - val_loss: 0.4369 - val_acc: 0.8112\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4002 - acc: 0.8272 - val_loss: 0.4363 - val_acc: 0.8128\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4004 - acc: 0.8245 - val_loss: 0.4384 - val_acc: 0.8161\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3993 - acc: 0.8290 - val_loss: 0.4368 - val_acc: 0.8144\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4005 - acc: 0.8265 - val_loss: 0.4361 - val_acc: 0.8128\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3970 - acc: 0.8265 - val_loss: 0.4367 - val_acc: 0.8144\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3985 - acc: 0.8281 - val_loss: 0.4370 - val_acc: 0.8144\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3967 - acc: 0.8243 - val_loss: 0.4371 - val_acc: 0.8144\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3967 - acc: 0.8258 - val_loss: 0.4357 - val_acc: 0.8128\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3970 - acc: 0.8267 - val_loss: 0.4359 - val_acc: 0.8128\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3962 - acc: 0.8303 - val_loss: 0.4361 - val_acc: 0.8128\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3954 - acc: 0.8287 - val_loss: 0.4362 - val_acc: 0.8128\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3940 - acc: 0.8312 - val_loss: 0.4348 - val_acc: 0.8095\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3941 - acc: 0.8283 - val_loss: 0.4343 - val_acc: 0.8128\n",
      "Epoch 114/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3924 - acc: 0.8314 - val_loss: 0.4350 - val_acc: 0.8112\n",
      "Epoch 115/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3948 - acc: 0.8300 - val_loss: 0.4342 - val_acc: 0.8095\n",
      "Epoch 116/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3921 - acc: 0.8307 - val_loss: 0.4337 - val_acc: 0.8112\n",
      "Epoch 117/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3922 - acc: 0.8312 - val_loss: 0.4349 - val_acc: 0.8144\n",
      "Epoch 118/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3911 - acc: 0.8309 - val_loss: 0.4332 - val_acc: 0.8112\n",
      "Epoch 119/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3912 - acc: 0.8289 - val_loss: 0.4331 - val_acc: 0.8128\n",
      "Epoch 120/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3883 - acc: 0.8334 - val_loss: 0.4347 - val_acc: 0.8112\n",
      "Epoch 121/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3901 - acc: 0.8314 - val_loss: 0.4354 - val_acc: 0.8112\n",
      "Epoch 122/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3902 - acc: 0.8292 - val_loss: 0.4328 - val_acc: 0.8128\n",
      "Epoch 123/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3876 - acc: 0.8311 - val_loss: 0.4324 - val_acc: 0.8112\n",
      "Epoch 124/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3904 - acc: 0.8281 - val_loss: 0.4344 - val_acc: 0.8112\n",
      "Epoch 125/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3884 - acc: 0.8321 - val_loss: 0.4333 - val_acc: 0.8128\n",
      "Epoch 126/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3882 - acc: 0.8325 - val_loss: 0.4326 - val_acc: 0.8112\n",
      "Epoch 127/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3876 - acc: 0.8298 - val_loss: 0.4320 - val_acc: 0.8128\n",
      "Epoch 128/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3881 - acc: 0.8323 - val_loss: 0.4329 - val_acc: 0.8112\n",
      "Epoch 129/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3875 - acc: 0.8318 - val_loss: 0.4329 - val_acc: 0.8112\n",
      "Epoch 130/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3852 - acc: 0.8334 - val_loss: 0.4320 - val_acc: 0.8128\n",
      "Epoch 131/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3858 - acc: 0.8305 - val_loss: 0.4323 - val_acc: 0.8161\n",
      "Epoch 132/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3833 - acc: 0.8334 - val_loss: 0.4329 - val_acc: 0.8112\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.2, 'lay_conf': [512, 128], 'lr': 0.0001, 'max_epochs': 300, 'out_act_func': 'relu', 'seed': 123456}\n",
      "Model: \"model_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_35 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,521\n",
      "Trainable params: 459,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.8152 - acc: 0.4264 - val_loss: 0.7089 - val_acc: 0.4319\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6996 - acc: 0.5098 - val_loss: 0.6657 - val_acc: 0.5731\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6733 - acc: 0.5749 - val_loss: 0.6465 - val_acc: 0.5747\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6535 - acc: 0.5902 - val_loss: 0.6192 - val_acc: 0.6043\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6263 - acc: 0.6459 - val_loss: 0.5963 - val_acc: 0.7406\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.6071 - acc: 0.6887 - val_loss: 0.5824 - val_acc: 0.6946\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5924 - acc: 0.7048 - val_loss: 0.5679 - val_acc: 0.7011\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5799 - acc: 0.7187 - val_loss: 0.5535 - val_acc: 0.7373\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5651 - acc: 0.7331 - val_loss: 0.5416 - val_acc: 0.7537\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5500 - acc: 0.7422 - val_loss: 0.5299 - val_acc: 0.7471\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5369 - acc: 0.7418 - val_loss: 0.5198 - val_acc: 0.7603\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.5268 - acc: 0.7572 - val_loss: 0.5117 - val_acc: 0.7865\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5131 - acc: 0.7694 - val_loss: 0.5038 - val_acc: 0.7865\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5045 - acc: 0.7732 - val_loss: 0.4968 - val_acc: 0.7915\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4928 - acc: 0.7781 - val_loss: 0.4904 - val_acc: 0.7947\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4870 - acc: 0.7780 - val_loss: 0.4838 - val_acc: 0.7931\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4781 - acc: 0.7864 - val_loss: 0.4787 - val_acc: 0.7931\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4737 - acc: 0.7845 - val_loss: 0.4731 - val_acc: 0.7980\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4681 - acc: 0.7876 - val_loss: 0.4680 - val_acc: 0.7980\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4624 - acc: 0.7933 - val_loss: 0.4650 - val_acc: 0.8046\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4583 - acc: 0.7964 - val_loss: 0.4598 - val_acc: 0.8079\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4522 - acc: 0.7971 - val_loss: 0.4601 - val_acc: 0.8062\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4486 - acc: 0.8017 - val_loss: 0.4545 - val_acc: 0.8128\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4448 - acc: 0.8064 - val_loss: 0.4526 - val_acc: 0.8128\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4428 - acc: 0.8048 - val_loss: 0.4533 - val_acc: 0.8112\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4378 - acc: 0.8068 - val_loss: 0.4480 - val_acc: 0.8210\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4345 - acc: 0.8124 - val_loss: 0.4495 - val_acc: 0.8112\n",
      "Epoch 28/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4316 - acc: 0.8112 - val_loss: 0.4456 - val_acc: 0.8144\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4262 - acc: 0.8126 - val_loss: 0.4447 - val_acc: 0.8177\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4255 - acc: 0.8132 - val_loss: 0.4442 - val_acc: 0.8161\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4264 - acc: 0.8101 - val_loss: 0.4420 - val_acc: 0.8210\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4204 - acc: 0.8130 - val_loss: 0.4416 - val_acc: 0.8194\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4213 - acc: 0.8179 - val_loss: 0.4406 - val_acc: 0.8210\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4167 - acc: 0.8210 - val_loss: 0.4377 - val_acc: 0.8276\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4140 - acc: 0.8212 - val_loss: 0.4406 - val_acc: 0.8177\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4125 - acc: 0.8239 - val_loss: 0.4355 - val_acc: 0.8276\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4111 - acc: 0.8214 - val_loss: 0.4363 - val_acc: 0.8177\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4084 - acc: 0.8208 - val_loss: 0.4343 - val_acc: 0.8210\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4076 - acc: 0.8234 - val_loss: 0.4345 - val_acc: 0.8177\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4050 - acc: 0.8225 - val_loss: 0.4335 - val_acc: 0.8177\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4045 - acc: 0.8261 - val_loss: 0.4318 - val_acc: 0.8194\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4033 - acc: 0.8272 - val_loss: 0.4312 - val_acc: 0.8194\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4000 - acc: 0.8274 - val_loss: 0.4336 - val_acc: 0.8177\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3996 - acc: 0.8267 - val_loss: 0.4296 - val_acc: 0.8177\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3934 - acc: 0.8281 - val_loss: 0.4305 - val_acc: 0.8161\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3951 - acc: 0.8334 - val_loss: 0.4294 - val_acc: 0.8210\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3955 - acc: 0.8276 - val_loss: 0.4291 - val_acc: 0.8161\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3937 - acc: 0.8323 - val_loss: 0.4286 - val_acc: 0.8194\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3897 - acc: 0.8314 - val_loss: 0.4289 - val_acc: 0.8144\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3889 - acc: 0.8343 - val_loss: 0.4289 - val_acc: 0.8144\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3860 - acc: 0.8352 - val_loss: 0.4270 - val_acc: 0.8161\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3833 - acc: 0.8338 - val_loss: 0.4285 - val_acc: 0.8161\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3831 - acc: 0.8338 - val_loss: 0.4261 - val_acc: 0.8144\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3828 - acc: 0.8387 - val_loss: 0.4287 - val_acc: 0.8144\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3796 - acc: 0.8347 - val_loss: 0.4254 - val_acc: 0.8128\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3778 - acc: 0.8378 - val_loss: 0.4269 - val_acc: 0.8128\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3749 - acc: 0.8380 - val_loss: 0.4255 - val_acc: 0.8128\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3746 - acc: 0.8373 - val_loss: 0.4261 - val_acc: 0.8128\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3730 - acc: 0.8389 - val_loss: 0.4253 - val_acc: 0.8128\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3758 - acc: 0.8376 - val_loss: 0.4250 - val_acc: 0.8144\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3713 - acc: 0.8453 - val_loss: 0.4281 - val_acc: 0.8128\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3695 - acc: 0.8429 - val_loss: 0.4240 - val_acc: 0.8128\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3674 - acc: 0.8400 - val_loss: 0.4295 - val_acc: 0.8144\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3644 - acc: 0.8398 - val_loss: 0.4229 - val_acc: 0.8144\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3657 - acc: 0.8456 - val_loss: 0.4262 - val_acc: 0.8112\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3630 - acc: 0.8440 - val_loss: 0.4264 - val_acc: 0.8095\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3590 - acc: 0.8442 - val_loss: 0.4260 - val_acc: 0.8112\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3579 - acc: 0.8464 - val_loss: 0.4242 - val_acc: 0.8079\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3557 - acc: 0.8453 - val_loss: 0.4269 - val_acc: 0.8112\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.2, 'lay_conf': [256, 32], 'lr': 0.0001, 'max_epochs': 300, 'out_act_func': 'relu', 'seed': 123456}\n",
      "Model: \"model_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_36 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 205,121\n",
      "Trainable params: 205,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.8583 - acc: 0.4304 - val_loss: 0.7668 - val_acc: 0.4253\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.7561 - acc: 0.4410 - val_loss: 0.7020 - val_acc: 0.4631\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6989 - acc: 0.5196 - val_loss: 0.6722 - val_acc: 0.6174\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6752 - acc: 0.5789 - val_loss: 0.6563 - val_acc: 0.6076\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6608 - acc: 0.6088 - val_loss: 0.6437 - val_acc: 0.6059\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6484 - acc: 0.6315 - val_loss: 0.6292 - val_acc: 0.6552\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6365 - acc: 0.6546 - val_loss: 0.6155 - val_acc: 0.7143\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6273 - acc: 0.6756 - val_loss: 0.6039 - val_acc: 0.7356\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6137 - acc: 0.6920 - val_loss: 0.5937 - val_acc: 0.7241\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6027 - acc: 0.6964 - val_loss: 0.5836 - val_acc: 0.7291\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5959 - acc: 0.7077 - val_loss: 0.5745 - val_acc: 0.7406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5863 - acc: 0.7179 - val_loss: 0.5661 - val_acc: 0.7504\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5785 - acc: 0.7267 - val_loss: 0.5583 - val_acc: 0.7537\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5682 - acc: 0.7296 - val_loss: 0.5503 - val_acc: 0.7521\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5581 - acc: 0.7375 - val_loss: 0.5425 - val_acc: 0.7471\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5559 - acc: 0.7311 - val_loss: 0.5351 - val_acc: 0.7570\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5460 - acc: 0.7462 - val_loss: 0.5283 - val_acc: 0.7652\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5330 - acc: 0.7515 - val_loss: 0.5213 - val_acc: 0.7701\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5275 - acc: 0.7615 - val_loss: 0.5147 - val_acc: 0.7816\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5273 - acc: 0.7559 - val_loss: 0.5095 - val_acc: 0.7750\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5163 - acc: 0.7614 - val_loss: 0.5052 - val_acc: 0.7767\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5122 - acc: 0.7685 - val_loss: 0.5007 - val_acc: 0.7750\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5015 - acc: 0.7730 - val_loss: 0.4955 - val_acc: 0.7734\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5012 - acc: 0.7685 - val_loss: 0.4921 - val_acc: 0.7783\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4958 - acc: 0.7741 - val_loss: 0.4887 - val_acc: 0.7800\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4897 - acc: 0.7818 - val_loss: 0.4845 - val_acc: 0.7833\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4894 - acc: 0.7710 - val_loss: 0.4803 - val_acc: 0.7915\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4790 - acc: 0.7885 - val_loss: 0.4770 - val_acc: 0.7915\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4778 - acc: 0.7847 - val_loss: 0.4736 - val_acc: 0.7931\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4715 - acc: 0.7913 - val_loss: 0.4701 - val_acc: 0.7931\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4739 - acc: 0.7898 - val_loss: 0.4677 - val_acc: 0.7964\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4682 - acc: 0.7935 - val_loss: 0.4669 - val_acc: 0.7980\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4656 - acc: 0.7953 - val_loss: 0.4623 - val_acc: 0.8013\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4590 - acc: 0.7968 - val_loss: 0.4608 - val_acc: 0.8013\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4535 - acc: 0.7999 - val_loss: 0.4604 - val_acc: 0.8013\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4577 - acc: 0.7993 - val_loss: 0.4568 - val_acc: 0.7980\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4518 - acc: 0.8004 - val_loss: 0.4562 - val_acc: 0.8030\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4537 - acc: 0.7944 - val_loss: 0.4548 - val_acc: 0.8046\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4460 - acc: 0.8037 - val_loss: 0.4533 - val_acc: 0.8062\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4413 - acc: 0.8077 - val_loss: 0.4524 - val_acc: 0.8046\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4416 - acc: 0.8072 - val_loss: 0.4525 - val_acc: 0.7997\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4396 - acc: 0.8092 - val_loss: 0.4512 - val_acc: 0.8013\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4414 - acc: 0.8068 - val_loss: 0.4476 - val_acc: 0.8112\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4372 - acc: 0.8095 - val_loss: 0.4491 - val_acc: 0.8013\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4353 - acc: 0.8030 - val_loss: 0.4481 - val_acc: 0.8046\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4367 - acc: 0.8104 - val_loss: 0.4442 - val_acc: 0.8095\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4345 - acc: 0.8112 - val_loss: 0.4461 - val_acc: 0.8062\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4304 - acc: 0.8115 - val_loss: 0.4449 - val_acc: 0.8079\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4293 - acc: 0.8135 - val_loss: 0.4460 - val_acc: 0.8046\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4261 - acc: 0.8101 - val_loss: 0.4423 - val_acc: 0.8095\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4272 - acc: 0.8150 - val_loss: 0.4413 - val_acc: 0.8112\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4245 - acc: 0.8170 - val_loss: 0.4417 - val_acc: 0.8112\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4233 - acc: 0.8148 - val_loss: 0.4414 - val_acc: 0.8112\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4230 - acc: 0.8163 - val_loss: 0.4401 - val_acc: 0.8128\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4234 - acc: 0.8150 - val_loss: 0.4385 - val_acc: 0.8128\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4192 - acc: 0.8137 - val_loss: 0.4390 - val_acc: 0.8112\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4194 - acc: 0.8177 - val_loss: 0.4367 - val_acc: 0.8095\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4194 - acc: 0.8152 - val_loss: 0.4360 - val_acc: 0.8144\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4149 - acc: 0.8176 - val_loss: 0.4388 - val_acc: 0.8062\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4158 - acc: 0.8179 - val_loss: 0.4369 - val_acc: 0.8144\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4159 - acc: 0.8192 - val_loss: 0.4355 - val_acc: 0.8161\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4155 - acc: 0.8194 - val_loss: 0.4342 - val_acc: 0.8161\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4148 - acc: 0.8181 - val_loss: 0.4359 - val_acc: 0.8144\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4104 - acc: 0.8234 - val_loss: 0.4328 - val_acc: 0.8144\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4122 - acc: 0.8241 - val_loss: 0.4358 - val_acc: 0.8095\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4105 - acc: 0.8236 - val_loss: 0.4320 - val_acc: 0.8177\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4096 - acc: 0.8197 - val_loss: 0.4317 - val_acc: 0.8177\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4085 - acc: 0.8238 - val_loss: 0.4356 - val_acc: 0.8095\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4064 - acc: 0.8256 - val_loss: 0.4328 - val_acc: 0.8194\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4052 - acc: 0.8217 - val_loss: 0.4310 - val_acc: 0.8210\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4052 - acc: 0.8287 - val_loss: 0.4323 - val_acc: 0.8144\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4023 - acc: 0.8270 - val_loss: 0.4295 - val_acc: 0.8210\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4024 - acc: 0.8232 - val_loss: 0.4295 - val_acc: 0.8194\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4014 - acc: 0.8301 - val_loss: 0.4317 - val_acc: 0.8112\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4000 - acc: 0.8269 - val_loss: 0.4286 - val_acc: 0.8194\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3979 - acc: 0.8265 - val_loss: 0.4299 - val_acc: 0.8177\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3947 - acc: 0.8316 - val_loss: 0.4298 - val_acc: 0.8161\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3997 - acc: 0.8294 - val_loss: 0.4274 - val_acc: 0.8194\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3957 - acc: 0.8276 - val_loss: 0.4289 - val_acc: 0.8194\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3959 - acc: 0.8269 - val_loss: 0.4298 - val_acc: 0.8128\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3963 - acc: 0.8274 - val_loss: 0.4263 - val_acc: 0.8177\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3917 - acc: 0.8296 - val_loss: 0.4279 - val_acc: 0.8161\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3917 - acc: 0.8311 - val_loss: 0.4257 - val_acc: 0.8194\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3902 - acc: 0.8292 - val_loss: 0.4269 - val_acc: 0.8161\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3939 - acc: 0.8329 - val_loss: 0.4256 - val_acc: 0.8161\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3904 - acc: 0.8298 - val_loss: 0.4268 - val_acc: 0.8161\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3862 - acc: 0.8327 - val_loss: 0.4268 - val_acc: 0.8161\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3903 - acc: 0.8287 - val_loss: 0.4243 - val_acc: 0.8177\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3876 - acc: 0.8351 - val_loss: 0.4250 - val_acc: 0.8144\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3829 - acc: 0.8374 - val_loss: 0.4263 - val_acc: 0.8144\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3848 - acc: 0.8363 - val_loss: 0.4271 - val_acc: 0.8161\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3835 - acc: 0.8345 - val_loss: 0.4249 - val_acc: 0.8194\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3821 - acc: 0.8420 - val_loss: 0.4266 - val_acc: 0.8161\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.2, 'lay_conf': [128, 128], 'lr': 0.0001, 'max_epochs': 300, 'out_act_func': 'relu', 'seed': 123456}\n",
      "Model: \"model_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_37 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.7605 - acc: 0.4370 - val_loss: 0.7131 - val_acc: 0.4319\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.7083 - acc: 0.4873 - val_loss: 0.6775 - val_acc: 0.6273\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6771 - acc: 0.5851 - val_loss: 0.6587 - val_acc: 0.6388\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6620 - acc: 0.6181 - val_loss: 0.6478 - val_acc: 0.5928\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6505 - acc: 0.6229 - val_loss: 0.6386 - val_acc: 0.5878\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6445 - acc: 0.6262 - val_loss: 0.6281 - val_acc: 0.6158\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6330 - acc: 0.6499 - val_loss: 0.6164 - val_acc: 0.6749\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6232 - acc: 0.6725 - val_loss: 0.6053 - val_acc: 0.7061\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6120 - acc: 0.6895 - val_loss: 0.5953 - val_acc: 0.7241\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6028 - acc: 0.7112 - val_loss: 0.5858 - val_acc: 0.7225\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5932 - acc: 0.7139 - val_loss: 0.5769 - val_acc: 0.7323\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5847 - acc: 0.7154 - val_loss: 0.5681 - val_acc: 0.7438\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5737 - acc: 0.7247 - val_loss: 0.5593 - val_acc: 0.7488\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5641 - acc: 0.7395 - val_loss: 0.5509 - val_acc: 0.7570\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5569 - acc: 0.7367 - val_loss: 0.5429 - val_acc: 0.7553\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5480 - acc: 0.7462 - val_loss: 0.5354 - val_acc: 0.7586\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5385 - acc: 0.7468 - val_loss: 0.5279 - val_acc: 0.7603\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5315 - acc: 0.7575 - val_loss: 0.5209 - val_acc: 0.7668\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5259 - acc: 0.7561 - val_loss: 0.5141 - val_acc: 0.7767\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5161 - acc: 0.7654 - val_loss: 0.5082 - val_acc: 0.7783\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5077 - acc: 0.7648 - val_loss: 0.5033 - val_acc: 0.7734\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5055 - acc: 0.7696 - val_loss: 0.4980 - val_acc: 0.7750\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4936 - acc: 0.7781 - val_loss: 0.4931 - val_acc: 0.7800\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4951 - acc: 0.7772 - val_loss: 0.4896 - val_acc: 0.7800\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4892 - acc: 0.7774 - val_loss: 0.4849 - val_acc: 0.7865\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4865 - acc: 0.7776 - val_loss: 0.4809 - val_acc: 0.7898\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4805 - acc: 0.7843 - val_loss: 0.4781 - val_acc: 0.7865\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4728 - acc: 0.7878 - val_loss: 0.4749 - val_acc: 0.7882\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4723 - acc: 0.7882 - val_loss: 0.4721 - val_acc: 0.7915\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4662 - acc: 0.7922 - val_loss: 0.4694 - val_acc: 0.7931\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4622 - acc: 0.7946 - val_loss: 0.4677 - val_acc: 0.7915\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4607 - acc: 0.7957 - val_loss: 0.4653 - val_acc: 0.7915\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4579 - acc: 0.7942 - val_loss: 0.4626 - val_acc: 0.7964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4560 - acc: 0.7957 - val_loss: 0.4608 - val_acc: 0.7980\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4533 - acc: 0.7978 - val_loss: 0.4604 - val_acc: 0.8013\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4484 - acc: 0.7997 - val_loss: 0.4573 - val_acc: 0.8013\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4507 - acc: 0.8004 - val_loss: 0.4562 - val_acc: 0.8013\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4494 - acc: 0.8035 - val_loss: 0.4533 - val_acc: 0.8062\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4449 - acc: 0.8015 - val_loss: 0.4529 - val_acc: 0.8046\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4404 - acc: 0.8039 - val_loss: 0.4526 - val_acc: 0.8062\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4392 - acc: 0.8081 - val_loss: 0.4499 - val_acc: 0.8062\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4356 - acc: 0.8110 - val_loss: 0.4498 - val_acc: 0.8062\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4390 - acc: 0.8064 - val_loss: 0.4497 - val_acc: 0.8079\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4362 - acc: 0.8110 - val_loss: 0.4474 - val_acc: 0.8062\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4341 - acc: 0.8070 - val_loss: 0.4466 - val_acc: 0.8079\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4310 - acc: 0.8084 - val_loss: 0.4466 - val_acc: 0.8079\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4294 - acc: 0.8099 - val_loss: 0.4455 - val_acc: 0.8079\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4274 - acc: 0.8117 - val_loss: 0.4448 - val_acc: 0.8079\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4253 - acc: 0.8177 - val_loss: 0.4444 - val_acc: 0.8079\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4247 - acc: 0.8172 - val_loss: 0.4431 - val_acc: 0.8112\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4295 - acc: 0.8135 - val_loss: 0.4417 - val_acc: 0.8144\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4202 - acc: 0.8155 - val_loss: 0.4413 - val_acc: 0.8112\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4218 - acc: 0.8221 - val_loss: 0.4438 - val_acc: 0.8046\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4213 - acc: 0.8152 - val_loss: 0.4394 - val_acc: 0.8112\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4175 - acc: 0.8186 - val_loss: 0.4385 - val_acc: 0.8079\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4168 - acc: 0.8177 - val_loss: 0.4400 - val_acc: 0.8128\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4200 - acc: 0.8132 - val_loss: 0.4375 - val_acc: 0.8095\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4173 - acc: 0.8196 - val_loss: 0.4381 - val_acc: 0.8128\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4167 - acc: 0.8201 - val_loss: 0.4372 - val_acc: 0.8112\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4099 - acc: 0.8238 - val_loss: 0.4370 - val_acc: 0.8112\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4121 - acc: 0.8241 - val_loss: 0.4363 - val_acc: 0.8095\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4083 - acc: 0.8208 - val_loss: 0.4372 - val_acc: 0.8095\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4075 - acc: 0.8207 - val_loss: 0.4357 - val_acc: 0.8095\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4071 - acc: 0.8254 - val_loss: 0.4350 - val_acc: 0.8095\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4032 - acc: 0.8199 - val_loss: 0.4373 - val_acc: 0.8079\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4066 - acc: 0.8238 - val_loss: 0.4354 - val_acc: 0.8062\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4033 - acc: 0.8216 - val_loss: 0.4338 - val_acc: 0.8128\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4036 - acc: 0.8232 - val_loss: 0.4364 - val_acc: 0.8079\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4008 - acc: 0.8269 - val_loss: 0.4356 - val_acc: 0.8046\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4011 - acc: 0.8261 - val_loss: 0.4331 - val_acc: 0.8046\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3997 - acc: 0.8289 - val_loss: 0.4337 - val_acc: 0.8030\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3983 - acc: 0.8280 - val_loss: 0.4354 - val_acc: 0.8062\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3997 - acc: 0.8290 - val_loss: 0.4335 - val_acc: 0.8062\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3965 - acc: 0.8252 - val_loss: 0.4314 - val_acc: 0.8095\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3965 - acc: 0.8298 - val_loss: 0.4345 - val_acc: 0.8062\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3945 - acc: 0.8270 - val_loss: 0.4325 - val_acc: 0.8030\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3957 - acc: 0.8303 - val_loss: 0.4324 - val_acc: 0.8046\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3930 - acc: 0.8263 - val_loss: 0.4330 - val_acc: 0.8095\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3919 - acc: 0.8307 - val_loss: 0.4319 - val_acc: 0.8062\n",
      "Training with parameters {'batch_size': 1250, 'dropout': 0.2, 'lay_conf': [256, 256], 'lr': 0.0001, 'max_epochs': 300, 'out_act_func': 'relu', 'seed': 123456}\n",
      "Model: \"model_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_38 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.8184 - acc: 0.4322 - val_loss: 0.7368 - val_acc: 0.4269\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7292 - acc: 0.4673 - val_loss: 0.6793 - val_acc: 0.5961\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6843 - acc: 0.5647 - val_loss: 0.6570 - val_acc: 0.5796\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6646 - acc: 0.6008 - val_loss: 0.6438 - val_acc: 0.5747\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6501 - acc: 0.6205 - val_loss: 0.6276 - val_acc: 0.5862\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6325 - acc: 0.6492 - val_loss: 0.6100 - val_acc: 0.6617\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6201 - acc: 0.6785 - val_loss: 0.5957 - val_acc: 0.7274\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6080 - acc: 0.6789 - val_loss: 0.5841 - val_acc: 0.7258\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5982 - acc: 0.6907 - val_loss: 0.5730 - val_acc: 0.7323\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5822 - acc: 0.7097 - val_loss: 0.5628 - val_acc: 0.7471\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5718 - acc: 0.7141 - val_loss: 0.5533 - val_acc: 0.7504\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5634 - acc: 0.7236 - val_loss: 0.5439 - val_acc: 0.7603\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5499 - acc: 0.7387 - val_loss: 0.5350 - val_acc: 0.7701\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5394 - acc: 0.7448 - val_loss: 0.5255 - val_acc: 0.7553\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.5287 - acc: 0.7469 - val_loss: 0.5169 - val_acc: 0.7685\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5225 - acc: 0.7572 - val_loss: 0.5099 - val_acc: 0.7767\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5094 - acc: 0.7643 - val_loss: 0.5025 - val_acc: 0.7734\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5026 - acc: 0.7725 - val_loss: 0.4959 - val_acc: 0.7783\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4932 - acc: 0.7734 - val_loss: 0.4920 - val_acc: 0.7915\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4865 - acc: 0.7803 - val_loss: 0.4878 - val_acc: 0.7882\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4837 - acc: 0.7783 - val_loss: 0.4824 - val_acc: 0.7898\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4738 - acc: 0.7849 - val_loss: 0.4783 - val_acc: 0.7931\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4704 - acc: 0.7885 - val_loss: 0.4755 - val_acc: 0.7964\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4648 - acc: 0.7920 - val_loss: 0.4719 - val_acc: 0.7997\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4597 - acc: 0.7938 - val_loss: 0.4688 - val_acc: 0.7964\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4506 - acc: 0.7991 - val_loss: 0.4670 - val_acc: 0.7964\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4521 - acc: 0.7944 - val_loss: 0.4636 - val_acc: 0.7997\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4473 - acc: 0.7991 - val_loss: 0.4620 - val_acc: 0.8030\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4424 - acc: 0.8030 - val_loss: 0.4589 - val_acc: 0.7980\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4432 - acc: 0.8019 - val_loss: 0.4599 - val_acc: 0.8062\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4381 - acc: 0.8020 - val_loss: 0.4541 - val_acc: 0.8013\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4369 - acc: 0.8061 - val_loss: 0.4576 - val_acc: 0.8013\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4329 - acc: 0.8115 - val_loss: 0.4511 - val_acc: 0.8046\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4319 - acc: 0.8093 - val_loss: 0.4513 - val_acc: 0.7997\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4327 - acc: 0.8084 - val_loss: 0.4515 - val_acc: 0.8013\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4291 - acc: 0.8099 - val_loss: 0.4478 - val_acc: 0.8062\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4262 - acc: 0.8088 - val_loss: 0.4518 - val_acc: 0.7997\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4261 - acc: 0.8099 - val_loss: 0.4462 - val_acc: 0.8128\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4234 - acc: 0.8166 - val_loss: 0.4470 - val_acc: 0.8095\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4178 - acc: 0.8137 - val_loss: 0.4455 - val_acc: 0.8112\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4192 - acc: 0.8170 - val_loss: 0.4438 - val_acc: 0.8128\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4165 - acc: 0.8203 - val_loss: 0.4437 - val_acc: 0.8128\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4127 - acc: 0.8179 - val_loss: 0.4419 - val_acc: 0.8144\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4128 - acc: 0.8212 - val_loss: 0.4421 - val_acc: 0.8177\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4108 - acc: 0.8225 - val_loss: 0.4406 - val_acc: 0.8194\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4112 - acc: 0.8179 - val_loss: 0.4420 - val_acc: 0.8128\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4100 - acc: 0.8146 - val_loss: 0.4406 - val_acc: 0.8161\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4090 - acc: 0.8247 - val_loss: 0.4405 - val_acc: 0.8128\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4051 - acc: 0.8208 - val_loss: 0.4408 - val_acc: 0.8144\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4052 - acc: 0.8223 - val_loss: 0.4383 - val_acc: 0.8210\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3984 - acc: 0.8245 - val_loss: 0.4389 - val_acc: 0.8177\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3988 - acc: 0.8248 - val_loss: 0.4381 - val_acc: 0.8144\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3997 - acc: 0.8267 - val_loss: 0.4362 - val_acc: 0.8177\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3974 - acc: 0.8296 - val_loss: 0.4387 - val_acc: 0.8112\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3994 - acc: 0.8219 - val_loss: 0.4350 - val_acc: 0.8194\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3951 - acc: 0.8259 - val_loss: 0.4377 - val_acc: 0.8144\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3934 - acc: 0.8274 - val_loss: 0.4344 - val_acc: 0.8194\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3921 - acc: 0.8290 - val_loss: 0.4348 - val_acc: 0.8161\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3908 - acc: 0.8274 - val_loss: 0.4331 - val_acc: 0.8210\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3881 - acc: 0.8305 - val_loss: 0.4334 - val_acc: 0.8161\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3853 - acc: 0.8312 - val_loss: 0.4336 - val_acc: 0.8144\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3881 - acc: 0.8283 - val_loss: 0.4317 - val_acc: 0.8177\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3824 - acc: 0.8331 - val_loss: 0.4334 - val_acc: 0.8144\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3828 - acc: 0.8336 - val_loss: 0.4299 - val_acc: 0.8194\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3829 - acc: 0.8332 - val_loss: 0.4314 - val_acc: 0.8144\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3783 - acc: 0.8360 - val_loss: 0.4290 - val_acc: 0.8194\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3772 - acc: 0.8347 - val_loss: 0.4302 - val_acc: 0.8144\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3759 - acc: 0.8320 - val_loss: 0.4324 - val_acc: 0.8112\n",
      "Epoch 69/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3796 - acc: 0.8301 - val_loss: 0.4283 - val_acc: 0.8161\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3783 - acc: 0.8332 - val_loss: 0.4288 - val_acc: 0.8128\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3745 - acc: 0.8334 - val_loss: 0.4341 - val_acc: 0.8128\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3714 - acc: 0.8380 - val_loss: 0.4272 - val_acc: 0.8112\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3732 - acc: 0.8373 - val_loss: 0.4310 - val_acc: 0.8112\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3687 - acc: 0.8402 - val_loss: 0.4281 - val_acc: 0.8128\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3689 - acc: 0.8398 - val_loss: 0.4287 - val_acc: 0.8128\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3676 - acc: 0.8418 - val_loss: 0.4285 - val_acc: 0.8095\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3655 - acc: 0.8415 - val_loss: 0.4278 - val_acc: 0.8095\n"
     ]
    }
   ],
   "source": [
    "mlp_perf_metrics7 = make_MLP_exp(X_train_vect, y_train.values, params7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_layers</th>\n",
       "      <th>layer_conf</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.832695</td>\n",
       "      <td>0.812808</td>\n",
       "      <td>0.386365</td>\n",
       "      <td>0.429664</td>\n",
       "      <td>44</td>\n",
       "      <td>0.818779</td>\n",
       "      <td>0.813205</td>\n",
       "      <td>0.759398</td>\n",
       "      <td>0.864802</td>\n",
       "      <td>0.785381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.838168</td>\n",
       "      <td>0.812808</td>\n",
       "      <td>0.371757</td>\n",
       "      <td>0.429004</td>\n",
       "      <td>73</td>\n",
       "      <td>0.814839</td>\n",
       "      <td>0.813421</td>\n",
       "      <td>0.747368</td>\n",
       "      <td>0.867133</td>\n",
       "      <td>0.778997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.833425</td>\n",
       "      <td>0.811166</td>\n",
       "      <td>0.383264</td>\n",
       "      <td>0.432887</td>\n",
       "      <td>132</td>\n",
       "      <td>0.816809</td>\n",
       "      <td>0.814332</td>\n",
       "      <td>0.751880</td>\n",
       "      <td>0.867133</td>\n",
       "      <td>0.781861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.845284</td>\n",
       "      <td>0.811166</td>\n",
       "      <td>0.355656</td>\n",
       "      <td>0.426885</td>\n",
       "      <td>69</td>\n",
       "      <td>0.816152</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.751880</td>\n",
       "      <td>0.865967</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.382131</td>\n",
       "      <td>0.426595</td>\n",
       "      <td>93</td>\n",
       "      <td>0.813526</td>\n",
       "      <td>0.814876</td>\n",
       "      <td>0.741353</td>\n",
       "      <td>0.869464</td>\n",
       "      <td>0.776378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.830688</td>\n",
       "      <td>0.806240</td>\n",
       "      <td>0.391884</td>\n",
       "      <td>0.431945</td>\n",
       "      <td>79</td>\n",
       "      <td>0.819435</td>\n",
       "      <td>0.818627</td>\n",
       "      <td>0.753383</td>\n",
       "      <td>0.870629</td>\n",
       "      <td>0.784652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.841452</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.365454</td>\n",
       "      <td>0.427760</td>\n",
       "      <td>77</td>\n",
       "      <td>0.812213</td>\n",
       "      <td>0.811166</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.865967</td>\n",
       "      <td>0.775510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_layers  layer_conf      lr  dropout  batch_size  accuracy_train  \\\n",
       "0         2  [512, 256]  0.0001      0.2        1250        0.832695   \n",
       "1         2  [256, 256]  0.0001      0.2        1250        0.838168   \n",
       "2         1       [128]  0.0001      0.2        1250        0.833425   \n",
       "3         2  [512, 128]  0.0001      0.2        1250        0.845284   \n",
       "4         2   [256, 32]  0.0001      0.2        1250        0.842000   \n",
       "5         2  [128, 128]  0.0001      0.2        1250        0.830688   \n",
       "6         2  [256, 256]  0.0001      0.2        1250        0.841452   \n",
       "\n",
       "   accuracy_val  loss_train  loss_val  epochs  accuracy  precision    recall  \\\n",
       "0      0.812808    0.386365  0.429664      44  0.818779   0.813205  0.759398   \n",
       "1      0.812808    0.371757  0.429004      73  0.814839   0.813421  0.747368   \n",
       "2      0.811166    0.383264  0.432887     132  0.816809   0.814332  0.751880   \n",
       "3      0.811166    0.355656  0.426885      69  0.816152   0.813008  0.751880   \n",
       "4      0.816092    0.382131  0.426595      93  0.813526   0.814876  0.741353   \n",
       "5      0.806240    0.391884  0.431945      79  0.819435   0.818627  0.753383   \n",
       "6      0.809524    0.365454  0.427760      77  0.812213   0.811166  0.742857   \n",
       "\n",
       "   specificity  f1_score  \n",
       "0     0.864802  0.785381  \n",
       "1     0.867133  0.778997  \n",
       "2     0.867133  0.781861  \n",
       "3     0.865967  0.781250  \n",
       "4     0.869464  0.776378  \n",
       "5     0.870629  0.784652  \n",
       "6     0.865967  0.775510  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_perf_metrics7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict and score Test set by using the best model and the best configurations found on the research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multilayer Perceptron** outperforms the metrics for all the models tested. Model with [256, 256] architecture, `batch_size=1250` and having `dropout=0.01` beats the higuest performances so, we'll try to use this coniguration to predict the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_39 (InputLayer)        [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,913\n",
      "Trainable params: 262,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.7933 - acc: 0.4277 - val_loss: 0.7078 - val_acc: 0.4698\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.6960 - acc: 0.5179 - val_loss: 0.6706 - val_acc: 0.5709\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.6582 - acc: 0.5933 - val_loss: 0.6546 - val_acc: 0.5407\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.6393 - acc: 0.5865 - val_loss: 0.6300 - val_acc: 0.5787\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.6180 - acc: 0.6595 - val_loss: 0.6015 - val_acc: 0.7507\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.6004 - acc: 0.7269 - val_loss: 0.5793 - val_acc: 0.7651\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.5860 - acc: 0.7216 - val_loss: 0.5612 - val_acc: 0.7677\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.5717 - acc: 0.7335 - val_loss: 0.5461 - val_acc: 0.7756\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.5581 - acc: 0.7484 - val_loss: 0.5293 - val_acc: 0.7835\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.5436 - acc: 0.7557 - val_loss: 0.5114 - val_acc: 0.7835\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.5297 - acc: 0.7573 - val_loss: 0.4959 - val_acc: 0.7913\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.5169 - acc: 0.7673 - val_loss: 0.4828 - val_acc: 0.7953\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.5041 - acc: 0.7719 - val_loss: 0.4693 - val_acc: 0.7940\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.4928 - acc: 0.7765 - val_loss: 0.4595 - val_acc: 0.8071\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4809 - acc: 0.7860 - val_loss: 0.4488 - val_acc: 0.8071\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4733 - acc: 0.7886 - val_loss: 0.4405 - val_acc: 0.8097\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4651 - acc: 0.7927 - val_loss: 0.4337 - val_acc: 0.8110\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4590 - acc: 0.7943 - val_loss: 0.4276 - val_acc: 0.8097\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4530 - acc: 0.8002 - val_loss: 0.4230 - val_acc: 0.8163\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4476 - acc: 0.7999 - val_loss: 0.4184 - val_acc: 0.8123\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4437 - acc: 0.8015 - val_loss: 0.4148 - val_acc: 0.8150\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4397 - acc: 0.8050 - val_loss: 0.4117 - val_acc: 0.8123\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4369 - acc: 0.8089 - val_loss: 0.4089 - val_acc: 0.8123\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4331 - acc: 0.8116 - val_loss: 0.4064 - val_acc: 0.8150\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4304 - acc: 0.8133 - val_loss: 0.4042 - val_acc: 0.8176\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4272 - acc: 0.8148 - val_loss: 0.4024 - val_acc: 0.8123\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4241 - acc: 0.8170 - val_loss: 0.4006 - val_acc: 0.8110\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.4230 - acc: 0.8155 - val_loss: 0.3996 - val_acc: 0.8123\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.4213 - acc: 0.8173 - val_loss: 0.3978 - val_acc: 0.8136\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4174 - acc: 0.8203 - val_loss: 0.3967 - val_acc: 0.8136\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4158 - acc: 0.8196 - val_loss: 0.3957 - val_acc: 0.8123\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4133 - acc: 0.8206 - val_loss: 0.3945 - val_acc: 0.8189\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.4117 - acc: 0.8219 - val_loss: 0.3934 - val_acc: 0.8163\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.4084 - acc: 0.8232 - val_loss: 0.3926 - val_acc: 0.8150\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.4068 - acc: 0.8224 - val_loss: 0.3916 - val_acc: 0.8163\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.4063 - acc: 0.8246 - val_loss: 0.3906 - val_acc: 0.8150\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.4040 - acc: 0.8238 - val_loss: 0.3907 - val_acc: 0.8110\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4021 - acc: 0.8246 - val_loss: 0.3898 - val_acc: 0.8163\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4010 - acc: 0.8260 - val_loss: 0.3891 - val_acc: 0.8176\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.3989 - acc: 0.8254 - val_loss: 0.3888 - val_acc: 0.8150\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.3972 - acc: 0.8273 - val_loss: 0.3882 - val_acc: 0.8123\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.3954 - acc: 0.8286 - val_loss: 0.3883 - val_acc: 0.8136\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.3944 - acc: 0.8270 - val_loss: 0.3889 - val_acc: 0.8241\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.3939 - acc: 0.8305 - val_loss: 0.3896 - val_acc: 0.8097\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.3928 - acc: 0.8295 - val_loss: 0.3864 - val_acc: 0.8176\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.3888 - acc: 0.8314 - val_loss: 0.3857 - val_acc: 0.8150\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.3882 - acc: 0.8326 - val_loss: 0.3854 - val_acc: 0.8163\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.3859 - acc: 0.8320 - val_loss: 0.3854 - val_acc: 0.8163\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.3852 - acc: 0.8329 - val_loss: 0.3849 - val_acc: 0.8163\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.3822 - acc: 0.8351 - val_loss: 0.3862 - val_acc: 0.8123\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.3831 - acc: 0.8345 - val_loss: 0.3852 - val_acc: 0.8202\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3811 - acc: 0.8335 - val_loss: 0.3842 - val_acc: 0.8150\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3767 - acc: 0.8386 - val_loss: 0.3837 - val_acc: 0.8176\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.3766 - acc: 0.8377 - val_loss: 0.3834 - val_acc: 0.8123\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.3760 - acc: 0.8377 - val_loss: 0.3838 - val_acc: 0.8136\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.3728 - acc: 0.8399 - val_loss: 0.3853 - val_acc: 0.8176\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.3721 - acc: 0.8393 - val_loss: 0.3841 - val_acc: 0.8150\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.3688 - acc: 0.8435 - val_loss: 0.3842 - val_acc: 0.8202\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.3670 - acc: 0.8412 - val_loss: 0.3835 - val_acc: 0.8136\n"
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "mlp_model = build_MLP_network(input_dim=X_train_vect.shape[1],\n",
    "                                      layers_dim=(256, 256),\n",
    "                                      dropout=0.01, lr=1e-4,\n",
    "                                      seed=123456)\n",
    "mlp_model.summary()\n",
    "\n",
    "# Fit model using train test keeping \n",
    "hist = mlp_model.fit(X_train_vect, y_train.values,\n",
    "                     batch_size=1250,\n",
    "                     epochs=300,\n",
    "                      validation_split=0.1,\n",
    "                      callbacks=[EarlyStopping(monitor='val_loss', patience=5,\n",
    "                                               restore_best_weights=True,\n",
    "                                              min_delta=1e-7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot performance evolution metrics\n",
    "fig, ax = plt.subplots(1,2, figsize=(20, 12))\n",
    "\n",
    "# Accuracy\n",
    "ax[0].plot(hist.history['acc'], label='train')\n",
    "ax[0].plot(hist.history['val_acc'], label='val')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_xlabel('Accuracy')\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Accuracy')\n",
    "\n",
    "# Loss\n",
    "ax[1].plot(hist.history['loss'], label='train')\n",
    "ax[1].plot(hist.history['val_loss'], label='val')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_xlabel('Loss')\n",
    "ax[1].legend()\n",
    "ax[1].set_title('Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.871916  ],\n",
       "       [0.934868  ],\n",
       "       [0.80780846],\n",
       "       ...,\n",
       "       [0.97993934],\n",
       "       [0.84976596],\n",
       "       [0.87949026]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = mlp_model.predict(X_test_vect)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3263,), (3263, 768), (3263, 1))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_test.shape, X_test_vect.shape, y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[y_pred >= 0.5] = 1\n",
    "y_pred[y_pred < 0.5] = 0\n",
    "y_pred = y_pred.squeeze().astype('int8')\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       1\n",
       "1         2       1\n",
       "2         3       1\n",
       "3         9       1\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       0\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       1\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge index and labels in dataframe\n",
    "pred_out = pd.DataFrame({'id':test['id'], 'target': y_pred})\n",
    "pred_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_out.to_csv('BERT_trans_mlp_pred_out.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
